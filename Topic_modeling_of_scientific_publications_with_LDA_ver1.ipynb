{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. Import Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nfrom fastcore.all import *\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport random\nimport pickle as pk\nfrom scipy import sparse as sp\nimport glob\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom gensim.models import Phrases\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport pyLDAvis.gensim\nimport warnings","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = Path('../input/coleridgeinitiative-show-us-the-data')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(dataset_path/'train.csv')\ntrain_df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                     Id  \\\n0  d0fa7568-7d8e-4db9-870f-f9c6f668c17b   \n1  2f26f645-3dec-485d-b68d-f013c9e05e60   \n2  c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29   \n3  5c9a3bc9-41ba-4574-ad71-e25c1442c8af   \n4  c754dec7-c5a3-4337-9892-c02158475064   \n\n                                           pub_title  \\\n0  The Impact of Dual Enrollment on College Degre...   \n1  Educational Attainment of High School Dropouts...   \n2  Differences in Outcomes for Female and Male St...   \n3  Stepping Stone and Option Value in a Model of ...   \n4  Parental Effort, School Resources, and Student...   \n\n                           dataset_title  \\\n0  National Education Longitudinal Study   \n1  National Education Longitudinal Study   \n2  National Education Longitudinal Study   \n3  National Education Longitudinal Study   \n4  National Education Longitudinal Study   \n\n                           dataset_label  \\\n0  National Education Longitudinal Study   \n1  National Education Longitudinal Study   \n2  National Education Longitudinal Study   \n3  National Education Longitudinal Study   \n4  National Education Longitudinal Study   \n\n                           cleaned_label  \n0  national education longitudinal study  \n1  national education longitudinal study  \n2  national education longitudinal study  \n3  national education longitudinal study  \n4  national education longitudinal study  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>pub_title</th>\n      <th>dataset_title</th>\n      <th>dataset_label</th>\n      <th>cleaned_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n      <td>The Impact of Dual Enrollment on College Degre...</td>\n      <td>National Education Longitudinal Study</td>\n      <td>National Education Longitudinal Study</td>\n      <td>national education longitudinal study</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n      <td>Educational Attainment of High School Dropouts...</td>\n      <td>National Education Longitudinal Study</td>\n      <td>National Education Longitudinal Study</td>\n      <td>national education longitudinal study</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n      <td>Differences in Outcomes for Female and Male St...</td>\n      <td>National Education Longitudinal Study</td>\n      <td>National Education Longitudinal Study</td>\n      <td>national education longitudinal study</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n      <td>Stepping Stone and Option Value in a Model of ...</td>\n      <td>National Education Longitudinal Study</td>\n      <td>National Education Longitudinal Study</td>\n      <td>national education longitudinal study</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n      <td>Parental Effort, School Resources, and Student...</td>\n      <td>National Education Longitudinal Study</td>\n      <td>National Education Longitudinal Study</td>\n      <td>national education longitudinal study</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(train_df.dataset_title)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"Counter({'National Education Longitudinal Study': 550,\n         'NOAA Tide Gauge': 441,\n         'Sea, Lake, and Overland Surges from Hurricanes': 312,\n         'Coastal Change Analysis Program': 326,\n         'Aging Integrated Database (AGID)': 3,\n         \"Alzheimer's Disease Neuroimaging Initiative (ADNI)\": 6144,\n         'Baltimore Longitudinal Study of Aging (BLSA)': 1589,\n         'Agricultural Resource Management Survey': 660,\n         'Beginning Postsecondary Student': 461,\n         \"The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)\": 22,\n         'Common Core of Data': 368,\n         'Survey of Industrial Research and Development': 104,\n         'Baccalaureate and Beyond': 306,\n         'International Best Track Archive for Climate Stewardship': 386,\n         'National Teacher and Principal Survey': 30,\n         'Higher Education Research and Development Survey': 34,\n         'Survey of Earned Doctorates': 509,\n         'School Survey on Crime and Safety': 44,\n         'World Ocean Database': 326,\n         'Program for the International Assessment of Adult Competencies': 65,\n         'Early Childhood Longitudinal Study': 1011,\n         'Survey of Graduate Students and Postdoctorates in Science and Engineering': 85,\n         'Trends in International Mathematics and Science Study': 1163,\n         'Education Longitudinal Study': 676,\n         'Optimum Interpolation Sea Surface Temperature': 288,\n         'National Assessment of Education Progress': 132,\n         'High School Longitudinal Study': 92,\n         'Survey of Doctorate Recipients': 309,\n         'Rural-Urban Continuum Codes': 490,\n         'Survey of Science and Engineering Research Facilities': 13,\n         'FFRDC Research and Development Survey': 3,\n         'Survey of State Government Research and Development': 2,\n         'Advanced National Seismic System (ANSS) Comprehensive Catalog (ComCat)': 29,\n         'Census of Agriculture': 743,\n         'North American Breeding Bird Survey (BBS)': 585,\n         'COVID-19 Open Research Dataset (CORD-19)': 185,\n         'Complexity Science Hub COVID-19 Control Strategies List (CCCSL)': 6,\n         'Our World in Data COVID-19 dataset': 223,\n         'COVID-19 Precision Medicine Analytics Platform Registry (JH-CROWN)': 2,\n         'Characterizing Health Associated Risks, and Your Baseline Disease In SARS-COV-2 (CHARYBDIS)': 10,\n         'COVID-19 Deaths data': 28,\n         'SARS-CoV-2 genome sequence': 860,\n         'COVID-19 Image Data Collection': 38,\n         'RSNA International COVID-19 Open Radiology Database (RICORD)': 4,\n         'CAS COVID-19 antiviral candidate compounds dataset': 4})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'There are {len(Counter(train_df.dataset_title))} dataset titles in this dataset')","execution_count":5,"outputs":[{"output_type":"stream","text":"There are 45 dataset titles in this dataset\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_json = (dataset_path/'train'/(train_df.Id.iloc[0]+'.json')).read_json()\n# print(example_json)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(example_json)):\n    position = example_json[i]['text'].find(train_df.iloc[0].dataset_label)\n    if position != -1:\n        print(f'Found in section {i}, {example_json[i][\"section_title\"]}:')\n        print(f'{example_json[i][\"text\"][position-400:position+len(train_df.iloc[0].dataset_label)+400]}')","execution_count":7,"outputs":[{"output_type":"stream","text":"Found in section 0, What is this study about?:\n\nFound in section 9, Degree attainment:\nAny college degree attainment The study author collected information on college degree attainment from the fourth follow-up of the National Education Longitudinal Study collected in 2000. Only students who attended college were included in the study.\nFound in section 10, Bachelor's degree attainment:\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of label\ngs = fig.add_gridspec(5, 6)\ndataset_label = train_df['dataset_label'].value_counts()[:20]\nfig = plt.figure(figsize=(15, 7))\n\nax = fig.add_subplot(gs[1:,:])\ndataset_label = train_df['dataset_label'].value_counts()[:20]\nax.bar(dataset_label.index, dataset_label,color='gray')\nax.set_xticks(range(len(dataset_label.index)))\nax.set_xticklabels(dataset_label.index, rotation = 90, fontsize = 15)\n\n\nplt.title('Dataset Label Distribution', fontsize=20)\nplt.show()","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x504 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud\n\ntitle = train_df.dataset_title.unique()\nn = 1 # 라벨 몇개까지 할껀지\nfor i in range(n):\n#     print(i,\"th\")\n    ind = np.where(train_df.dataset_label == title[i])\n    if i!=2 :\n        txt = ''\n        ind = np.array(ind)\n        id = train_df.Id[ind[0,0]]\n        json = pd.read_json(dataset_path/'train'/(id+'.json'))\n        for j in range(len(json['text'])):\n            txt += json['text'][j]\n#         print(txt)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Modeling - LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path to the JSON files\ntrain_files = glob.glob(\"../input/coleridgeinitiative-show-us-the-data/train/*.json\")\ntest_files = glob.glob(\"../input/coleridgeinitiative-show-us-the-data/test/*.json\")\n\n# load json file using pandas\ndf_train = pd.DataFrame()\nfor count,ele in enumerate(train_files,len(train_files)):\n    df_train = pd.concat([df_train, pd.read_json(ele)])\n\ndf_train.to_csv(\"df_train.csv\",index=False)\n\ndf_test = pd.DataFrame()\nnum = []\nfor count,ele in enumerate(test_files,len(test_files)):\n    df_test = pd.concat([df_test, pd.read_json(ele)])\n    num.append(len(df_test))\n# convert dataframe to csv file\ndf_test.to_csv(\"df_test.csv\",index=False)","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":131,"outputs":[{"output_type":"execute_result","execution_count":131,"data":{"text/plain":"                                        section_title  \\\n0                                            Foreword   \n1                                       Introduction'   \n2   The Importance of International Data Comparabi...   \n3      Feasibility of Crosswalking U.S. Data to ISCED   \n4   Historical Efforts at International Data Cross...   \n..                                                ...   \n34  Future plans, related initiatives, and conclus...   \n35                                    Acknowledgments   \n36                                 Supplementary data   \n37                                RESEARCH IN CONTEXT   \n38                                 Future directions:   \n\n                                                 text  \n0   The International Standard Classification of E...  \n1   A Guide to the International Interpretation of...  \n2   Several persuasive arguments can be made for i...  \n3   The U.S. educational system differs in detail ...  \n4   In 1978, the National Center for Education Sta...  \n..                                                ...  \n34  ADNI-2 is now about to enter its final year, a...  \n35                                                     \n36  Supplementary data related to this article can...  \n37  1. Systematic review: Here we provide an updat...  \n38  We also discussed conceptual directions, such ...  \n\n[258714 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>section_title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Foreword</td>\n      <td>The International Standard Classification of E...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Introduction'</td>\n      <td>A Guide to the International Interpretation of...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Importance of International Data Comparabi...</td>\n      <td>Several persuasive arguments can be made for i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Feasibility of Crosswalking U.S. Data to ISCED</td>\n      <td>The U.S. educational system differs in detail ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Historical Efforts at International Data Cross...</td>\n      <td>In 1978, the National Center for Education Sta...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Future plans, related initiatives, and conclus...</td>\n      <td>ADNI-2 is now about to enter its final year, a...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Acknowledgments</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Supplementary data</td>\n      <td>Supplementary data related to this article can...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>RESEARCH IN CONTEXT</td>\n      <td>1. Systematic review: Here we provide an updat...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Future directions:</td>\n      <td>We also discussed conceptual directions, such ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>258714 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"[17, 30, 92, 118]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('./df_train.csv').sample(1000)\ndocs_orig = array(df['text'])","execution_count":33,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.output_area { max-width:100% !important; }</style>\"))\ndisplay(HTML(\"<style>.input_area { max-width:100% !important; }</style>\"))","execution_count":34,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container { max-width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.output_result { max-width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.output_area { max-width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.input_area { max-width:100% !important; }</style>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs_orig","execution_count":35,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"},{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"array(['Data used in this study are from the 1992, 1998, and 2004 USDA Agricultural Resource Management Survey (ARMS) of the hog sector. Because of broad differences in production techniques among various types of hog operations, we limit the sample to feeder pig-to-finish hog operations. 2 Over the period of this study, hog operations have become more specialized, with production shifting from farrow-to-finish operations to separate farrowing, nursery, and finishing operations. This study does not capture efficiency gains resulting from this specialization, but instead captures gains in efficiency within the feeder-to-finish product cycle. The analysis focuses on two major hog producing regions: the \"Heartland\" (IA, IL, IN, KY, MO, OH) and the \"Southeast\" (AL, AR, GA, NC, SC, VA). Producers located in the remaining surveyed states (CO, KS, MI, MN, NE, OK, PA, SD, TN, TX, UT, WI) were placed in the \"Other regions\" category. Table 1 lists the distribution of observations, farms, and output by region and farm size for the three survey years. The 1992 to 1998 period is characterized by a shift in production from the Heartland to the Southeast and Other regions. Over this period, the share of output produced by farms in the Southeast increased by 12.2 percentage points, even though the share of feeder-to-finish operations located in this region declined by 5.6 percentage points. This increase in output despite a relative decline in farm numbers is explained by a large increase in scale of production: average farm size in the Southeast increased almost ten-fold. 3 Farms in the Heartland, while representing roughly half of all feeder-to-finish hog farms in both 1992 and 1998, experienced a relatively small proportional increase in average farm output over this period, and consequently suffered a 22.5 percentage points decline in output share. or more hogs. There were several exceptions to this moratorium, including for new construction using \"innovative animal waste management systems that do not employ an anaerobic lagoon.\" 4 The moratorium, which was originally to expire in 1999, was extended several times in modified form through 2007. Table 2 provides summary statistics for the output and input variables by region. Output is defined as \"hog weight gain\" -the weight added to purchased/placed hogs and existing hog inventory in the calendar year prior to the year of the survey. Hog weight gain, unlike the alternative measure of output \"number of head removed,\" accounts for changes in inventory and differences in weights of feeder and finished pigs between operations. Feed is defined as the total weight of feed applied. 5 The labor input is a Tornqvist quantity index comprised of paid labor and unpaid farm household labor using the labor expenditure shares for paid and unpaid labor as weights. 6 Capital is the \"capital recovery cost\" -the estimated cost of replacing the existing capital equipment (barns, feeding equipment, etc.). \"Other inputs\" is defined as expenditures on veterinary services, bedding, marketing, custom work, energy, and repairs. Price indices from official statistics are used when price information is not directly available from the farm survey. Labor wages are deflated using the Bureau of Labor Statistics (BLS) Blue Collar Total Compensation index; feed prices are deflated using a weighted average of the BLS corn and soybean PPI; Capital is deflated using the BLS farm machinery PPI, and other inputs are deflated using the CPI. In the estimation we rescale all logged values of the variables as deviations from the sample mean to facilitate interpretation of the coefficients. Table 3 provides an overview of the advances in factor productivity during the study period for the three regions. Except for \"other inputs\" in the Southeast, all partial factor productivity measures increased at roughly the same annual rates between 1992 and 2004. during the two sub-periods. While all regions began in 1992 with approximately the same levels of factor productivity, from 1992 to 1998 farms in the Southeast experienced much larger increases in feed, labor, and capital productivity than did farms in the Heartland. Between 1998 and 2004, this pattern is reversed, with farms in the Heartland increasing their feed, labor and capital productivity at a much more rapid rate than farms in the Southeast. The next section examines whether these shifts in productivity were caused mainly by changes in the scale of production, which was illustrated in table 1, or whether the shifts were caused by differences in rates of technological change, allocative efficiency change, or technical efficiency change. Table 4 presents the estimated coefficients of the stochastic production function. Because the variables are expressed as deviations from their means, the first-order parameters of the translog function can be directly interpreted as estimates of production elasticities evaluated at the sample means. The production elasticities with respect to feed, capital, and other inputs have plausible values and are statistically significant. The estimated elasticity of output with respect to labor is quite low, but this finding is consistent with other studies that also found low labor elasticities (e.g., Brummer, Glauben, and Thijssen, 2002). Labor, particularly unpaid labor, is difficult to quantify and value using a survey instrument and the resulting low elasticity and relatively low statistical significance level for labor could reflect these empirical challenges.',\n       'The modeled water levels were output every 5 s, compared to observed water levels, and used to calculate the sea surface slopes (Figure 4b) . Modeled water-level elevations closely matched observations Measured instantaneous (gray), envelope of ± δη around 5.7 min running mean (brown), and low-pass filtered (black) slopes, and modeled low-pass filtered (green) slopes. (c) Root-mean-squared (rms) wave heights estimated from the water-level spectra at the two wells (solid) and modeled rms wave heights at the same locations (dashed).',\n       'This category is concerned with the production, propagation (breeding/growing), gathering, and catching of animals, animal products, and plant products (timber, crop, and ornamental); the provision of services associated with agricultural production; and game farms, fisheries, and wildlife conservation. \"Other agricultural and related occupations\" include occupations concerned with the production and propagation of animals, animal products, plants, and products (crops and ornamental).',\n       'The primary roads (i.e., study roads) on SCI included Ridge Road (24.3 km) that ran north-south through the center of the island, and Perimeter Road (7.9 km) that encircled the airfield at the northern tip of the island (Fig. 1) . The maximum speed limit was 56 km/hr (35 mph), although 40 km/hr (25 mph) was posted in some urban areas and on some curved sections. Depending on the segment of road, speeds averaged 47-60 km/hr and the volume of traffic averaged 60-366 vehicles/day (Snow et al. 2011) . Approximately, 90% of all traffic volume occurred during daytime (Snow 2009) . During this study, these roads were a mixture of paved (76%) and gravel (24%) surface. The study roads were 2 lanes, and most did not have maintained shoulders and were slightly or not elevated above the surrounding landscape. Vegetation typically grew to the edge of the roads. Secondary roads were present, but were infrequently used and were not developed or maintained, and therefore were not considered during our study.',\n       \"This research project aims to improve the quality of U.S. Census Bureau data and to increase knowledge about the determinants of manufacturing establishment performance. These objectives will be accomplished by linking an external dataset of establishments receiving business assistance between 1999 and 2007 to census datasets. The project uses the external dataset of business assistance recipients to both validate and improve the quality of census data. The data will be used to identify limitations of the census sampling frame, measure data quality, estimate nonresponse bias, and improve imputations for nonresponse in census datasets. The business assistance dataset is valuable because it contains data at the same level of resolution and contains some of the same data elements as census datasets. The project uses the linked datasets to explore determinants of establishment productivity not currently measured in census datasets. Specifically, the research explores how manufacturing establishment performance is affected by business assistance and how the effects vary across measurable dimensions. The analysis focuses on business assistance provided by the National Institute of Standards and Technology's Manufacturing Extension Partnership between 1999 and 2007. The project employs a variety of econometric approaches, primarily relying on an instrumental variables approach to estimate how measures of establishment performance such as productivity, output, and employment growth are affected by different types and levels of business assistance. Access to the census datasets is required to provide a valid control group and to provide key performance and control variables for this analysis. This study analyzes the link between the characteristics of firms' labor forces (the human factor) and corporate restructuring decisions, such as plant closure and acquisitions. Specifically, it will analyze whether the labor composition and the wage structure of a plant or firm affects its probability of being shut down or of becoming a takeover target, and how factors such as experience, human capital, and wages explain the cross-sectional differences in worker retention decisions and firm performance following an acquisition or plant closure. worker statistics, such as those produced by the LEHD program. Moreover, since the SDC data documents the time when restructuring becomes effective in the legal sense and census databases record the time when real changes are made (in terms of labor and capital), comparing these two sources will provide valuable information on the length of the integration process. Statistics on the changes in workforce composition and the change in the wage distribution after restructuring will be produced. Econometric models will estimate the effects of employee characteristics and the wage distribution on the likelihood of plant closure or takeover. . These links will be constructed for the years 1996-2004 and will enhance the data by examining nonresponse to MEPS IC and evaluating possible methods to impute MEPS-IC variables that are not currently included in imputation procedures. Currently, the procedures for imputing missing or invalid values exist only for variables included in the MEPS-IC published estimates because developing these procedures requires resources. In the process of constructing the linked data sets, comparisons will also be made across these three nationally representative datasets: the MEPS IC, the MEPS HC, and the LEHD-ICF.\",\n       'Cognitive performance was analyzed from assessments administered to BLSA participants every two years. Memory was assessed using the California Verbal Learning Test (CVLT), including learning (total recall over 5 learning trials), immediate free recall, and long delay free recall. Attention was assessed using the Trails Making Test Part A and the WAIS-R Digits Forward test. Executive function was measured using the Trails Making Test Part B and the WAIS-R Digit Backward test. Language was measured using letter fluency and semantic fluency tests. Visuo-spatial ability was measured using the Clock Drawing Test and the Card Rotation Test.',\n       'This involves knowing the conventions for the representation of data in tables, charts, graphs and diagrams (such as tree diagrams, scale and perspective drawings, and other visual representations of spatial or conceptual entities), for example: -bar, pie, scatter plots, -ways of showing scale on diagrams and maps, and ways of representing 3D objects in perspective.\\n2 Identifying and distinguishing 2.1 Identifying connections and distinctions between different representations of quantitative concepts This involves locating the relevant connections between different representations, for example:\\nlocating the parts of a graph referred to in the verbal description of a process which is also represented graphically, -recognising that a table of data and a chart depict the same information, and noticing significant differences between superficially similar representations.\\n2.2 Identifying the mathematics to be done and strategies to do it This involves identifying which mathematical concepts or methods are relevant in a context, for example:\\nknowing which arithmetic operations to perform, and knowing when to formulate an equation.',\n       'In practice, administrative agencies decide to share personally identifiable information with statistical agencies by first protecting their \"business interests\" with their clients (program participants) and with funding sources (Administration, Congress).\\n3 Since violations of privacy or breaches of confidentiality can damage their mission, administrative agencies will assess the current climate and risk mitigation measures proposed by the statistical agency as key factors in deciding the conditions for sharing their data. Similarly, statistical agencies will decide how to use administrative data they receive based in part on how the uses fit within agreed upon privacy and confidentiality constraints and an assessment of their ability to recover should future access be denied. Finally, decisions by statistical agencies to release these data to researchers are dependent on the same assessments of confidentiality and privacy.\\nUnlike surveys, administrative records typically represent entire populations (e.g., all tax filers, all food stamp recipients). When used for statistics, these records are also frequently linked to other administrative records or to survey records for the same individuals. The extent of sharing and linkage is generally not apparent to the data subjects. For these reasons, there are unique privacy and confidentiality risks that must be addressed. Agencies are dealing with these risks through policies and statistical and procedural tools. Such tools include techniques for data masking, alternative data access procedures, and methods to reduce sensitivity. The Government Accountability Office\\'s (GAO) 2001 report on Record Linkage and Privacy provides an excellent discussion of these tools and how they are applied.\\nTo help inform decisions from the perspective of privacy and confidentiality, U.S. statistical agencies have sponsored research into disclosure risk and privacy attitudes. From the perspective of privacy, research suggests that although the public does not trust the government to protect their personal information, they are more likely to respond favorably to statistical studies involving their personal information when they understand the uses and potential benefits (Gerber, 2003) and (Guarino et al., 2001) . Research has also determined that the public\\'s knowledge and opinions about privacy in this context are fluid and quickly become out of date (Singer et al., 2001 ). Research on disclosure avoidance has led to new data products and alternative methods for researcher access, but demands for even greater access require creative solutions. Despite the progress made through privacy and confidentiality research, there remains considerable uncertainty about the degree of public understanding and acceptance of the statistical use of administrative records and whether current confidentiality protections are appropriate. As a result, we may not be realizing the full potential of these records.\\nIn this paper I explore a broad range of policy, public opinion, and methodological issues surrounding the sharing and use of administrative records for federal statistics. My primary goal is to encourage new research on privacy and confidentiality in order to provide agency decision makers with relevant information to better understand the benefits and risks of sharing and using administrative data. It is also my goal to encourage a more open process that respects the interests of all parties, in particular the individuals whose records are to be shared.\\nIn Section 2, I provide a summary of some important new uses of administrative records, and in Section 3 I discuss legal and policy support for such uses. The reader desiring more details on the relevant laws and policies is directed to the appendices. In Sections 4-6, I describe the role that privacy and confidentiality play in acquiring, using, and providing researcher access to administrative records. In Section 7, I discuss past research related to privacy and confidentiality in the context of record linkage, and I offer suggestions for new research. Section 8 provides some examples of how other countries\\' laws and policies on the statistical use of administrative records compare to the U.S. Finally, in Section 9 I suggest a government-wide approach to fostering administrative records use in U.S. federal statistics.\\nA few caveats are worth noting:\\nFirst, many of the administrative records projects discussed in the paper involve the U.S. Census Bureau. There are three reasons for this: 1) the Census Bureau is the leading agency in conducting surveys and has a long history of linking administrative data from multiple sources; 2) much of the research on privacy and confidentiality has been conducted or funded by the Census Bureau; and 3) I have firsthand knowledge of negotiations involving the Census Bureau. I have included some examples in the text of work by the National Center for Health Statistics and I acknowledge that a great deal of important research with administrative records is being conducted in many other federal statistical agencies. The issues I discuss are similar for these agencies and proposals for additional research and policy steps should be coordinated across, and are applicable to, all statistical agencies.\\nSecond, although administrative records acquired for statistical uses can relate to either individuals or businesses, for the most part, this paper will focus on administrative records pertaining to individuals since privacy is a concept inherent only to individuals. Much of the discussion pertaining to confidentiality also applies to records of businesses.\\nThird, the paper focuses on administrative data collected and held by government entities and does not specifically consider issues related to data collected and held by private sector organizations. Private sector records, such as those derived from credit reports and public records, are increasingly being used by U.S. statistical agencies, and privacy and confidentiality will play an important role in accessing and using these records as well.\\nFinally, throughout the paper I refer both to data linkage and data integration as the process of integrating administrative records and statistical records at the individual level.\\n2 Recent Progress in Administrative Records Use in U.S.',\n       \"The final section of the survey instrument asked about landowners' demographic characteristics. This information is an important determinant of forest and woodland owner behaviors and is important for understanding other phenomenon, such as the likelihood of transferring land due to old age. Space for up to two owners who were a part of the ownership to provide demographic information was included, and 67 percent of the family and individual owners responding to the 2011-2013 NWOS indicated they had two or more owners. To facilitate comparison with other data sources, demographic categories and terms were chosen to correspond with those used by the U.S. Census Bureau (2012) Age (Question 31)-An owner's age can be an important determinant of how the forest and woodland is used. As the age of an owner increases, the probability increases that his or her land will be sold or transferred in the near future. Respondents were asked to write in their age. \",\n       'It is well documented that high quality teachers are one of if not the most important schoolbased component in the production of student achievement (D. J. Boyd, Grossman, Lankford, Loeb, & Wyckoff, 2009; Rockoff, 2004; etc.) . For that reason, a strong body of research has sought\\nto better understand what makes highly qualified or effective teachers decide to stay or leave a school, or exit the profession altogether (i.e. Clotfelter, Ladd, & Vigdor, 2011; Scafidi, Sjoquist, & Stinebrickner, 2007; Feng 2010; Feng, Figlio, & Sass, 2010) . We briefly discuss findings from recent teacher retention bonus research.',\n       \"College graduates were asked about their perceptions regarding the skill level required by their April 1997 job and its career potential. In particular, they reported on whether a bachelor's degree was required, whether the job had definite career potential, and whether the job built on skills from a previous job. Reports varied among graduates across undergraduate fields of study. For example, graduates who had majored in education, engineering, nursing, and other health fields were more likely than all graduates to report that a degree was required for their April 1997 job (75 to 81 percent versus 61 percent) (table 7). In contrast, those with majors in business, humanities and arts, or social sciences were less likely to report that a degree was required. While just over one-half (55 percent) of college graduates reported that their April 1997 job had definite career potential, only education majors were less likely than all graduates to report 9For definition of service occupations, see entry for the variable B2AJOBR in appendix A.\",\n       'The NARR and ERA reanalysis products estimate GLW average summer evapotranspiration peaks of 4 and 3.5 mm d −1 , respectively, and winter minima of 0.6 mm d −1 (Figure 2b). RegCM captures the seasonal trend but peaks 41 and 29% lower than NARR and ERA, respectively, on average across the three simulations in the summer. Land evapotranspiration reflects the GLW average (i.e., combined land and lake) seasonal cycle with a maximum in the summer (Figure 4a) due primarily to transpiration from the canopy followed by soil evaporation (not shown). Satellite-observed land evapotranspiration measurements from the MODIS show a summer peak evapotranspiration rate of 3.4 mm d −1 , which is 18% higher than RegCM simulations and Journal of Geophysical Research: Atmospheres 10.1002/2014JD022316 18 and 35% lower than ERA and NARR, respectively. On average throughout the year, RegCM shows better agreement with MODIS observations than the reanalyses, particularly during the transition seasons. Latent heat fluxes measured by eddy covariance at the three FLUXNET stations show that all three RegCM simulations reproduce observed seasonal cycle and magnitude at the WCr and Syv FLUXNET site (Figures 5b  and 5c), whereas NARR and ERA overestimate observations at all three sites by a factor of approximately 1.5 and 2, respectively. Sheffield et al. [2012] find similar overestimates (28% annually) by the NARR product, and Ruane [2010] notes that NARR evapotranspiration is too intense. Midsummer evapotranspiration rates observed at the UMB site ( Figure 5a) peak 24-34% higher than at the two Wisconsin-based sites, potentially due to differences in soil moisture content, though soil moisture data are unavailable at UMB during this time period. Neither RegCM nor the reanalyses capture the enhanced evapotranspiration at UMB. These results show that the model land surface in RegCM more closely simulates observed evapotranspiration than the reanalyses, although both struggle to capture the variability across the three observation sites. As noted by previous studies [e.g., Trenberth and Guillemot, 1998], the evapotranspiration estimates in the reanalysis products are poorly constrained and model dependent. Oleson et al. [2008] note several improvements to land-based evapotranspiration in version 3.5 of CLM relative to its predecessors that may explain the improved performance of RegCM at the WCr and Syv sites. We note, however, that uncertainties in the eddy covariance measurement of evapotranspiration (ranging from 7-12% [Baldocchi, 2003] to 40% Point-based observations (black) are shown with simulated spatial averages of a 3-by-3 grid (excluding lake points) centered on the tower coordinates (Table 1) for NARR (dashed grey), ERA (solid grey), RCM-ERA (red), RCM-GFDL (blue), and RCM-HADGEM (green). Observed values are averaged across all nine buoys (Table 1 and Figure 1b) for the months in which < 10% of the data is missing (June-October). GLSEA is averaged spatially over all five lakes. Simulated values are averaged spatially over the model lake points. [Vickers et al., 2010]) and soil water limitations on evapotranspiration may also lead to observation-model discrepancies. In addition, tower-based point measurements may not be representative of a 25 km model grid cell due to their relatively small footprint (e.g., 100-200 m at UMB) [Pressley et al., 2005]. Therefore, heterogeneities in the landscape may partially explain discrepancies between the observed and modeled data. Lake evaporation lags land evapotranspiration due to the high heat capacity of water, as shown in previous modeling studies [Lofgren, 1997] and eddy covariance measurements from midlake islands and lighthouses [Blanken et al., 2011]. In fall and winter, LSTs are warmer than the overlying air, which raises vapor pressure deficit (VPD) and induces lake evaporation (Figure 4b). In contrast, cool LSTs suppress evaporation in the spring and summer by stabilizing the atmosphere. Estimates of lake evaporation from the Great Lakes Evaporation Model from NOAA GLERL averaged spatially over the five lakes demonstrate this offset seasonal cycle that peaks 4 months after land evaporation at 4 mm d −1 (Figure 4b). NARR, ERA, and RCM-ERA compare well with the GLERL estimates during the late winter and early spring with respect to both phase and magnitude. From the beginning of summer to midwinter, these products underestimate the estimates from GLERL by as much as 50%. While RCM-ERA resembles the reanalyses, RCM-HADGEM simulates 2-3 times higher lake evaporation than the reanalyses (Figure 4b), and the RCM-GFDL simulation exhibits a seasonal cycle that is shifted in phase. Despite the fact that the lakes make up only 11.8% of the domain, the variability in lake evaporation across the three simulations is evident in the GLW average evaporation (Figure 2b), particularly in the winter when land contributions are small. To explain the differences in lake evaporation across the three models, particularly the biases in the GCM-constrained simulations, we evaluate the simulated lake and air temperatures with buoy observations and the model-derived GLSEA data ( Figure 6). June-October buoy measurements show that LSTs generally follow the air surface temperature seasonal cycle with a 2-3 week lag in warming and cooling during the transition seasons (e.g., March-April-May (MAM) and September-October-November (SON)). The lake surface exerts a strong influence on the surface air temperature measurement, and thus, LSTs may lag behind air temperatures farther than the buoy data suggest, particularly higher in the atmosphere. In addition, the offset seasonal cycle is more apparent when November-May data are included (not shown). GLSEA June-October temperatures show a similar phase as the buoy measurements with 3 K warm biases in the summer and a winter minimum temperature of 275 K. We note, however, that the buoys only capture the deep, midlake conditions while the GLSEA data also include the warm shallow waters and thus appear warmer than the buoys. RCM-ERA captures the GLSEA LSTs in winter and buoy lake and air temperatures in summer, demonstrated by the agreement in lake-air temperature differences (Figures 7a and 7b) and evaporation rates (Figure 4b). In RCM-GFDL, LSTs follow air temperatures, leading to weak lake-air temperature differences year round and a weak seasonal offset in evaporation. RCM-HADGEM LSTs are 8 K warmer than the overlying air in winter and 3 K cooler in summer, leading to a stronger temperature gradient and evaporation rates that are twice that of the other two simulations. We note that these biases are computed from spatial averages across all five lakes and that each lake exhibits strong individual variability, particularly in the RCM-HADGEM case (Figures 7a and 7b), as discussed in section 4.3.',\n       \"The textures in the image or texels are the repetitive patterns or spatial sequence to analyze all real environment objects as images by Randen and Husoy (1999) . The spatial analysis in the small Texel region or micro-textures is complicated with the existing statistical approaches such as Gauss-Markov random fields, second-order statistics or local linear descriptor transform based on Smith and Chang (1994) . It discards the templates of internal frequency varying regions. The spatial filter banks such as Gabor, S-filter and Gaussian fails to correlate the texture features for evaluating the features by Unser (1995) . The statistical textural features in estimating the spatial relationship of various types of data sets of aerial and satellite image analyzed by Haralick et al. (1973) . The calculation of spectral features of texture image of terrain based on power spectrum to estimate the roughness in the ring and wedge based region yields inaccurate towards different type of data determined by Chen and Young (1982) . Later, the discrete Broadatz textures are characterized to yield optimal transform from the set of histogram obtained through the neighbourhood pixel or cooccurrence based classification that results in overlapping applied by Unser (1986) . By using wavelet transform to extract the multi-component texture method improves orthogonality but highly time consuming for due to multi-scale strategy. The images with different features are marked as indices based on relative similarity based on the research by Monay and Gatica-Perez (2007) . The Content Based Image Retrieval (CBIR) for object or feature analysis exploit much research developments with application. Among them, the multi-texton histogram integrates with the cooccurrence matrix based on Julesz's textons theory to describe image features by Liu et al. (2011) . Feature extraction method consists of gray level co-occurrence matrix and GMRF features. The classification is done by using Support Vector Machine (SVM). A detailed literature review is presented by Tou et al. (2009) for the classification of texture images based on various feature extraction techniques. In our previous work (Vivek and Audithan, 2014 ) that we integrated the characterization of textures based on Discrete Shearlet Transform (DST) by extracting entropy measure and to classify the given Brodatz database texture image using K-Nearest Neighbor (KNN) classifier by Fang et al. (2010) . Although such adaptation improves the classification accuracy, it also severely increases the feature space complexity. Similarly, the an exclusive machine learning algorithm with strong supervised LP Boost classifier to train the ADNI database of MR images in a hyper-plane shows improved in classification compared with other methods that developed by Fang et al. (2011) . Similarly, the LP Boost algorithm optimized for weak classifier while ignoring strong classifier through mini-max theory that revokes on the edge constraint shows higher convergence rate and accuracy for real world applications. Later, the same algorithm updated with strong classifier with the limited range that the training set of 5-fold-cross validation shows higher accuracy in the research by Easley et al. (2008) and Liu et al. (2010) . Thus, we propose the combination of multitexton histogram with the Discrete Shearlet Transform (DST) to discriminate the Brodatz album based on feature extraction and then it undergoes classification with the mini-max theory based LPboost classifier the accuracy of this system very well compared to other state of art techniques.\",\n       'First, we ran shape regression component optimization with each optimization searching for shape regression components associated with one of the clinical variables. For each clinical variable we ran linear regression models that assessed the strengths of associations between the shape regression component coefficients and the clinical variable. We then ran PCA on the HP to provide shape components that accounted for the greatest amount of HP shape variability possible; we used linear regression to test the strengths of associations between each clinical variable and the PCA coefficients. Finally, we ran pointwise regression [3] , i.e. we calculated a separate linear regression model at each HP surface point that associated the local HP radius there to each clinical variable. Figure 1 illustrates the key differences between PCA and shape regression components in terms of associations with CSF total tau, a key biochemical marker of AD. Exactly one regression shape component has coefficients whose correlation with total tau is non-zero, while many principal component coefficients are correlated with total tau. In fact, the strength of association between the principal component coefficient and total tau decreases very gradually from component to component, making it unclear how many of these principal components should be analyzed in further detail by the end user. In addition, the one regression component whose coefficients were significantly correlated with total tau (green line) reflected a large amount of population variability in HP shape, while the principal component whose coefficients were maximally correlated with total tau reflected very little population shape variability. This suggests that while the principal component is associated with tau, the association may be irrelevant because it represents a relatively rare, under-represented shape feature in the population. Figure 2 shows the p values of linear regressions between clinical variables and shape features, including principal component coefficients, shape regression component coefficients, and individual HP surface point radii. They are plotted on a logarithmic scale. For the shape regression components and principal components, the minimal p value over all such components is plotted. For pointwise regression, the average p value over all HP surface points with p < .05 is plotted. For all clinical variables, shape regression components had p values closer to zero, suggesting a superior ability to identify shape features that are the most strongly associated with clinical variables.',\n       'We tested two major hypotheses: (1) greater WMH load will lead to more extensive and faster decline in cognition of the PD patients (2) patients with a higher WMH load (WMHL) will show more cortical thinning in their follow-up visit after one year.\\nSurvival analysis was used to investigate the relationship between WMH burden and decline in cognition. It has been previously shown that a threshold of WMHs should be present before cognitive deficits are observed 28, 29 . The question of interest was whether there is a significant difference between the cognitive survival curves of subjects (normal controls and PD patients) with low versus high WMHL. The threshold for differentiating between high and low WMHL was set at 5 cm 3 (median value, 0.7% of WM volume, 0.27% of brain volume). Similar to previous studies [30] [31] [32] [33] Gaussian random field theory with a threshold of p<0.05 37 . Similar to the survival analysis, the threshold for differentiating between high and low WMHL was 5 cm 3 .\\nObserved differences in cortical thickness were then correlated to cognitive measures using Pearson partial correlations correcting for age. and 56% (95 CI=0.45-0.67) in controls, respectively (NPD-Low=186, NPD-High=174, NHC-Low=79, NHC-High=83). In PD, the high WMHL cohort experienced a significantly lower survival rate than the low WMHL cohort (χ 2 =30.9, p<0.00001, hazard ratio= 2.42).',\n       'In this section, we brie ‡y summarize the key patterns of R&D spending in the United States. 14 Figure   1 shows the share of total R&D spending in U.S. GDP over the period 1953-2009. Between 1953and 1964, R&D investment rate increased rapidly from 1.36 percent to 2.88 percent. Since then, it has been maintained between two and three percent. Figure 2 shows the distribution of total R&D spending among basic research, applied research and development. Over the entire sample period, development accounted for more than 60 percent of total R&D spending in the United States, while applied research accounted for another 20 percent. Despite the importance of non-basic research, the share of basic research spending has been persistently increasing over the years. In 1953, basic research accounted for 8.9 percent of total R&D spending. This increased to 19.0 percent in 2009. Next, we turn to the importance of government and private industries in funding and performing these research activities. Figure 3 shows the breakdown of basic research spending by funding source. Table 1 shows the distribution of basic research output (measured in terms of scienti…c publications) by performing sector. Throughout the entire sample period, the U.S. government played a predominant role in funding basic research. In particular, the government has funded about 70 percent of all the basic research performed in academia, which is the primary contributor of scienti…c publications. Another important observation from Figure 3 and Table 1 is that, while private industries have funded and performed a substantial amount of basic research, they have produced only a small fraction of the output. This seems to suggest that the amount private …rms spend on basic research may not truly re ‡ect their importance in the creation of fundamental knowledge. In a well-cited article on industrial R&D, Rosenberg (1990) suggests two possible reasons why private …rms would choose to invest in and maintain a basic research capability: First, this type of capability is crucial for planning and evaluating non-basic research. Second, it is also essential for understanding and exploiting the knowledge created by academic research. 15 Thus, according to this view, the main reason why private …rms invest in basic research is to enhance their performance in non-basic research, not to create fundamental knowledge. While it is di¢cult to test this hypothesis rigorously, it does seem to explain the empirical evidence mentioned above. Given the relatively insigni…cant role of private industries in the creation of fundamental knowledge, we choose to 1 4 Unless otherwise stated, all the data reported in this section were taken from National Patterns of R&D Resources: 2009 Data Update compiled by the NSF. 1 5 The same author also pointed out that for industrial R&D the distinction between basic and non-basic research is often not that clear. For instance, in the Survey of Industrial Research and Development conducted by the NSF, industrial basic research is de…ned as \"the pursuit of new scienti…c knowledge or understanding that does not have speci…c immediate commercial objectives, although it may be in …elds of present or potential commercial interest.\" This de…nition can be found in Research and Development in Industry: 2006-07, Appendix A. abstract away from basic research funded by private …rms. This assumption helps keep the dynamics of the model tractable and allows us to focus on the growth and welfare e¤ects of government basic research spending. Figure 4 shows the distribution of non-basic research spending by funding source. It is evident from this diagram that the importance of government spending on applied research and development has been declining since the 1960s. Figure 5 shows that this decline is coincident with the cutbacks in defenserelated and space-related R&D programs. In the present study, we only take into account two types of research activities: (i) government-…nanced basic research, and (ii) applied research and development performed by private industries. Thus, when comparing the model to data, we use the sum of R&D expenditures incurred by these activities as our measure of total R&D spending (see Appendix C for more details). For the period 1953-2009, our measure of total R&D spending represents 77.3 percent of the U.S. total. The implied R&D investment rates are shown in Figure 1 (labelled as \"Our Measure\"). Over the past several decades, the U.S. has also witnessed a rapid increase in the employment of researchers. Figure 6 shows the growth factor of the number of full-time R&D scientists and engineers employed by private industries and the growth factor of total employment in the United States. 16 Between 1953 and 2009, the employment of private-sector researchers has increased by a factor of 8.75, while total U.S. employment has increased by a factor of 2.29.',\n       '1 0 Bowen and Sosa (1989) use enrollment as part of their calculation of faculty workload predictions and their impact on supply and demand. While they start with enrollment projections based on the IPEDS EF survey, they use data aggregated by institution. This is probably because they relied on data available to the public in the late 1980s and few researchers were using the CIP-specific data which were collected but not released. They write that \"Data showing enrollments by field of study exist only at the level of the individual institution, and even then they are often incomplete or incompatible with data from other institutions\" (p. 46). To obtain discipline-specific enrollments, they used the IPEDS Completions (C) survey and applied percentages of degrees conferred by clusters of disciplines to their enrollment projections. These were then used to calculate discipline-specific student-faculty ratios. There are some problems in using degree data as a proxy variable for enrollment. Bowen and Sosa (1989) recognize that \"students who go on to receive one kind of degree can crossregister in courses taught by faculty members who are in other fields of study,\" so that \"we almost certainly underestimate shares of enrollment in the arts and sciences when we look only at degrees conferred\" (p. 46). A similar argument may be made about the problem of using enrollment by major. An Induced Course Load Matrix (ICLM) model shows the relationships of departmental consumption and contribution. Many majors take courses outside of their major and many departments serve non-majors in their courses. This presents a problem with using enrollment data by major for documenting faculty workload. Clearly these studies recognize the importance and also some of the problems of using enrollment data to predicting faculty demand. However, none make use of the disciplinespecific enrollment data which are available. The IPEDS EF data files available on the NCES web site and on WebCaspar are at the level of the first two digits of the CIP code. The IPEDS LI CD-Rom provides access to the full 6 digit CIP code data. It is also possible for researchers to request special cross-tabulations of these data from the National Data Resource Center (NDRC). Future researchers using the methodology of Massy and Goldman (1995) and Bowen and Sosa (1989) now have discipline-specific, enrollment data readily available. These data are critical to calculating faculty workload and to projecting faculty demand based on workload. Both studies use student full-time equivalencies (FTE) instead of headcount, using the standard NCES calculation which equates 1 full-time headcount to 1 FTE and 3 part-time headcount to 1 FTE. While the IPEDS Institutional Characteristics Survey (IC) has in the past included questions about student credit hours (SCH) by level, these are not at the discipline level and these data are not being reported in the raw data files, in part because of recognized problems in institutional reporting methods. Without SCH data, researchers must rely on the NCES calculation. The results of this calculation are suspect for those institutions which have significant part-time enrollments such as urban institutions and community colleges. The benefit of using the IPEDS EF Survey is that collects data on the entire student population, not just a sample. While some institutions do not respond, these are usually proprietary and technical schools and the previous year\\'s data may be substituted. When assumptions about time-to-degree and graduation rates are applied to cohorts of students, it is possible to predict the enrollment component of supply by discipline. The \"BA-PhD Nexus\" is described by Bowen and Rudenstine (1992) Bowen and Sosa\\'s (1989) calculation of faculty workload and applied to overall enrollment ratios, no special significance is given to masters enrollment data. Masters students are one consideration of departmental activity considered by Bowen and. Rudenstine. Still, most of these complex models do not incorporate masters enrollment in any significant way in predicting faculty supply and demand. However, meaningful statistics may arise from calculating student-level, faculty workload. With this method, assumptions could be made about the pipeline of graduate students. The NSOPF, SDR, and SED surveys collect data on graduation dates by degree for each respondent and could be used to calculate time to the BA, MA, and doctorate. While time-to-degree and cohort tracking methodologies are quite complicated, these survey data have not been used to their fullest extent. Degrees conferred and enrollment data for masters programs are also useful in projecting potential community college faculty, which traditionally do not need the doctorate. Most faculty demand models do not adequately capture community college needs. Massy and Goldman (1995) only address research and doctoral institutions and Bowen and Sosa (1989) only predict results for four-year institutions and above. In addition to the SDR, the NSF SESTAT system includes the National Survey of Recent College Graduates (NSRCG) and the National Survey of College Graduates (NSCG). All three SESTAT surveys include occupation codes for postsecondary faculty in 29 clusters of disciplines. These provide some estimates of the faculty population weighted to census estimates. With models based on the NSRCG, masters enrollment data could be used to project faculty supply and demand for those with masters degrees, similar to the ways in which the NSOPF, SDR, and SED could be used to project supply and demand for those who will earn doctorates. This discussion suggests that masters enrollment and degree data are useful but little used components of complex faculty supply and demand models. While Bowen and Rudenstine pay great attention to the \"BA-PhD Nexus,\" a similar potential relationship for predicting enrollment exists with MA-Ph.D. cohort enrollment tracking. Once established, there are other sources for data besides the IPEDS EF and IPEDS C reports which can be used to project enrollment and degree variables for supply and demand. Perhaps the most important and consistent of these is the annual CGS-GRE Graduate Survey, which collects full-and part-time data by discipline/ program by institution. The GSS is similar and includes relevant data on financial support, but only collects data on science and engineering disciplines.',\n       \"Land's contribution to growth in agricultural output was negative for all recent time periods but 1973-79. Over 1948-2009, the contribution of land to output growth was -0.08 percentage points per year. The positive growth in materials (including fertilizer, pesticides, fuel, and purchased services) reflects the substitution of those inputs for other inputs. Material inputs' contribution averaged 0.69 percent per year over 1948-2009. This offsets the negative contributions of labor and land, making the contribution of all inputs essentially flat. Among the materials, agricultural chemicals grew faster than others at an annual rate of 2.54 percentage points. While labor input declined and capital investment slowed in recent years, purchased services, including contract labor and machinery custom work services, increased at an annual rate of 2.54 percentage points over 1948-2009. Material growth was faster and contributed more to output growth before 1979 than after.\",\n       'No significant difference was observed in CSF clusterin levels between the three diagnostic groups (Table 1 and Figure 1A ). However, there was a significant difference in CSF NG levels between the three groups ( Table 1 and Figure 1B ).',\n       'As indicated previously, a subset of students was selected for administration of interviews to their parent(s) to obtain supplemental interview data (e.g., parent demographics, finances, and postsecondary decision making regarding their child) that could not be reliably obtained from the student and that were not available from institutional or CPS records. This supplemental information was needed (and sought) for students who were: (1) dependent undergraduates who had not received federal aid; (2) dependent undergraduates who had received federal aid but for whom not all applicable data were obtained from extant records; and (3) \"newly independent\" undergraduate students (i.e., 24-and 25-year old students, who recently passed the age at which they could still be defined as dependent under Federal definitions).',\n       \"Despite the significance of skills and abilities, in practice situations where teams with high-quality individuals are ineffective are often. Beside cognitive ability, personality traits are a good predictor of performances. Most reliable predictor is team consciousness that reflects as reliability and regularity in performing task related activities (Barrick & Mount, 2005). Van Vianen and De Dreau (2001) found that harmony and consciousness in teams are related to performances. Same conclusions produced meta analysis of other authors (Peeters, Van Tuijl, Rutte, & Reymen, 2006). Interesting conclusion of these authors is that changes in harmony and consciousness have negative impact on team performance. Researchers analyzed influence of personality traits on team performances in respect with type of task given to team members. LePine, Colquitt i Erez (2000) found negative correlation between consciousness and effective decision making when decision making rules demand the need to adjust (Le Pine, Colquitt, & Erez, 2000). Adjustment to modified conditions is difficult for individuals with need for stability. Other authors came to the conclusion that effect of personality traits varies upon task as well (English, Griffith, & Steelman, 2004). Another argument why personality is important is it's' influence on team climate. George concluded that teams differ on affective tone determined upon individual personalities (George, 1990). The fact that personality influence team climate is not surprising considering the fact that personality traits of all individuals affect organizational climate (Schneider, 1987). Specific combination of personalities in team has a great influence on team performances. There could be conflicting personalities in team environment, and it is also possible that one individual has a bad influence on team performances just because he/she possesses certain personality trait. Barrick et al., (1998) found that teams with emotionally unstable members have lower values in cohesion, flexibility, communication, conflicts and division of labor, in comparison to teams without that type of member. However, one emotionally unstable team member does not influence performances in negative manner (Barrick, Stewart, Neubert, & Mount, 1998). Bell conducted meta analysis of relationship between personality traits and performances. Her results indicate the fact that average team consciousness, cooperation, extraversion, emotional stability and openness to experience have positive correlation to performances (Bell, 2007). Likewise, teams consisting of highly conscious members who cooperate achieve higher performances (Halfhill, Sundstrom, Lahner, Calderone, & Niels, 2005). At the same time, teams with members who have low need for cooperation are not skilled in acquiring new knowledge (Mathieu, Maynard, Rapp, & Gilson, 2008). Spectrum of personality traits analyzed in relation to team performances is extremely wide and surpasses the most popular model in the field -Big Five model, that considers neuroticism, extraversion, openness, agreeableness and consciousness (Costa & McCrae, 1995). Many authors include orientation to achievement and dependability (LePine J. A., 2003), assertiveness (Pearsall & Elis, 2006), and locus of control (Boone, Van Olffen, Van Witteloostuijn, & De Brabander, 2004) as significant personality traits in teamwork and team effectiveness.\",\n       'DISTRICT O F COLUMBIA Three years after its 2009 adoption, the District\\'s high-profile value-added teacher-evaluation model known as IMPACTdeveloped during Michelle Rhee\\'s colorful stint as Chancellor-continues to rattle the Washington Teachers Union (WTU). IMPACT was the first such system to be implemented in the country, and although the WTU grudgingly agreed to it, the union did not have much of a choice since evaluations are strictly within the purview of DCPS management. Union leaders contended that it was implemented prematurely and was unreliable, punitive, and based on a false premise: that student test scores accurately measure teacher ability and only teacher ability (as opposed to conditions outside the classroom). 6 \"The scores don\\'t reflect the existing conditions that students bring into the classrooms, issues pertaining to family dysfunction, economic circumstance, poverty,\" said WTU president Nathan Saunders. 7 But the union had no actual say over evaluation and dismissal rules, and consequently, the results of IMPACT led to the termination of 542 teachers for poor performance between 2010 and 2012. 8,9 On the other hand, D.C. also ranked 988 of its approximately 4,100 teachers \"highly effective\" in 2012, making them eligible for bonuses of up to $25,000-which have so far been funded by outside donors but, as of 2013, will be paid for from the district\\'s pocket. 10 Unlike teacher evaluations and dismissals, performance pay is a part of the collective-bargaining agreement-and was the subject of a high-profile and prolonged round of contract negotiations that was finally resolved in April 2010 with the help of AFT president Randi Weingarten. When Henderson took over from Rhee as chancellor of DCPS, WTU leaders pressed to de-emphasize the role of standardized testing in the IMPACT metric. 11 Beginning in 2013, student achievement will still be half of a teacher\\'s evaluation, but standardized test scores will be just one part of that score (35 percent of the total evaluation, down from 50 percent); teacherdeveloped assessments of student learning will make up the rest (15 percent of the total). Henderson explained that the change was \"in response to feedback from teachers,\" and she clarified that \"while we believe strongly that value-added is the fairest and most accurate method of capturing a teacher\\'s impact on student achievement, we recognize that this measure does not reflect everything your students have learned.\" 12 The WTU applauded the move. 13 In another successful campaign, the WTU ensured that the District made good on its obligations to teachers who had their positions eliminated due to budget cutbacks and declining enrollments. A provision in the 2010 collective bargaining agreement specified that those \"excessed\" teachers with good evaluations and twenty years of service were eligible for early retirement with full benefits. When first asked to hand over the funds to carry out this provision, the District said it lacked the money. 14 WTU then accused the District of questionable accounting; the District replied that the union had not filed the necessary paperwork; but by May 2012, they had come to agreement, announcing that the District will allocate $10.2 million over the next five years to fund these benefits. 15 Politics appear to be alive and well in the nation\\'s capital.  Area and Indicator). Second, we take a weighted average of the sub-indicators in each of five areas. In each area, we use that average to place the states in rank order: For example, in Area 1: Resources and Membership, the District is ranked 17th of 51 based on the weighted average of its sub-indicators. To generate the state\\'s overall rank, we average the five area ranks together, then re-order the states. For a more detailed description of data sources and methodology, see Appendix A. 2 We understand that this percentage appears very low, especially given the attention the District receives for its high per-pupil expenditures and teacher salaries. There are numerous explanations that might account for this: First, D.C. is unique because the majority of employer pension contributions to teacher pensions is the obligation of the U.S. Treasury, not the school district. Second, the NCES cautions against comparing the District of Columbia to other states because it has only one school district, and therefore per-pupil expenditures (and allocations for salaries and benefits as compared to other operating expenses) are not similar to those in other states. Third, per-pupil spending in urban districts tends to be considerably higher than non-urban districts in the same, but D.C. does not see its high expenditures averaged with lower ones from non-urban districts. Finally, the District used external funding sources for its teacher performance bonuses (although as of 2013 that will no longer be the case).',\n       'We developed two separate proportional-hazards models to elucidate the individual effects of hospital distance and EQI on survival time. Covariates included in both models were gender, race, BMI, alcohol use, tobacco use, rurality, and age. When we added average distance to the nearest hospital into our model (Table 2) , we found increased survival time for patients living over 20 miles from the nearest hospital compared to under 10 miles (hazard ratio (HR) = 0.90, 95% confidence interval (CI) = 0.89-0.91). Separately, when we added EQI into our model Access to healthcare, environmental quality, and end-stage renal disease survival time (Table 2) , we found no significant difference in survival in the worst EQI compared to the best EQI (HR = 1.01, 95% CI = 0.98-1.05). Stratifying these separate models into rural versus urban categories did not change these associations (not shown). Our full model included distance from the nearest hospital and EQI mutually adjusted to assess possible effects on survival time in patients diagnosed with ESRD. Two adjusted survival curves were generated from the full model: one stratified by distance to the nearest hospital (Fig 1a) and one stratified by total EQI (Fig 1b) . Adding total EQI into the model did not affect the inverse association between survival and distance to the nearest hospital (Over 20 miles compared to under 10 miles HR = 0.88, 95% CI = 0.87-0.89). Adding hospital distance into the model did not influence the associations between environmental quality and patient survival time as there was no substantial difference between the best and worst EQI strata (Fig  1b) . Repeating this analysis with the five individual domains of the EQI instead of the overall EQI did not clarify these associations. Just as with the total EQI, survival time was significantly greater farther from the nearest hospital. The direction of association between the EQI and individual domains was inconsistent with no significant difference between the best and worst EQI percentiles for the air and built EQI domain and an improvement in survival in worse EQI percentiles compared to the best EQI percentile for the water and land EQI domain. While survival was decreased in the worst EQI percentiles compared to the best EQI percentile for the sociodemographic domain, there was no uniformity to this association between the remaining strata (data not shown).',\n       \"Our in-house WMH segmentation protocol is illustrated in Fig. 1 and detailed here. First, we created a reference standard segmentation that was visually similar to a manual segmentation in a sub-sample of four training participants who had minimal WMHs on the FLAIR image. We chose participants with minimal WMHs, because for these participants, WMHs were clearly defined and unambiguous, and they contributed minimally to the overall mean white matter intensity for that participant. In our four training participants, we automatically identified WMHs by applying a participant-specific intensity threshold at 99th percentile of the signal intensity in the total white matter for each participant, using the fslmaths function in FSL. We arrived at this 99th percentile threshold by visually assessing which threshold adequately segmented these clearly delineated lesions in our test participants. If we had included participants having extensive WMH in this training set, their mean white matter intensity would be low, because the WMHs themselves would reduce the mean signal intensity in the WM mask. Therefore, in participants with extensive WMHs, a 99th percentile intensity threshold would not adequately identify WMHs. Once the WMHs were identified in these four participants, we used their data to calculate a study-specific intensity ratio that could be used to identify high intensity WMHs, even in participants who also have more extensive and diffuse lesions. To calculate a study-specific intensity ratio, across the four training participants, we divided the mean minimum intensity of the WMHs by the mean intensity of the normal-appearing white matter (excluding the WMHs). The voxel, volume, and intensity information derived from the four training participants is tabulated in Table 2 . This resulted in a study-specific WMH intensity ratio indicating how much greater the minimum intensity of WMHs was compared with the mean intensity of normal-appearing WM. We then obtained a participant-specific WMH map, by calculating the mean intensity value of each participant's FLAIR image within the peripheral white matter mask (see 2.5.1 Creating white matter masks) and multiplied it by our study-specific WMH intensity ratio to obtain a threshold, which we then applied to that participant's original FLAIR image.\",\n       \"S protein is a large multifunctional protein forming the exterior of the CoV particles [50, 51] . It forms surface homotrimers and contains two distinct ectodomain regions known as S1 and S2. In some CoVs, the S protein is actually cleaved into these subunits, which are joined non-covalently, whereas an additional proteolytic cleavage within the N-terminal part of the S2 subunit that takes place upon virus endocytosis generates spike proteins S2'. Subunit S1 initiates viral infection by binding to the host cell receptors, S2 acts as a class I viral fusion protein that mediates fusion of the virion and cellular membranes and thereby promotes the viral entry into the host cells, whereas S2' serves as a viral fusion peptide [52, 53] . Spike binds to the virion M protein through its C-terminal transmembrane region [54] . Belonging to a class I viral fusion protein, S protein binds to specific surface receptor angiotensin-converting enzyme 2 (ACE2) on host cell plasma membrane through its N-terminal receptor-binding domain (RBD) and mediates viral entry into host cells [55] . The S protein consists of an N-terminal signal peptide, a long extracellular domain, a singlepass transmembrane domain, and a short intracellular domain [56] . A 3.60 Å resolution structure (PDB ID: 6ACC) of S protein from Human SARS complexed with its host binding partner ACE2 has been obtained by cryo-electron microscopy (cryo-EM The biophysical analysis reported in previous study has also revealed that the S protein from SARS-CoV-2 has a higher binding affinity to ACE2 than S protein from Human SARS [58] . which has been calculated by averaging the disorder scores from all six predictors is represented by a short-dot line (sky-blue line) in the graph. The light sky-blue shadow region signifies the mean error distribution. The residues missing in the PDB structure or the residues for which PDB structure is unavailable are represented by the grey-coloured area in the corresponding graphs. (E) Aligned disorder profiles generated for spike glycoprotein from SARS-CoV-2 (black line), Human SARS (red line), and Bat CoV (green line) based on the outputs of the PONDR ® VSL2.\\nMSA analysis among all three coronaviruses demonstrates that S protein of SARS-CoV-2 has a 77.71% sequence identity with Bat CoV and 77.14% identity with Human SARS (Supplementary Figure S1A) . All three S proteins are found to have a conserved C-terminal region. However, the N-terminal regions of S proteins display noticeable differences. Given that there is significant sequence variation RBD located at the N-terminal region of S protein, this might be the reason behind variation in its virulence and its receptor-mediated binding and entry into the host cell. According to our intrinsic disorder propensity analysis, S protein from all three CoVs analysed in this study are highly structured, as their predicted disorder propensity lies below 10% ( Table 1 ). In fact, the mean PPID scores of SARS-CoV-2, Human SARS CoV, and Bat CoV are calculated to be 1.41%, 1.12%, and 1.85%, respectively. Figures 3B, 3C , and 3D represent the intrinsic disorder profiles of S proteins from SARS-CoV-2, Human SARS and Bat CoV obtained from six disorder predictors. Finally, Figure 3E shows aligned disorder profiles of S proteins from these CoVs and illustrates remarkable similarity in their disorder propensity, especially in the C-terminal region. It is of interest to map known functional regions of S proteins to their corresponding disorder profiles. The maturation of S protein requires specific posttranslational modification (PTM), proteolytic cleavage that happens at two stages. First, host cell furin or another cellular protease nicks the S precursor to generate S1 and S2 proteins, whereas the second cleavage that takes place after the viral attachment to host cell receptors leads to the release of a fusion peptide generating the S2' subunit. In Human SARS CoV, the first and second cleavage site is located at residues R 667 and R 797 , respectively, whereas in Bat CoV, the corresponding cleavage sites are residues R 654 and R 784 . As it follows from Figure 3 , these cleavage sites are located within the IDPRs. In Human SARS CoV S protein, fusion peptide (residues 770-788) is located within a flexible region, is characterized by the mean disorder score of 0.232±0.053. Similarly, in Bat CoV S protein, fusion peptide (residues 757-775) has a mean disorder score of 0.320±0.046. S protein contains two heptad repeat regions that form coiledcoil structure during viral and target cell membrane fusion, assuming a trimer-of-hairpins structure needed for the functional positioning of the fusion peptide. In Human SARS CoV S protein, heptad repeat regions are formed by residues 902-952 and 1145-1184, which have mean disorder scores of 0.458±0.067 and 0.353±0.062, respectively. The analogous situation is observed for the S protein from Bat CoV, where these heptad repeat regions are positioned at residues 889-939 (0.44±0.11) and 1132-1171 (0.353±0.062). Another functional region found in S proteins is the receptor-binding domain (residues 306-527 and 310-514 in Human SARS CoV and Bat CoV, respectively) containing a receptor-binding motif responsible for interaction with human ACE2. In human S protein of Human SARS CoV this motif (residues 424-494) is not only characterized by structural flexibility, possessing a mean disorder score of 0.30±0.16, but also contains a disordered region (residues 461-466). Since S protein is known as spike glycoprotein, it contains numerous glycosylation sites. Due to rather close similarity of disorder profiles of S proteins analysed here, we can assume that all the aforementioned indications of the functional importance of disorder and flexible regions in S proteins from SARS CoV and Bat CoV are also applicable to SARS-CoV-2 S protein.\\nFinally, Table 2 shows that S protein from SARS-CoV-2 contain one MoRF region at its Cterminal (residues 1265-1272) by MoRFchibi_web, two MoRF regions ((residues 2-6) & (residues 819-823)) by MoRFPred, and one MoRF region at N-terminal (residues 1-10) by DISOPRED3. These results indicating that intrinsic disorder is important for its interaction with binding partners. Interestingly, the N-terminal region of S protein (residues 1-10) from all three viruses are observed to be a disorder-based protein binding region by two predictors (MoRFPred and DISOPRED3). N-terminal MoRF displays its role in viral interaction with host receptor and C-terminal MoRF displays its role in M protein interaction and viral assembly. Moreover, MoRF region mainly lies in the N-and C-terminal regions suggesting a possible role during cleavage as well. In addition to protein-binding regions, S protein also shows many nucleotide-binding residues. Tables 9, 10, and 11 shows that numerous RNA binding residues predicted by PPRint in all three viruses and a single RNA binding residue were predicted by DisoRDPbind in human SARS. Further, DRNApred and DisoRDPbind predicted the presence of many DNA binding residues in S protein of all three viruses. These results signify the role of S protein functions related to molecular recognition (protein-protein interaction, RNA binding, and DNA binding) such as interaction with host cell membrane and further viral infection. Therefore, identified IDPs/IDPRs and residues/regions from S protein crucial for molecular recognition can be targeted for disorder-based drug discovery.\",\n       nan,\n       'generally embraced community feeling less strongly. Compared to Boomers, Millennials and GenX\\'ers viewed goals concerned with money, fame, and image as more important, and goals concerned with self-acceptance, affiliation, and community as less important. Concern for others declined a small amount over the generations, and civic orientation declined by more than a third of a standard deviation, with Millennials continuing declines begun by GenX. Millennials were less willing to participate in collective or personal change even in areas reported to be of special interest to them, such as the environment. These results are consistent with a recent large-scale study concluding that only about 4% of modern young people are genuinely civically and politically engaged (Smith et al., 2011, p. 208) . The findings, however, are nuanced. Overall, these results primarily support the \"Generation Me\" view and are consistent with previous research finding increases in individualistic traits and declines in civic engagement over time (e.g., Malahy et al., 2009; McPherson et al., 2006; Putnam, 2000; Smith et al., 2011; Twenge & Foster, 2010) . However, the \"Generation We\" model receives some support, especially in the increasing rate of volunteering reported by younger generations. The generational similarities model is correct that not all items demonstrate considerable differences, though many exceeded Cohen\\'s (1988) revised cutoff for a small effect (d ϭ |.10|) and several exceeded or approached his d ϭ |.50| cutoff for a large effect (including all three of the items on helping the environment).\\nThe largest exception to the trend away from less community feeling was in community service and volunteering, which increased d ϭ .20 in MtF and d ϭ .22 in AF between GenX and the Millennials. Why would only these items increase, when other items measuring concern for others and civic orientation decreased or stayed the same? High schools increasingly required community service for graduation over this time period (Planty et al., 2006) . The number of public high schools with organized community service programs jumped from 9% in 1984 to 46% in 1999 (Newmann & Rutter, 1985; Skinner & Chapman, 1999) . In addition, many high school students participate in community service work to improve their college applications; thus, part of the generational increase could also be due to increasing competition in college admissions (Planty et al., 2006) . As MtF asks students about their college plans, we were able to address this possibility with data. In the most recent data (2006 -2008) , those who planned to attend a 4-year college were significantly more likely to perform community service: t(7360) ϭ 16.70, p Ͻ .001, d ϭ .39. Nearly twice as many of those who planned to attend a 4-year college (vs. those who did not) did volunteer work once a month or more (40% vs. 24%).\\nAmong the life goals items, some of the largest declines appeared in \"developing a meaningful philosophy of life\" and \"finding meaning and purpose in my life.\" Although the first of these was correlated with the intrinsic value of self-acceptance, these items are not clearly linked to community feeling. Instead they seem to capture an element of self-introspection and abstraction that was more characteristic of the Boomers than of the two generations that followed. Future research should further explore generational differences in this area.\\nThese findings may provide a partial explanation for the generational increase in anxiety, depressive symptoms, and poor mental health found in other studies (e.g., . Kasser and Ryan (1996) found that an emphasis on extrinsic values over intrinsic values was correlated with distress and decreased psychological well-being. With young people less focused on intrinsic values such as community feeling and more focused on extrinsic values such as money, mental health issues may follow. Kasser and Ryan speculated that this may occur because extrinsic values are contingent on outside forces that may be uncontrollable, whereas intrinsic values are more under the control of the self. For example, being very well off financially may be difficult to attain, but becoming involved in community affairs often requires little more than initiative. In addition, community feeling satisfies inherent human needs for connection and meaning, whereas money may not.',\n       'All statistical analyses were conducted with SPSS Statistics v.21. Prior to analysis, continuous summary variables were converted to zscores based on mean and standard deviation of the baseline CN values.\\nFor each composite score (ADNI-EF and ADNI-MEM), regularized ridge regression models of baseline data were run to guide selection of co-linear variables for linear mixed models (e.g. MD or FA, total GM or hippocampal volume, AV45 or Aβ 1-42 , t-tau or diagnostic group). Variables included in the ridge regression analyses included DTI FA and MD, mean regional FDG-PET, global AV45-PET, CSF measures of Aβ 1-42 and t-tau, hippocampal, gray matter, normal appearing white matter, white matter hyperintensities, and cerebrospinal fluid volumes (see eTable 1 in Supplement for a complete list of variables). Demographic information, diagnosis, APOE ɛ4 genotype, and intracranial vault (ICV) were also included in the ridge regression to model their effects in evaluating the contributions of the imaging measures to cognition. However, these measures were also included in all later analyses regardless of whether they were significant in the ridge regression. Imaging variables from the ridge regression that survived either the ADNI-EF or ADNI-MEM models were retained for further analyses.\\nNext, for each composite score, step-wise linear mixed models nested by diagnostic group were built to determine whether any of the imaging variables were associated with baseline scores or modified the rate of change in scores. Every model adjusted for age, sex, education, APOE ɛ4 genotype, and ICV and accounted for random effect of subject. Modifiers of change over time (years) were the selected imaging variables from the ridge regression. All independent variables were fixed baseline-associated values. Models were built step-wise to determine the cumulative variance explained by each for composite scores in the following order: MD (Model A), MD + Hippocampus (Model B), MD + Hippocampus + FDG (Model C), and MD + Hippocampus + FDG + AV45 (Model D). Secondary analyses substituted by RD or AxD for MD in Model A to test whether either of these components of MD was significantly associated with the composite scores.\\nLastly, the specificity of the association between overall white matter MD and composite scores was tested in two steps. First, the association between composite score and each ROI MD were tested in independent linear mixed models (eTable 4-5 in Supplement). Second, a principal components analysis (PCA) was run on baseline ROI MD to determine subsets of correlated tracts independent of composite scores. Four components were estimated and optimized with varimax rotation. ROIs were associated with the component with greatest factor loading N0.5; ROIs with a loading b0.5 were not grouped with a component. The regression coefficients for each component were extracted and used as predictors nested by diagnosis in linear mixed models for composite scores analogous to Model A.',\n       'SAs have various derivatives of more than 50 chemically different structures formed from the basic N-acetylneuraminic acid (Neu5Ac) on the main ring of pyranose and the glycerol side chain. SA are modified by acetyl-, lactyl-, methyl-and sulfo-groups individually or in multiple combinations [28] . Multiple enzymes are involved in the modifications [29] . Historically, the first discovered SA was crystallized by Gunner Blix via a hot mild acid extraction of bovine submaxillary mucin in 1936. It consisted of two acetyl groups. Among these, only one acetyl group was attached to nitrogen [30] . Blix isolated a 9-O-acetyl SA of the common SA N-acetylneuraminic acid (Neu5Ac), chemically described as 9-O-acetyl-N-acetylneuraminic acid (Neu5,9Ac2). Neu5,9Ac2, Neu5Ac and Neu5Gc are naturally occurring SA species in mammals. A common modification is O-acetylation. In fact, O-acetylation of SAs is common in organisms.\\nThe O-acetyl modification occurs in single positions of C-4, C-7, C-8 and C-9 of SA as well as in combined C-positions to yield Neu4,5Ac2, Neu5,9Ac2 and Neu5,7,9Ac3 SAs. Neu5Ac9NAc is a chemical and biologic mimic of Neu5,9Ac2 in the SA-glycans. The C-7 and/or C-9 O-acetylations are catalyzed by the SA O-acetyltransferase enzyme, Cas1 domain containing 1 (CasD1) ( Figure 6 ). CasD1 catalyzes the addition of acetyl groups to the SA C-7 at the late Golgi apparatus compartment [31] . Thereafter, an enzyme termed \"migrase\" transfers the additional acetyl-group from C-7 to C-9, although this enzyme has not been identified [29] . CasD1 uses acetyl-coenzyme A as a donor substrate and CMP-Neu5Ac as an acceptor substrate, but with weak activity on CMP-Neu5Gc. and susceptible to esterase enzymes. Sialic acid cleavage of the di-acetylated Neu5,7,9Ac3 by bacterial NAs decreases two-fold, when compared to mono-O-acetylated Neu5Ac. The O-acetylated glycan modification invites interaction with viruses, antibodies and mammalian lectins [32] . Therefore, the SA O-acetylation modification confers specific functions to organisms. ',\n       '• Individuals with AD underestimate their memory and language impairment.\\n• Individuals with AD are better aware of their language than their memory decline.\\n• Individuals with AD are better aware of their language abilities than informants.\\n• Self-ratings of language functioning predict performance on language tests in AD.',\n       nan,\n       'For the remainder of the report, we use the term \"predominantly minority\" to designate schools where at least 50% of the student body is minority. Likewise, we use the term \"intensely segregated minority\" to designate schools where at least 90% of the student body is minority.',\n       nan,\n       'Another source of instrumental variables is the deviation from cohort trends. This type of instrument was first used by Hoxby (2002) to investigate the causal effect of class size and class composition on student achievement. As in Angrist and Lavy\\'s (1999) study, class size and composition were the most likely endogenous variables. As a consequence, students in classes of different sizes and compositions might differ in unobserved ways that affect students\\' test performance. Hoxby\\'s (2002, p. 59 ) strategy for overcoming this problem was to use the \"cohort composition surprise\" as an instrument because year-toyear random fluctuations in local births can appreciably alter both class sizes and class compositions. If a cohort is larger than the previous cohort, the school must allocate the \"extra\" students to its classrooms. Similarly, if there are more females in the cohort than expected, some students in the cohort will have a peer group that has more females than is typical. Thus, \"surprises\" constitute an intriguing instrument. On one hand, because \"surprises\" are unexpected, they should not influence parents\\' and administrators\\' decisions that might be connected with student achievement and have no causal relationship to achievement. On the other hand, such surprises are correlated with class size and composition. As the IV, Hoxby used predicting models for annual enrollment and the prediction error for a given school in a given year. The results showed that class size did not have a statistically significant effect on student achievement; however, class composition did. For instance, both boys and girls performed better in reading when they were in classes that had larger proportions of girls.\\nEstimating the effect of class size was also explored by Wößmann and West (2006) , who used TIMMS data. They used school fixed effects 3 to account for between-school sorting, and IVs for within-school sorting and estimating causal effects. As the instrument for the class size, they used the average class size at different grade levels in a particular school. Because they controlled for the fixed effect, their instrument deviated from the predicted size of the classrooms forecasted from different grades, which is similar in principle to Hoxby\\'s idea. This IV was highly correlated with actual class size, but after controlling for school fixed effects, there was little evidence that grade and average class size would affect student performance.',\n       'Linear mixed-effects models were fit to the longitudinal BP indices of SBP, DBP, PP and MAP with a bivariate model used for SBP+DBP along with MAP+PP. The estimates of the fixed effects are shown in Table 4 for the six different models applied separately to BLSA females and males. The values in the table represent estimates of initial population averages or average rates of change in blood pressure with regard to the initial age in the study (Fage), time in the study (Time), disease status (CHD) and necessary interaction terms corresponding to these variables. For example, in the SBP models, the intercept values of 85.9 for females and 101.8 for males represent the predicted average initial SBP levels independent of the other variables or terms in the models. Thus, a 40 year-old female at baseline (Time = 0) with no occurrence of CHD during follow-up would have a predicted average SBP value of 114.8 mmHg compared to a similar aged male with no CHD having a predicted SBP value of 120.8 mmHg. Note that smoking was not a significant variable in any of the BLSA models (all p>0.25) due to the small overall percentage of smokers in the BLSA. Figure 1 shows receiver operator characteristic (ROC) curves for the CHD classifications in the BLSA obtained from the posterior probabilities from the corresponding marginal distributions of the different BP indices models. For females SBP+DBP performed slightly better than PP alone with corresponding AUC values of 0.88 compared to 0.83 (Table 5). Also, SBP alone (AUC = 0.79) had considerably better accuracy in predicting CHD than either MAP or DBP (AUC = 0.60) alone. For BLSA males, however, all the BP models had a similar predictability for CHD. Figure 2 shows ROC curves for the different BP indices for the VHM&PP. For females, PP and SBP gave the best accuracy in predicting CHD mortality with AUC values of 0.83 and 0.81, respectively. Models of MAP+PP (AUC = 0.79) and SBP+DBP (AUC = 0.78) also performed well followed by MAP alone (AUC = 0.75). As with BLSA females, DBP alone performed the worst in VHM&PP females with a value little better than by chance (AUC = 0.52, Table 5). A comparison of the results from the BLSA and VHM&PP with regard to sex suggests that the prediction accuracy for both SBP and PP alone were better for females in both populations, while both DBP and MAP alone performed better for males in both studies. In addition, the bivariate predictions of SBP+DBP and MAP+PP performed much better for females in the BLSA and only slightly better for females in the VHM&PP (Table 5). When applying the mixed-model estimates obtained from the BLSA (Table 4) to perform CHD mortality predictions for the VHM&PP, AUC values were very similar indicating good generalizability of our model to other populations. In VHM&PP females, AUC estimates derived from BLSA model estimation vs. VHM&PP model estimation were 0.85 vs. 0.83 for PP, 0.83 vs. 0.79 for MAP+PP, 0.76 vs. 0.75 for MAP, 0.85 vs. 0.78 for SBP +DBP, and 0.69 vs. 0.52 for DBP. In VHM&PP males, BLSA estimates gave slightly lower AUC values than using the VHM&PP model estimates for all the BP variables with the smallest difference in AUC values for SBP (0.71 vs. 0.76) and PP (0.70 vs. 0.75, Table 5).',\n       'ApoE ε2 may protect against AD development [51] . In fact, when ApoE ε3/ε3 carriers have a reference odds ratio of 1.0, the odds ratio for AD development for ApoE ε2/ε4 carriers is 2.4, with its 95% confidence interval including the value 1.0 (0.4 to 15.4); in contrast, the odds ratios (95% confidence interval) for AD development for ApoE ε3/ε4 carriers and ApoE ε4/ε4 carriers are 5.6 (3.9 to 8.0) and 33.1 (13.6 to 80.5) in Japanese subjects, respectively [52] . However, in the present study, the frequency of ApoE ε2/ε4 carriers was very small: 1.3% both for AD and CN participants while no MCI individuals were carriers (Table 1) . Therefore, we regarded ApoE ε2/ε4 carriers as ApoE ε4 carriers.',\n       'Sex of the sample member is taken from the base-year student questionnaire; if missing, it is supplemented by the parent questionnaire or school-provided sampling roster. If the sex indicated by any of these three sources was inconsistent, coders checked the names attached to each case to determine whether the names were gender-specific, and if so, to determine sex.',\n       nan,\n       'The work here seeks to understand the extent to which commercialization in academic science research is associated with gender disparities in the way men and women graduate students in STEM are funded. Given that the increased attainment of doctoral degrees in STEM fields among women in recent decades has not necessarily been associated with proportionate representation of women in tenure or tenure-track research faculty positions (Kulis et al., 2002) and that this has occurred during the same period in which commercial influence in academic science has grown (Slaughter & Leslie, 1997;Slaughter & Rhoades, 1996), research investigating these trends is warranted. This section will detail expectations for the current work and connect it to the context discussed previously. First, this work expects to find that women are funded via reproductive labor mechanisms, primarily teaching assistantships funded via institutional support, at a disproportionately high rate. As well, this work expects to find that women are funded via productive labor mechanisms, primarily research-oriented work funded by \"Other U.S. Source\" and federal sources, at a disproportionately low rate. The divisions that are expected to be evident in the analysis mirror broader trends in which women are disproportionately employed in teaching college and performing disproportionately high levels of teaching labor (Baker, 2012;Misra et al., 2010;Umbach, 2007;Winslow, 2010). In this way, this research seeks to provide some evidence that women receive socialization for teaching careers at a higher rate than men. The second component of this research relates to the value of work performed by women and the influence of commercialization for reinforcing gender roles and hierarchies. Slaughter and Metcalf\\'s (2008; work suggests that market proximity is an emerging component of perceptions of prestige, relating to the transition toward an academic capitalist regime. Further, scholars argue that the increasingly commercial context in which academic research is being done has resulted in increased commodification of academic research (Irzik, 2013;Jacob, 2009;Radder, 2010;Slaughter & Leslie, 1997;Slaughter & Rhoades, 1996). Because of this, the current work predicts that as commercial influence grows, research labor will increasingly be perceived as masculine, productive labor. Further, this labor will be prioritized over feminine, reproductive labor (most associated with teaching labor) (Acker, 1990(Acker, , 1992. Through this process, labor undertaken by STEM graduate students is expected to become increasingly gendered. Therefore, women engaged in productive labor may be perceived as resisting gendered norms, which may be associated with consequences. Therefore, disadvantages relating to socialization toward teaching labor may be compounded in highly commercial settings. The compounded undervaluing of teaching labor in highly commercial settings may be associated with reinforcing and widening gender gaps in the broader academic workforce. Although not investigated in this analysis, such findings may be used to explain women\\'s high representation in academic positions of low prestige, performing less prestigious work like teaching and high representation among part-time, and lowerrank positions (Baker, 2012;Kulis et al., 2002;Misra et al., 2010;Umbach, 2007). Therefore, my study aims at investigating disparities in academic labor pipeline that could help to explain broader gender gaps, as well as why these gender gaps are not decreased simply through increased attainment of doctoral education among women in STEM fields.\\nThe primary motivator for this research was to understand whether disparities exist in the way that men and women graduate students are funded in STEM fields. Furthermore, this research took on an additional layer by critically evaluating the role that industry influence plays for changing how research is conceived, and thereby affecting the gendered perception of labor among graduate students. Given this theoretical framework, drawing on academic capitalism and resource dependence, one would expect to find a restructuring of the workplace as departments become more reliant on industry sources of funding (Pfeffer & Salancik, 2003;Slaughter & Rhoades, 1996;Slaughter & Rhoads, 2004). As such, using feminist institutionalism (Kenny, 2007;Mackay et al., 2010) and the theory of gendered organizations (Acker, 1990(Acker, , 1992, I theorized that existing gender dichotomies apparent in graduate student funding mechanisms, would likely be reinforced as marked influence increased. Of particular importance for the current work is to analyze the extent to which patterns in men and women\\'s funding mechanisms in STEM departments, aligned with the predictions of the critical theories used in this analysis. However, the statistical did not provide evidence that increased commercialization leading to increased commodification of research was associated with increased P-GFD. Furthermore, this study found evidence that increased commercialization, leading to increased commodification of research, was associated with decreased R-GFD. These findings, while contrary to the theoretical perspectives used, point to the need for further analysis of the assumptions used in for this study. As detailed in the discussion section, further 123 work is needed to better understand the process of commodification of academic research, as well as the extent to which the used of proportions in this analysis may mask inequalities that are evident when analyzing the raw numbers of students. As is identified throughout this work, gender gaps in STEM are multidimensional and persistent. The use of critical theories to develop this model, and critical perspectives used to analyze the results, requires of researchers to consider such multidimensional approaches to understanding inequality. Although the current work failed to find support for its\\' predicted outcomes, the results do provide rich insight into the interrelationships between the various variables used in this analysis. As such, this work represents a jumping off place of sorts for future analyses of gender funding disproportionality among STEM graduate students. Figure 1. Rates of Doctorates Awarded to Males and Females in STEM, 1986-2016  1986 2016 1986 2016 1986 2016 1986 2016 1986 2016 Life Sciences Physical Sciences and Earth Sciences',\n       '1980s, theoretical and clinical developments have shifted research interest earlier in the natural history to include pre-dementia syndromes, especially mild cognitive impairment (MCI) [1] [2] [3] [4] [5] [6] . It is now thought that intervening to slow or stop the progression of disease will be more effective than waiting until severe neuropathology and dysfunction have developed [1, 4, [7] [8] [9] . Thus, many research studies, both observational and experimental, are being conducted in pre-dementia populations, whereby cognitive ability may range from normal cognition (NC) to MCI [5, 8, [10] [11] [12] [13] [14] . The ADAS-Cog is often employed in these pre-dementia studies; however, the ADAS-Cog was developed for use in studies of dementia where cognitive impairments are more severe. Concerns have been raised about whether the ADAS-Cog is able to detect important changes at earlier stages of disease progression [2, 7, 15, 16] . If not, studies may miss disease progression or regression, or miss potential treatment benefits. Using an outcome measure in a population or context where it does not perform well threatens both internal and external study validity.\\nThis narrative review has two main objectives: first, to review the performance of the ADAS-Cog in subjects with pre-dementia status such as MCI, and second, to assess responsiveness of modified ADAS-Cog versions in dementia and pre-dementia populations.',\n       \"More than 3000 phone calls were conducted in nine states to arrange for the mailing of 4466 surveys with 1561 returned (Table 1) , representing a 35% survey response rate (75% and 25% Spanish and English, respectively). In several states, a sample of 30 was never reached (Table 1) . Florida and Ohio represented the only states where 30 packets were mailed (Table 1) . Many nurseries contacted by phone did not want to participate in the study. Acuña and Mathers (2009) also identified employer indifference and reluctance to participate in programs dealing with Hispanic nursery employees. Response rates by state (surveys returned/surveys sent) varied from 13% in Tennessee to 51% in Michigan (Table 1) . There is no consensus regarding acceptable response rates for mailed surveys and, according to Lohr (1999) , giving absolute guidelines for acceptable response rates is dangerous and can lead to complacency. Of nurseries contacted in Florida, a total of 67% returned surveys to OSU (Table 1) . Seventytwo percent of the nurseries contacted in Ohio returned surveys (Table 1) . Rhode Island and Delaware had fewer nurseries to survey, 87 and 131, respectively (Table 1) , and few that met the size restriction of the study. In these states, researchers visited nurseries and administered the surveys onsite resulting in a 100% nursery packet response rate. The response rates (returned/ sent) were 22% and 29% for Rhode Island and Delaware, respectively. A 25.2% response rate in Ohio (Table 1) was lower than the nursery response rate reported by Mathers (2003) of 52% conducting an on-site survey. However, the nursery contacted/response rate in Ohio of 72% (Table 1 ) was higher than the Mathers (2003) study.\\nHigh employee turnover. Hispanics constituted 70% of the average nursery workforce, including general laborers (76%), crew leaders (61%), and sales/managers (others) (21%) (Acuña et al., 2010) ; a significant correlation existed between time working in the nursery industry with current job position (RSCS, P < 0.0001). The polynomial correlation r = 0.8478 was only significant for the laborer level. Corresponding polynomial correlation coefficients (r) for crew leader and other job positions were r = 0.4991 and 0.6801, respectively, as calculated by Excel (Microsoft Inc., Redmond, WA). Comparing type of job (general laborer, crew leader, or other) and number of years working, at the laborer level, the turnover rate was more than 51% after 5 years of employment [37% (0 to 5 years) to 18% (5 to 10 years)/37 · 100)] (Fig. 1) . The highest worker turnover rate in the United States is in the accommodation and food services sector at 56% (Nobscot Corp., 2006) . At more than 51%, the nursery industry ranks second only to this traditionally unstable employment industry. With an estimated cost of $4000 to $7000 to replace an hourly worker (Lousberg, 2005) , efforts to stabilize the nursery industry workforce are crucial to secure the industry's economic survival. Additionally, in Ohio, where the nursery/landscape industry is one of the largest employers, surpassed only by Florida, California, and Texas in terms of worth contributed to the state's economy (Hall et al., 2005) , improved economic stability for the industry would translate into economic stability for the entire state. The high worker turnover rate was limited to the laborer level. The majority of crew leaders and others (sales, managers, and so on) were working longer than 10 years and many working longer than 15 years (Fig. 1) . The indifference associated with obtaining employers to participate in this survey and lack of employer participation in other labor force programs conducted by OSU 2003 (Acuña and Mathers, 2009 ) indicate major outreach efforts are required at the managers/owner level to reduce apathy and increase sensitivity to issues of the current labor supply.\\nSalary is often noted as the most important issue for nursery workers, especially with migrant laborers; however, in this survey, ''lack of benefits'' received the highest response rate (87%) when ''very'' important and ''somewhat'' important categories were totaled versus 81% for ''low salary.'' DaleOlsen (2006) also noted a positive correlation existed between higher wages and benefits and reduced worker turnover. Job accidents, as the second ''very'' important issue (Fig. 2) , agrees with Yeoman (2000) , which indicates immigrant workers often perform dangerous jobs and/or jobs requiring repeating the same action continuously for hours in high temperatures, causing development of carpal tunnel syndrome or back injuries that require returning home and/or surgery to correct. The rating of job accidents (second) and lack of benefits (fourth) as ''very'' important would be related issues to an employee (Fig. 2) . Coupled together, these factors reduce earning potential, negating the main reason stated for seeking U.S. employment: increased remuneration.\\nIssues deemed to be ''not important'' included ''no job continuity'' and ''work not challenging'' (Fig. 2) . However, the question asking about job continuity (lack of yearround employment) also had the highest level of responses in the ''somewhat'' important category and marked the only time ''somewhat'' important positively outmeasured the ''very'' important category (Fig. 2) . This possibly indicates if other issues such as job safety, salary, and so on were corrected, lack of year-round employment would be more important to the employee. This is pertinent because many nursery owners have indicated they would like to retain workers for yearround employment but cannot convince employees to stay. Migrant workers have discovered how they can extract wealth from nursery operations on a short-term basis, maintain home bases in Mexico and elsewhere, and leave their options open for future employment. They, therefore, contribute to the high worker turnover observed in this study.\\nCountry of origin. Seventy percent of survey respondents were born outside the United States, including 57% in Mexico, 5% in Guatemala, 3% in Puerto Rico, and 5% elsewhere (Acuña et al., 2010) . Sixty-five percent of the Mexican workers filled general laborer positions and 48% held crew leaders positions (Fig. 3) . Seventy percent of the ''other'' job classification was filled by U.S. native-born employees. Responses in the ''elsewhere'' classification included mainly other Hispanic countries such as Venezuela, Colombia, Panama, Dominican Republic, Paraguay, El Salvador, Ecuador, and Chile. These results are similar to those reported in NAWS (U.S. Department of Labor, Employment and Training Administration, 2002) in which 78% of all crop workers were born outside the United States. According to Mosisa (2002) , the ethnic and racial composition of the U.S. workforce is diversifying at a rapid pace. Much of that change reflects the proportion foreign-born workers versus U.S. native-born workers. In 1960, this proportion was 1:17; today the proportion is 1:8. Additionally, the birth country of these workers HORTSCIENCE VOL. 45(1) JANUARY 2010 has shifted. In 1960, 75% of the foreignborn labor force came from Europe. Today, European workers represent less than 16%, largely reflecting the influx of immigrants from Latin American and Asian countries. In 1965, the Immigration and Nationality Act was passed, eliminating quotas for national origin, race, or ancestry. This has allowed a large influx of immigrants from Mexico, Philippines, India, China, Cuba, El Salvador, Vietnam, South Korea, Canada, and the Dominican Republic (Mosisa, 2002) . In 2000, people of Mexican origin were the largest Hispanic group in the United States representing 59% of the U.S. total Hispanic population (Ramirez, 2004) .\\nAdvancement and language. The primary language spoken at work by survey respondents was Spanish (63%) followed by English (30.3%), whereas only 6% of the workers indicated an ability to speak English and Spanish and a very small percentage (0.1%) indicated they could speak another language such as Mixteco or Portuguese. These language responses were similar to those reported in NAWS (U.S. Department of Labor, Employment and Training Administration, 2002) , in which 80% of the workers indicated Spanish was their primary language, 18% English, and 2% other. There was no report of language spoken at work in the 2000 U.S. Census. However, the Census did report that 75% of Hispanics spoke a language other than English at home with 99% of those speaking Spanish (U.S. Census Bureau, 2002) .\\nA significant association was found between job position and ability to speak (RSCS =106.7, F = 13.4, P < 0.0001), understand (comprehend) (RSCS =172.2, F = 21.7, P < 0.0001), and read (interpret) English (RSCS =123.9, F = 15. 5, P < 0.0001). Fifty-seven percent of laborers reported ''no English''-speaking ability (33%) or ''a few words'' (24%) ( Table 2) . By contrast, 22% of crew leaders and 14% of others reported ''no English''-speaking ability or only ''a few words'' ( Table 2 ). The percentages obtained were lower for ''I speak no English'' (33%) compared with NAWS (U.S. Department of Labor, Employment and Training Administration, 2002) in which 44% of the crop workers said, ''They could not speak English.'' This difference may be the result of workers in the nursery industry having more time and exposure to English and technical terms. The nursery workforce is not as ''seasonal'' as in other agricultural sectors such as fruit or vegetable production in which significant worker numbers are needed only for short periods of 1 or 2 months, coinciding with harvests. In the nursery sector, workers are employed on average 6 months with many staying for 10 months (Mathers, 2003) . In a survey of low-skilled immigrants, Chiswick (1991) found that verbal and comprehension fluency increased with duration in the United States. He also observed the greatest increase duration was for those who had more schooling in their home country in all groups except Hispanics. Our results concur with Chiswick (1991) because no relationship between previous education level and language skill development was ascertained (data not shown). Skill development. There were several other important findings from the survey questions regarding language in relation to skill development, interest, and employee turnover. A difference in comprehension, verbal, and reading skill development was evident with job position. Ranked from highest to lowest (a negative response question), comprehension~verbal~reading skills were 26.7%, 33%, and 38.6% and 2.6%, 7.2%, and 8.7% for general laborers and crew leaders, respectively (Table 2 ). This finding that verbal skills are lagging behind comprehension (understanding English by Spanish speakers) ( Table 2) is consistent with the ''preproduction period'' of second language acquisition or ''the silent period'' as defined by Haynes (2008) . A large body of literature supports this ''silent period'' hypothesis (Krashen, 1987 (Krashen, , 1988 . The silent period varies in duration because a newcomer is unwilling to speak in the second language. Nearly all students go through a silent period. This stage can last for as long as 1 year (Haynes, 2008) .\\nIn the ''early production phase'' or second phase of second language acquisition (Haynes, 2008) , general laborers are still indicating greater comprehension of English [''I understand a few words'' (31.7%)] than ability to speak [''I speak a few words'' (24.1%)] (Table  2 ). Most researchers support that the natural language acquisition order is for listening/ comprehension to develop before speaking (Krashen, 1987 (Krashen, , 1988 . The early production phase may last up to 6 months during which time students develop a receptive and active vocabulary of 1000 words and can usually speak in one-or two-word phrases. They can use short language chunks that have been memorized, although these chunks may not always be used correctly.\\n''Speech emergence'' is the third phase of second language acquisition (Haynes, 2008) . At this stage, students develop a vocabulary of 3000 words and can communicate with simple phrases and sentences. General laborers at this phase are indicating an equal ability to comprehend English [''I understand half of the words'' (10.2%)] as to speak [''I can speak simple phases'' (11.4%)] (Table 2) . This equivalency would be unusual but not without precedence. An Army Specialized Training Program (World War II) used verbal drills based on behavioral psychology theory to train solders to deliver specific types of messages (such as troop locations) in a second language. They were able to perform very well within a narrow context but had very little listening comprehension in general and no speaking ability in unfamiliar contexts (Agard et al., 1945) . People who volunteer in second language clinics have observed nonnative speakers may understand and function effectively within the narrow context of their tasks; however, their ability to function is not indicative of their overall English proficiency. To potentially earn more money and advance in the workplace, general laborers may feel external pressure to increase their verbal skills beyond their comprehension. This finding is of interest because many nursery employers feel that if their staff can speak English, they can understand English and therefore they can conduct technical training sessions in English versus Spanish.\\nThe phase of second language acquisition, in which crew leaders make the greatest gain in verbal and comprehension skills versus laborers, is in the ''intermediate fluency'' or the fourth stage (Haynes, 2008) . This fourth stage was evaluated in our survey with the questions, ''I can maintain a simple conversation'' (26.6% crew leaders; 9.4% laborers) ( Table  2) . At this stage, English language learners have a vocabulary of 6000 active words and the ability to speak and comprehend is near equivalent (Haynes, 2008 ) consistent with our findings. However, employers should be aware that even crew leaders are far from mastering the fifth stage or ''advanced fluency'' in English as described by Haynes (2008) . In fact, crew leader verbal and comprehension fluency is only 15% (36.8% to 22.1% and 38.3% to 22.8%, respectively) ( Table 2) , respectively, more than general laborers.\\nEmployee turnover and training. Another important finding from the language question responses dealt with the association of language development and worker turnover and the consequences of this for worker training. Haynes (2008) indicated it takes second language students 4 to 10 years to achieve the fifth stage or ''advanced fluency.'' This is interesting because 51% of the laborers leave after 5 years and only 5% remain employed after 10 years. There seems to be a relationship between developing fluency and worker turnover. Of course, this makes the worker attrition even more unfavorable for employers. Employers not only lose all the tacit knowledge, technical training, and company familiarity that employees may have acquired over 5 years, but they also lose employees who have attained advanced verbal and comprehension skills, making them the kind of employees they want to retain and advance. Haynes (2008) indicates that students at this stage will be near native in their ability to perform in content area learning.\\nTraining is known to play a significant role in employee retention, advancement, increased remuneration, and productivity (Witty, 2007) . Mexican and other Hispanic immigrants have significantly lower levels of education than any other American groups, including their American native-born counterparts. Therefore, another potential benefit of employee training might be to reduce the educational gap for Hispanic nursery employees. A recent study with migrant workers in Alabama's horticultural industry indicated a 1% increase in a firm's total number of employees raised gross sales by 0.69% (Bellenger et al., 2008) indicating migrant employees are one of the most success means of increasing company productivity. This finding also was supported by Posadas et al. (2006) in which hiring an additional full-time-equivalent increased annual sales by $69,513 versus a 1% increase in mechanization ($3384/year) or adding one more acre in production ($1207/year).\\nSixty percent of nursery employees surveyed had not received work-related training and if training was provided, it was most often delivered in English (data not shown). Although 81% of men and 72% of women were interested in training (Table 3) , and an association between training and employee retention existed, training in the majority of nurseries did not occur. The highest rated training topic of interest was English/Spanish regardless of gender (respective of Spanish/ English primary language) (75%) ( Table 3 ). Other topics of interest were plant identification, plant disease identification, and control and equipment safety (Acuña et al., 2010) . Seventyeight percent of female respondents and 70% of males would prefer training in Spanish (Table 3) . Only 30% of men and 22% of women preferred training in English (Table 3) .\\nWe have shown there is a correlation between developing fluency and nursery worker turnover. With 52% of the laborers leaving after 5 years and only 5% remaining employed after 10 years, employers not only lose all the technical training and company familiarity that employees have acquired in the past 5 years, but they also lose an employee that has attained advanced verbal and comprehension skills. Previous surveys have indicated employer-provided training opportunities were positively received by Hispanic nursery employees, improved worker/manager relations, and improved company loyalty (Mathers, 2003) . Some nurseries report that approximately twothirds of their annual expenses are directly related to labor. With significant slowing in the U.S. nursery industry growth rate, it follows that innovations in labor retention in such a labor-intensive industry could have more profound impacts than any other modernization. Nursery workers are interested in technical classes. Seventy-seven percent of workers (pooled over gender) indicated an interest in attending training courses. In addition, it was observed in the nurseries visited that the workers are open to learning more. The preferred topic for a course was English (75%). This finding related to the association of job position and English proficiency. Those with higher English proficiency were obtaining better jobs in the industry. The nursery workers see English as a way to advance economically and professionally and therefore they indicated this was their greatest technical need. Future studies are needed to evaluate the effects of Hispanic employee training, principally the effects of English training on worker retention. Currently, there are no studies evaluating these effects in the agricultural or horticultural sector. Methods to increase employer/employee relationships are also needed to improve work efficiency.\",\n       'The index of online reading activities (ONLNREAD) was derived from the frequency with which students involved in the following reading activities (ST26): reading emails, <chat on line>, reading online news, using an online dictionary or encyclopedia, searching online information to learn about a particular topic, taking part in online group discussions or forums and searching for practical information online. The higher values on this index indicate more frequent online reading activities.',\n       'research builds on existing FEST-C system and uses the bidirectional flux version of CMAQ (bidi-CMAQ), which represents the integration of EPIC and CMAQ models driven by WRF meteorology. The CMAQ version employs a compensation point approach to estimate the flux of NH 3 (emission or deposition) from underlying soil and vegetated surfaces to air Cooter et al., 2012) . EPIC was modified to take daily time series of total wet oxidized N (g ha −1 ), total wet reduced N (g ha −1 ), total dry oxidized N (g ha −1 ), total dry reduced N (g ha −1 ), and total wet organic N (g ha −1 ) from WRF/CMAQ (Cooter et al., 2012) .\\nThe FEST-C system guides users through generating land use and crop data needed for EPIC (BLED4 in Fig. 1 ), creat ing daily weather and N deposition input from WRF/CMAQ; preparing EPIC site, soil, and management inputs (Spatial Allocator Tools in Fig. 1 ) for EPIC simulations; and extracting EPIC output for quality assurance. In addition, it also extracts initial soil and pH conditions and daily N information required by CMAQ bidirectional NH 3 modeling. The Spatial Allocator Tools connect EPIC with WRF/CMAQ (Fig. 1 ). Our effort in this study further enhanced the FEST-C system to generate SWAT-needed inputs from EPIC/WRF/CMAQ.\\nThe target EPIC simulation resolution for integration with a gridded regional air quality model is 144 km 2 (i.e., 12 km by 12 km rectangular grid-cells); land use at the start of the simulation period (2002) is used throughout. The 2002 county-level Census of Agriculture (fractional distribution of crops within the county with the total agricultural land use) was constrained by NLCD 2001 (Cooter et al., 2012) and the model was configured to simulate fertilization based on the plant demand using computed N stress level in simulation. The area of each crop land on a given 12 km by 12 km grid cell is known, but the exact location is not. EPIC produces edge-of-field outputs including runoff, sediment, and nutrients on a daily basis for each crop within a grid cell; outputs are unit loadings (kg ha −1 ).',\n       nan,\n       \"Beginning June 19, a period of vigorous thunderstorm activity resulted in an unprecedented weeklong lightning event with 36 000 strikes in three days. During this period, 65 000+ strikes in Alaska gave rise to nearly 270 ignitions of the preconditioned fuels. Burned acreage increased by 3.8 million acres (Fig. 4.1b) in the two and a half weeks following those starts ( Fig. 4.1c). Lightning ignitions caused 99.5% of the acreage burned in Alaska in 2015. A westerly shift in upper-level winds by mid-July brought cool and damp weather that curtailed fire growth, and most extant fires burned little acreage after July 15. This pattern highlights a significant difference between Alaska's top two fire seasons: 2004 burned significant acreage in July and again in August during extended warm and dry late summer weather, while 2015 saw the bulk of fire activity concentrated from mid-June to mid-July. These different pathways to large fire seasons demonstrate the importance of intraseasonal weather variability and the timing of dynamical features. Yet, underlying each case are the common requirements of: heat, extremely dry fuels, and ignition. One question that arises is whether the extremely warm and dry, yet convective, conditions of 2015 might be driven by anthropogenic climate change. This attribution study is a model-based test of the hypothesis that anthropogenic climate change increases the likelihood of fire seasons as extreme as 2015 through increasing flammability of fuels. Measuring Fire Risk through the Buildup Index. This assessment uses the Buildup Index (BUI; Lawson and Armitage 2008), which is part of the Canadian Forest Fire Danger Rating System's Fire Weather Index system and represents potential fuel availability and flammability, based on cumulative scoring of daily temperature, relative humidity, and precipitation. \",\n       \"The purpose of this study was to test the effectiveness of Van Hiele's phases of learning geometry using the Geometer's Sketchpad (GSP) on students' levels of geometric thinking. This quasi-experiment involved 94 students and two teachers. A total of 47 students were in the control group and the rest were in the treatment group. The students in the treatment group learned Form Two's Transformation topic through the Van Hiele's phases of learning using the GSP, while the students in the control group learned the same topic conventionally. Before the study started, students from both groups were given Van Hiele's Geometry Test (VHGT) to identify their initial levels of geometric thinking. The experiment took place for 6 weeks. At the end of the study, the students in both groups were given the VHGT for the second round to analyse their final levels of geometric thinking. Wilcox on-t test for the design of repeated measurement was used for the data analysis. The results found that the students in both groups showed increment in their post-VHGT as compared to the pre-VHGT. However, the students in the treatment group achieved better levels of geometric thinking compared to the students in control group (t = 34.50, p<0.05). Thus, the Van Hiele's phases of learning geometry can be applied in classrooms in order to help students achieve better level of geometric thinking.\",\n       'Administrative records from Schleswig-Holstein\\'s school entrance examinations (SEE) are our main data source. The data is structured in school entrance cohorts. A school entrance cohort comprises all children who turn 6 years old between July of the previous year and June of the same year as school entrance. 14 This study draws on data for the school entrance cohorts 2004 to 2012. As described in the previous section SEE data contain medical assessments on, among other dimensions, children\\'s motor skills and socio-emotional maturity. The medical diagnosis can take five forms: \"normal development\", \"some problems, but no treatment is necessary\", \"some problems, already in treatment\", \"problems, treatment necessary\", and \"problems which will reduce the child\\'s performance in school\". Based on this diagnosis, we construct a binary indicator for each of the two dimensions assessed in the SEE (motor skills and socio-emotional maturity), which is equal to one if the child does not exhibit any problem in the assessed tasks.\\nThe SEE provides us furthermore with the pediatrician\\'s assessment whether a child is ready to follow the school curriculum or not. The recommendation can take the following three forms:\\n\"ready for school\", \"school enrollment only with support provided by the teacher\", and \"special needs education required\". We construct again a binary indicator which is one if the child is ready for school.\\nThe SEE also contains parental reports on child and family background. Among other questions, parents answer a question regarding whether their child attended child care. Yet, they do not provide any information on the amount of hours their child attended child care. In other words, we do not possess of any direct information on the intensive margin on an individual basis -i.e. whether the child attended care on a full-day basis. Instead we rely on the average rate of full-day slots among all slots available in care centers on the municipality level provided by our second data source described in turn. Thus, similar to previous studies investigating the impact of universal child care on children\\'s development (Baker et al., 2008; Cascio, 2009; Fitzpatrick, 2008; Havnes and Mogstad, 2011) , we can only provide estimates for the intention-to-treat effect (ITT), but not for the treatment effect itself.\\nAdministrative records of all child care centers are our second data source (the so-called Kinder-und Jugendhilfestatistik or Statistics on Child and Youth Services). These records contain detailed information about the provider, the children enrolled and the staff employed and thus, allow us to construct the following series of indicators describing the care centers:\\ncoverage rates among 0-2-year-old children and 3-6-year-old children, the share of full-day and 14 Children who were not ready for school in one year undertake a special examination one year later and thus are not included in the baseline SEE. Parents whose children turn 6 years between July and December of the same year can ask their child to be examined a year before the official SEE would have taken place. We exclude these children from our analyses.\\nhalf-day slots, the provider (public provider vs. the church vs. other providers, which are mostly welfare organizations), as well as structural quality features such as group size and the staff composition in terms of age, gender, pedagogical degree and workload. All information is available at the care center level. Since a substantial share of care centers host children of different age groups, 15 we cannot distinguish between slots offered to 0-2-year-old children and slots offered to 3-6-year-old children. As such our treatment -the share of full-day slots -as well as any other information on care centers refer to children age 0-6. Nevertheless, as pointed out before, until 2006 only up to 7% of all slots in care centers are offered to 0-2-year-old children, while more than 93% of all slots in care centers are offered to 3-6-year-old children. In other words, the focus of our analysis lies on the effect of expanding full-day care which is mostly available to 3-6-year-old children.\\nThe smallest regional level available in both data sources is the municipality. 16 Data protection, however, restricts the number of municipalities available for scientific research. First, administrative data on care centers are only released if municipalities contain at least three care centers, otherwise only averages of care centers in neighboring municipalities are available.\\nSecond, not all municipalities can be identified in the SEE data. In fact, counties -the second smallest regional level in Germany -are in charge of gathering the results of the SEE and delivering them to the respective state office (which is the Ministry of Social Affairs, Health, Family and Equality in Schleswig-Holstein). When delivering the data to the ministry, counties have the right to anonymize municipalities and some of them do so. In fact, we possess identifiers for 75 municipalities (belonging to 8 out of 15 counties) which allow us to merge the available administrative data on child care centers to the SEE data. In addition, we use the municipality identifiers to add information on the demographic and socio-economic composition of the municipalities.\\nThe fact that information on our treatment -the share of full-day slots -is only available on the care center level which are accessible for children age 0-6 years old, rises the question of when to measure the share of full-day slots. In other words, we have the choice of measuring the supply of full-day slots at any age between zero and six. To circumvent any endogeneity of care center features to the parental decision of enrolling their child in child care and in particular, in child care on a full-day basis, we choose to control for the share of full-day slots available in children\\'s birth year and thus prior to children\\'s own enrollment in child care.\\nRestricting our sample to children for whom we possess information on all assessed dimensions -school readiness, motor skills and socio-emotional maturity -leads to a sample of 93,570 children belonging to nine school entrance cohorts and residing in 75 municipalities. % of all children are assessed to be socio-emotionally mature. Finally, 87 % of all children are assessed to be ready for school, i.e. are able to follow the curriculum taught in primary school.',\n       nan,\n       \"Before European settlement, about 162 × 10 6 ha of grasslands occurred in North America in three major ecoregions: tallgrass prairie, shortgrass prairie, and mixedgrass prairie (Samson & Knopf 1996 , Herkert & Knopf 1998 . Loss of native grassland habitat has been nearly complete in all regions (Samson & Knopf 1996) , resulting from conversion to cropland, planting of exotics, overgrazing, and encroachment by shrubs (Vickery et al. 1999) . The biogeographic history of North American grassland is complex, but these ecosystems undoubtedly developed with frequent fire from lightning strikes and fires set by Native Americans (see several chapters in Samson & Knopf 1996) . The frequency of fire and other disturbance agents such as grazing or mowing (a detrimental form of anthropogenic disturbance depending on when it occurs; see Bollinger et al. 1990 ) is a major determinant of the structure and plant species composition of most types of prairie. Vickery et al. (1999) identified 32 bird species that are obligate to temperate-zone grasslands in North America, and they listed an additional 52 species that regularly occur in grasslands or ecotonal grasslands such as savannas.\\nManaging disturbance is the foundation of grassland restoration and conservation (Leopold 1949) . Surprisingly few studies have been designed to assess fire effects on grassland birds, but it is evident that the temporal and spatial scales of disturbance have important consequences for habitat suitability. A 23-y study by Johnson (1997) in mixed-grass prairie in North Dakota assessed the effects of a 3-5 y burn rotation on avian abundances. Burning initially rid sites of vegetation and created open ground or short herbaceous vegetation; succession then led to taller vegetation and encroachment of woody vegetation. Avian responses to fire disturbance and subsequent succession fall into three groups: 1. those species that immediately colonized such as the Killdeer (Charadrius vociferous), Marbled Godwit (Limosa fedoa), or Upland Sandpiper (Bartramia longicauda); 2. species that used sites about 2 y after burning, but before woody encroachment, such as the Bobolink (Dolichonyx oryzivorus) and Grasshopper Sparrow (Ammodramus savannarum); and 3. species that required woody vegetation and relatively long-term protection from disturbance such as Common Yellowthroats (Geothlypis trichas). Similar patterns of turnover were found by Madden et al. (1999) in mixed-grass prairies in North Dakota that burn frequently. Therefore, even within the constraints of what is a prairie or grassland, there is an important short-term successional cycle that is disturbance-dependent, with significant turnover in avian community structure. Fire is particularly important in mesic grasslands that are comparatively productive and where woody encroachment proceeds rapidly (Madden et al. 1999) .\\nTime elapsed since fire disturbance also influences bird populations in tallgrass prairie. Local abundances of Henslow's Sparrows (Ammodramus henslowii), a species of conservation concern throughout much of its range (Peterjohn & Sauer 1999) , are highly sensitive to near-term fire history. In Illinois, newly burned sites are generally avoided and greatest abundances are attained on sites 2-4 y postburn (Herkert & Glass 1999) within a relatively specific range of vegetation heights and litter depths.\",\n       \"In this cohort, we used data from the Japanese Alzheimer's Disease Neuroimaging Initiative (J-ADNI). The clinical and imaging protocol is described elsewhere [6] . Most participants in the J-ADNI underwent 1.5-tesla MRI scans; we selected only participants who underwent 3-tesla MRI scans. The clinical demographics are also shown in Table 1 . \",\n       'CATI BRR weights for graduate/first-professional students BRCBWT01BRCBWT64: CATI BRR weights for baccalaureate students. To create the replicate weights, student-level replicate weights were defined. For each replicate set, student weights of one PSU within each analysis stratum were set to zero and the student weights of the other PSUs were doubled to approximately preserve the population weight total. The number of replicates was set equal to the number of analysis strata to achieve the correct degrees of freedom for variance estimation. Then each set of replicate weights was poststratified to the control totals, similar to the description in Section 6.1, with a couple of exceptions to allow the models to converge. First, there were model convergence problems for some replicates when we attempted to control to total Pell grant recipients and also to Pell grant amounts. Therefore, we could not control the mean value and could only control to Pell amounts. Second, for several of the replicates, we had to collapse some control totals, such as enrollment by sector, for two sectors because some replicates had small sample sizes for certain poststratification groups.',\n       'Therapies to combat the spread of SARS-CoV-2 and the lethality caused by the resulting COVID-2019 are currently focusing primarily on S, the viral spike protein (Kruse, 2020; Liu et al., 2020; Wrapp et al., 2020) . However, despite high similarities between proteins S between SARS and SARS-CoV-2 viruses, existing neutralizing antibodies are ineffective against SARS-CoV-2 (Wrapp et al., 2020) . Hence, new antibodies that bind specifically to the SARS-CoV-2 spike protein need to be developed, tested, and approved for human use, which would be a time-consuming process. Our pangenomic analysis suggests that the protein E of all SARS viruses preserves its critical motifs used for pathogenesis, and should be considered as an alternative target to be tested for therapies to mitigate COVID-2019.',\n       'The applications in this category contain mainly customized or specifically designed search portals and information retrieval systems that search and analyze thousands of documents to extract the relevant information. The types of documents vary from published peer-reviewed articles to preprints, commentaries, case studies, letters, and editorials.\\nThe iSearch COVID-19 application (Table 1 , No. 2) can search indexed documents based on simple keywords. Using advanced search options, it gives more control over search operations through additional content type search including types of devices, chemicals, drugs, and conditions. Moreover, using filters, one has greater power over export of results tailored to the requirements. A potential issue with this application is that novice users can have erroneous results if the And/Or criterion is not used correctly. The system is also not robust to special symbols, e.g., \"covid-19\" and \"covid 19\" show different results as well as the use of various other special symbols that lead to unpredictable results.\\nLitCovid (Table 1, No. 3) is also managed by the NIH entity like iSearch (Table 1, No. 2), but unlike the latter, it is more organized around topics. In addition to using open search of articles using keywords, LitCovid provides division of articles under eight topics: General, Mechanism, Transmission, Diagnosis, Treatment, Prevention, Case Report, and Forecasting. By clicking on any topic, a researcher gets a sorted (reverse chronological) list of articles in that topic. The results can be further narrowed down by chemicals, journals, and countries. Another useful feature of LitCovid is the ability to download articles with six different annotations (BioConcepts): gene/protein, drug/chemical, disease, cell type, species, and genomic variants. The interface allows to select/unselect any of these BioConcepts by the researcher and the results will be highlighted accordingly. The annotated articles can be visualized in PubTator [29] . Finally, researchers can create collections of articles using article ID or a search query. This is useful when working on several collections with different characteristics simultaneously.\\nCovid Scholar (Table 1 , No. 4) also provides keyword-based searching of articles. However, the search can be more precise by using phrases inside quotes and some additional symbols or operators, e.g., +(\"SARS-COV-2\" \"coronavirus 2\" \"novel coronavirus\"). For researchers having previous experience of using similar search operators (like those in information technology), this feature can bring flexibility in search but for many researchers, this feature can be prohibiting. One more advanced, and similarly complex, feature is the use of Word Embeddings [31] , which is a cluster of words with associated frequencies and that can be visualized. This feature is of interest mainly for exploring connectivity between concepts related to diseases, symptoms, diagnosis, etc.',\n       'A weight was also constructed for analyzing the cases with transcript data. Of the 18,640 students who were eligible for BPS:04/06, 110 were deceased, 16,960 had some transcript data and met the definition of a \"transcript respondent,\" and the remaining 1,580 were considered nonrespondents for this weight. As with the weights described in sections 6.1.1 and 6.1.2, the initial weight was the BPS:04/06 analysis weight. An adjustment was made for nonresponse using a model-based constrained logistic weighting procedure, then the weights were calibrated to the sums of the BPS:04/09 study weights for eligible cases. The procedure WTADJUST in SUDAAN was used to implement the nonresponse and calibration adjustments. The first adjustment was for nonresponse, that is, not having transcript data. The adjustment model included the 18,540 cases who were not deceased, with the response indicator set to 1 for the 16,960 cases with transcript data and set to 0 for the 1,580 cases who were nondeceased transcript nonrespondents. Predictor variables were chosen if considered to be predictive of response status and were nonmissing for both study respondents and nonrespondents. Variables used in the nonresponse adjustment models for NPSAS and BPS were also included. Candidate predictor variables included the same set of variables that was used for the study weight (see section 6.1.1). Variables included in the nonresponse modeling included all of the candidate predictor variables as well as certain important interaction terms identified using. CHAID was run for up to three segments, resulting in the identification of two-way and three-way interactions. Seven variables that made up the CHAID interaction terms for the student transcript weight adjustment were: the state in which the base year institution is located, type of institution, whether the student was a BPS:04/06 interview respondent, retention and attainment through 2006, base year institution undergraduate enrollment, race/ethnicity, and type of aid package. Table 41 shows the predictor variables used in the model to adjust the weight and the average weight adjustment factors resulting from these variables. The nonresponse weight adjustment factors have the following characteristics: • Minimum: 1.00; • Median: 1.01; and • Maximum: 3.45    Chi-square automatic interaction detection (CHAID) segments Base year institution located in AR, AZ, NY, TX and is public 2year, public 4-year non-doctorate-granting, or public 4-year doctorate-granting 1,840 98.13 1.02 Base year institution located in WA, LA, NE, OR, UT and is public 2-year, public 4-year non-doctorate-granting, or public 4-year doctorate-granting 970 94.96 1.04 Base year institution located in DE or PA and is public 2-year, public 4-year non-doctorate-granting, or public 4-year doctorate-granting 420 99.76 1.00 Base year institution located in AR, AZ, PA, NY, WA, DE, LA, NE, OR, TX, or UT Institution type is public less-than-2-year, private nonprofit less than 4-year, private nonprofit non-doctorate-granting, or private for-profit 2 years or more   To ensure population coverage and consistency with the BPS04/09 study weight, and the BPS:04/06 and NPSAS:04 weights, the BPS:04/09 panel transcript was adjusted to control totals determined by the BPS:04/09 study weight sums. Cases which were deceased were not included in either the control totals or in the cases included in the adjustment. This adjustment was also implemented using the SUDAAN WTADJUST procedure. Variables used to define the control totals were the same as those used for the poststratification coverage adjustments for the BPS:04/09 study weight. The control totals for the BPS:04/09 transcript weights were established by the weighted sums from the BPS:04/09 study weights. Table 42 gives the variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable. Statistics for the weight adjustment factors are the following: • Minimum: 0.67, • Median: 1.00, and The response adjusted, calibrated transcript weight is the variable WTC000 on the data file. Table 43 summarizes the weight distributions and the variance inflation due to unequal weighting by type of institution. The median student study weight ranges from 63 for students whose base year institution was private nonprofit less than 4-year to 219 for students whose base year institution was public 4-year doctorate-granting. The mean student study weight ranges from 87 for students whose base year institution was private nonprofit less than 4-year to 277 for students whose base year institution was public 2-year. The unequal weighting effect overall is 2.01, and ranges from 1.33 for students whose base year institution was public 4-year doctorate-granting to 2.77 for students whose base year institution was public less-than-2-year. To assess the overall predictive ability of the nonresponse model, an ROC curve was again used to provide a measure of how well the model correctly classified individuals of known response type. The plot of the first probability against the second (that is, the proportion of respondents with a predicted probability of response greater than c versus the proportion of nonrespondents with a predicted probability of response greater than c), for c ranging from 0 to 1, resulted in the ROC curve shown in figure 24. The area under the ROC curve is 0.66, such that 66 percent of the time (or almost 7 of 10 pairings), the predicted probabilities give the correct classification. The ROC area of 0.66 equals the value of the Wilcoxon test statistic; the Wilcoxon test fails to support the null hypothesis of no predictive ability (p < 0.05). This level of discrimination implies that the variables used in the model are highly informative but not definite predictors of a sample student\\'s overall response propensity. ',\n       \"We used the same depth and temperature proxies to define habitat extent at the LGM. Again, we assume that a water depth of less than 127.5 m and a range of annual mean SST of 5.4uC to 11.7uC define the grey seal habitat, but also used the 2.7uC to 12.6uC temperature range as extreme values. We then calculated the habitat areas using the GLAMAP SST climatology (Fig. 1 and 8). Assuming a sea level drop of 123 m between today and the LGM, the habitat is reduced to 4.74?10 4 km 2 for the smaller temperature range and 6.87?10 4 km 2 for the extreme SST range. These values correspond to a habitat loss of nearly 97%. Figure 8 shows the available habitat to the grey seals during the LGM. The previously large shelf areas were not available and the habitat was restricted to a narrow belt along the coastline. In the western North Atlantic the grey seal range was limited to the continental shelf edge between 40uN and 45uN and around Flemish Cap, which was an island during the LGM. On the eastern side, the coast of the Bay of Biscay and the north-eastern coast of the Iberian Peninsula would have been suitable. In contrast to today, the simple model also highlights the coasts of the western Mediterranean as suitable grey seal habitat during the LGM (Fig. 8).\\nThe major loss of suitable foraging habitat as defined by our study must have had a major impact on the grey seal population size. At present there are around 300,000 grey seals over 1 year old in the entire North Atlantic [72]. There are indications that over much of their range grey seal populations are approaching carrying capacity with either stable populations or gradually declining rates of increase [72,73]. However, we do know that the carrying capacity has not been reached in the Baltic Sea, the southern North Sea or the Northwest Atlantic and, in any case, it would be dangerous to assume that current carrying capacities are indicative of conditions before human perturbation of marine ecosystems. Hence, it is not possible with any confidence to estimate the natural maximum world population size for grey seals. However, if for illustration we speculate that the total number of grey seals (age 1 and older) could reach 500,000-700,000 we can estimate the LGM population assuming an even distribution of these seals across the possible habitat. We would then estimate the LGM population to have been around 15,000-21,000 seals. This would represent a very small global population for a phocid seal species, especially since it would have been split into two separate populations on either side of the North Atlantic. For comparison, this would represent a population smaller than the current estimates for any phocid species other than the critically endangered monk seals [74] and would have qualified as an endangered species under IUCN criteria [2,75]. A larger population may have been possible if there was a major change in grey seal behaviour. The absence of shallow shelf water would have required a shift to a more pelagic or bathy-pelagic feeding strategy. Grey seals today appear to prefer shallow shelf (,200 m) areas crossing deep troughs or channels to other shallow areas only infrequently (Figs. 2 and 4). Only a few dives (,0.8%) deeper than 200 m were recorded within the western population, when most of the shallow shelf is covered by winter sea ice. In today's habitat, only hooded seals (Cystophora cristata), another large abundant pinniped that winters in these areas, use such shelf-slope habitat with water depths between 150 m and 500 m. In the western North Atlantic grey and hooded seals occur at the same latitude (seasonally) and do overlap in a few areas, in which only one depth range is available. However, within the Gulf of St. Lawrence satellite telemetry data from this region shows a remarkable distinction between grey and hooded seal distributions, with an apparent border coincident with the 200 m contour and little movement by either species across this boundary (Fig. 9). There is no reason to suspect that the habitat preferences and or relative abilities to exploit shallow and deep shelf waters of grey and hooded seals would have been different at the LGM. However, if we make the extreme assumption that grey seals could have out-competed hooded seals during the LGM and we relax the water depth constraint to 500 m for the predicted paleoclimatic grey seal habitat (Fig. 8), the total calculated area increases to approximately 2.11?10 5 km 2 , which is three times more than the extended area. This could then have potentially supported up to 63,000 seals in ideal conditions. However, this scenario seems unlikely given current seal distribution patterns. Further extension of the suitable habitat to include deep ocean waters to forage does not seem reasonable as there is no evidence that grey seals feed over very deep water, regularly travel across ocean basins or go on extended foraging trips covering great distances. So, it seems likely that grey seal population fell to very low levels during the LGM and it would have remained low for several thousand years before expanding into current habitats over the last 12,000 years or so. We have shown that we can describe current grey seal habitat using two simple proxies. This produces an accurate description of the effective range of grey seal populations based on observations and telemetry studies. Using these two proxies to define the extent of grey seal habitat during the LGM, indicates that it was only about 3% in size compared to today. We therefore conclude that the grey seal population during the LGM must have been very low for a considerable period of time. In the future, Arctic permanent sea ice levels are predicted to get smaller and SST is predicted to increase in the Arctic [11]. For grey seals, this is likely to result in new available habitat and opportunities to extend their range onto the extensive Arctic shelf. However, grey seals will have to compete with other species either occupying this habitat currently or extending their range as well. It is also important to note that the apparent relationship between foraging habitat and some measures of water temperature is not likely to indicate direct physiological limits on grey seals. It is more likely that these relationships result from responses of their major prey items. Significant shifts in prey distributions in response to changing temperatures have already occurred in parts of the grey seal range [76]. Human impact and exploitation also need to be considered. At present we do not have sufficient information to allow us to predict the effects of these and future changes in prey distributions on grey seal foraging success.\",\n       \"The ATT and Rosenbaum bounds for spillover program effectiveness are displayed in Table 4. Participation in a spillover program increases high school graduation and college matriculation rates, specifically at four-year, public, and private institutions. The negative relationship reflected in the two-year ATT values may be the result of more students attending four-year institutions, but this analysis did not aim to confirm those results. Additionally, program participation had an overall negative impact on deviant behaviors, especially in the form of idleness and tardiness. Each of these outcomes were statistically significant in at least one of the algorithms and demonstrated a significant stability against unobservable bias, with only one of the models being below the desired 1.5 threshold (public college attendance at 1.4). Spillover programs are performing as hypothesized by enhancing the social context of the educational environment. Referring back to Table 1, the populations these programs are assisting are in lower SES, higher racial/ethnic minorities with limited English proficiency. Schools offering spillover programs do have a higher percentage of certified teachers, more of whom are able to teach in their specified fields. Because the propensity models were able to find schools with comparable quality teachers, it is safe to say that the positive outcomes are a result of program participation and not having a higher caliber teacher in each classroom. The propensity models also account for the likelihood that these schools may be able to offer intervention programs because they are more adequately staffed. However, based on the descriptive statistics, while these programs are being implemented in schools with populations characteristic of requiring interventions, the enhanced teaching staff needs to be explored further. Referring now to the first column in Table 6, several matching variables significantly determine the schools propensity to offer spillover programs. As seen with the descriptive statistics, the relationship between the propensities to offer an intervention program is negatively associated with school SES, limited English proficiency, and the number of out-of-field teachers. The percentage of racial/ethnic minorities, student-teacher ratios, levels of perceived crime, and the number of certified teachers have a significant positive relationship with respect to the propensity to offer spillover programs. While the sample size was smaller for school-wide intervention programs, the results are similar. Table 5 displays the ATT and Rosenbaum bounds for school-wide adolescent intervention programs. As seen with spillover programs, school-wide programs increase high school graduation and college matriculation rates, specifically at four-year, public, and private institutions, though private institution attendance can be easily confounded by unobserved variables. Additionally, school-wide programs experienced the benefits of decreasing deviant behaviors, specifically those of idleness and tardiness. The propensity models are more resistant to outside influence with respect to deviant outcomes than were the spillover models. Again, as hypothesized, in the social context, school-wide programs enhance educational outcomes while decreasing deviant behavior at the school level. When referring to Table 2, the school-wide program participation descriptive statistics, schools offering intervention services are lower SES, have a higher percentage of racial/ethnic minorities, and perceptibly higher levels of crime, though only marginally so for each. On the other hand, these schools also have lower levels of limited English proficient students, substantially more students enrolled in college preparatory curriculums 13 Referring to the second column in Table 6, the significance of the matching characteristics in determining propensity to offer a school-wide program is listed. As seen with spillover programs, SES is inversely associated with the propensity to offer a program, though to a much lesser extent. Urbanicity is a significant contributor, though it's nominal coding makes the direction difficult to interpret. Offering a college preparatory curriculum with certified teachers teaching within their subject matter expertise increases the propensity to offer a program. These results will be summarized and further clarified through a discussion of the significance of this research. , and more certified teachers practicing in their field. The availability of a higher quality staff provides students with access to information necessary to attend postsecondary education.\",\n       'Every research study is limited in some way. Fraenkel & Wallen (2003) define a limitation as an \"aspect of a study that the researcher knows may influence the results or generalizability of the results, but over which he or she has no control\" (p. G-4). This study included the limitation that the IT capacity and IT capabilities of each college and university were treated equally in terms of the quantity or speed of the IT infrastructure of the institution. However, in reality $1 spent on a given type of IT infrastructure/support does not equate to $1 spent on another. Not all pieces of the IT infrastructure that comprise the IT capacity and IT capabilities of the institution have similar value or costs or even uses associated with the other pieces of the IT infrastructure. The costs to purchase each piece of the infrastructure will differ for each institution based on the quantity purchased and the time of the purchase since the costs of a similar piece of IT infrastructure decreases significantly over time and purchases of greater quantity typically cost less than smaller purchases. In short, the acquisition costs and money spent to acquire and maintain the IT infrastructure would differ at each institution. However, since IT expenditure data was not available, this required the IT infrastructure of each college and university to be treated equally.',\n       'The nonresponse bias analysis of the household sample revealed differences in the characteristics of respondents who participated in the background questionnaire compared with those who refused. In a bivariate unit-level analysis at the background questionnaire stage, estimated percentages for respondents were compared with those for the total eligible sample to identify any potential bias owing to nonresponse. Multivariate analyses were conducted to further explore the potential for nonresponse bias by identifying the domains with the most differential response rates. The three samples (Main Study, National Supplement area sample, and National Supplement list sample) were analyzed separately to inform the separate nonresponse weighting adjustments for each sample. For the Main Study, these analyses revealed that the subgroup with the lowest response rates for the background questionnaire had the following characteristics: (1) Hispanic, (2) age 26 and older with no children in the household, and (3) reside outside the Northeastern United States in areas with low levels of linguistic isolation (a low percentage who have some difficulty speaking English) and with unemployment rates exceeding approximately 5 percent. For the National Supplement area sample, the lowest response rates to the background questionnaire were for persons with the following characteristics: (1) age 25 to 34 or older than 55, (2) sampled as not unemployed (age 16 to 34) or older (age 66 to 74), (3) no children in the household, (4) reside in the Northeastern United States in a census tract with an employment rate exceeding approximately For the National Supplement list sample, the subgroup with the lowest background questionnaire response rates corresponded to the following: (1) female with no children in the household, (2) reside in a Metropolitan Statistical Area in the Western or Northeastern United States, and (3) reside in a census tract in which less than approximately 29 percent of the population has a high school education. In general, persons with children in the household were found to be more likely to participate, as were persons in areas with a high percentage of the population below 150 percent of poverty. However, the variables found to be significant in the multivariate analysis-those used to define areas with low response rates-were used in weighting adjustments in an effort to reduce bias. See chapter 7 in the U.S. PIAAC Main Study and National Supplement Technical Report for the complete nonresponse bias analysis.',\n       \"Before data can be obtained from sampled participants, evaluators need to decide what data are needed: what are the key independent, dependent, and control variables that need to be measured to conduct the outcome evaluation. Typically, the independent variable or exposure will be the intervention of interest (eg, a policy or ordinance change, tax, subsidy, label, marketing change). Although the ultimate goal of many natural experiments may be to change health outcomes, it is important to match the outcome of interest to the evolution of the intervention itself. 47 As noted above, like any intervention, natural experimental evaluations require a conceptual model that details expected changes, pathways to change, and potential factors that may affect outcomes. In this way, researchers can identify the most salient outcomes, mediators, process measures, and control variables to measure. Depending on the time available and the stage of the intervention, researchers may decide to focus on food environment outcomes, dietary behaviors, and/or health outcomes.\\nFood environment outcomes. In some cases, it may be more appropriate to look at food environment outcomes first before examining dietary measures or health outcomes. This may be the case if the natural experiment is unproven or in a pilot phase, the main focus of the evaluation is on understanding implementation, or the natural experiment is a policy designed to cause food environment change (eg, industry initiatives to remove calories from the food system). Prioritizing measuring environmental change may also make sense if there is not a strong indication that the natural experiment will change diet in the given timeframe.\\nIn selecting food environment measures, it is important to consider that there are multiple dimensions to food access, including availability, affordability, accessibility, and acceptability. 48 Although ideally researchers would choose validated measures, most published measures of the food environment do not contain information on reliability or validity. 49, 50 Those that do exist may need to be modified for specific contexts. If resources are available, researchers should consider conducting a validation substudy before or during the data collection process; ideally, validation occurs before the outcome evaluation begins so that a validated measure can be obtained prior to the beginning of data collection. The following case study highlights the importance of selecting culturally appropriate measures and collecting data on multiple food environment indicators as a change in factor can have detrimental effects on others.\\nCase study: evaluating the impact of the 2009 federal revisions to the Special Supplemental Nutrition Program for Women, Infants, and Children on the retailer environment. Efforts to evaluate the health impact of recent, deliberate changes to federal food assistance programs such as the Special Supplemental Nutrition Assistance Program for Women, Infants, and Children (WIC) highlight challenges with selecting and measuring appropriate outcomes in the retail food store environment. In 2009, federal revisions to WIC increased cash vouchers for fruit and vegetable purchases and updated cost containment, administrative, and WIC food packages. [51] [52] [53] The new policy, set forth by the US Department of Agriculture, sought to influence both the broader retail food environment and individual behavior. Consequently, researchers attempting to evaluate the impact of the new policy had to grapple with whether to focus on measures of the retail food environment (eg, availability, accessibility, and affordability of foods) or measures related to individual-level dietary consumption (eg, purchase and consumption of various foods). Lu et al. 54 recently undertook an evaluation of the WIC program in Texas and opted to examine the broader retail food environment. The researchers also considered geographic differences (ie, urban vs rural) in their evaluation due to urban-rural disparities in the resources of food stores. 55 Moreover, although examining the local retail food environment (eg, in a specific city) is important, WIC policy is driven by state guidelines, 55 so the state, rather than the city, was used as the geographic unit of analysis for the evaluation. One of the first challenges was finding a validated instrument that would capture foods that were culturally relevant for the area and reflected dietary patterns. In this instance, the authors first adapted and field-tested one of the popular validated store survey tools, the Nutrition Environment Measures Survey, for use in the study area. Conceptualizing and measuring the availability, accessibility, and affordability of foods also presented a challenge for the researchers. Availability measurements included the visibility and amount of shelf space allocated to each item, the variety of produce, the stocking and quality of products, and the availability of culturally specific (Hispanic) foods. Accessibility was defined as the visibility and labeling of WIC foods based on marketing principles, such as whether specific foods were at eye-level, and affordability was defined as the price of the least expensive brand item for a particular product. Lu et al. 54 found improved accessibility and availability of food items following the WIC policy update but did not find an improvement in affordability. It is possible that the addition of vouchers may have increased purchasing power among WIC participants but did not make food more affordable to the broader community, which presumably would include many other lowincome families who were not eligible for WIC eligible. This underscores the importance of examining multiple outcomes in the evaluation of food and beverage policies to accurately assess the full impact on the retail food store environment.\\nDietary behaviors. Public health evaluators will typically be most interested in whether a healthy food retail intervention changes dietary behaviors, but dietary behavior can itself be complex to measure and define, and which dietary behaviors are of interest will vary with the specifics of the intervention being evaluated. For example, a subsidy program for fruits and vegetables will likely be most interested in measuring fruit and vegetable consumption or nutrients related to these items (eg, fiber), whereas a menu-labeling policy might wish to examine caloric intake at restaurants subject to the policy change. An additional challenge is that even if an intervention is expected to change dietary behaviors in the short term, some have argued that a better outcome measure is whether healthy habits are sustained, rather than just initiated. 56 Although the focus in this article is more on the types of outcomes researchers should consider measuring rather than the specific measurement tools, publications exist that debate the merits of different dietary intake tools. 57 Because collecting individual dietary intake data can be challenging, many food retail studies have chosen to focus instead on looking at changes in store purchases as a proxy for dietary change, relying on the assumption that if people are purchasing healthy food, they are likely to be eating healthier food. A benefit to this approach is that purchase data, when objectively collected, are less subject to desirability bias than self-reported dietary data. Pilot studies or evaluations with limited time to collect data prior to the start of a natural experiment may also favor purchase data. Challenges to using purchase data include limited or incomplete data from smaller markets, such as farmers' markets, corner stores, and mobile markets, because these venues often lack sophisticated point-of-sale systems to track customer purchases. Retail inventory records may be an alternative (ie, looking at trends in wholesale purchases of target items by the participating retailers) in this situation but still may not give a full picture of the effect of the program on diet. Using store-level data on food purchases may indicate that healthy food purchases are increasing but could also represent a change in the customer base at those retailers. To reduce the likelihood of this alternative explanation, researchers can track individuals' purchases over time by collecting individual-level purchase data in the form of customer receipts, personally identifiable purchase data from loyalty cards, or sales records that record customer identity (such as at some mobile markets).\\nIn addition, a final challenge is that researchers may be interested in understanding how the policy impacts total dietary intake because individuals consume many foods and beverages -not just the food(s) or beverage(s) targeted -as well as consume foods from a variety of sources (food retail outlets as well as awayfrom-home sources like restaurants, fast food outlets, or schools). Individuals may respond to policy change by substituting one food or beverage for another or shifting their allocation of in-store and away-from-home food purchases, but sales and food purchase data only capture in-store purchase and, if product categories are restricted, may not capture the full range of substitutions individuals might make. Supplementing purchase or sales data with dietary intake measures such as 24-hour recalls or food-frequency questionnaires may provide a better assessment of the total dietary change. With sales and purchase data, it is preferable to collect data on all products rather than only those targeted by the policy, and it is also useful to collect data from a variety of retail types (supermarket, convenience store, locally owned shop, or tienda) in order to understand potential shifts across retail outlets.\\nHealth outcomes. Individual and population nutritionrelated health outcomes (eg, obesity, diabetes, cardiovascular disease, and certain types of cancer) are also often of interest; however, literature looking at the effects of food environment interventions on health outcomes is generally limited to the collection of body mass index (BMI). Other potential health targets (eg, diabetes, blood pressure) generally require longer-term exposure and thus are not easily collected during the typical evaluation that lasts a year or less. Given the expensive and often invasive procedures required to measure these health outcomes, they would be most appropriate for longer-term evaluations of established interventions, which have already shown changes in purchasing and/or diet. In many cases, longer-term outcomes are studied using other methodologies, such as simulation modeling (see Basu et al. 58 and Wang et al.\",\n       'A critical assumption in our assessment of the optimum location for paleo-IOD reconstructions is that the spatial patterns of IOD-forced SST anomalies remain stationary over time. It is likely that this is a reasonable assumption, particularly for the eastern Indian Ocean sector as the location of coastal (Ekman) upwelling here is constrained by the geography of Java and Sumatra. Nevertheless, we examined this further using the preindustrial climate simulations to calculate the distribution of DMI versus optimum-site SST correlations at different window lengths (Figure 8). We find that all of the models examined here display correlation distributions that show a significant inverse relationship between the model DMI and SST at the South Pagai grid cell. Across models, the correlations are strongest and most highly stationary in models with more frequent pIOD events (e.g., ACCESS and CSIRO Mk3.6.0; Table 4). The models with less frequent pIOD events, and where the DMI accounts for a lower proportion of the SST variability in the South Pagai grid cell (e.g., CSIRO Mk3L and MPI ESM-LR; Table 4), display a larger spread in the range of negative DMI versus SST correlations. However, the full distribution of correlations remains highly significant even in these models. Assessment within each model of the impact of reconstruction window length shows that even at short (20y) window lengths the distribution of correlation coefficients exhibits very little additional spread compared to the same analysis performed using much longer (100y) windows (shading versus solid lines in Figure 8). This demonstrates that the climate models assessed here do not indicate any multidecadal scale fluctuations in the impact of the DMI on SST variability at the optimum-site grid cell that could affect IOD interpretations based on short paleoclimate reconstructions. ',\n       'NO. 32 over the past century (Table 2). In addition to mean water-level changes, storm frequencies have been predicted to increase by 5-10% along the Atlantic, with possible increases in intensity of storms as well (Giorgi et al. 2001). In addition to strong directional winds and normal lunar-seasonal cycles (Fig. 1), a longterm metonic or nodal cycle of 18.6 yr may also play a role in coastal fl ooding of marshes (Kaye and Stuckey 1973). Stumpf and Haines (1998) report that, with metonic, or lunar-nodal, cycles coupled with an annual tidal signal, some Gulf Coast wetlands may experience discrepancies of up to 5 cm from predicted tidal charts. In the mid-Atlantic region, this suggests that increases in water levels, and hence marsh fl ooding, probably occurred more often in the mid-1970s and again the early 1990s (Panel D, Fig. 1). Unfortunately, we were unable to locate any long-term data on breeding success by any marsh-nesting birds to confi rm this prediction. However, Eyler et al. (1999) did fi nd high nest fl ooding losses of marsh-nesting Gull-billed Terns during the 1994-1996 breeding seasons in Virginia. For long-lived marsh specialists such as Laughing Gulls that might breed for up to a decade, an intriguing question arises: does evidence exist for any long periodicities in reproductive effort that might be associated with these tidal cues?',\n       'The resistivity results from the seasonal surveys and the corresponding conductivity data collected from the wells ( Figure 5) were used to obtain a formation factor for the island. The electrical conductivity measurements were converted to pore water resistivity in Ω-m using ρ=1/σ. Resistivity after inversion at the corresponding depth and location of the wells were used for the bulk rock resistivity. A scatterplot was produced of the bulk rock resistivity vs. the well water resistivity to see if they followed a linear trend. These scatterplots were then separated by island and by transect. An orthogonal regression line through the origin was fit to the scatterplots with their uncertainties. Since the well conductivity measurements and the resistivity measurements are both variable, orthogonal regression was the best choice for the slope and uncertainty. Unlike linear least squares regression lines which compute the least square distance of the vertical offset, orthogonal regression computes the smallest distances of the perpendicular offset. Since both variables contain errors, the slope of these regression lines were calculated as m=Σρ O /Σρ W . The slope is the formation factor for that given data set. To determine the uncertainty, the residual of the data points had to be determined with respect to the perpendicular offset. The equation to determine the residual was as follows: Then the standard deviation of the residuals determined the uncertainty. Salinity profiles were created utilizing the bulk formation factor. First the bulk resistivity was converted to the pore water resistivity given the determined formation factor. The pore water resistivity results were then converted to conductivity and corrected to 25°C using the temperature in well B11 at the time of the measurement with the equation where C 25 is the temperature corrected conductivity in mS/cm and T is the temperature in Celsius (Radtke et al. 2005). The conductivity was converted to salinity with the equation S = 0.0120 + (−0.217 * R 1/2 ) + (25.33* R) + (13.77 * R 3/2 ) + (−6.479 * R 2 ) + (2.584 * R 5/2 ) (Wagner et al. 2006) where R is the ratio of the C 25 to the conductivity of standard seawater (35 ppt) at 25°C.',\n       nan,\n       \"Father's reports were used to compute father involvement in instrumental activities (e.g., feeding, diapering) at 9-months (11 items, α = .86), 2-years (10 items, α = .83), and preschool (5 items, α = .83) and enrichment / play activities (e.g., read books, play peekaboo, play chase) at 9-months (8 items, α = .62), 2-years (12 items, α = .78) and preschool (5 items, α = .67). Direct child assessments using the Bayley Short Form-Research Edition (BSF-R) were used to assess children's mental ability and motor ability at 9-months and 2-Father-child Interaction Page 7 years. Preschool teachers provided measures of children's peer competence (e.g, accepted by other children, makes friends easily; 6 items, α = .81) and pre-academic kindergarten readiness skills (e.g., knows colors, alphabet; 8 items, α = .84).\",\n       \"The most comprehensive source of information about phased retirement plans in higher education is the Survey of Changes in Faculty Retirement Policies (SCFRP), conducted in 2000 by the American Association of University Professors with financial support from the TIAA-CREF Institute. The survey examined US institutions of higher learning in all five Carnegie categories with 75 or more full-time faculty members. At the time of the survey, 27 per cent of the responding institutions had a phased retirement program in place. 2 These programs had the following characteristics: • Faculty in most programs (64 per cent) must obtain administrative approval to participate. • Most programs require faculty to reach minimum levels of age (75 per cent) and years of service (73 per cent) to be eligible for phased retirement. The modal age requirement is 55, but some programs are open to 50-year-olds whereas many others require participants to be at least 60. Programs typically require 10 or more years of service to be eligible. • A minority of programs (21 per cent) put a ceiling on age of eligibility, in most cases 62 or higher. • Roughly two-thirds of the programs provide special financial benefits. In most cases the special benefit is full contribution to health insurance, although some institutions provide extra salary payments or extra retirement payments or credits. • Most plans (60 per cent) do not require professors to relinquish tenure before they enter phased retirement. Faculty members generally must lose their tenured status after three or five years in phased retirement. One way potentially to better understand the motivation behind phased retirement plans is to compare the characteristics of universities which offer such plans to those which do not. It is reasonable to expect that the value of phased retirement to the institution should be a function of its mission. Doctoral institutions must be especially sensitive to the orderly replacement of aging faculty to be competitive for graduate students and research contracts. Presuming that phased retirement polices provide improved capacity to make long-run plans for staffing, one would expect doctoral universities to be more likely to adopt such policies. In contrast, universities and colleges that focus on teaching, especially baccalaureate and two-year institutions, have more flexibility in how they replace aging faculty members and thus may not value phased retirement as much. These patterns are borne out in the SCFRP data, as shown in Table 9.1. Phased retirement programs are most commonly observed among doctoral universities (35 per cent). The percentage of masters and baccalaureate institutions with phased retirement plans is slightly lower (29 per cent), whereas the percentage of two-year institutions is considerably lower (16 per cent). The age and tenure structure of the faculty also should have some bearing on the value of phased retirement to the institution. Colleges and universities with a high percentage of faculty who are near retirement have much more to gain from successfully managing the transition to retirement than those with relatively fewer faculty in their 50s and 60s. There is no systematic relationship between the age structure of the faculty and the adoption of phased retirement policies across the campuses in the SCFRP sample (Table 9.1). Phased retirement policies are most prevalent among schools with 30 to 39 per cent of faculty in the 55-and-over age bracket, and least prevalent among schools with fewer than 30 per cent of faculty in this age bracket, as expected. However, phased retirement policies are least common among schools with 40 per cent or more of faculty in this age bracket, which runs contrary to expectations. Holding age structure constant, tenure has an obviously large impact on the degrees of freedom the institution has available to manage an aging workforce. Institutions with a high percentage of tenured faculty have much less flexibility than those with relatively low percentages of tenured faculty. To the extent that tenured faculties are also highly compensated faculties, phased retirement plans also generate greater opportunities for cost savings. Empirically, there appears to be a strong relationship between the percentage of full-time faculty with tenure and the adoption of phased retirement policies. Table 9.1 shows that the percentage of schools with phased retirement steadily increases with the percentage of full-time faculty with tenure. Phased retirement policies are in place at only 16.5 per cent of the schools where less than 40 per cent of the faculty have tenure. In contrast, phased retirement is available at 25.4 per cent of the schools where the tenure ratio is 40 to 49 per cent , at 30.2 per cent of the schools where the tenure ratio is 50 to 69 per cent , and 33.7 per cent of the schools where the tenure ratio is 70 per cent or higher. The fit between a phased retirement plan and the pension plan must be assessed carefully. Defined benefit plans are a poor fit with phased retirement for two reasons. First, the pension payment under defined benefit plans is a function of final average salary, which means that a move to a half-time job on campus is also a move to as much as a 50 per cent cut in one's pension (unless the retiree can start receiving the annuity upon entering phased retirement or a special exemption for phased retirees can be created in the benefit formula). Second, the formulas for most defined benefit plans are set up in such a way that the present value of the pension annuity is maximized at the time the individual becomes eligible for full retirement benefits. The pension-based incentives facing these individuals for retirement are extremely powerful and are likely to reduce the value of retirement-management policies to the university. Phased retirement plans are likely to be a better fit on campuses with defined contribution plans because they offer managers a mechanism to productively influence retirement decisions. The data in Table 9.1 show a strong relationship between type of pension plan and the adoption of phased retirement plans. Phased retirement is available at 38 per cent of schools which exclusively have a defined contribution plan, versus 19 per cent of schools which exclusively have a defined benefit plan. Private colleges and universities are likely to have more degrees of freedom to adopt phased retirement plans than their public counterparts. Public colleges and universities tend to be less autonomous, facing some degree of oversight from state government. In some states, public institutions are part of a statewide system, which would have mixed effects on the odds of adopting a phased retirement plan. On the one hand, there would be greater transactions costs associated with adopting a new policy in a statewide system than a single campus. On the other hand, a successful launch of a statewide plan would lead to earlier adoption on campuses that, left on their own, would not have adopted a plan. Further, private schools may have such an advantage in dealing one-on-one with individual faculty about their compensation and workloads that they may have little need for a phased retirement policy. In the SCFRP data in Table 9.1, there is very little difference in the odds of having a phased retirement plan between public and private institutions. 3 The adoption of phased retirement policies is also likely to reflect the management style of the organization. Institutions that have decided to take active steps to manage the age structure of their faculty are likely to consider a variety of steps, rather than focus on a single policy. If true, one would expect campuses that have phased retirement policies to also have taken other steps, including seminars on retirement planning, financial incentives for early retirement, and targeted buyouts. These patterns are born out in the SCFRP data, as shown in Table 9.1. Phased retirement plans are available in 29.2 per cent of the schools that offer retirement planning seminars or programs, in contrast to 11.6 per cent of schools that do not offer such programs. Phased retirement policies are in place in 31.9 per cent of the schools that offered financial incentives for retirement since 1995, versus 22.7 per cent of the schools that did not offer such incentives. Phased retirement is available at 32 per cent of the schools that offered buyouts to faculty since 1995 (either on a college-by-college or case-by-case basis), in contrast to 24 per cent of the schools that did not offer buyouts. The simple comparisons made in Table 9.1 could be misleading. For instance, it is well known that the tenure ratio is much lower at baccalaureate and two-year schools than at doctoral and master's level institutions. Probit analysis is a tool that can be used to control for additional variables. It is very much like multiple regression analysis, except that it is designed for situations where the dependent variable (odds of having a phased retirement program) is measured in binary categories (either you have a phased retirement plan or you do not). Table 9.2 reports the results of a probit analysis of the odds that a campus will have a phased retirement plan, using the variables from Table 9.1 along with number of full-time faculty. The number of full-time faculty is included in the analysis to determine whether there are economies of scale involved with developing and implementing phased retirement plans. If the costs of developing a phased retirement plan increase less rapidly with size than the benefits, one would expect that small schools would be less willing than large schools to adopt the plans. In the probit analysis, three variables stand out as significant predictors of the odds that a campus will have a phased retirement plan. Institutions that exclusively offer a defined contribution plan have a 25 to 27 percentage point greater probability of offering phased retirement than schools that offer defined benefit plans. This reflects both the poor fit between defined benefit plans and phased retirement policies, as well as the possible use of phased retirement for strategic human resource management on campuses that only offer defined contribution plans. 4 Schools with a large percentage of tenured faculty are much more likely to have phased retirement plans than schools with a relatively small percentage of tenured faculty. At the extreme, a school where all faculty have tenure would have a 38 to 40 per cent greater chance of having a phased retirement plan than a school where none have tenure. Percentage of faculty with tenure is strongly correlated with Carnegie categorization, so the analysis was repeated by estimating separate probit models for each Carnegie class (these results are not reported in Table 9.2; they are available from the author upon request). The decrease in sample size makes these results more fragile, but percentage tenured was still statistically significant at or near the p ϭ 0.10 threshold in all levels of institutions except two-year institutions without faculty ranks. Note that whereas there is a significant relationship between the odds of offering phased retirement and percentage of faculty with tenure, there is no relationship between the odds of offering phased retirement and Carnegie class once one controls for percentage of faculty with tenure. 5 Phased retirement plans are much more likely to be in place on campuses which followed a management strategy of actively managing faculty retirement. Schools that have offered retirement planning seminars are 12 per cent more likely to have phased retirement plans than schools that have not offered such seminars. Schools that have offered financial incentives for retirement before age 70 are 10 per cent more likely to have phased retirement plans than schools that have not offered such incentives. There is no significant relationship between buyouts by college or individual and the odds of having a phased retirement plan. Public institutions have a 10 percentage point greater probability of having a phased retirement plan than private institutions. This effect is statistically significant at the 6 to 10 per cent level, a bit below the standard threshold used in social science. There was no evidence in the probit analysis of economies of scale in the offering of phased retirement plans. There was also no relationship between the age structure of the faculty and the odds of having a phased retirement plan. This conclusion was robust across a number of other measures of age structure (e.g., percentage above 60, percentage below 50). To sum up, this discussion shows that phased retirement plans have not been cropping up on a random basis. There is clear evidence that campus leaders have carefully considered preexisting conditions such as type of retirement plan and the percentage of the faculty with tenure. The fact that campuses with a high percentage of faculty members with tenure are most likely to have phased retirement plans (as well as retirement planning seminars and offers of financial incentives for retirement) 6 implies that phased retirement is being viewed as a tool to give management more flexibility to manage a difficult-to-manage workforce.\",\n       \"This section of the field test report reviews the effort and burden associated with the B&B:2000/01 field test student interview. We examine the interview's length by considering the timing analysis statistics. This information is useful because it provides evidence that can reduce respondent burden, reduce data collection effort and cost, and improve data quality. Then we consider the effort required to locate and interview sample members for the study using the average interview time. During CATI instrument development, project staff embedded time stamps at the start and end of the interview, as well as the beginning and end of each interview screen, which could include up to eight related items. The time stamps measured the elapsed time to complete each segment of the interview, and enabled project staff to monitor the time required to complete specific interview items, the online coding programs, sections of the interview, and the entire interview. The time (in minutes) needed to conduct a student interview is shown by interview section in table 3.10. Sections are listed in the table in the order in which they were presented. Certain sections of the interview applied to selected groups of respondents (see figure 2.2,) so timing results are presented for the overall cohort, and by subgroup. For example, Section A was designed for base-year nonrespondents, so the number of cases in that group was less than for the rest of the instrument. Respondents who were currently teaching skipped the post-baccalaureate employment section and proceeded directly to the teaching section. Table 3.10 presents timing results for the B&B:2000/01 field test cohort. Overall average administration time to complete the student interview was 18 minutes. There was no difference in average completion time due to base-year response status (see table 3.11). Both respondents and nonrespondents to the NPSAS:2000 field test took an average of 18 minutes to complete the interview.5 For respondents who had taught since graduating (see table 3.12), the average interview time was 21 minutes compared to 17 minutes for those who had not taught. The Technical Review Panel reviewed the administration time and then recommended certain items for deletion in the full-scale study. Items to be excluded typically showed a lack of temporal stability or extremely low variance of responses (see chapter 5). Interview administration time, however, reflected only a small fraction of the time required to obtain a completed interview. Time was spent by locator/interviewers in locating, scheduling call-backs, attempting refusal conversion, and other related activities. This time was spent whether or not interviews were obtained. The average locator/interviewer time requirement for each completed interview was slightly more than 2 hours. 'All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, 1 in section B, 3 in section D, 1 in section F, and 2 in section G).  'All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, 1 in section B, 3 in section D, 1 in section F, and 2 in section G).  All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, I in section B, 3 in section D, 1 in section F, and 2 in section G \",\n       'In the first set of models, Aβ-converters were also more likely to be APOE-ε4 carriers, have more education, and longer duration of follow-up. Age was not significantly associated with progression to Aβ-positivity in either model. After accounting for covariates, individuals with poorer performance on either cognitive composite at baseline showed higher odds of progressing to Aβ-positivity at follow-up (ADNI_MEM: OR=1.66, P=0.013; PACC: OR=1.66, P=0.01). Full results of the regression models are presented in Figure 1 .\\nThe second set of models included a dichotomous classification for baseline CSF p-tau ( Figure 2) . As in the first set of models, Aβ-converters were more likely to be an APOE-ε4 carrier, have more education, and longer duration of follow-up. Age and dichotomous classification of p-tau status were not significantly associated with progression to Aβ-positivity in either model. After controlling for covariates, poorer baseline performance on either cognitive composite remained significantly associated with increased odds of progressing to Aβ-positivity at follow-up (ADNI_MEM: OR=1.64, P=0.016; PACC: OR=1.67, P=0.011).\\nThe third set of models addressed the question of whether subthreshold levels of AD pathology could account for the effect of lower cognitive performance on progression by including continuous CSF Aβ and p-tau measures (Figure 3) . More abnormal levels of baseline CSF Aβ and p-tau were associated with increased odds of progression to Aβ-positivity (CSF Aβ:\\nOR=2.53 -2.59, P<0.001; CSF p-tau: OR=1.51, P=0.03). In the case of CSF Aβ, we note that these values were all in the normal range according to standard cut-offs. After controlling for baseline biomarkers, the performance on the ADNI_MEM remained a significant predictor (OR=1.61, P=0.03), but the effect of the PACC was reduced to trend level (OR=1.49, P=0.071).\\nEducation and length of follow-up remained significant predictors of progression, whereas the effect of APOE-ε4 status was reduced to trend level.\\nTo determine whether these results may be driven by the MCI participants, we conducted follow-up analyses on CN and MCI groups separately. The large drop in sample size resulted in non-significant results for most analyses, but the effects of cognition predicting progression to Aβ-positivity tended to be larger for the CN group.',\n       'The high-throughput computational analysis was conducted. First, we estimated morphometric estimates using the Freesurfer image analysis pipeline (Fischl, 2012 ) (v6) from T1 and T2-FLAIR images. Morphometric measures (N=948 per subject) include volumes of the hippocampal subdivisions, and thickness, surface area, and volume of cortical/subcortical regions using two different atlases available in Freesurfer (Desikan-Killiany atlas and Destrieux atlas; https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation). The technical details of these procedures are described in previous studies (Desikan et al., 2006; Destrieux et al., 2010;  A C C E P T E D M A N U S C R I P T Fischl and Dale, 2000; Fischl et al., 1999) . In brief, the image processing includes motion correction, removal of non-brain tissue, Talairach transformation, segmentation, intensity normalization, tessellation of the gray matter-white matter boundary, topology correction, and surface deformation. Deformation procedures use both intensity and continuity information to produce representations of cortical thickness. The maps produced are not restricted to the voxel resolution and are thus capable of detecting submillimeter differences between groups.',\n       'Human brain is a highly interactive system, in which a connectivity may link multiple high-related brain regions rather than only two of them. Such connective patterns were found in both functional and structural brain networks (Ryali et al., 2012; Zheng et al., 2018) . In the present study, we took a further step to extract networks based on multiple imaging modalities to facilitate the diagnosis of MCI-to-AD conversion. The multimodal network is denoted as G = (V, E), with a node set V and an edge set E. Here, V is a set of brain regions and E is consisted by the MMC. The diagram illustrated in Figure 1 outlines the pipeline of the multi-modal network construction. Briefly, the MRI and PET images were first preprocessed and registered to a prior template. A principal component analysis (PCA) with a bagging strategy was then performed to each imaging modality in the auxiliary domain, consisted by AD and NC subjects, to extract the PCs of each brain region based on resampling the subjects with replacement. Regional data in the target domain, consisted by sMCI and pMCI subjects, were then projected to that feature space using the corresponding PCs of the brain region. The projected data were submitted to a multi-task sparse regression model to extract the connectivity between one target brain region and other regions. The technical details are provided below.',\n       \"First, longitudinal data analyses would be useful as the current study, based on cross-sectional data, does not establish causality between out-of-school time use and achievement. Second, the present study examined a variety of out-of-school activities in the U.S. and Korea, yet still there could be other types of activities, such as attending cram school or private tutoring in Korea. Future studies should collect data about various activities by asking students to list all of their activities beyond the given activities in the survey questionnaire. Third, parents and family members have responsibility for students' behavior during after-school hours. Future studies should include more family background variables such as number of siblings, two or single parents, level of parental involvement in out-of-school activities, and socioeconomic status that is measured by multiple items, such as income and parents' employment status or types of their profession or occupation.   Note. All activities were presented with hours spent per day.\",\n       'The approach outlined above presumes that we have specific information about how responses map to codes for major field of study. Fortunately, some information on this is available from previous surveys. We know what strings were collected on the previous surveys, and what major field of study was finally coded for each of those strings. Starting with this information we constructed a database of word to code mappings, using our judgement about whether to delete or preserve links between particular words and codes. The final database structure consists of an index of words related to their associated codes, so that for any word the associated codes can easily be identified. The coding system has been designed so that it has the ability to \"learn\" new words or new codes for existing words in a controlled way. Because of the importance of the database to the coding system, updating of the database in this way is normally performed by very experienced personnel. The response string, assigned root words, and final major field of study code are 7/21/93 DRAFT gathered and reviewed. In addition, the system indicates what words in the response string were or were not associated with root words, and whether the CATI operator overrode the set of \"reasonable\" codes presented by the coding system. A computer program identifies potential new database words, and potential new codes to be associated with existing database words. As new database entries are identified, they are presented on a screen and can be added or not added to the database. The update system only presents words or word/code combinations that are (1) not already present in the database and that (2) have not previously been refused entry into the database by the operator. There are 739 word to code maps in the current word association database. The database has 406 unique words and 113 unique codes. The distribution of the number of word/code maps for the unique words in the dictionary is shown in Figure 1. Figure 1 clearly shows that most words are associated with only a very few major field of study codes. Also, the number of words in the database quickly decreases as the number of maps increases so that very few words map to more than three or four codes. However, as shown in Figure 2, the distribution of the number of word/code maps across codes is quite different. Although generally speaking the number of codes in the database decreases as the number of maps increases, there are some exceptions to this general trend. Most of the codes have more than three or four words which map to them. Figure 2 also shows that there are some words which map to a unique code, for example the word \"philosophy\" maps only the major field of study code for \"philosophy\".',\n       'Cape Blanco South to north lag Figure 2b. Mean seasonal patterns of density and alongshelf velocity at selected depths along the northern CCS midshelf. Velocity data were rotated to a natural isobath reference frame, positive poleward. Means were averaged over the period -2004. Note different velocity range at GH. The dashed lines connect summertime property extremes.',\n       'The macro approaches to policy transfer sketched above reflect on the global mechanisms (namely international evidence, legitimation and economic competitiveness) that are behind the international spread and growth of NLSAs and TBA. In this section, we reflect on how a political sociology approach to policy instruments can complement these macro theories by bringing meso-level factors into the analysis. The political sociology approach to policy instruments aims to understand, on the one hand \"how instruments are chosen, how they develop and how they are operationalised\" (Kassim & Le Galès, 2010, p. 2) and, on the other, what are the political implications of policy instruments\\' choices (Peters, 2002) . This approach is contextually grounded, in the sense that is compatible with historical institutional premises on the role of institutions in the mediation of global forces and agendas, but also in the sense that it provides actors operating at different scales with voice and agency in understanding policy adoption. The political sociology approach to policy instruments, due to its embeddedness within a constructivist epistemology, emphasises that meaning-making processes importantly interact with political, institutional and economic factors in the production of policies and policy change.\\nFrom this perspective, some of the factors that could explain the spread and growth of NLSAs and TBA are the politically rewarding, economically convenient and semiotically malleable nature of these policy instruments.\\n• Politically rewarding. Enacting quality assurance and accountability systems in education allows politicians to signal to their publics that they are working seriously towards education change and that they are concerned about education quality, learning outcomes and the future of children, while simultaneously putting the reform pressure on schools and teachers rather than on the government. Furthermore, for political actors, it is easier to discuss and agree on the adoption of policy instruments than on deeper or more fundamental policy transformations (Kassim & Le Galès, 2010) . Overall, adopting new instruments might be a way to avoid addressing controversial political debates, or a way of addressing complex education debates (such as the causes of and solutions to educational inequalities) from the perspective of technical and instrumental choices. For instance, it is quite common that, in current policy narratives, external assessments are portrayed as key devices to address learning gaps and assure that all children achieve minimum learning standards. • Economically convenient. Policy instruments are easier to adopt if they are administratively and economically viable or, at least, their cost is not apparent to the public (Peters, 2002) . Promoting testing and accountability is somehow \"cheaper and quicker than alternative reforms\" (Smith, Miller-Kahn, Heinecke, & Jarvis, 2004, p. 50) , and in particular than the adoption of more profound and structural equity reforms. Something similar happens with the adoption of other components of the GERM, such as curricular standards, which according to Hargreaves and Shirley expand like \"wildfire\" because they are \"easy to write, inexpensive to fund\" (2009, p. 9; as quoted in Sahlberg, 2016, p. 131 ). • Semiotically malleable and culturally acceptable. Policy instruments spread more quickly if they can be attached to positive symbols (such as \"quality assurance\") and politically powerful slogans (such as \"addressing the learning crisis\" or the \"learning gap\"). Policy instruments also expand faster when they have a polysemic nature and/or can operate as empty signifiers, i.e. the same instrument can serve very differenteven contradictinggoals (Steiner-Khamsi, 2016) . This is the case of instruments such as NLSAs and TBA, which allow different political groups to advance different objectives through them. For instance, the fact that the same accountability instrument might contribute to meet a diversity of goals such as quality, equity, transparency, school choice, etc. facilitates that governments with different ideological orientations converge in conceiving test-based accountability as an adequate instrument through which to articulate educational change.\\nThe selection of policy instruments is contingent to political, discursive and economic factors, such as those just mentioned. Political strategies are behind instrument choices. Nonetheless, policy instruments are far from being technical technologies that political actors manipulate at their will (Lascoumes & Le Galès, 2007) . Despite their socially constructed character, or maybe because of this, policy instruments, once adopted, have the potential of affecting the politics, semiotics and economics of education policy. There are at least three specific premises that can be tested regarding the constitutive and relatively-autonomous role of policy instruments; first, \"policy instruments have an existence independent of the decisions that created them\" (Kassim & Le Galès, 2010, p. 11) . It is difficult to predict the form that any instrument will assume since policy \"instruments create their own structures of opportunity in ways that were typically unforeseen when they were adopted\" (Kassim & Le Galès, 2010, p. 12) , and generate political activity and political consequences that go beyond the initial instrument ontology (Peters, 2002) .\\nWithout going any further, our data implies that NLSAs tend to be increasingly used for accountability and monitoring purposes, even in places where they were not initially adopted with these objectives. In many places, NLSAs, at the time of adoption, are not seen as intrusive instruments by school actors due to their education diagnostic and/or student grading purposes; however, once the complex large-scale assessment infrastructure is in place, accountability policies start being adopted and are increasingly attached to the assessment results. This is, for example, the case of the end-of-primary test in the Netherlands. While the test was originally aimed at providing information on the secondary-school type most appropriate for each student, new uses have been attached to the test over recent years. More specifically, the test has become an instrument for school assessment purposes, as aggregate results are now used by inspectors as indicators of school quality (Nusche, Bran, Hálasz, & Santiago, 2014; OECD, 2013a; Scheerens, Ehren, Sleegers, & de Leeuw, 2012) . There are also actors that, beyond national governments, might start doing accountability policy with NLSAs. In Denmark and Norway, the government stopped producing rankings with school scores due to teachers organisations\\' and other educational stakeholders\\' contestation. However, the media and some local governments, taking advantage of transparency rules in public administration, have access to NLSA data and produce and disseminate their own school rankings (Hatch, 2013; Ydesen & Andreasen, 2014) .\\nSecondly, once selected, policy instruments privilege certain actors and their interests by \"determining resource allocations, access to the policy process, and problem representations\" (Menon & Sedelmeier, 2010, p. 76) . Here, it is important to observe which educational actors are empowered by TBA systems, and which are disempowered by the new instruments in place. It is also relevant to analyze whether and under what circumstances new instruments contribute to the emergence of new political subjectivities such as the opt-out movement, which is increasingly influential in several US localities (Hutt & Schneider, 2018; Pizmony-Levy & Saraisky, 2016) . Economic interests also emerge around new policy instruments, as both thirdparty producers (for instance, testing companies in charge of NLSAs) and parties supporting policy implementation (for instance, consultants offering data analysis, instructional improvement or test preparation services to schools). When these economic interests are strong, there are more reasons to expect that policy instruments will endure in time, independently of how effective the instruments are.\\nFinally, the effects produced by policy instruments \"depend on how the aims and purposes ascribed to them, and the meanings and representations they carry, are perceived, understood and responded to by key actors\" (Skedsmo, 2011, p. 7) . Overall, a political sociology approach to policy instruments aims to go beyond the study of the impact of policies (in terms of, for instance, educational access, learning achievement, and so on), and rather aims to analyse the types of social relations, behaviours and responses that instruments trigger among communities of practice (Lascoumes & Le Galès, 2007) . Policy instruments are forms of power that aim at disciplining actors\\' behaviour, but they are also resisted and/or creatively transformed by educational actors. Thus, to understand how NLSAs evolve, we need to go beyond the level of formal policy formulation and observe how practitioners also do policy (cf. Ball, Maguire, & Braun, 2012) by enacting these instruments and the implicit educational mandates that come with them, as well as whether they do so in unpredicted or unintended ways.',\n       'This study describes the results with a promising set of cognitive assessment tools that can be used to measure general as distinct from domain-specific reasoning and writing abilities that are valued as outcomes of college attendance. These open-ended measures can be administered in a few hours and scored reliably. A machine can even grade some and perhaps eventually all of the answers. This makes these measures very efficient relative to other outcomes assessment batteries. The measures appear to assess important abilities that are applicable across major fields. In addition, the scores on them appear to be sensitive to between-institution effects. Such findings are rare in the higher education research literature (Pascarella and Terenzini, 1991) . These instruments may also prove useful for benchmarking purposes if future studies with larger numbers of institutions replicate our findings of statistically significant between-institution effects.\\nOne of our next steps will be to investigate how scores on the outcome measures used in this research relate to engagement and participation measures at both the student and school levels. This will allow us to estimate the extent to which those practices that the literature espouses to be educationally effective (Chickering and Gamson, 1987; Kuh, 2001 Kuh, , 2003 translate into cognitive payoffs. With data from enough colleges and universities it also may be possible to develop residual models, whereby we can compare how particular institutions or types of colleges actually score with how they would be predicted to score, given the nature of their students and institutional characteristics. This would open up a potentially instructive approach to measuring institutional effectiveness. In addition, combining these value-added measures with other information about students (e.g., NSSE) and institutions will allow us to learn much more about the impact of college on student learning as well as the kinds of educational experiences that contribute to desired college outcomes.\\nFinally, we found that the measures we used produced reasonably reliable student-level scores that correlated as highly with GPAs as did SAT scores. Future studies with larger and more representative samples of institutions and students may therefore find a role for such measures in the college admissions process (see Klein et al., 2004 for a discussion of this topic). Indeed, some of the prompts we used are already an integral part of the GRE and the Graduate Management Admissions Test.',\n       'The analysis simulating what would happen if all CRP contracts expired in 2002 estimates the probability that each CRP contract would return to crop production if the program were no longer available. Multiplying these estimates by the acres in each contract and aggregating to the county level yields predictions for the amount of CRP land in each county that would return to production. For the purposes of estimating the economic impact of these changes, we first estimate associated revenue changes for the following land-use activities: grains, oilseeds, cotton, hay and pasture, and other crops. To do so, we allocate CRP lands predicted to return to crop production to specific crops based on the current use of cropland within each county. 54 We allocate other CRP lands to pasture based on actual landuse patterns of parcels dropping out of the CRP between 1992 and 1997. We then estimate changes in annual revenues by multiplying our predicted acreage changes by county-level estimates of expected 2002 revenues per 54 The current crop mix in a county presumably reflects the current profitability of those crops. NRI parcels that returned to crop production in 1997 after dropping out of the CRP typically did not return to the same crop that was planted before the parcel was enrolled in the CRP. While the most profitable crop for each acre of CRP land exiting the program might differ from the county average due to unique land characteristics, the current crop mix in the county should be a reasonable proxy for crop allocation on acres exiting the CRP. acre for each land use. To calculate revenues from crops we use 5-year average yields for each county, adjusted for the productivity of CRP acreage, and 2001 commodity prices. County-level revenue estimates for pasture and hay employed a similar approach (for details, see Lubowski and Roberts, 2003). One potential shortcoming of this approach is that it does not allow the price effects of increased production to feed back into land-use decisions. That is, since land released from the CRP will increase production, we would expect commodity prices to drop, lowering expected revenue for all affected crop farmers and discouraging some farmers from planting a crop. If this happens, our national estimates of the production and revenue impacts of CRP expiration will be overstated and our regional estimated impacts may be over-or understated, depending on interregional shifts in cultivation. This is slippage in reverse. Over the years, researchers have argued that the production-control impacts of land retirement and diversion programs are reduced as rising commodity prices encourage uncultivated land into production. Slippage rates of 20 to over 50 percent have been reported, varying greatly by crop, land quality, and geography (see Leathers and Harrington, 2000;Love and Foster, 1990;and Wu, 2000). Others have found evidence suggesting that local slippage rates are much lower (Hoag et al., 1993;Roberts and Bucholtz, 2002). If reverse slippage follows a similar pattern, CRP land coming into production in one area may cause non-CRP land to drop out of production in other areas. To check on the likely size of price effects as CRP land returns to production, the analysis was supplemented with an assessment of how the overall agricultural economy might change if CRP expired, based on the U.S. Regional Agricultural Sector Model (USMP; see House et al., 1999). As described in Appendix D, the USMP is a comparative-static market equilibrium model. While much more aggregated than the land-use model we estimated econometrically, as an equilibrium model it is able to capture the dynamic response of the agricultural economy as policies and programs change. For this analysis, the USMP model was constrained to force CRP land to return to production to determine the likely price and revenue impacts if CRP contracts expired. 55 The results suggest that as CRP acreage is released from conservation uses, crop production will increase and crop prices will fall. There is considerable variation among crops, with corn showing the greatest response with production increasing by 4 percent and market prices falling by about 6 percent. 56 As producers make further adjustments in response to these market conditions, one would expect fewer total acres to be planted, with prices moderating. But our concern is with the initial shock of eliminating CRP contracts, so we make no attempt to predict a new longrun equilibrium for farm commodity markets or the broader economy. We estimated crop revenue impacts using two alternative scenarios: (1) no commodity price effects, which is consistent with early input-output modeling efforts; and (2) allowing prices to decline as predicted by USMP, but not allowing further slippage in planting intentions. The first case overestimates the revenue impact because it does not account for a reduction in revenue occurring on all cropland stemming from a fall in commodity 55 The USMP model and the econometric model discussed previously are not strictly comparable and were not designed to work with each other. Furthermore, the USMP model only accounts for about two-thirds of the land in the CRP, so this simulation provides only rough estimates of what would happen if only 51 percent of CRP land returned to production. 56 The price response for other crops ranges from close to 0 to about 4 percent. These production and price responses are similar in magnitude, but in the opposite direction of, those estimated when the CRP program was just prices. The second case exaggerates the price response, and therefore underestimates the revenue impact, because total acres planted to crops will not increase one-for-one as CRP acres are returned to production. Together these two approaches should provide a reasonable range of revenue shocks associated with the expiration of all CRP contracts. We used the econometric model to estimate the changes in agricultural output and the social accounting matrix (SAM) model to analyze the effects of these changes on the linked sectors. If CRP rental payments end, household expenditures would also be affected. Data from the Agricultural Resource Management Survey are used to apportion CRP rental payments going to low-, middle-, and high-income households, using farm operator wealth to measure permanent income. 57 The size of each of these economic shocks is estimated for the United States and for three multicounty regions likely to be most affected by expiration of the CRP. For the regional models, we assume that all transfer income is spent within the region. (CRP rental payments accruing to nonoperator landlords living outside the region represent expenditure leakages that diminish the regional impact of the CRP.) Table 4.3 presents the changes in final demand affecting producers, households, and factor income flows for the Nation and the three regional economies used to define our two scenarios. Scenario 1 is called the \"traditional scenario\" because it assumes that agricultural price changes do not affect farm incomes-the traditional approach adopted by previous analyses. With no agricultural price effects accounted for, post-CRP shifts in land use generate $3 billion in increased agricultural production nationally. Partially offsetting this is a net reduction in outdoor recreational expenditures of $7 million (using a trips-based model) and the loss of $1.6 billion in CRP rental payments. Scenario 2 is called the \"augmented scenario\" because it allows for agricultural price changes to also affect farm enterprise incomes. In this scenario, post-CRP shifts in land use lead to a $7.46-billion reduction in the value of current agricultural production at the national level in addition to increasing agricultural production by $3 billion. 58 However, we assume that this reduction in farm enterprise income merely represents a transfer from the farm sector to the rest of the economy. 59 Hence, from a national perspective, the two effects offset each other. Nevertheless, since the regional economies we will be examining later in the section are not closed economies, farm enterprise income losses are not likely to be offset by other consumer expenditures within the region. We therefore include the loss of farm revenue stemming from lower prices as part of the agricultural shock to these regions. We also include in the augmented scenario estimates a loss of $293 million in rural recreation expenditures (using the receipts-based model) and a loss of $1.6 billion in CRP payments to U.S. households. 60 57 Changes in household consumption patterns derive from changes in the perceived level of permanent income rather than transitory income which, particularly for farm households, can fluctuate widely from year to year. Low-income households with little net worth did not receive any CRP payments. This is consistent with information on the source of income among farm households categorized by the ERS farm household typology ( fig.  2.5). Seventy-two percent of CRP funds accrue to farm households with moderate average incomes: retirement, residential lifestyle, and low-sales farming occupation farms. In contrast, 71 percent of total farm program payments accrue to farm households with high average incomes: high-sales farming occupation, large, and very large farms. 58 The $7.46-billion decrease occurs on land that was in production while the CRP was in place. Inelastic demand for food (and our assumption that all cropland in production stays in production) means a small change in price leads to a substantial drop in revenue. The $3-billion dollar increase comes from land that was in the CRP but shifts to crop production. Therefore, farm income for the entire agricultural sector is down approximately $4 billion. 59 We are assuming this income transfer stays within the United States. To be able to quantify the extent to which a portion of this $7.46 billion in consumer surplus accrues to foreign purchasers of U.S. agricultural products requires further study. 60 In both scenarios, loss of CRP rental payments are treated as household income transfer losses. To treat them as value-added losses would be equivalent to assuming that they are linked to producer decisions at the margin. In fact the CRP program payments are decoupled from producer decisions at the margin.',\n       'Diffusion tensor brain networks were built following a similar methodology to previous work (Marinazzo et al., 2014; AlonsoMontes et al., 2015; Amor et al., 2015; Diez et al., 2015 Diez et al., , 2017 using FSL (FMRIB Software Library v5.0) and the Diffusion Toolkit. First, all the selected images were downloaded in DICOM and transformed to Nifti format for further analysis. Next, an eddy current correction was applied to overcome the artifacts produced by variation in the gradient field directions, together with the artifacts produced by head movements. Next, using the corrected data, a local fitting of the diffusion tensor was applied to compute the diffusion tensor model for each voxel. Next, a Fiber Assignment by Continuous Tracking (FACT) algorithm was applied (Mori et al., 1999) . Next, a transformation from the Montreal Neurological Institute (MNI) space to the individual-subject diffusion space was computed and applied to the brain hierarchical atlas (BHA) with M = 20 modules, which was shown in Diez et al. (2015) to have the best correspondence between functional and structural modules. This atlas developed by the authors is available to download at http://www.nitrc.org/projects/biocr_ hcatlas/. This allowed building 20 × 20 structural connectivity (SC) matrices, each per subject, by counting the number of white matter streamlines connecting all module pairs. Thus, the element matrix (i,j) of SC is given by the streamlines number between modules i and j. As a result, SC is a symmetric matrix, where the connectivity from i to j is equal to the one from j to i.',\n       \"This study was designed to test the interaction between amyloid-β and tau proteins as a determinant of metabolic decline in preclinical Alzheimer's disease (AD 18 F]florbetapir SUVR and CSF phosphorylated tau (p-tau) measurements, rather than the sum of their independent effects, was associated with a 24-month metabolic decline in basal and mesial temporal, orbitofrontal, and anterior and posterior cingulate cortices (P o0.001). In contrast, interactions using CSF amyloid-β 1-42 and total tau biomarkers did not associate with metabolic decline over a time frame of 24 months. The interaction found in this study further support the framework that amyloid-β and hyperphosphorylated tau aggregates synergistically interact to cause downstream AD neurodegeneration. In fact, the regions displaying the metabolic decline reported here were confined to brain networks affected early by amyloid-β plaques and neurofibrillary tangles. Preventive clinical trials may benefit from using a combination of amyloid-β PET and p-tau biomarkers to enrich study populations of cognitively normal subjects with a high probability of disease progression in studies, using [\\n18 F]FDG as a biomarker of efficacy.\",\n       'what they can and cannot do, how they look and feel, how they compare to others. This collective set of ideas is called the self-concept, and it affects the individual\\'s behavior, choices, and relationships (Bee, 1995;Damon & Hart, 1982). Research on academic self-concept has indicated that individuals who think well of themselves are more motivated to succeed academically. In an extensive review of the literature, Graham (1994) concluded that African Americans are equal to or higher than European Americans on an array of self-concept measures. These data become more significant when viewed in relation to reported measures of actual achievement or academic ability. When judged by their reported educational and vocational aspirations, there is no indication that African Americans do not value achievement. Ford (1992) proposed that African American students\\' perceptions of how well they fit socially, psychologically, and culturally are linked directly to their academic achievement. Further, the level of support these students hold for the achievement ideology plays an important role in their academic behaviors. This suggests that self-concept, in particular the aspect focusing on how the individual perceives his or her own academic ability, is a major factor in academic achievement. Even in the face of achievement failure, African American students maintained \"undaunted optimism and positive self-regard\" (Graham, 1994, p. 103). It may be that African American students have unrealistic expectations about the process of achieving academically, rather than low self-concept. This notion may contradict the commonly held belief that it is low self-concept that affects African Americans academically. Socioeconomic Status. African Americans are over-represented among economically disadvantaged groups in the United States, and researchers who make comparisons between African American students and other groups need to incorporate socioeconomic status into their research designs in order to separate race and social class effects (Graham, 1994). Current research has placed more emphasis on the disadvantaged or lower socioeconomic level African American child. Economically deprived parents generally have few resources to provide an educational home environment that includes books, reference materials, and study space (Rhodes, 1992). Children who live in such \"pedagogically poor surroundings\" can appear to be less intelligent than they actually are (Kylen, as cited in Rhodes, 1992, p. 109). Furthermore, African American children may appear to be low achievers because measurement instruments used in determining intelligence and academic achievement are based on middle-class, mainstream values and language rather than on cognitive or intellectual functioning. In reality, these \"underachievers\" may be potential gifted children (Rhodes, 1992). Gender Identity. An important part of gender identity is the ideal of role models. Gender identity and role modeling are particularly relevant for at-risk African American students because many are from single-parent homes. If a boy has no father, he will need to find a positive role model in a grandfather, uncle, neighbor, teacher, or church member. The same holds true for girls. Unfortunately, young, urban, economically disadvantaged children are often raised in families and in a society in which positive role models are few, distant, or not approachable (Wright & Borland, 1992). Research Questions In an attempt to assess the impact of the five predictor variables generated from the literature review, the following three research questions guided this investigation: What is the initial level on the criterion variables (mathematics and reading achievement) and the predictor variables (parental involvement, religious socialization, self-concept, socioeconomic status, and gender) for African American students?',\n       'To assess IRR for the two forms of rater evaluation (direct measurement and holistic measurement), an intra-class correlation coefficient (ICC) was calculated for each set of dentation measurements using scores from the two raters. All values between both raters were compared for consistency in a two-way random model. Direct labeling performance by raters was evaluated using total quantity of dentations, and was found to be very high, ICC = 0.924. Holistic assessment performance by raters was assessed using overall scores for each participant (taken as a sum across hemispheres, following Beattie et al., 2017) and was also very high, ICC = 0.921.\\nNext, descriptive statistics were then computed to characterize the quantity and prominence of dentations along the longitudinal axis of the hippocampus and between each hemisphere. Overall dentation quantity was compared to performance scores on visual and verbal recognition tests for each participant to explore the relationship between hippocampal dentations and episodic memory function. Results were computed using Matlab 2018a software.\\nTo evaluate the reliability of each test statistic, bootstrap resampling was employed to establish 95% confidence intervals around the relationship around each variable or between each pair of variables. The use of bootstrap-based statistics provides maximal statistical power when working with smaller samples and avoids making normality assumptions associated with parametric statistics (McIntosh & Misic, 2013) . Briefly, 1000 bootstrap samples were obtained, with each representing a random resampling with replacement of the original group, and a correlation coefficient was recorded from that bootstrap sample. Where dependent measures were pooled (e.g. with all dentations being entered into long-axis analysis), resampling was still conducted on a subject-wise basis, with all entries corresponding to a particular participant being entered at once with replacement (i.e., \"super-subject\" analysis; see also Detre, Natarajan, Gershman, & Norman, 2013) . The bootstrap standard error was established by evaluating the variance across samples and a bootstrap ratio (BSR) was computed in which the correlation obtained from the full group was divided by the bootstrap standard error, yielding a value analogous to a z-score that could be used to look up a corresponding p-value. Correlations were considered reliable only when this p-value fell below 0.05 and their 95% confidence intervals did not encompass zero.',\n       'A course that focuses on the analysis and systematic study of the intersection between leadership and operational art at the tactical and/or strategic levels. Includes instruction in leadership case studies, the dynamics of battle and associated responsibilities, the use of opportunities and resources, the human and physical dimensions of warfare, the effects of technology and logistics, doctrine, logistics, stress and the legal authority of command.',\n       'Objective: Late-life depressive symptoms (DS) increase the risk of incident mild cognitive impairment and probable dementia in the elderly. Our objectives were to examine the relationship between elevated DS and regional brain volumes including frontal lobe subregions, hippocampus and amygdala, and to determine whether elevated DS were associated with increased subclinical cerebrovascular disease in postmenopausal women. Methods: DS were assessed an average of 8 years prior to structural brain MRI in 1372 women. The 8-item Burnam regression algorithm was used to define DS with a cut-point of 0.009. Adjusting for potential confounders, mean differences in total brain, frontal lobe subregions, hippocampus and amygdala volumes and total ischemic lesion volumes in the basal ganglia and the cerebral white and gray matter outside the basal ganglia were compared between women with and without DS. Results: Depressed women had lower baseline global cognition and were more likely to have prior hormone therapy history. After full adjustment, DS at baseline were associated with smaller superior and middle frontal gyral volumes. Hippocampal and amygdala volumes, and ischemic lesion volumes were similar in depressed and non-depressed women. Limitations: Depression was not assessed based on semi-structured interview, and MRI scans were obtained cross-sectionally rather than longitudinally. Longitudinal MRI assessments will be necessary to define the temporal relationships between DS and frontal lobe volumes. Conclusions: Elevated DS were associated with lower volumes in certain frontal lobe subregions but not in the medial temporal lobe structures. Our findings support the role of frontal lobe structures in late-life DS among women.',\n       \"In this section, we describe the mastery innovation, including the Mathematics Teacher Exchange initiative. First, we consider the different ways the term 'mastery' is used. Second, brief descriptions of East Asian practices are provided, with a focus on the Shanghai approach. An important issue here is that East Asian pedagogy is embedded in systems and cultures with specific characteristics.\",\n       \"We use data from the USDA's 2011 and 2015 Agricultural Resource Management Survey, which is representative of all farm households. Using a difference-in-difference framework allows us to isolate the effects of Medicaid expansion on household-level insurance coverage. Our results are based on the following model: \",\n       'Adaptations made to the school, teacher, and student questionnaires were of the following five main types: \\uf06e changes to general instructions made in the interests of enhancing clarity; \\uf06e changes designed to make question text more readable to U.S. students, similar to those made to the assessment items as described above; \\uf06e changes to response alternatives where the international response set did not adequately reflect the U.S. context; \\uf06e additional questionnaire items included to address particular issues of national interest; and \\uf06e International items that were omitted from the U.S. questionnaires because they violated the federal Pupil Privacy Rights Act (for example, questions on bullying and violence in the school).',\n       'EDUCATION STATISTICS QUARTERLY -VOLUME 4, ISSUE 2, SUMMER 2002 ,d0=211209 and Secondarty Education There were 141,407 school administrators (mostly principals and assistant principals), 58,891 school district administrators, and 380,655 school and district administrative support staff. Administrators and administrative support staff made up 10.1 percent of all education staff. On average, there were 15 teachers and 13 other staff for each district and school administrator. What was the racial/ethnic background of students enrolled in public schools? In the 2000-01 school year, racial/ethnic data were reported for 47.0 of the 47.2 million students enrolled in public elementary and secondary schools in the 50 states and the District of Columbia (table 4). White, non-Hispanic students made up the majority of students (61.2 percent3), followed by Black, non-Hispanic and Hispanic students (17.2 and 16.3 percent, respectively) (figure 3 and table 5). Asian/Pacific Islander students made up 4.1 percent of the public school population and American Indian/Alaska Native students made up 1.2 percent. In six states (California, Hawaii, Louisiana, Mississippi, New Mexico, and Texas) and the District of Columbia, 50 3Based on the 47.0 million students with reported racial/ethnic data. percent or more of students were non-White. Black, non-Hispanic students made up more than 50 percent of all students in the District of Columbia and Mississippi. New Mexico reported 50.2 percent of its students as Hispanic, and Hawaii reported 72.3 percent of its student body as Asian/Pacific Islander. On the other hand, five states (Iowa, Maine, New Hampshire, Vermont, and West Virginia) reported that over 90 percent of their students were White, non-Hispanic. How many students graduated from high school during the 1999-2000 school year? Some 2.5 million students received a high school diploma in the 50 states and the District of Columbia during the 1999-2000 school year and subsequent summer (table 6). Another 41,638 received other high school completion credentials (e.g., a certificate of attendance). These \"other high school completers\" only made up 1.6 percent of all high school completers (diploma recipients and other high school completers, not including recipients of high school equivalencies). In addition, there were students who earned a high school equivalency certificate; however, a national total cannot be computed because of missing data from a 62  (table 2). Over 50 percent of the DoD school students were White, non-Hispanic (table 5). Of the students in the overseas schools, 21.6 percent were Black, non-Hispanic; 7.8 percent were Hispanic; and 10.1 percent were Asian/Pacific Islander. Of domestic students, 26.0 percent were Black, non-Hispanic; 18.4 percent were Hispanic; and 3.5 percent were Asian/ Pacific Islander. Approximately 47,000 students attended the Department of the Interior, Bureau of Indian Affairs (BIA) schools (table  1). The governance of BIA schools differs from that of the federal DoD schools. The Education Amendments Act of 1978 (P.L. 95-561) and further technical amendments (PL. 98-511, 99-89, and 100-297) mandated major changes in BIA-funded schools. These amendments empowered Indian school boards, provided for local hiring of teachers and staff, and granted the direct funding of schools. The BIA does not report the number of staff or graduate counts. How many students were educated in outlying areas? Five outlying areas participate in the CCD collection: American Samoa, Guam, the Northern Marianas, Puerto Rico, and the Virgin Islands. Puerto Rico, considered the third largest school district, educated 612,725 public school students (table 1). The other four outlying areas are much   Grade 1  Grade 2  Grade 3  Grade 4  Grade 5   United States  \\'47,222,778  \\'795,597  3,381,629  3,634,724  3,632,608  3,673,058  3,707,931  3,702,792   Alabama  \\'740,176  \\'11,020  55,112  59,669  58,887  59,263  59,749  60,123  Alaska  133,356  1,210  9,677  9,786  9,817  10,700  10,646  10,743  Arizona  877,696  2,037  68,347   74,491  71,402  72,603  72,     hood @aoin:02 @COM arD1932dirCO V2C17 2000-01   Georgia New Hampshire Oregon This article was originally published as a Statistics in Brief report. The universe data are primarily from the \"National Public Education Financial Survey\" (NPEFS), part of the NCES Common Core of Data (CCD). Technical notes and definitions from the original report have been omitted. Nearly $373 billion of revenues were raised to fund public education for grades prekindergarten through 12 in school year 1999-2000. Current expenditures (those excluding construction, equipment, and debt financing) came to almost $324 billion. Three out of every five current expenditure dollars were spent on teachers, textbooks, and other instructional services and supplies. An average of $6,911 was spent on each studentan increase of 6.2 percent from $6,508 in school year 1998-99 (in unadjusted dollars).* Total expenditures for public education, including school construction, debt financing, community services, and adult education programs, came to nearly $382 billion. These and other financial data on public elementary and secondary education are collected and reported each year by the National Center for Education Statistics (NCES), U.S. Department of Education. The data are part of the \"National Public Education Financial Survey\" (NPEFS), one of the components of the Common Core of Data (CCD) collection of surveys.',\n       'One assumption is to consider that the local spreading of the disease gives a small or null contribution. Thus, we consider the functions\\nHere, the rate constants a and c have units 1/(day) ν , while ν > 0 is a dimensionless positive parameter. This functional form of a(t) is justified from the solution it gives rise. Eq. (1) can be integrated as n(t) = n(0) + t 0 dt ′ a(t ′ ), with n(0) = 0. Therefore,',\n       'We are acutely aware that only a small subset of people with expertise in land use policy and planning participated in our June 2015 meeting; an even smaller number contributed a chapter to this book. Consequently, we acknowledge that there will be perspectives that are not represented, either in part or in full, in this volume. In many respects, this is positive, for it means that there is more to be said and written about land use policy and planning, and how information and tools can be better generated, managed and used to improve policy and decision-making. If this book stimulates additional dialogue that fosters support for better informed land use policy and planning, we believe that the exercise will have been a valuable one.',\n       \"In this cross-sectional study of healthy adults in the Framingham Offspring Study, four dietary patterns were derived using cluster analysis ('Fruits, Reduced Fat Dairy and Whole Grains', 'Refined Grains and Sweets', 'Beer' and 'Soda'). The 'Refined Grains and Sweets' dietary pattern comprised \\nValues within a row with unlike superscript letters are significantly different (P,0·05; Tukey-Kramer's adjustment for multiple comparisons). * Adjusted for age, sex, smoking, physical activity, treatment of hypertension and total energy. † Lowest mean in the row. ‡ Highest mean in the row. § Adjusted for age, sex, waist circumference, smoking, physical activity, treatment of hypertension and total energy intake. k Excluded those taking blood pressure medications.\\nthe largest number of individuals, while the 'Soda' pattern comprised the smallest number. The four patterns derived in the present study are similar to those identified using cluster analysis in other American populations (31 -34) . For example, a 'heart healthy' pattern and an 'empty calorie' pattern, which resemble our 'Fruits, Reduced Fat Dairy and Whole Grains' and 'Soda' patterns, were identified as two of six patterns in women participating at the third examination cycle of the Framingham Offspring Study (33) . Consistent with the present study, a 'healthy' and 'alcohol' pattern were identified among five patterns using cluster analysis in 459 men and women in the Baltimore Longitudinal Study of Aging (32) . Often, an 'alcohol' pattern is captured in dietary pattern analysis, particularly when men are included in the analysis (32, 35, 36) . In the present study, beer was the predominant contributor to alcohol intake and thus the rationale for naming it the 'Beer' dietary pattern. Although dietary patterns derived in different studies are assigned different names, they often reflect similar food behaviours and nutrient profiles. For instance, our 'Fruits, Reduced Fat Dairy and Whole Grains' pattern is comparable in nutrient content with the 'dark bread, rice and pasta, vegetables' pattern identified in the Insulin Resistance Atherosclerosis Study (34) and a 'milk, cereals, and fruits' pattern identified among elderly Boston area residents (31) . In the present study, compared with individuals in the 'Fruits, Reduced Fat Dairy and Whole Grains' dietary pattern, those in the 'Refined Grains and Sweets' dietary pattern had significantly higher waist circumference and BMI. The 'Refined Grains and Sweets' pattern also had a relatively higher energy contribution from high-fat dairy, meat, refined grains, sweet baked foods, candy, and had a lower energy contribution from reduced-fat dairy, fruits, vegetables and whole grains compared with the 'Fruits, Reduced Fat Dairy and Whole Grains' pattern. This result is consistent with previous findings that a diet high in refined grains, high-fat dairy, meat and sweets, and low in whole grains, fruits and vegetables, was associated with a high risk of obesity (37, 38) . Prospective studies also found that consuming a diet high in fruits, vegetables, reduced-fat dairy and whole grains was associated with lower increases in BMI and waist circumference (32,39 -42) . In the present study, none of the dietary patterns was associated with improved blood pressure after the exclusion of those on hypertensive medication.\\nWe found individuals in the 'Soda' pattern had significantly higher fasting insulin concentrations than those in the 'Fruits, Reduced Fat Dairy and Whole Grains' pattern, consistent with our previous finding that the consumption of sugar-sweetened drinks is positively associated with fasting insulin (43) . Individuals in the 'Soda' pattern also had a lower dietary fibre intake and a higher dietary glycaemic index. The protective effect of dietary fibre on insulin resistance (4, 6, 44) , the metabolic syndrome (5) and type 2 diabetes mellitus (34, 45) has been well documented. McKeown et al. (46) observed that a higher dietary glycaemic index was unfavourably associated with surrogate measures of insulin resistance. In contrast, no association was found between dietary glycaemic index and measures of insulin resistance in either the Insulin Resistance Atherosclerosis Study (6) or the Zutphen Elderly Study (47) . Inconsistencies between studies may in part be attributed to the different methods used to determine insulin sensitivity, the different populations studied, and methods of dietary assessment.\\nSome, but not all, observational studies have found a significant association between sugar-sweetened soft drinks and obesity (48 -50) . Using cluster analysis, Wirfalt & Jeffery (28) observed that individuals in the 'soft drinks' pattern had a significantly higher mean BMI compared with individuals in the'skim milk' and'meat-cheese' patterns. The 'Soda' pattern had the highest energy contribution from carbohydrate and the highest glycaemic index. Previous studies have demonstrated that high-carbohydrate diets contribute to decreasing HDLcholesterol and increasing TAG concentrations (51, 52) . Findings from several observational studies have consistently found that glycaemic index is inversely related to HDL-cholesterol concentration (53, 54) . Using data from an earlier examination in the Framingham Offspring Cohort, Sonnenberg et al. observed that obese and non-obese women in the 'empty calories' cluster had a higher prevalence of the metabolic syndrome (14) . Despite differences in the type of FFQ administered and the defining of the clusters, this dietary pattern characterised by a higher intake of sugar-sweetened beverages and lower intakes of dietary fibre and vegetables is similar to the 'Soda' dietary pattern derived in the present study. Individuals in the 'Soda' pattern displayed several metabolic syndrome abnormalities, i.e. higher waist circumference and TAG and lower HDLcholesterol. However, these differences were not statistically significant compared with the 'Fruits, Reduced Fat Dairy and Whole Grains' pattern, in part due to the small number of subjects in this pattern (n 210).\\nTwo prospective studies have observed that diet soda consumption is positively associated with the metabolic syndrome (16, 50) . It is possible that diet soda consumption may be associated with a particular dietary pattern, or alternatively a reflection of a dietary change due to some underlying metabolic disease. In the present study, only those individuals in the 'Soda' cluster differed with respect to diet soda consumption, i.e. they consumed significantly less diet soda compared with the other clusters. We previously found that diet soda consumption was not associated with any surrogate measures of insulin resistance after adjustment for potential confounders (43) . In the present study, individuals in the 'Beer' pattern had significant higher HDL-cholesterol concentration compared with those in the 'Fruits, Reduced Fat Dairy and Whole Grains' pattern. The inverse association between alcohol consumption and CHD has been observed in many observational studies around the world (55) . It is estimated that approximately 50 % of the benefit of moderate alcohol consumption could be explained by the direct effect of alcohol on HDLcholesterol (56, 57) . A meta-analysis of experimental studies found that a dose of 30 g alcohol per d increased the concentration of HDL-cholesterol by 39·9 (95 % CI 32·5, 47·3) mg/l (58) . Although a higher beer intake was associated with a better HDL-cholesterol level, individuals in the 'Beer' pattern had a poorer overall diet, for instance, fewer fruits, vegetables, whole grains, and less reduced-fat dairy. In addition, individuals in the 'Beer' pattern had a significantly lower overall diet quality score compared with individuals in the 'Fruits, Reduced Fat Dairy and Whole Grains' pattern.\\nStrengths of the present study include its large sample size and inclusion of multiple insulin-resistant phenotypes measured in a clinical setting. The main limitation is the cross-sectional study design. Since the diet and insulin-resistant phenotypes data were collected at the same time, the causal relationship cannot be assessed due to uncertainty regarding the timing of exposure and outcome, so further prospective studies are needed to examine the effect of diet on insulin resistance using dietary patterns to characterise exposures. Although we adjusted for physical activity, residual confounding caused by lifestyle behaviours and social economic status factors may have arisen in the present study. The characterisation of the food groups used in the present study is limited by the dietary data obtained by self-reported FFQ. Although the FFQ has limitations in estimating absolute intake for individuals, it is a feasible and valid method to rank (or differentiate) individuals according to their usual diet. Differences in food and nutrient intake profile across clusters, as depicted in Tables 1 and 2 , demonstrate the utility of the cluster analysis for discriminating dietary exposure within the cohort. In addition, dietary patterns derived by cluster analysis using FFQ data have been validated against 3 d food records specifically in the Framingham Offspring Study (33, 59) . Because the participants of the Framingham Offspring Study are predominantly white Americans, the results from the present study may not be readily generalised to other populations who have different dietary behaviours.\\nIn conclusion, our findings suggest that the consumption of a diet rich in fruits, vegetables, whole grains and reduced-fat dairy protects against insulin-resistant phenotypes; displacing these healthy choices with meat, refined grains, high-fat dairy, sweet baked foods, candy and sugar-sweetened soda promotes insulin-resistant phenotypes.\",\n       'Objective. This study investigated the association of the ACTN3 polymorphism with sarcopenia and osteoporotic status in older Korean adults. Methods. Older Korean 62 men and 270 women (mean age 73.7 ± 6.6 years) participated in this study. Body mass index, percent body fatness, appendicular skeletal muscle mass, and bone mineral density of the lumbar spine, femur, and total body were analyzed with dual-energy X-ray absorptiometry. ACTN3 R/X genotyping was determined using TaqMan probes. Results. Determination of odds ratios (ORs) and 95% confidence intervals (CIs) using binary logistic regression analyses showed that XX homozygotes were at a significantly higher risk of sarcopenia (OR = 2.056, 95% CI = 1.024-4.127, = 0.043) and osteoporosis (OR = 2.794, 95% CI = 1.208-5.461, = 0.016) than RR homozygotes (reference group, OR = 1). The OR of XX homozygotes for having sarcopenia remained significant (OR = 2.237, 95% CI = 1.044-4.836, = 0.038) after adjustments for age, gender, body fatness, and serum vitamin D. The OR of XX homozygotes for having osteoporosis was no longer significant (OR = 2.682, 95% CI = 0.960-7.942, = 0.075) after adjustments for the covariates. Conclusion. Our findings suggest that the ACTN3 R577X genotype may influence decline in muscle and bone health phenotypes in older Korean adults.',\n       'Our primary outcome was the total number of health-related community resources available within each tribe. We created a score to capture the number of health-related community resources by first grouping resources into 5 subdomains: 1) community infrastructure, 2) health care and education, 3) social determinants, 4) recreational infrastructure, and 5) recreation programs. The community infrastructure subdomain included 5 resources: presence of sidewalks, parks, playgrounds, community gardens, and community centers. The health care and education subdomain included 8 resources: tribal health clinics, health insurance, chronic disease prevention programs, healthy living campaigns, wellness programs, nutrition courses and resources, weight loss programs, and health fairs. The social determinants subdomain included 5 resources: housing assistance, college scholarships, promoting higher education, promoting GED programs, and after school programs. Recreational infrastructure comprised 8 resources: community baseball or softball fields, basketball courts, swimming pools, running paths, walking/hiking trails, tribal-owned gyms, gym availability in the community, and sports scholarships. Finally, the recreation program subdomain included 4 resources: fitness classes, schoolbased sports leagues, community-based sports leagues, and sports tournaments.\\nWe decided that we had no reason to weight any of these subdomains more heavily than the others. Therefore, each subdomain had a maximum of 10 points (range, 0-10), regardless of the number of resources in each subdomain so that, in the total score, each subdomain had equal weight. For example, each resource in the social determinants subdomain was worth 2 points, because the social determinants subdomain comprised 5 resources; whereas each resource within the health domain was worth 1.25 points, because 8 resources made up this domain. To create the total resource score, we summed each of the subdomain scores, for a maximum score of 50 (range, 0-50).',\n       '1 In 1 In these models, typically labeled nonparametric structural equation models, the arrows signify inclusion in kernel functions f(.) that generate effects and where no functional form is placed on the kernels. Thus, if A and B have arrows that point to Y, then the structural relation is specified as Y=f Y (A,B,e Y ), where the right-hand side can be parameterized variously by any function in A, B, and e Y , including cross-product terms such as A*B.\\nthis article, the black arrows represent causal effects that we assume exist. The gray arrows represent causal effects that many other researchers assume exist and which we accordingly allow even if, as we discuss below, we are not convinced that they exist.\\n[ The model asserts that performance in high school is caused by a direct effect of family background and by a general mechanism represented by a chain of three unmeasured variables:\\ninformation (I), beliefs (B), and commitment (C) . 3 This mechanism is intended to capture a welldocumented phenomenon in adolescence: many students move in fits and starts through high school, eschewing all-or-nothing grand decisions about their futures and responding only in a 2 We also assess math learning, measured as the difference between 2004 and 2002 math test scores, and timely high school completion. These results are available in the Supplemental Appendix to this article. Reading test scores are not available in the 2004 ELS wave, and as a result we cannot assess reading learning. 3 The direct effect of family background on performance is properly interpreted as a collection of unspecified mechanisms. We take no position on which of the many proposed mechanisms in the literature constitutes a portion of the arrow that defines this direct effect in Figure 1 . Possibilities include (1) differences in resources that affect learning and performance but that are unrecognized by students, (2) biased assessments of teachers that generate an association between ascriptive characteristics such as race and subjective performance evaluations such as grades on written assignments, and (3) structures in schools, such as tracking and course sequences, that harm the achievement growth of students from disadvantaged social origins, without such students recognizing these effects. Notice, however, that we do allow the Wisconsin model to have an explicit place in the causal graph, which includes its master variable of educational expectations (see Sewell, Haller, and Portes 1969; Sewell, Hauser, Springer, and Hauser 2004) . Accordingly, the Wisconsin model mechanism, where significant others define status expectations that students then adopt as their own aspirations, is not embedded within the direct effect arrow that emanates from family background.\\nlimited way to the educational plans defined for them by others. Students make consequential everyday choices of whether to commit to schooling, and they do so under information deficits and with goals that are susceptible to social influence (see Bozick et al. 2010; Fredericks et al. 2004; Grodsky and Riegle-Crumb 2010; Morgan 2005; Schneider and Stevenson 1999) .\\nThe causal graph in Figure 1 posits that the unmeasured information, I, that informs educational choices is generated by exogenous factors in Z and V. 4 This information, which presumably includes information about the fairness of the education system and about the costs and benefits of higher education, is also determined directly by family background. We assume that this effect of family background on I emerges because those who occupy advantaged social positions are more comfortable searching for information beyond that which is available to them because of joint structural determinants, V, of both family background and the distribution of information.\\nBeliefs, B, are then formed on the basis of this differentially available information, although in interaction with family background. Here we assume that students from different family backgrounds may process their acquired information differently. They may also feel that the costs and benefits of education depend on their social origins. This perception may or may not be accurate, and indeed the academic literature offers contradictory findings regarding the direction and magnitude of class-differentiated costs and benefits (see Breen and Goldthorpe 1997 and Brand and Xie 2010) . For our purposes, the critical point is not so much whether beliefs about education are accurate, only that these beliefs vary by both family background and information, I.\\n4 Typically in this tradition of causal graphs, nodes such as Z would be suppressed, since it is assumed that all nodes have exogenous sources that give them distributions and that are independent of the other variables in the model. Here, we give Z an explicit place in the model in order to reinforce the point that differences in information are not reducible to differences in family background or correlates of it.\\nThe key mechanistic behavioral variable in the causal model is commitment, C, which transmits the effects of beliefs to performance in high school. The model in Figure 1 does not require a particular model of commitment, and there are many on offer. Morgan (2005) provides one possible model in his concepts of prefigurative and preparatory commitment, where the latter follows from the former. Another possible model of commitment emerges from the \"aligned ambitions\" perspective (Schneider and Stevenson (1999) . This model maintains that motivation and effort in high school are determined partly by the alignment of students\\' educational and occupational ambitions, which Schneider and Stevenson argue are shaped by a diverse set of factors that structure students\\' beliefs about their futures. A third alternative is the Bourdieuinspired model of habitus utilized by Grodsky and Riegle-Crumb (2010:18) , where \"a collegegoing habitus may increase the likelihood that students engage in behaviors that increase their probability of attaining their goals.\" For Grodsky and Riegle-Crumb, a college-going habitus can be measured by indicators of how beliefs for future educational attainment were constructed, either as taken-for-granted scripts for the future or as conscious choices arrived at during primary or secondary schooling. The critical point here is not which model of commitment the analyst adopts, but that he or she adopts some belief-based model of everyday behavioral orientations to schooling that can account for some subsequent differences in levels of educational performance.\\nFinally, the causal graph in Figure 1 includes two additional observed variables that reflect the underlying beliefs in B. As discussed in the next section, Educational Requirements of Expected Jobs will be the key predictor variable in our empirical models, and College\\nExpectations will be used to test for the robustness of our conclusion that the underlying model of commitment has empirical support.',\n       'Teachers\\' purposes for the role-play simulation. Both Bender and Kramer explained the purpose of the Ellis Island Role-Play Simulation was to \"hook\" students and get them \"involved\" and \"interested\" in the topic of immigration in turn of the century America. Kramer remarked, \"After they [students] go through that experience of Ellis Island, they want to learn more about it.\" Their comments reflected their constructivist view that students learn best by doing. As Bender stated, \"if you are not involving your learner[s], they aren\\'t going to learn.\"\\nBender said that the Ellis Island Role-Play Simulation was also an attempt to assist students in thinking about the challenges immigrants faced at the turn of the century and today:\\nThe goal of the Ellis Island Simulation was to allow them [students] to understand what actually happened at Ellis Island, the larger issue there is to show the problems immigrants faced. The secondary larger issue is to show that we are still an immigrant nation and immigrants still face those problems.\\nBecause the purpose of the simulation was primarily to encourage students\\' interest in immigration and to deepen their thinking about the immigrant experience at Ellis Island, it lasted only one class period. However, Bender and Kramer prepared students for the simulation beforehand.\\nPreparing for the role-play simulation. Two class periods before the enactment of Ellis Island, the teachers went over background information with students to provide them with the factual knowledge they would need to participate in the simulation. Using a guided note sheet on immigration, Bender and Kramer assisted students in writing answers to several questions, including: \"What is an immigrant?\" \"Who are the immigrants we are studying in 1890-1924?\" \"What made people leave their homes?\" and \"What attracted people to the U.S.?\" They explained what Ellis Island was and showed them a PowerPoint that included historic images of the immigrant experience at Ellis Island.\\nThe day before the simulation, the teachers asked each student to create their role for the simulation. They handed students a list of questions, including: \"What country will you come from?\" \"What will be your immigrant name?\" \"What will be your marital status?\" \"What will be your education level?\" \"What will be your career?\" \"Do you have a job in America?\" \"What is your current wealth?\" and \"Why are you coming to America?\" The handout also asked students to describe their health, political views, and English speaking skills. For homework, students wrote responses to the questions. Additionally, the teachers showed students a three-minute video clip of the Ellis Island Role-Play Simulation from the previous year to demonstrate how students could dress for the simulation. Students were told they would receive \"spirit points\" toward their eighth grade trip to Washington, D.C., later that year for coming to class dressed as their characters.\\nThe Ellis Island Role-Play Simulation. The day of the simulation students seemed excited as they came into the classroom. All but one student in Kramer\\'s class came dressed as an immigrant. Most of the female students wore long skirts, shawls, and tied their hair into a bun at the top of their head. Three students carried dolls to show they were mothers with young children. Many of the male students donned feather hats. Three students posed as frail, elderly immigrants, wearing gray wigs and walking with canes and limps. For the first five minutes of class, as the teachers were taking attendance and getting organized, students were asked to introduce themselves to each other in their native languages. Half-sheets of paper with basic expressions, greetings, and questions in various languages were provided. Students seemed to enjoy the activity. We overheard one male student relate to another male student that he was Hungarian and his grandfather had taught him to speak a Hungarian dialect, which he eloquently demonstrated. Many students conversed with each other in English with Italian-or Polish-sounding accents.\\nNext, Kramer and Bender\\'s second period classes combined for the simulation and lined up in the hallway outside the school library, which would become Ellis Island. Kramer and Bender, who did not always participate in the simulations, took an active role in this one. In a loud voice so that all 45 students could hear, Kramer said, \"I am not Mr. Kramer and Mr. Bender is not Mr. Bender. We are inspectors at Ellis Island. You are an immigrant, and you don\\'t speak English.\" Bender then introduced the school\\'s technology coordinator, Mr. Hughes, who was dressed in a blue inspector uniform. He said Mr. Hughes would be the lead inspector. As students proceeded into \"Ellis Island,\" Bender and Kramer said things to them such as \"I am not sure if you look healthy enough to get in\" and \"Do you have a limp?\" Before school on the day of the simulation, Bender and Kramer transformed the library into \"Ellis Island.\" Projected on the back wall of the library was a historic image of the waiting area at Ellis Island.\\nThe middle section of the room had been cleared for seven inspection stations, which consisted of a folding table and two chairs. The stations were where the immigrants went for balance, memory, intelligence, literacy, vision, health, and legality tests. The stations were staffed by inspectors: Kramer, Bender, the school librarian, and four eighth grade students from other class periods, all supervised by Mr. Hughes. Upon entering Ellis Island, the immigrants received a blue slip of paper that said, \"Ellis Island Checklist: You must go through all these stations.\" They were required to have their checklist initialized and stamped by an inspector at each station.\\nStudents rushed to get in line at all the stations. Mr. Hughes, the lead inspector, walked around the room telling students where the lines were shorter. Although there was some laughing and joking among students while they were in line for the tests, all 45 students participated in the tests and seemed to take the simulation seriously. For example, at the literacy test station, we noted a female student read a passage from a history textbook in broken English, prompting the female student inspector to say, \"You stupido! Can\\'t you read faster?\" Kramer and Bender occasionally circulated around the room to keep students on-task. They also created some excitement. For example, 20 minutes into the simulation, Kramer grabbed a doll from one of the female students and said, \"Oh no, I\\'m not sure if this baby is going to make it. I don\\'t think she is breathing.\" Then he laid the doll on the ground and pretended to give her CPR. The student played along and said, \"My baby … please save her!\" Toward the end of the period, Bender began blowing a whistle and said that he would now take \"the anarchists\" (students who had received a yellow dot at one of the stations) to a detention center. Three males and one female followed him to a side area of the library.\\nWith five minutes left in the class period and most immigrants having completed their tests, Bender, Kramer, and Hughes asked the immigrants to sit in chairs lined up in the front of the library so that they could begin debriefing the simulation. Mr. Hughes asked students if they knew how many immigrants were allowed into Ellis Island in one day. After several incorrect guesses by students, he related that the number was \"approximately 11,000.\" Bender asked students who were in the detention center to explain how they felt when they were pulled out of the simulation. One male student said \"surprised,\" while another said \"angry.\" Next, Bender began asking students about the tests they took and whether they would be difficult for immigrants with limited English proficiency. He said he bought three of the tests at the Ellis Island gift shop when he visited. Unfortunately, the teachers\\' oral debriefing was shortened by the end of the class period, but students had an assignment to further their thinking about what they experienced at Ellis Island.\\nDebriefing and assessment of the role-play simulation. For homework the evening of the simulation, Bender and Kramer asked students to write responses to several questions: \"Which test was the hardest for you today?\" \"If you were a real immigrant, what test would have made you the most nervous?\" \"Ellis Island has been described by immigrants as being both \\'heaven and hell wrapped together.\\' After experiencing it for yourself, explain why that statement is true or false.\" and \"In one word, describe an immigrant\\'s experience through Ellis Island.\" The next day, they went over students\\' responses to the questions and discussed the simulation. Additionally, they showed students a PBS video about immigration at the turn of the century.\\nTo further assess students\\' learning from the role-play simulation, Bender and Kramer asked students to write a letter home to a relative living in an Eastern European country in 1911, based on their experiences in the simulation as well as their class notes. The assignment required students to describe push and pull factors, the boat ride to America, how they felt when they saw the Statue of Liberty, their experience at Ellis Island, their experiences after they left Ellis Island, and their hopes and dreams for the New World.\\nWhen asked how the simulation went, both Bender and Kramer said they thought it went well and that they had achieved their desired learning goals. Kramer explained: I think they [students] were able to internalize the experience of an immigrant, and, even today, they remember their immigrant names and the stories they created, so that has become part of their deeper memory here. After we did that simulation, there was definitely a higher interest, and they wanted to learn more about immigration; they wanted to learn more about their own family histories, so I think it made it real.\\nThe teachers\\' comments suggested the role-play simulation did, indeed, make the topic of turn-ofthe-century immigration interesting, engaging, and relevant.',\n       'Missing Data (No reason provided on the questionnaire.) 62 99.9 TOTAL (Missing Data is not counted in percent total.) 1/ Percent total may be less than or greater than 100% due to rounding.',\n       \"For longitudinal studies, response status to a preceding interview is typically a good predictor of a sample member's likelihood to participate in the current interview. Table 4 shows the response status of B&B:93/03 field test sample members by their response status to the last follow-up interview, B&B:93/97. 4 Almost 76 percent of B&B:93/97 respondents participated in the B&B:93/03 interview. In contrast, only 48 percent of B&B:93/97 nonrespondents participated (χ 2 =27.0, p<0.0001). Thus, respondents in B&B:93/97 were considerably more likely than nonrespondents to participate in B&B:93/03. \",\n       \"Education is a report that will examine the representation of military students in undergraduate and graduate education and to present how their demographic and enrollment characteristics compare with their nonmilitary peers. • Web Tables-Trends in the Pell Grant Program, 2000 to 2012 will examine trends in Pell Grant awards between 1999-2000 and 2011-12 and present information on the changes in the amount of Pell awarded and changes in the proportion of students' total cost of attendance met by Pell. • Contraction of Private Loans is a report that will examine trends in borrowing from commercial lenders for postsecondary education, the characteristics of undergraduate and graduate private loan borrowers, and combining private and federal loans. • Web Tables-Trends in Nonfederal Aid will examine trends in state and institution aid between 1999-2000 and 2011-12 by institution and student characteristics.. • New Americans in Postsecondary Education is a report that will describe the characteristics and undergraduate experiences of 2011-12 undergraduates who immigrated to the United States or who had at least one immigrant parent (second-generation Americans). • Undergraduate PowerStats contains the data on a sample of about 95,000 undergraduates from about 1,690 institutions. The data represent all undergraduate students enrolled between July 1, 2011, and June 30, 2012, in postsecondary institutions in the 50 states and the District of Columbia that were eligible to participate in the federal financial aid programs under Title IV of the Higher Education Act. • Graduate PowerStats contains the data on a sample of about 16,000 graduate students from about 1,690 postsecondary institutions. The data represent all graduate students enrolled between July 1, 2011, and June 30, 2012, in postsecondary institutions in the 50 states and the District of Columbia that were eligible to participate in the federal financial aid programs under Title IV of the Higher Education Act. Survey data files and associated codebooks and file documentation, are available to researchers who have obtained a restricted data license from NCES from the website: http://nces.ed.gov/statprog/instruct.asp. Information on obtaining a restricted data license is available in the NCES Restricted-Use Data Procedures Manual at http://nces.ed.gov/statprog/rudman/. The general public may use NCES web tools, found at http://nces.ed.gov/datalab, to analyze NPSAS:12 restricted-use data. These tools permit analysis of the derived file without disclosing its contents to the user, and, as necessary, suppress or flag estimates that fail to meet reporting standards, or both. QuickStats allows casual users to generate simple tables and graphs quickly and easily. PowerStats is available for users who wish to generate complex tables or estimate simple linear or logistic regression models.\",\n       'The median 1998 doctorate recipient graduated from high school in 1980, at age 18, was about 34 (33.7) years of age when receiving his or her doctoral degree, and had been enrolled on a full-time basis for 6 years in the doctoral program. Women were, on average, about 18 months older than their male counterparts (34.8 years of age versus 33.1 years for males). While twothirds (67.9 percent) of recent Ph.D.s received their high school diploma at 18 years of age, 3.8 percent were 16 years old or younger, and 2.4 percent were at least 20 years old. The amount of time taken by doctoral students to earn their degrees can be expressed in several ways. The survey collects data on three statistics in particular: (1) the elapsed time between receipt of the baccalaureate and conferring of the doctorate; (2) the number of years actually registered in a doctoral program; and (3) the age at which the doctorate was awarded. None of these \"clock times\" is necessarily an accurate measure of the time and effort required to complete a doctorate, for each measure can be affected by such factors as the job markets for new doctorates, child care responsibilities, or requirements governing access to loans (and the repayment schedule) and health insurance through the university. Nevertheless, taken together, these three offer a complementary picture of the path and process of doctoral study. (Tables 16,  17, and 18 and figures 13 and 14 provide the data and graphical illustrations for the discussion on time to degree below, both for 1998 levels and longitudinal comparisons.)  Doctorate recipients in the physical sciences had the shortest total time to degree (8.0 years) for students in any of the seven broad fields of study, with engineering Ph.D.s second (8.9 years); both fields had the lowest registered time (6.7 years). Within the general arts and sciences areas, humanities students took the longest median time to earn their doctorates (11.6 years), and they were registered for the longest period as well (8.7 years). Overall, education doctorate recipients had the longest average time to degree (20.0 years), although they were actually registered in their doctoral program for less than half of that time (8.4 years). (See figures 13 and 14.) 29 Figure 14. Age distribution at doctorate by broad field of study  Table 18 Source: NSF/NIH/NEH/USED/USDA, Survey of Earned Doctorates',\n       'This section uses results from Blomquist et al. (1988) to estimate a value for ecosystem services that is capitalized in the wage and housing markets. Unlike the results in Table 1 , values calculated in this section are for services provided by ecosystems directly to consumers; that is, we are now valuing inputs to consumption rather than production. One might think, therefore, that these estimates should be added to the numbers derived so far to find a total value for ecosystem services, but that would be erroneous -values to households are conditional on household income, which includes excess rents due to ecosystem services. Our purpose in presenting these numbers is merely to indicate the likely relative importance of these non-market ecosystem benefits.\\nThe techniques used here are due to Rosen (1979) , who suggested that locations are best viewed as tied bundles of wages, rents and amenities. More recently, evidence of the influence of amenities on wages has been researched by Roback (1982) , Graves (1983) and Blomquist et al. (1988) , amongst others. Their findings suggest that wages are negatively affected by positive amenities and positively affected by negative amenities (workers are generally willing to give up higher wages to live in more amenity-rich regions).',\n       'We performed univariate analysis to assess the relationship between demographic and clinical characteristics and rural-urban status among all patients and to evaluate the association between rural-urban status and regional lymph node examination and adequate lymphadenectomy among patients who received a hysterectomy. Chi-square test for independence was performed for categorical variables, and Wilcoxon rank-sum tests were performed for continuous variables, as these variables were non-normally distributed. Analyses were stratified by type, grade, and race/ethnicity (non-Hispanic Whites and non-Hispanic Blacks only). Cochran-Armitage test for trends was performed to assess trends in adequate lymphadenectomy overall and by cancer type. We performed unadjusted and adjusted logistic regression analyses to determine rural-urban differences in odds of receipt of surgery, nodal examination, and adequate lymphadenectomy for all cancers and stratified by type, grade, and race/ethnicity. Adjusted analysis controlled for age, race/ethnicity, marital status, year of diagnosis, and clinical stage. Kaplan-Meier curves were constructed to assess rural-urban differences in survival. Additionally, Cox proportional hazard models were used to further assess rural-urban differences in all-cause mortality accounting for race/ethnicity, marital status, year of diagnosis, clinical stage, receipt of hysterectomy, and receipt of adequate lymphadenectomy. All analyses were performed in SAS 9.4 (SAS Institute, Inc.). All tests were two sided using an alpha level of 0.05.',\n       nan,\n       'Generalized linear regression models (McCullagh and Nelder 1989) \\nNotice that the quantity B that satisfies U(B) ¼ 0 is always a meaningful population quantity even if the model is misspecified, since it is a linear approximation of x i to h i . A first-order approximation of E(Bjy, X) is given based on B where\\n, and E(Bjy, X) is obtained by solving the weighted score equation for population regression parameter B',\n       'A method was developed for the segmentation of lacunar infarcts utilizing both FLAIR and T1 images. The method first detects candidate locations via localizing \"holes\" in a T1 image, and then classifies these holes based on the intensities and contrasts in T1 and FLAIR images.\\nIn order to find the holes, the tissue segmentation of T1 image is performed using two approaches: 1) EM-based classification and 2) multi-atlas segmentation as in Section 2.4.2. EM-classification is based mostly on the voxel intensities, i.e., a voxel with low intensity is typically classified as CSF. On the other hand, small holes get easily misclassified in multi-atlas segmentation if they are in the middle of WM and a strong probabilistic prior term is used. Consequently, holes can be detected as the voxels that are classified as CSF in EM-classification and as WM or GM in multi-atlas segmentation.\\nA hole is classified as a lacunar infarct if 1) in FLAIR, the contrast of the hole and the surrounding tissue is large, 2) the surrounding tissue in FLAIR is bright, and 3) the intensity in T1 image is low. However, in basal ganglia the condition 2 is not expected. Finally, only the infarcts with diameter larger than 3 mm and smaller than 15 mm are regarded as lacunar infarcts.',\n       \"The Coastal Observing SYstem for Northern and Arctic Seas [COSYNA; http://www.hzg.de/institute/coastal_research/ cosyna/] has been deployed in the German Bight, integrating near real-time measurements with numerical models and providing continuous coastal ocean state estimates and forecasts. COSYNA, which is operated by the Institute of Coastal Research, Helmholtz Zentrum Geesthacht (HZG), shows many similarities with advanced coastal observatories in the US and Europe (e.g. Glenn & Schofield 2009; Proctor & Howarth 2008) . It consists of observational nodes, a data management system and data assimilation capabilities, streamlined towards meeting the needs for high quality operational products in the German Bight. The individual in-situ observing subsystems used are: FerryBox, gliders, buoys and HF-radar. The forecasting suite includes nested 3D hydrodynamic models running in a data assimilation mode, forced with meteorological forecast data.\\nUnlike most systems assimilating HF-radar data, which are concerned with low-pass filtered surface velocity measurements, COSYNA focuses on intra-tidal scales, which can be justified by the need to a) develop a better knowledge on the short-term coastal ocean variability, and b) enhance quality of data needed for special coastal operations. The blending of data and models (see also next sub-section) uses a spatio-temporal optimal interpolation (STOI) which enables dynamically consistent smoother within an analysis window of one or two tidal cycles. This method maximizes the use of available observations, as a step towards 'best surface current estimate'. Patchy observations over part of the German Bight sampled every 20 mins from three WERA radars are used to prepare 6-hr and 12-hr forecasts. COSYNA modeling products also include regular maps of wind, waves, salinity, and temperature. The latter two are enhanced by the assimilation of FerryBox data (Stanev et al. 2011 ).\\nExample 4: National initiatives in coastal ocean forecasting (NOAA/USA) Coastal wave and surge modeling systems typically make use of phase-averaged spectral wave models such as SWAN (Simulating Waves Nearshore), and increasingly WAVEWATCH III, coupled to varying degrees with circulation models such as SLOSH, ADCIRC, FVCOM (Finite Volume Coastal Ocean Model) and SELFE (Semiimplicit Eulerian-Lagrangian Finite-Element), typically run in twodimensional, depth-integrated mode (Booij et al. 1999; Tolman et al. 2002; Jelesnianski et al. 1992; Luettich et al. 1992; Chen et al. 2003; Zhang & Baptista 2008) . The US federal agency that oversees operational oceanic prediction (NOAA) has developed operational guidance systems where these models are currently run in uncoupled (or one-way coupled) mode: ADCIRC-based Extra-tropical Surge and Tide Operational Forecast System (ESTOFS), the SLOSH-based Probabilistic Hurricane Storm Surge (P-Surge) and the SWAN/WAVEWATCH III-based Nearshore Wave Prediction System (NWPS) (Feyen et al. 2013; Taylor & Glahn 2008; Van der Westhuysen et al. 2013) . In these systems, aspects of physical phenomena are shared between models (e.g. ESTOFS and P-Surge water levels are included in the NWPS wave model, see Figure 3 ), but there is no process feedback. An example of a fullycoupled wave-surge system is the ADICRC and SWANbased ADCIRC Surge Guidance System (ASGS). Here the surge model transfers water levels and depth-integrated currents to the wave model, which, in turn, transfers wave radiation stresses and enhanced bed friction to the surge model.\\nCoastal three-dimensional baroclinic circulation modeling systems are designed to provide guidance on water levels, currents, salinity and temperature. Examples of such systems are NOAA's national network of Operational Nowcast and Forecast Hydrodynamic Model Systems (called OFS). An OFS consists of the automated integration of observing system data streams, hydrodynamic model predictions, product dissemination and continuous quality-control monitoring. Within these systems, hydrodynamic models such as ROMS, FVCOM and SELFE are driven by real-time data and meteorological, oceanographic, and/or river flow rate forecasts, receiving boundary conditions at the coastal shelf from NOAA's Global-RTOFS (Real-Time Ocean Forecast System), based on HYCOM [HYbrid Coordinate Ocean Model, http://hycom.org] (Chassignet et al. 2007) .\\nTo promote the next generation of operational forecasting, NOAA's Integrated Ocean Observing System [IOOS; This example is based on one of the RCOOS mentioned above, namely the Southeast Coastal Ocean Observing Regional Association (SECOORA). An integrated highresolution, three-dimensional, coupled (ocean-atmosphere-wave) Nowcast/Forecast system has been developed for the Northwest Atlantic Ocean by North Carolina State University (NCSU). Covering the entire US east coastal ocean, the Gulf of Mexico and Caribbean Sea, the system is implemented based on the Coupled Ocean-Atmosphere-Wave-Sediment Transport (COAWST) modeling system (Warner et al. 2010) . COAWST couples ROMS, WRF and SWAN models representing the ocean, atmosphere, and wave environments. ROMS/SWAN is spatially collocated with the WRF domain (7-10 km grid, fine enough to resolve atmospheric forcing from tropical cyclones) (Halliwell et al. 2011 ). Boundary and initial conditions are provided by the global HYCOM. These three models were coupled using the Model Coupling Toolkit (MCT), resulting in a COFS that exhibits several advantages over global forecasting systems (Larson et al. 2004; Jacob et al. 2005 ). These include: (a) finer resolutions in both horizontal and vertical directions that can better resolve regional and coastal processes (Hurlburt & Hogan 2000) ; (b) fully coupled model physics that include the interactions/feedbacks among ocean circulation, marine meteorology, and ocean waves; (c) an improved representation of coastal/shelf dynamics (e.g. tides). The coupled system performs routine nowcast and 3-day forecast on daily basis [http://omgsrv1.meas.ncsu. edu:8080/ocean-circulation-useast2] with an example provided in Figure 4 . Near-real time model predictions are validated against HF radar surface currents, NOAA sea level data and buoy measurements. Interactive functions include: visualizations of user defined virtual station profiles or hydrographic transects and 72-hour surface trajectory 'virtual particle' simulations.\\nExample 6: From the ocean to the reef scale (Australia)\\nReaching reliable model forecasts in fine coastal scales requires careful downscaling (see also Kourafalou et al. 2015) . The eReefs project [http://www.emg.cmar.csiro. au/www/en/emg/projects/eReefs.html] is highlighted here as an example associated with a unique set of challenges (Schiller et al. 2014 ). This initiative aims to provide an information system, underpinned by models, for the iconic Great Barrier Reef (GBR) on Australia's northeast coast. The GBR is the longest stretch of coral reef in the world, one of the seven natural wonders of the world, a UNESCO world heritage site and home to abundant biodiversity. Reef cover has continued to decline over the last several decades, due to the effects of cumulative stresses, primarily nutrient loads from terrestrial runoff, Crown-of-Thorns Sea-star infestations and damage from tropical cyclones (Brodie & Waterhouse 2012) . Some of Journal of Operational Oceanography s135 these stresses can be mitigated by targeted management strategies (e.g. nutrient load input), whereas others cannot (e.g. extreme weather events). The eReefs system, therefore, aims to provide managers relevant information to assist in the development of informed mitigation strategies to improve reef health. Consequently, any models contributing to the overall modeling system must span scales (from the catchment, through estuaries, across the lagoon, over the reef matrix and across the shelf to the deep ocean) and disciplines (catchment modeling, hydrodynamics, waves, sediment transport and biogeochemistry).\\nNesting from the global to the reef scale requires several downscaling nests: eReefs ( Figure 5 ) employs a 4 km 'bridging model' in global products and a 1 km regional model, with nested re-locatable models of estuaries/reefs (100s of meters) (Herzfeld 2009; Herzfeld et al. 2011; Herzfeld & Andrewartha 2012) . The reef matrix can generate fine scale structure in the flow, which can feed back to the larger scale (Wolanski & Hamner 1988; Wolanski et al. 1996; Wolanski et al. 2003a; Wolanski et al. 2003b ). To optimize runtime, complex curvilinear grids utilizing branching are employed to represent only areas of interest. Additionally, an unstructured coordinate system is utilized to 'house' state variable matrices within the model; this facilitates the representation of wet cells only in the state vector, which improves computational efficiency. Although the coordinate system is unstructured, the model is based on finite differences. The reef creates large topographic gradients, and individual reef lagoons are isolated from the surrounding waters by exposed fringing reefs at every tidal cycle (tidal range can be 6 m at the coast). Differential heating/evaporation can significantly modify water properties in these isolated lagoons, which feed back to the larger scale when the reef becomes wet again at high tide. These dynamics promote the use of a 'z' vertical coordinate system with true wetting and drying.\",\n       nan,\n       \"During 2014-16, team members conducted and evaluated various formats of extension programming in Spanish, including interactive workshops, model demonstration plots, on-farm trainings and tours, fact sheets and field guides, and videos. During the 2nd year of the project, we conducted surveys and interviews of Hispanic/Latino horticulturists who had participated in a variety of the educational formats offered in Spanish. All educational programs were designed to follow researchbased guidelines for building trust/ positive relationships and providing culturally responsive outreach to firstand second-generation Hispanic/ Latino communities (Farner et al., 2005; Hobbs, 2004; Olsen and Skogrand, 2009; Vega et al., 2016) .\\nIN-DEPTH WORKSHOPS. A team of bilingual adult educators designed and implemented workshop lesson plans that incorporated role play, case study simulations, and various other interactive, problem-solving activities as suggested by Knowles et al. (2005) and Ota et al. (2006) . Role play was used to help learners practice scenarios relevant to their farm interests. For instance, during a session on identifying potential markets for horticultural crops, learners role-played various strategies for interviewing restaurant and store managers. Short case studies (also referred to as critical incidents) facilitated critical thinking skills relative to crop production situations or problems. For example, a case study simulation following a group discussion on protecting pollinators engaged learners in identifying possible reasons for the discovery of dead and disoriented honeybees (Apis mellifera) in front of a hive. Other participatory activities included introductory discussions to engage attendees in defining the scope of a workshop, ice-breaker games that helped gauge prior experiences relative to a topic, and round table sessions facilitated by specialty crop grower mentors.\\nMODEL DEMONSTRATION PLOTS AND STUDY CIRCLES. On-farm, model vegetable demonstration plots served as ''living classrooms'' where Hispanic/ Latino learners practiced sustainable growing methods and compared production and economic performance of various cropping systems. The demonstration plots also served as sites for study circle networks that engaged a broader community of learners. The model plot and study circle curricula are described in detail at the Penn State Extension ''Start Farming'' website (Penn State Extension, 2016) under ''Courses and Workshops.'' ON-FARM TRAININGS AND TOURS. Learners who participated in this project had the opportunity to visit with other Hispanic/Latino farmers during on-farm tours and trainings. The tours provided a variety of experiential learning formats including hands-on demonstrations on fruit tree pruning and field walks to learn crop scouting skills.\\nOur team developed fact sheets, field scouting guides, and videos to supplement the hands-on, problemsolving learning modules. These were produced in both Spanish and English, and in both hard copy and electronic formats. The videos were used during in-depth workshops and also serve as two initial modules in online horticulture trainings for Hispanic learners.\\nTwo educators on our team had formal training in adult learning, and they engaged the learners in postprogram evaluations to assess presentation techniques used during the face-to-face field and classroom trainings. All program participants contributed to the follow-up evaluations by completing a standardized questionnaire designed to provide feedback relative to four key adult learning principles: 1) opportunities for active participation, 2) depth of engagement, 3) relevance to life and work, and 4) development of problemsolving skills. During 30-min open discussions at the conclusion of each program, participants informally shared details on relevance of the specific program topics to their professional goals.\\nPurposive surveys and interviews were conducted to assess 1) methods of learning preferred by Hispanic/ Latino growers, 2) ways extension might increase and improve education and engagement with Hispanic/ Latino growers, and 3) factors that might limit Hispanic/Latino specialty crop growers from participating in extension educational activities and/or using extension resources. Written surveys that began with a consistent set of five multiple choice questions (followed by program-specific questions) were conducted following three workshops for Hispanic/Latino specialty crop growers. Sixty-one participants (out of a total of 70) voluntarily completed the surveys after being advised of the purpose of the study. Nine Hispanic/Latino learners who had attended a variety of extension offerings in Spanish were individually interviewed. The same five questions were asked, and participants were encouraged to provide reasons for the answers selected.\",\n       'The purpose of NPSAS is to serve as a comprehensive, nationwide study to determine how students and their families pay for postsecondary education; it features a nationally representative sample of both aided and nonaided students in postsecondary education institutions in the United States. The sample includes undergraduate and graduate students. These students attend all types and levels of postsecondary institutions that are eligible to distribute student aid authorized under Title IV of the Higher Education Act, including public and private institutions, for-profit and nonprofit institutions, and less-than-2-year institutions to 4-year colleges and universities. NPSAS also serves as the base-year data collection for two longitudinal studies, BPS and B&B. The current NPSAS:12 serves as the base year for the BPS:12 cohort of first-time beginning (FTB) college students, with two follow-up studies planned over the subsequent 5 years. Consequently, a set of items in the NPSAS:12 student interview captured information about student experiences in the first year and their perceptions of the costs and benefits of education in order to support longitudinal analysis of student choices related to persistence and completion.',\n       \"Since not all brain regions are related to AD, those irrelevant features derived from the unrelated brain regions are better removed by feature selection before performing classification and regression. However, to our knowledge, most existing feature selection methods are designed for the single-time-point image, i.e., each subject has only the single-time-point data with the corresponding targets. This feature selection method cannot be easily extended for feature selection on multiple time-point images (i.e., baseline plus longitudinal data). To distinguish from the existing feature selection methods based on the single-time-point data, we call our feature selection on multiple time-point (baseline plus longitudinal) data as longitudinal feature selection method, as formulated below. It's worth noting that, in this study, we focus on the linear feature selection based on feature weight learning as detailed next.\\nAssume that we have N training subjects s 1 ,:::,s i ,:::,s N f gand each subject s i has T imaging data at T different time points, represented as\\n, where\\nas the training data matrix at the j-th time point, and y j ([R N ) is the corresponding target outputs at the j-th time point. Longitudinal feature selection learns a feature weight vector w j ([R D ) from X j and y j , with a 'group regularization' constraint on the corresponding elements of w j across T time points, as formulated in the following objective function:\\nwhere W~w 1 ,:::,w j ,:::\\n, and w d is its d-th row vector. The regularization parameter l balances the relative contributions of the two terms and also controls the 'sparsity' of the linear models. In fact, the last term in the above objective function is equivalent to the l 2,1 -norm of the matrix W, i.e., first computing l 2 -norm on each row vector and then computing l 1 -norm on column vector with l 2 c-norms of row vectors. It's worth noting that the use of l 2 -norm on row vectors forces the weights corresponding to the d-th feature across multiple time points to be grouped together and the further use of l 1 -norm tends to select features based on the strength of T time points jointly. In other words, features (in brain regions) will be selected as a group across all time points together. This formulation is important for tracking the longitudinal changes of brain regions with progression of disease. Fig. 2 gives an illustration on the longitudinal feature selection. Here, at each time point j, we have baseline (for time point j = 1) or longitudinal (for time point j.1) image data X j (each row denotes a subject with features gotten from different brain regions), and corresponding clinical scores y j , e.g., MMSE or ADAS-Cog. By imposing a 'group regularization' on the corresponding elements of each feature weight vector w j , longitudinal feature selection achieves the goal of 'group selection' of features (or brain regions).\\nIt is easy to know that the above objective function in Eq. (1) reduces to the standard l 1 -norm regularized optimization problem in Lasso [37] when only a single (baseline) time point of data is available, i.e., T = 1. Also, because of the use of l 2,1 -norm for W in the above objective function, it will result in a weight matrix W with elements in some rows being all zeros. For the goal of feature selection, we can just keep those features with non-zero weights. For implementation, there have been a number of algorithms available in machine learning and statistics communities to solve the linear regression problem with l 2,1 -norm regularization [38] [39] . In this paper, the SLEP toolbox [40] is used to solve the objective function in our longitudinal feature selection method.\\nIn this paper, to distinguish from conventional feature selection methods that are often based on the single (baseline) time point of data, we call our new feature selection method that works on multiple (baseline plus longitudinal) time points of data as the longitudinal feature selection method. The key characteristics of our longitudinal feature selection method is that the features are jointly selected from the longitudinal data across multiple time points, to better reflect the longitudinal change patterns of the brain with the progression of disease. To the best of our knowledge, this new type of feature selection problem was not investigated in the previous studies, and we solved this problem by formulating it as a linear feature weigh learning with l 2,1 -norm regularization, which can be efficiently solved by the existing multi-task learning methods [38, 39, 40] . It's worth noting that, a few recent works also use the similar multi-task learning techniques based on l 2,1 -norm regularization as used in our proposed longitudinal feature selection method, but they are developed for different purposes. For example, in [25] and [47] , joint regression and classification is performed via multi-task learning, where the estimation of each regression or classification variable is regarded as a different task. However, both methods use only the baseline data and thus cannot reflect the longitudinal change patterns of the brain across different time points, which are apparently different from our longitudinal feature selection method. Predicting future clinical scores\\nA number of high-dimensional regression methods have been used for predicting future clinical scores (or changes) for MCI subjects, based on the baseline neuroimaging data. For example, in [14] , a principal component analysis (PCA) based model was used on the baseline MRI data of 49 MCI subjects (including 20 MCI-C and 29 MCI-NC) to predict the 12-month change in MMSE score, and a correlation coefficient of 0.31 was reported. In [17] , a Bagging relevant vector machine (RVM) was adopted to predict the future decline of MMSE score from the baseline MRI data and a correlation coefficient of 0.537 was achieved on 16 MCI-C, 5 MCI-NC, and 5 AD subjects. More recently, in our previous work, a multi-modal multi-task (M3T) model has been proposed to predict the 24-month change in MMSE and ADASCog scores, and the correlation coefficients of 0.511 and 0.531 are achieved on 38 MCI-C and 42 MCI-NC, as well as 40 AD and 47 HC subjects, respectively. In contrast, by using the longitudinal data, our proposed method achieves much better correlation coefficients of 0.786 and 0.777 on 38 MCI-C and 50 MCI-NC subjects, for predicting 24-month MMSE and ADAS-Cog scores, respectively. All these results further validate the importance of using the longitudinal data for improved prediction of future clinical scores of MCI subjects.\",\n       'ABSTRACT. Accurate quantification of ecosystem services (ES) at regional scales is increasingly important for making informed decisions in the face of environmental change. We linked terrestrial and aquatic ecosystem process models to simulate the spatial and temporal distribution of hydrological and water quality characteristics related to ecosystem services. The linked model integrates two existing models (a forest ecosystem model and a river network model) to establish consistent responses to changing drivers across climate, terrestrial, and aquatic domains. The linked model is spatially distributed, accounts for terrestrial-aquatic and upstreamdownstream linkages, and operates on a daily time-step, all characteristics needed to understand regional responses. The model was applied to the diverse landscapes of the Upper Merrimack River watershed, New Hampshire, USA. Potential changes in future environmental functions were evaluated using statistically downscaled global climate model simulations (both a high and low emission scenario) coupled with scenarios of changing land cover (centralized vs. dispersed land development) for the time period of 1980-2099. Projections of climate, land cover, and water quality were translated into a suite of environmental indicators that represent conditions relevant to important ecosystem services and were designed to be readily understood by the public. Model projections show that climate will have a greater influence on future aquatic ecosystem services (flooding, drinking water, fish habitat, and nitrogen export) than plausible changes in land cover. Minimal changes in aquatic environmental indicators are predicted through 2050, after which the high emissions scenarios show intensifying impacts. The spatially distributed modeling approach indicates that heavily populated portions of the watershed will show the strongest responses. Management of land cover could attenuate some of the changes associated with climate change and should be considered in future planning for the region.',\n       \"Two TD parameters computed from temperature profile observations and synthetics made during 2001-03 are listed in Table 2 . The parameters are based on two isothermal layer depth (ILD) algorithms computed using freely available software described in Lorbacher et al. (2006) and Kara et al. (2000) . These methodologies can also be applied to density profiles to compute MLD, but this is not considered here because not all profiles have both temperature and salinity and we are primarily considering thermocline depth.\\nThe ILD algorithms have been well tested, and their characteristics are known. For example, the Lorbacher et al. (2006) ILD selects the first curvature peak of the temperature profile and tends to be relatively shallow. Because the Lorbacher et al. (2006) ILD method is curvature based, the label we use is ILD $ , where the subscript $ represents the gradient associated with curvature. The Kara et al. (2000) ILD method selects the depth where a change in temperature relative to the near surface exceeds a threshold. Because the Kara et al. (2000) ILD is based on a change in temperature, the label we use is ILD D , where the subscript D indicates that it is a threshold methodology. Hereafter, the unadorned ILD will refer to the isothermal layer depth in general. The ILD D values tend to be deeper than ILD $ and are most closely associated with the seasonal ILD (e.g., Helber et al. 2008 ). BEST The difference of the dynamic height computed from the observation profile and the dynamic height of the MODAS background climatology. This is the SSHA value that corresponds to MODAS's best possible representation of the observation. This is unavailable in practical applications and represents the upper limit on prediction accuracy. 5 A0 NLOM SSHA produced without SSHA assimilation. Surface fluxes and SST assimilation is still used. 6 A1J The NLOM SSHA assimilation is from Jason-1 only. 7 A1G The NLOM SSHA assimilation is from GFO only. 8 A2EG The NLOM SSHA assimilation is from ERS-2/Envisat and GFO. 9 A3EGT The NLOM SSHA assimilation from all available altimeters: Jason-1, ERS-2/Envisat, and GFO. 10 A3EGT2 As in A3EGT, but the initial conditions are perturbed slightly to help identify the nondeterministic response to the initial conditions. \\nFor a small percentage of profiles, each ILD method has certain shortcomings. For example, for profiles where the water column is mixed all the way to the bottom of the profile, ILD $ returns a value of zero. This is because the algorithm cannot find a curvature peak (Kara et al. 2009 ). For similar cases, particularly at high latitudes, the threshold value for ILD D is too large and returns an ILD that is unrealistically deep or at the bottom of the profile. To eliminate this problem, profiles are discarded if the ILD estimates are at the bottom of the profile and more than 50 m from the ocean bottom (based on the navy's 2-min bathymetry) because in these cases the observations do not allow for an accurate estimate of the ILD.\\nThe TD parameter is computed by first determining two temperatures: 1) the temperature of the profile at the ILD T 0 and 2) the change in temperature between T 0 and the temperature 100 m below the ILD, DT. The depth below the ILD where the temperature is equal to T 0 2 DT/2 defines TD. Because we have two estimates of the ILD, there are two corresponding estimates of TD, TD D and TD $ , which correspond to ILD D and ILD $ , respectively.\\nThe TD D is shown graphically with labels in Figs. 2a,b. For profiles with sharp thermoclines just below the ILD D , the TD D is relatively close to the ILD D (Fig. 2a) . For weak thermoclines, the TD D is further from the ILD D (Fig. 2b) . In this way, the TD D characterizes the location of the highest gradients of the upper thermocline from a simple, widely applicable calculation. Because the computation is a bulk estimate of the high temperature gradient depth, it is insensitive to random noise in the profile. Finite difference computation of vertical gradients in observation profiles is inherently noisy and thus avoided in this analysis. The strength of the thermocline (TS) is the gradient of the upper thermocline between the ILD and TD (Table 2; Fig. 2 ).\",\n       'We illustrated the patterns in postdoc training in engineering disciplines over the past two PhD, retaining graduates in the engineering workforce is important for the sustainability of the field. An interesting extension of our study is to investigate the fields of postdoc training in more detail and how their postdoc training contributes to these \"new\" fields of study.',\n       \"To build the strongest possible recruitment team, RTI assembled a blend of in-house recruitment specialists, off-site experienced recruiters, and institutional contactors from RTI's Call Center to serve as recruiters for HSLS:09. The recruitment team was charged with securing the cooperation of districts, dioceses, and schools, and to coordinate the logistics of test day. The team received 3 days of training to learn the recruitment process. The training agenda is shown in figure 4.\",\n       'The Census Bureau\\'s Economic Programs Directorate has adopted two standard definitions for response rates. The first is useful for monitoring progress. The second is useful for monitoring value or quantity coverage from actual respondents. The first response rate measures the proportion of attempted cases that provide a response, where an attempted case is a case for which data collection has been attempted. It is defined as follows: Response rate #1 = R / M, where: R = the number of units which provide a response, and M= the number of units for which one attempts to obtain a response. The second response rate measures the proportion of an estimated total (not necessarily the published total) that is contributed by respondents for an individual variable. It is defined as follows: where: w i = the design weight of the i th unit before adjustments for nonresponse, t i = the reported value for the i th unit of variable t for which the response rate is to be computed, and T = the estimated (weighted) total of the variable t over the entire population represented by the sampling frame. Response Rate Number 1 is frequently labeled \"return rate.\" It is generally calculated only at disaggregated levels-for example, disaggregated by mode of data collection, questionnaire version, or sizebased strata. Among Census Bureau surveys, there have been varied interpretations of whether returned forms that do not contain any respondent data or do not contain respondent data for specified key items should be included in the numerator. Response Rate Number 2 excludes imputed data from its numerator. Consequently, the quantity (1 -Response Rate # 2) is frequently calculated and is labeled \"imputation rate.\" The Economic Directorate includes imputation rates in the explanatory notes of press releases and in the \"Reliability of the Estimates\" section of publications. The Census Bureau\\'s publication guidelines encourage the discussion of sources and magnitudes of errors in published estimates. Imputation rates are easily discussed in connection with the published estimates for different items. Response rates, on the other hand, are associated with the response process and not as easily discussed in connection with the published estimates. StEPS calculates response measures similar to Response Rate Number 1 in its management information module. This module is primarily used to monitor the progress of data collection operations and initial data editing. StEPS calculates imputation rates equal to (1-Response Rate Number 2) in its estimates and variances module. The definition of Response Rate Number 2 excludes from the denominator \"administrative records used in place of a planned attempt to collect data.\" The StEPS imputation rate, however, includes administrative data in the denominator when administrative data are included in the published estimate. For surveys that use weight adjustment to handle unit nonresponse, StEPS treats this as a type of imputation because the adjusted weights allow units that report to represent both reporting and non-reporting units. For surveys that use imputation to handle unit nonresponse or item nonresponse, StEPS calculates imputation rates based on the outcomes of processing performed in the StEPS general-imputation module. The general-imputation module imputes data using estimator type techniques (Giles and Patrick, 1986) and adjusts data items associated with additive relationships so that detail items sum to total items (Sigman and Wagner, 1997). With one exception, all reported item data changed by the imputation module are flagged as being imputed item data. This includes (1) items for which no data were reported, and the general-imputation module creates data; and (2) reported data that fail defined edits, and as a result the general-imputation module changes some of the data. The one exception is when reported or missing data are replaced by administrative data that are considered to be equivalent in quality to respondent-provided data. In this case, the changed data are treated neither as imputed data (used to calculate the imputation-rate numerator) nor as reported data (used to calculate the numerator of Response Rate Number 2) but are used to calculate the numerator of an associated administrative-data rate. The calculation of imputation rates requires that StEPS maintain tracking information indicating if a case is active, if the case has responded, and if the case has not responded how nonresponse is to be handled during processing. StEPS stores a status code for each case indicating if the case is active or inactive. StEPS also stores a \"coverage code\" for each case that specifies a reason why StEPS handles the case the way it does during data collection and subsequent data processing. The first and second columns of Figure 3 list the StEPS coverage codes for active cases, and the third column lists the coverage codes for inactive cases. To indicate response status, StEPS maintains a response code for each case indicating if the case has responded and if it has not, whether it is to be imputed or if it is contained in a subpopulation for which weight adjustment will be used to handle unit nonresponse. For nonrespondents, StEPS has the capability to track the classification of a case as a \"hard refusal,\" in which the respondent informs the Census Bureau that it will not participate, or as a \"soft refusal,\" in which the respondent does not report over a period of time but never actually informs the Census Bureau that it will not participate.',\n       \"Item nonresponse for key employment items, such as employment status, sector of employment, and primary work activity, ranged from 0.0% to 5.3%. Nonresponse to questions deemed sensitive was higher: nonresponse to salary was 6.8%, and nonresponse to earned income was 12.0%. Personal demographic data, such as sex, marital status, citizenship, ethnicity, and race, had item nonresponse rates ranging from 0.0% to 6.2%, with sex at 0.0%, birth year at 0.6%, marital status at 6.2%, citizenship at 3.4%, ethnicity at 0.4%, and race at 1.3%. Item nonresponse was imputed using logical imputation and hot-deck imputation methods. Logical imputation often was accomplished as part of editing. In the editing phase, the answer to a question with missing data was sometimes determined by the answer to another question. In some circumstances, editing procedures found inconsistent data that were blanked out and therefore subject to statistical imputation. During sample frame building for the SDR, some missing demographic variables, such as race and ethnicity, were imputed before sample selection by using other existing information from the sampling frame. Imputed values for race and ethnicity that were used for sampling were not included in the survey's data collection; therefore, race and ethnicity were imputed in post-data processing if this information remained missing. However, sampled cases with imputed values for race and ethnicity who responded in either the Web or CATI mode were asked these questions. The 2013 SDR primary method for statistical imputation was hot-deck imputation. Almost all SDR variables were subjected to hot-deck imputation, with each variable having its own class and sort variables structured by a multiple regression analysis. However, imputation was not performed on critical items or on text variables. For some variables, there was no set of class and sort variables that was reliably related to or suitable for predicting the missing value. In these instances, consistency was better achieved outside of hot-deck procedures using random imputation.\",\n       'The last part of the project is to run a SLOSH (Sea, Lake, and Overland Surges ',\n       \"The first step in translating the new strategy and law into a practical implementation plan is to set out a few clear goals critical for national development, like improving education access, equity and quality. These goals will provide clear objectives to report against to help ensure continuity over the medium to long term. This would help to address the current challenge of discontinuity linked to political change by ensuring the overall focus of the education system remains unchanged despite government changes. Using simple goals with measureable, time-specific targets (see below) is essential to help ensure that each government's commitment to these goals is not only rhetorical but measured in terms of impact.\\nA long-term approach is important in the field of education since reform can take years to take hold and yield results. Many countries establish new strategies over 10-15 year periods, with periodic reviews and space for appropriate adjustments to ensure they continue to be relevant. Given the experience of the 2011 law, and the frequent changes in education leadership that have been common in Romania, establishing an agreed adequate timeframe will be critical. One country that has taken a very comprehensive approach to setting national goals and integrating them into system evaluation is Australia (Box 5.3).\",\n       \"The primary source for any health analytic solution is the EMR also known as EHR. This is a hospital-based system that combines all the entries that are logged by health practitioners. To obtain an effective health analytic solution, it is paramount that the other sources of data be able to synchronize with the EMR. The EMR is a collection of entries that include doctor notes, diagnosis history, pharmacy data, and the insurance company's data. This aggregation of actors makes its design so complex that its adoption is sometimes hindered.\\nThe challenges faced in designing a proper EMR start at the stage of data entry by the physicians. The traditional method for data entry which is straightforward for practitioners is using an easy to use Word document and a spreadsheet. however, this method exhibit difficulties in providing a meaningful insight through an appropriate analytic algorithm. The analytic solution should be able to process unstructured data while the EMR contains mainly structured data hence it is necessary to transform these unstructured entries.\\nYang [38] proposed an XML-based scheme that consists of facilitating both the physicians as well as analytical solution designers. The solution consists of using an XML schema to process entries that were recorded in the usual word processor or spreadsheet fashion and transform it into XML data to be processed as structured data inside the EMR.\\nAnother challenge with the EMR is the interoperability. A patient is treated at different hospitals, which may leave his health data scattered across various EMRs. Interoperability problems between hospitals still pose a big barrier for systems integration. This hindrance is a big obstacle to healthcare as with the absence of prior treatments and the tests, patients undergo different and repeated treatments for the same illness. There are two big questions to consider. The first is the access infrastructure, which is mostly standalone for each hospital and the second is the complexity of the integration of health systems in the context of a country or any other geographical entity.\\nTo solve the access challenge, Wan and Sankaranarayanan [39] proposed a cloud-based EMR that can help all stakeholders to access the EMR by making use of three cloud computing components: Software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). The system allows mobile access for all stakeholders. Additionally, another big challenge to consider is confidentiality as not many hospitals permit their records to be accessed by others.\\nTo solve the compatibility problems, various studies have been conducted and one of the most trusted technologies is Blockchain [169] . The Blockchain technology solves the challenge of personalization and ubiquitous access to records which are the two traits that are required in the healthcare industry. Blockchain ensures data integrity by allowing an immutable way to update records and prevents the tampering with them once they have been logged. Each entry is saved in a block and the content of each block is hashed to constitute the content of the next record.\\nAs depicted in Fig.3 , to solve the mistrust between health providers, Azaria et al. [40] proposed MedRec a solution that decentralizes the EMR access using Ethereum smart contracts. The smart contract will not only help the patient but can also help other stakeholders. A use case would be to help insurance companies to verify if certain medical treatments like surgeries have occurred before paying for the services.\",\n       \"This chapter covers the situation for the majority of UNECE countries. At the time of writing the UNECE comprises the whole of Europe as well as the USA, Canada, New Zealand, Australia and Brazil. The work of Eurostat (the statistical office of the EU) covers in full the 27 member states and the four countries of the European Free Trade Association. The requirement for the acceding and pre-acceding countries to comply with the regulations at the moment of accession means that the western Balkan countries and Turkey are still developing their standards; in these countries the situation in both agriculture and agricultural statistics is more traditional, and at different stages en route to the EU model. For the EU's other neighbours in Europe, the model is more based on the Commonwealth of Independent States (CIS) approach to statistics, but also strongly harmonized and converging to the EU model. The contribution of the USA can be considered partly valid for Canada, Australia and New Zealand. Nevertheless, especially for these countries, some specific circumstances might not be fully covered. Finally, the situation in Brazil is that of a specific country with a strong development in new technologies for statistics and a very specific agricultural situation. The term 'agricultural statistics' is here taken to include statistics on forestry and fisheries. It implicitly also includes statistics on trade in agricultural products (including forest and fishery products) as well as issues related to food safety. The definition of agricultural statistics is based on three conditions, all of which have to be met. In this definition, agriculture consists of the use of land, the culture of a living organism through more than one life cycle, and ownership. Land is used for many purposes, ranging from mining to recreation. Agricultural land supports the culture of living organisms and their ownership. This separates aquaculture from capture fishing and tree farming from forestry. Agriculture includes the management of water, the feeding and raising of organisms through several growth stages. Tree farming includes the management of the soil, fertilization, and pest management as the trees or other ornamental plants are raised through varies stages of growth. In both cases, farmers can choose to use the land for other purposes than aquaculture or raising tree crops. The raising of awareness of the effects of globalization and the impact of climate change have led in the UNECE region to a greater understanding by statisticians as well as the politicians of the need to analyse different societal developments in relation to each other rather than in isolation. However, the interrelatedness of agriculture with, for example, land use and rural development, and also with environmental sustainability and overall well-being, is considered to be not yet fully reflected in available statistical information. Agriculture in the UNECE region is in general characterized by the use of highly advanced technologies. Machinery, new production methods, fertilizers, pesticides and all kinds of supporting instruments have created a sector that is more businesslike than some traditional primary industries. At the same time, the recent emphasis on sustainability, environmental protection and ownership has led to more attention being given to the important role of rural areas. The increased use of modern technologies and the increase of scale has created a farming sector with relatively strong relations to other sectors of society, both at the level of the sector as a whole as well as at the level of individual farmers and their households, for example, with regard to employment and time use. The paradox of increased efficiency on the one hand and more emphasis on sustainability and environmental protection on the other is considered one of the main challenges for current agricultural statistics that justifies a reflection on their future. The burden on farmers and decreasing response rates have forced the statistical institutes and other agricultural data collectors to enhance their efforts to deploy new data collection techniques and to make more use of other types of information, for example, from administrative sources. Growth in the availability of advanced IT tools for data collection, analysis and dissemination techniques is also forcing the statistical institutes to review the methodologies applied in agricultural statistics. Furthermore, the pressure to lower the administrative burden by simplifying regulations in agricultural statistics has created an emphasis on changes in the fundamental legal and methodological bases for agricultural statistics. The enlargement of the EU in 2004 and 2007 and enhanced cooperation with future acceding countries and other neighbouring countries has visibly increased the impact of decisions on the organization and content of agricultural statistics in the EU. The greater variety in crops and methods demands a review of the existing statistics, specifically in the EU context. In agricultural statistics in the UNECE region, the number of international and supranational organizations involved is currently quite limited. In the UNECE context, only the Food and Agriculture Organization (FAO) in Rome and Eurostat in Luxembourg play a role of any significance. The UNECE secretariat in Geneva and the Organisation for Economic Co-operation and Development (OECD) in Paris are no longer heavily involved. At a global level, the number of organizations involved is also very limited, this being the main reason for the decision of the UN Security Council in 2006 to close down the Inter-Secretariat Working Group on Agricultural Statistics. Both in Northern America and in Europe, however, there are many other organizations outside statistics involved in agricultural statistics and information. Traditionally, many of the agricultural organizations as well as the agricultural ministries are involved -as part of, for example, the Common Agricultural Policy -in collecting and using information on agriculture. This chapter has been written from the viewpoint of the national statistical offices, related governmental bodies as well the international and supranational organizations mentioned above. However, for a complete overview of ongoing work in agricultural statistics, this has to be complemented with information from other international and branch organizations. The chapter is structured as follows. In Section 1.2 the current state and political and methodological context of agricultural statistics in the UNECE region is described. The main items discussed are the infrastructure for agricultural statistics, the information systems for collecting structural information, the statistics on production, the monetary elements, the added value in the production of agricultural statistics, other important sources, the relations with other statistics and the use of administrative data. Fishery and forestry statistics are also briefly discussed. In Section 1.3 governance and horizontal issues are discussed in more detail. In Section 1.4 some developments in the demand for agricultural statistics and some challenges are discussed. Finally, Section 1.5 focuses on the main recommendations for agricultural statistics in the UNECE region, regarding both content and governance.\",\n       \"Having recruited a high quality faculty, each college and university must attempt to retain these valuable employees. Competitive salaries and benefits are essential. Without competitive levels of compensation, some institutions will have their best faculty bid away by peer universities, while others will find their faculty leaving the academy entirely for more lucrative employment in other sectors of the economy. Molly Broad, President of the University of North Carolina, noted the difficulty of retaining high quality faculty at the University of North Carolina after three consecutive years without a salary increase. She also expressed the need to continue to offer competitive health benefits when these costs are rising more rapidly than inflation. Understanding the preferences and attitudes of faculty is central to the adoption of appropriate employment and compensation policies to retain productive faculty. The TIAA-CREF Institute has funded several surveys in an effort to ascertain faculty preferences and attitudes toward compensation and employment policies of their universities. Faculty aged 50 and older at member institutions of the Associated New American Colleges, the University of North Carolina, and the University of Minnesota were asked about their work patterns, professional interests, institutional relationships, and compensation preferences. The faculty were interested in flexible workload policies and they were very concerned about health insurance while working. In addition, they worried about the ability to maintain universityprovided health insurance if and when they leave the university. The surveys indicated that senior faculty in these institutions were 'hardworking, institutionally-motivated, and flexible'. Review of the responses illustrate that senior faculty provide universities with a resource 'that institutions could benefit from taking advantage of faculty interests in new roles and their seeming willingness to cooperate in retirement transitions beneficial both to institutions and to faculty members'. Surveys combined with other areas of institutional research could provide the basis for developing future compensation and employment policies. It is important that organizations provide their employees with the most value for each dollar of compensation. With faculties continuing to age, understanding the preferences of senior faculty for flexible assignments, phased retirement, and their willingness to continue to contribute to their university is very important. Knowledge of faculty preferences is important to those institutions seeking to facilitate the orderly retirement of senior faculty, and also to those colleges and universities that are anticipating increases in enrollments, and who would like to entice senior faculty to delay retirement to meet the growing student demands. Changes in the economic environment and faculty aging are requiring institutions to reassess rigid employment rules to be more consistent with faculty preferences and to reevaluate employee benefits. Institutions must reform employment and compensation policies to meet new budgetary realities, but they must also continue to pay competitive salaries and benefits if they are to maintain the quality of their faculty. Chapters in this volume illustrate the importance of faculty participation in reviewing existing policies, determining the need to amend them, and developing new benefit plans. Financial pressure of rapidly escalating health care costs is requiring many institutions to reduce the generosity of their health plans and to shift more of the cost to faculty participants. Acceptance of these policy changes requires faculty buy-in, and the inclusion of faculty in the developmental phase of policy changes enhances the chances that such modifications will be accepted. Equally important is the introduction of new employment policies such as phased retirement and family benefits. Institutions should develop a process for faculty participation and policies should be based on faculty preferences as well as institutional needs. Many colleges and universities have prominent faculty members who are experts on compensation policies, employment practices, and health and retirement benefits. Using these faculty as a resource can facilitate the adoption of optimal compensation packages for a changing workforce. Amending compensation policies to provide less generous benefits should be done carefully with substantial faculty input. The process will be smoother if it is transparent and the decisions are based on surveys of faculty preferences.\",\n       'According to TV host and self-described cheerleader for trade workers, Mike Rowe, \"Not all knowledge comes from college, but not all skills come from degrees\" This popular piece of wisdom indeed rings true both when describing jobs available to students after high school as well as those graduates seek after college How to better prepare our youth for the new workplace has recently taken on greater urgency among educators and policymakers Across the country high schools are providing different program -such as career pathways and certifications -to acquaint teenagers with workplace demands Yet we seem to be short on a potentially effective strategy -apprenticeships -and how students may benefit from such programs The purpose of our report is to provide an overview of apprenticeships in the United States, from the following four dimensions: I What does the 21st century apprenticeship look like in the United States? II What does data from the Program for the International Assessment of Adult Competencies (PIAAC) say about apprenticeships in the U S ? III What are the challenges that educators, both international and national, are facing to expand apprenticeship programs? IV What perspectives should school districts consider when developing apprenticeship programs? The concept of apprenticeship remains a practical and effective method of teaching a trade to young people, but technology in the 21 st century has changed what apprentices learn and do during the apprenticeship program In this section, we provide some contextual information pertaining to apprenticeships in the United States Our reviews focus on apprenticeship-related policies, national trend of apprenticeships, youth apprenticeship, federal funding system that supports apprenticeships in K-12 education, and apprenticeship information management at federal and state level A brief review of apprenticeship-related policies • Historically, the boom or decline of apprenticeship programs has been related to industry needs and economic change • The current national goal for expanding apprenticeship programs is not only to prepare workers to fill both existing and newly created jobs, but also to prepare workers for the jobs of the future • Both previous and current administrations have stressed the need to expand access, equity, and career awareness when promoting, funding and regulating apprenticeship programs',\n       'To date, there is no definitive FDA-approved antiviral-drug for the COVID-19treatment. Besides, most of the treatment strategies concentrate only on symptomatic management and supportive therapy [98] .Various research organizations are tirelessly working hard to evaluate multiple compounds that can inhibit the spread of SARS-CoV-2 in humans. Nevertheless, these efforts are tedious and involve a meticulously extensive process. Thus, in addition to experimental work, specialized computational resources focusing on antiviral compounds/peptides can be utilized to evaluate the identified antivirals and repurpose them against single agents or in combinations to effectively control the spread of the virus [98, 99] .',\n       'Standard errors for text table 6: Percentage distribution of 1989-90 beginning students according to their highest degree attained or enrollment status in 1994, by persistence or departure status in 1989-90 and first institution attended  Stopout path for 1989-90 beginning students in 4-year institutions and outcomes in 1994 for first-year persisters and stopouts',\n       'Academic activities are more and more diverse It is probably an oversimplification to say that in the past academic tasks meant two main tasks: teaching and research. The combination of these two activities allowed to categorise faculty members on an axis starting with those teaching only, continuing with those mixing both teaching and research and ending with those mainly involved in research. Two principles of differentiation organised the academic profession: one separating academics according to their main activities (research or teaching); another drawing territories around the different tribes (Becher 1989) constituted by the disciplines and sub-disciplines. An in-depth investigation of academic work would probably have shown that many academics were already engaged in many other activities. It is at least what can be deduced from reading biographies of Pasteur by B. Latour (2001), Nash by S. Nasar (1998) or autobiographies (Friedel 1994, Mendras 1995, Crozier 2002: whatever the period concerned, they were all engaged in a multiplicity of activities. This confirms the importance of what Latour (1987) and Callon (1989) would describe as the building of socio-technical networks, for their careers and in turn for their scientific reputation, today and in the past. Thus, even if one could distinguish, as Bourdieu (1984Bourdieu ( -1988, between two types of careers 2 the core activities were teaching in classes and publishing results in academic journals. Other activities were necessary but were not expected and were not explicitly rewarded. Today, this is no more the case. Writing proposals, developing contracts, elaborating e-learning programmes, being engaged in technology transfers, etc. are part of the tasks achieved by faculty members nowadays and they are no longer considered as peripheral, not compelling and secondary, but recognised as important aspects of academic work. Academics are expected to make these endeavours in order to gain scientific reward. In Germany and in the USA for instance, the ability to raise money and to manage research projects based on external funding is one of the criteria of judgment when hiring professors (Musselin 2005b). It is no longer something academics can do: it is something they must do. For example, scientific productivity (in terms of number and impact of publications) is of course one of the explicit criteria expressed by the direction of the INRA (a French national research institution in agronomy) in order to be promoted from the corps of chargés de recherche (tenured research fellow) to the corps of directeur de recherche (senior research fellow). But management competencies are as important as the scientific profile: being responsible for a small research team within a lab, leading a European research project, being in charge of contracts are necessary in order to have a chance for promotion (Carrère et al. 2006). This diversification of tasks also holds true for teaching. Activities around teaching have evolved and represent a larger scope of tasks nowadays. Giving a class and supervising doctoral students are only one part of the training work. Teaching engineering, designing e-learning programmes, finding internships for students, for instance, also belong to \"teaching\" today. Furthermore, new missions (or the so called \"third mission\") are emerging. They include links with regional, national or international bodies and decision makers, interaction between scientists and the public at large and involvement in public debates, public expertise, support to public policy at large, etc. Such tasks also participate in the diversification of academic activities.',\n       \"..=.44, 2.4.;61,rvatmfr,41 A Profile of the American High School Sophomore in 1990 21. Scott, L.A., Rock, D.A., Pollack, J.M., and Ingels, S.J. Two Years Later: Cognitive Gains and School Transitions of NELS:88 Eighth Graders, 1994, NCES 94-436. This report describes the growth in cognitive Skills and achievement, and the continuities and discontinuities experienced in school and at home by the NELS:88 eighth-grade cohort during the two years between the study's base year (1988) and first follow-up (' 990) surveys. Four distinct topics are addressed, involving both school dropouts and persisters. (1) By 1990(1) By , some 1988 eighth graders were dropouts; this report describes their characteristics and the reasons they gave for dropping out of school. 2This report presents findings on patterns of school transition changing from a public eighth-grade school to a private high school or vice versa--and the changes in perception of safety and overall learning environment cohort members experienced after moving from a typically more homogeneous middle school environment to a more heterogeneous high school environment. 3Additionally, this report summarizes major changes in home life and family, such as the divorce or remarriage of a parent, that also occurred during cohort members' transition to and/or early years of high school. (4) Finally, this report examines the 1988-90 achievement gain of the eighth-grade cohort, thus addressing several basic questions: How much did students gain in achievement in the two years following eighth grade?; Who gained, in what subjects, and (for mathematics) in what way (that is, at what skill or proficiency level)? The analysis of growth in achievement illustrates use of the NELS:88 continuous measure of probability of mathematics proficiency (see Rock, Owings and Lee [1994, entry 15] for an illustration of mathematics achievement gain analysis using NELS:88 dichotomous proficiency scores). 22. Green, P.J., Dugoni, B.L., and Ingels, S.J. Trends Among High School Seniors, 1972. NCES, forthcoming, 1994NCES 94-380. This report compares the NLS-72 1972, HS&B 1980, and NELS:88 1992 senior cohorts. It supplies a sociodemographic description of the three senior cohorts. The report compares the cohorts' high school program placement, course-taking and achievement, as well as participation in extracurricular activities. It also compares 1972, 1980 and 1992 seniors' plans for the next year, noting the proportions who planned to work full-time in the year following graduation, the type of postsecondary institution seniors planned to attend, college selection, and major field of study. Finally, the report compares the future educational and occupational aspirations of the three senior cohorts. \",\n       'There was considerable variation among tenth graders in the extent to which they saw themselves as being religious and the extent to which they took part in church services. Most, almost three-fourths of the tenth graders, thought of themselves as being at least a somewhat religious person. About two-fifths of the tenth graders said they attend church services at least twice a month. One-fifth said they never attended services. Most, nine out of ten, indicated some kind of religious affiliation. The largest percentages were 25% Catholic and 23% Baptist. Only one in ten said that they thought their friends believed religidus participation was very important. Four in ten said their friends thought it not important. Overall, over a third of the comparisons, 35%, showed substantial differences. The percent of the 11 comparisons per variable that yielded differences of ten percent or more when variables were examined were as follows: family type, 73%; ethnicity, 64%; socio-economic status, 45%; and region, 55%. Sex of students and urbanicity did not show any substantial differences.',\n       \"Phase 3 of the Agricultural Resource Management Survey (ARMS) is the one of the longest and most detailed sample survey data collections conducted by the US Department of Agriculture's National Agricultural Statistics Service (NASS). For this survey, NASS collects highly detailed economic data covering a calendar year from agricultural producers nationwide. Previous research with a self-administered mail-out/mail-back version of the ARMS questionnaire (Beckler, Ott and Horvath, 2005) showed benefits of indirect monetary incentives ($20 ATM cards). Face-to-face interviews are used to collect data from the Costs and Returns Report (CRR) sample of the ARMS Phase 3. For 2005, use of ATM cards was continued as standard practice with the mail-out/mail-back Core form sample and an incentive experiment was conducted comparing several types of incentives provided to ARMS Phase 3 CRR sampled operations. Five treatment groups (including a control group) in the CRR sample were used for this incentive experiment. All experimental groups received a pre-survey letter and were then contacted for a face-to-face interview. Treatment groups received either 1) a standard pre-survey letter with no incentive, 2) pre-survey letter with a prepaid indirect cash incentive (in the form of $20 automated teller machine (ATM) cards), 3) pre-survey letter with a promised individual financial profile comparing the operation with aggregated information about other operations, 4) presurvey letter with a NASS logo wall clock or 5) pre-survey letter with a pre-paid $20 ATM card and a promised individual financial profile. The entire ARMS Core version was provided with a $20 incentive but was not included in this experiment. Information for the Core form sample is provided along with the experimental groups. Response rates for the CRR incentive groups were slightly higher than the control group. However, the differences were not significantly different, with response rates for all groups in the low 70 percents. Each incentive tested required an additional cost for the incentives themselves and also for processing and delivery of the incentive. Used as tested, these incentives do not appear to be effective in increasing response rates on the face-to-face ARMS CRR sample. This is contrary to our prior evaluation of their use with the mail-back Core Version of the form where response rates were increased. For the Core form sample, incentives raised response rates and were also cost effective since they increased mail response and reduced costs from face-to-face interviews. In addition, the number of cards cashed was low, and the Core Version non-respondents cashed their ATM cards at a much lower rate than respondents (both in the previous research and in the current year). In the current research, Phase 3 CRR interview respondents and non-respondents cashed the ATM cards at comparable rates. This suggests that incentives, while increasing costs, are of limited effectiveness when combined with a face-to-face interview mode. Careful consideration should be made regarding how incentives are used in the future as they do not appear to be universally effective.\",\n       'One-third of the global population is currently subjected to social distancing measures to slow the spread of the severe acute respiratory syndrome coronavirus (SARS-CoV)-2 disease (COVID). The COVID-19 pandemic is not only having profound effects on healthcare systems but also on global economies, world trade, tourism, and social restrictions. These restrictions are directly impacting mental health [1, 2] , food security [3] , food waste [4] , purchasing behaviors [5] , and physical activities [6] .\\nA joint statement on COVID-19 impacts on food security and nutrition was recently released by the Food and Agriculture Organization of the United Nations (FAO), the International Fund for Agricultural Development (IFAD), the World Bank, and the World Food Program (WFP) on the occasion of the Extraordinary G20 Agriculture Minister\\'s Meeting, which concluded that the \"pandemic is already affecting the entire food system and collective action is needed to ensure that markets are well-functioning\" [7] . In addition, a joint statement on nutrition in the context of the COVID-19 pandemic in Asia and the Pacific by the FAO, WFP, World Health Organization (WHO), and United Nations Children\\'s Fund (UNICEF), also emphasized the importance of healthy diets, micronutrient supplementation, and nutrition surveillance especially among those most affected, such as the poor and physically vulnerable [8] . Maintaining healthy dietary and lifestyle behaviors during the COVID-19 pandemic is important for combating viral infections and maintaining mental health and well-being [9] .\\nGlobally, an estimated 3.4 billion people have access to the internet, and online information has grown in popularity since 1990 [10] . The accessibility of the internet and the rise of social media have affected our social lives and also our dietary and lifestyle behaviors [11, 12] . The internet provides immediate access to an enormous amount of information, and infodemiology has been used to assess human behaviors related to the COVID-19 pandemic [13] . Google Trends is the most popular tool to gather information on web-based behaviors, and it can be used to predict or prevent health-related issues [14] . Currently, limited data are available as to how the COVID-19 pandemic is affecting our dietary and lifestyle-related behaviors at the global level. To address this shortcoming, we used Google Trends to analyze relevant keywords related to these topics.',\n       nan,\n       'Mosquitoes have four life stages: egg, larva, pupa and adult. With the exception of extreme conditions, all stages of mosquito maturation occur more rapidly with warmer temperatures. Mosquitoes are unable to regulate their body temperature and, thus, are dependent on the temperature of their surroundings for warmth and growth. The concept of growing degree-days for mosquito population forecasting involves the amount of accumulated heat required for mosquitoes to complete their development from one stage in their life cycle to another. This measure of accumulated heat for development is known as physiological time [41] . Growing degree-days are essentially heat units based on the product of excess temperature (in degrees) beyond the base value and its length (in days). The conditions that are required to complete each life stage of C. tarsalis and C. pipiens, obtained from existing studies, are summarized in Table 2 .\\nStudies on mortality show that larva, pupa and adult mortality is similarly temperature-dependent. For example, the estimated mortality curve provided by Eisenberg et al. [35] , a summary of existing studies, gives a minimum mortality rate of 0.35 at 20°C. Generally, each life stage has an optimal temperature at which mortality is minimized, and mortality increases as the ambient temperature departs from the optimal temperature. One exception is the egg stage, wherein mortality is influenced more by density than by temperature [35] . In nature, however, the predominant factor for mosquito mortality is predation. In a study from California by Reisen et al. [42] , it was observed that predators are the greatest cause of mosquito mortality: 60-85% of immature C. tarsalis mortality was due to predation, while abiotic factors and lack of food resulted in only around 20% mortality. In our model, mortality of C. tarsalis due to predation and lack of food is estimated based on the results of Reisen et al. [42] . This is 89%, 86% and 73% from January until June, during July/August and during September/October, respectively. For C. pipiens, a fixed mortality value of 98%, derived by calculating a best-fitting line to the BCCDC observations, was used since there were no existing studies on mortality caused by predators and lack of food for this species.\\nAnother important factor that influences mosquito abundance is the seasonal change in oviposition (egg laying) behavior that ultimately influences abundance. Mosquitoes have a biological mechanism that increases their procreative power in the early summer. Bennington et al. [43] found that the percentage of engorged and gravid C. tarsalis females drops rapidly in May, and then stabilizes at around 25% throughout the summer before eventually decreasing to zero at the end of summer (they are fat with stored energy at this point preparing to overwinter). This leads to greater emergence during the summer months.\\nFuthermore, Madder et al. [44] found that temperature, density and day length influence the percentage of adult C. pipiens females in diapause (a period of suspended or dormant development, characterized by inactivity and decreased metabolism). Thus, temperature, density and day length also influence mosquito activity.\\nThe date at which the growing degree-days calculations start is also important for the model. Bennington et al. [45] reported that the first female C. tarsalis in 1954 was observed on Apr. 6 after emerging from hibernation when the soil temperature at a depth of 1-2 m was around 7°C. Due to the absence of soil temperature data, we assumed that a three day average of average daily temperature above 7.0°C is sufficient to break hibernation and initiate the calculation.\\nAn adult female can have multiple gonotrophic cycles. Burdick and Kardos [46] observed the numbers of female C. tarsalis, after overwintering, that experienced oviposition. At most four cycles were observed-no mosquitoes survived for a fifth oviposition, and the average number of gonotrophic cycles was 1.10, which is the value used in our model. Tamarina [47] observed an average of 4.08 gonotrophic cycles with 105 eggs per raft for C. pipiens (observed until one-half of the sample population died). In our model, which considers the oviposition of survivors, 5.00 is used as a reasonable number of gonotrophic cycles for C. pipiens. Also, all eggs (calculated as a product of the average number of gonotrophic cycles and average eggs per raft) from a female adult are laid as soon as the condition for the first oviposition is fulfilled. Daily mortality is not used in the model explicitly since the concept of gonotrophic cycle numbers includes the mortality of adults. However, in determining the total number of active adult mosquitoes, a temperature-dependent daily mortality variable for adults is applied.\\nAll of the parameters used in our model are summarized in Table 1 . Note, however, that some of the parameters required to model C. pipiens do not exist in the literature, so values derived from C. tarsalis studies are substituted.\\nIn the following analysis, the daily mean temperature is used in the calculation of degree-days. Also note that in some studies, the temperatures reported were the temperature of the water -in our model we assumed that the average air temperature was an adequate surrogate.',\n       'The characterization of spatial distribution and temporal progression of amyloid deposits and neuronal hypometabolism is essential for elucidating pathological mechanisms that underlie AD. This study based on the analysis of cross-sectionally modelled disease stages tends to support the early and diffuse amyloid accumulation and later glucose metabolism alteration in AD natural history. It suggests that, at least over a given time range in the evolution from the prodromal state to the demented state, amyloidopathy and hypometabolism evolve concomitantly in specific cortical areas (temporal cortex, precuneus, and posterior cingulate). In a prognostic perspective, amyloid PET profile appears as the main predictor of subsequent cognitive decline and AD conversion in cognitively impaired subjects. In subjects with a normal amyloid profile, FDG profile allows to detect about two thirds of amyloid PET false negatives. When forecasting AD conversion using binary linear classification, the optimal predictive accuracy is obtained by combining baseline cognitive status, regional amyloid SUVR, and regional FDG SUVR.',\n       'JAMES M. NOVAK, KAREN F. GAINES, JAMES C. CUMBEE, JR., GARY L. MILLS, ALEJANDRO RODRIGUEZ-NAVARRO, AND CHRISTOPHER S. ROMANEK Abstract. Clapper Rails (Rallus longirostris) can potentially serve as an indicator species of estuarinemarsh health because of their strong site fi delity and predictable diet consisting predominantly of benthic organisms. These feeding habits increase the likelihood of individuals accumulating significant amounts of contaminants associated with coastal sediments. Moreover, since Clapper Rails are threatened in most of their western range, additional study of the effects of potential toxins on these birds is essential to conservation programs for this species. Here we present techniques (DNA strand breakage, eggshell structure, and human-consumption risk) that can be used to quantify detrimental effects to Clapper Rails exposed to multiple contaminants in disturbed ecosystems as well as humans who may eat them. Adult birds collected near a site contaminated with polychlorinated biphenyls (PCBs) and metals in Brunswick, Georgia had a high degree of strand breakage, while those collected from a nearby reference area had no strand breakage. Although, results showed that eggshell integrity was compromised in eggs from the contaminated sites, these results were more diffuse, reemphasizing that multiple endpoints should be used in ecological assessments. This study also shows that techniques such as eggshell integrity on hatched eggs and DNA strand breakage in adults can be used as non-lethal mechanisms to monitor the population health of more threatened populations such as those in the western US. We also present results from human-based risk assessment for PCBs as a third toxicological endpoint, since these species are hunted and consumed by the public in the southeastern US. Using standard human-risk thresholds, we show a potential risk to hunters who consume Clapper Rails shot near the contaminated site from PCBs because of the additional lifetime cancer risk associated with that consumption. Key Words: Clapper Rail, DNA strand breakage, eggshell integrity, indicator species, metals, polychlorinated biphenyl, Rallus longirostris.',\n       \"Autosomal and X-chromosome linkage analyses were performed using Merlin [15] and included parametric two-point affected individuals-only and age-dependent penetrance models and a nonparametric multipoint analysis. Parametric multipoint analysis was performed on significant overlapping regions between the families in this report and a companion analysis in Hispanics (Barral et al. [16] ). The package MINX (Merlin in X) was used for analysis of X-chromosome SNPs. Heterogeneity LOD (HLOD) models were applied to the two-point analyses to allow for detection of linkage in the presence of locus heterogeneity [17] . Whittemore and Halpern NPL-pair and NPL-all statistics [18] and Kong and Cox linear model logarithm of odds (LOD*) scores [19] were calculated for the nonparametric multipoint analysis.\\nPower analyses using SIMLINK [20] on the 41 families in the linkage analysis, with a dominant model and disease allele frequency of 0.001, showed we have .80% power to generate an LOD .3 for a fully informative (a 5 1) age penetrance model with marker locus allele frequencies equal to 0.2 (MAXLOD 5 5.62) and 0.4 (MAXLOD 5 6.86). Using these same paramaters, the affected individuals-only model has .80% power to generate an LOD .2 with a marker locus frequency of 0.4 (MAXLOD 5 3.61) and 0.71% power to produce an LOD .2 with a marker locus frequency of 0.2 (MAXLOD 5 3.00). Using a heterogeneity model (a 5 0.5) reduced power to generate an LOD .2 to 41% and 18% for the age-penentrance and affected individualsonly models, respectively (Supplementary Table 1) .\\nParameters for the parametric two-point models assumed dominant inheritance, a disease allele frequency of 0.001 and penetrances of 0.01, 0.90, and 0.90 (representing NN, NA, AA genotypes, respectively). Age-dependent penetrances used in the analysis are listed in Supplementary  Table 2 . Two-point parametric analysis used all SNPs for each of the analyses. The nonparametric multipoint scan included a linkage disequilibrium (LD)-pruned set of 119,555 SNPs common to all genotype platforms. LD pruning was done using the independent pairwise LD pruning option in PLINK (default settings). Mean distance between markers for the set of nonparametric multipoint markers is 4.55 cM. As some pedigrees were too large for MERLIN to perform nonparametric linkage analysis, uninformative family members (based on an individual's position in the pedigree and/or absence of genotyping) were trimmed before performing analyses using the program PowerTrim [21] . Allele frequencies for all SNPs were based on CEU HapMap data [22] .\\nA significance threshold of HLOD 3.5 was set for the parametric two-point linkage scans taking into account testing of two separate parametric models. This is above the Lander and Kruglyak [23] recommendations for significance (LOD 3.3; P value 5 4.9 ! 10\",\n       'Brought to you by | Iowa State University Authenticated Download Date | 8/23/18 3:41 PM arise, for example, if a producer intending to operate on a fully integrated basis finds that random fluctuations in death loss rates lead to feeder pig output that sometimes exceeds and sometimes falls short of the capacity of the farm\\'s feeder-to-finish facility. Maintaining nearcapacity operation might then require both purchases and sales of feeder pigs at different times in a single year. Only one of the sampled producers displayed this kind of operation for the sample year and it was excluded from estimation. 11 The survey reported, for each farm, whether it was involved in each of four stages of production: breeding and gestating, farrowing, nursery, and growing/finishing. So, for example, a farm that reported sales of market hogs but did not claim involvement in the \"growing/finishing\" stage of production was considered to be inconsistent and was not used in estimation. 12 The breakdown of the final sample of 744 operations is as follows: fully integrated -31 %, completely specialized farrow-to-feeder -8 %, completely specialized feeder-to-finish -51 %, partially integrated backward -3 %, partially integrated forward -7 %. 13 Equation 8gives costs for active operations with > 0 and/or > 0. With = = 0, cost is zero. 14 The ARMS survey solicited information about the period of operation corresponding to calendar year 2004. Thus the cost and output variables are totals for this year. 15 The construction of these variables is detailed in the Appendix. 16 Our feed price variable, , in dollars per hundred-weight, was determined by dividing total feed cost (\"COSTFEED\" in the ARMS dataset) by the total hundred weight of feed fed (\"CWTFEED\"). Feed cost included the opportunity cost of homegrown feed as well as expenditure on purchased feeds. Opportunity costs of homegrown feed were based on market price data from secondary sources. Our price of labor, , in dollars per hour, is ARMS\\' unpaid wage rate (\"UWAGE\") variable which estimates the opportunity cost of operator-supplied labor using wage data from secondary sources. 17 The first term in the incremental cost function introduces a discontinuity at = 0 to account for the fact that cost is zero when no output is produced at either stage. Thus, when no second-stage output is produced, the incremental cost function for the farrow-to-feeder stage is 1 ( , 0) = 0 + 1 + ( 3 + 8 + 10 ) ⋅ + 5',\n       'Invertebrate communities within freshwater wetlands change with the changing water level. During periodic droughts isopods dominate, but as water level rises and subsequent emergent vegetation surfaces, amphipods, chironomid larvae, and other insect larvae dominate. As water levels continue to rise, emergent vegetation gives way to floating aquatic plants and copepods dominate the marsh system (Craft, 2001). Major consumers found in freshwater marshes typically include the Virginia Rail (Rallus limicola) and King Rail (Rallus elegans) (Tunnell et al., 1996), while alligator gar (Atractosteus spatula) and common carp (Cyprinus carpio) are among the dominant freshwater fishes (TPWD, 2009).',\n       'Complete case data were captured and edited under the three separate data collection modes for the 2006 SDR. A computer assisted data-entry system was used to process the SAQ paper forms. In contrast, the CATI system, including an additional CATI instrument used to collect critical-item follow-up data, and the Web survey had internal editing controls. Mail questionnaire data and Web-based returns were reviewed for any missing critical items (working status, job code, or resident status in United States). Telephone callbacks were initiated to obtain this information, in order to consider the response complete. All completed CATI responses included critical items. After receipt of this information, data from the three separate modes were merged into a single database for all subsequent coding, editing, and cleaning. Following established SESTAT guidelines, staff were trained in conducting a standardized review and coding of occupation and education information, \"Other/Specify\" verbatim responses, state and country geographical information, and postsecondary institution information. For standardized coding of occupation, the respondent\\'s occupational data were reviewed along with other work-related data from the questionnaire by specially trained coders to correct known respondent self-reporting problems to obtain the best occupation codes. The assignment of an education code for a newly earned degree was based solely on the verbatim response for degree field.',\n       'This appendix contains proofs of the theoretical results discussed in section II.',\n       'It is seductive to judge these purposes of assessment as being inherently positive or negative and even independent of one another. In reality, each of these three basic purposes of assessment serves an essential role in a well-functioning education system and must be balanced with the other purposes. The over-emphasis, under-emphasis, or absence of assessment for any of these purposes may negatively impact the overall quality of education. This paper does not prescribe a specific balance of resource allocation to the three purposes but aims to sensitize stakeholders at all levels to the considerations necessary in the three purposes. The manner of distribution of resources will be determined by the needs of the specific education system and the level at which decisions must be made. By way of illustration, consider the government official who might not consider the value of formative assessment. Through an understanding of the assessment purpose triangle, this stakeholder will be better equipped to make informed decisions about how the allocation of resources to assessment for accountability, reduces the availability of resources for other assessment purposes. In the same manner, an educator may be sensitized to the importance of accountability practices and incorporate time and resources in classroom planning to ensure this requirement is met.\\nThis required balance can be conceptualized as the assessment purpose triangle shown in Figure 1 below. This is an adaptation of the project management Triple Constraints model, or project management Iron Triangle which is built on the premise of resource scarcity (Atkinson, 1999) . This article is therefore based on the premise that assessment for any purpose takes place in an environment of scarcity in which all resources must be shared. If external or internal motivators skew the emphasis to focus on one, or two of the purposes of assessments, the other will have to sacrifice importance and resources. The affected purpose of assessment may be neglected to the detriment of the overall systemic educational quality if this does not serve the current needs of the particular system (Brown and Harris, 2016) .\\nThis graphical aid of the assessment purpose triangle depicts each of the basic purposes of assessment on opposing sides: assessment to support learning; assessment for accountability; assessment for certification, progress, and transfer. This positioning is important because, while each purpose of assessment may contribute to the quality of education, the resources such as educator time, student time, marking load, administrative burden, and technology employed to operationalize these purposes belong to a shared pool. An over-emphasis of any one of the purposes of assessment will affect the other sides by diverting resources from one or both of the other essential assessment functions, thereby adversely influencing the quality of education. Such an outcome is not purposefully planned, but comes about owing to a differential need within the system (see Figure 2) . Nevertheless, educational quality requires attention to each assessment purpose and an equitable distribution of assessment resources.',\n       'The measurement model consisted of seven variables, i.e., the socioeconomic impact of student families, student truancy, years of experience, administrative support, teacher autonomy, and the intentions of math and science teachers to remain in the profession, and the 24 items comprising those variables. Table 2 shows what items comprised each variable and their respective pattern coefficients.\\nThe disturbances and measurement errors for each variable were assigned a scale using a unit loading index that fixed the residual path coefficient of one of the items to one. SOCIO3, TRUANT3, ADMIN5, AUTO2, SALARY1, and INTENT2 were used as reference indicators in this model. A pair of items in teacher autonomy, i.e., AUTO1 and AUTO2, was allowed to estimate error covariance. In addition, the error variance for years of experience was set to zero (i.e., its standardized factor loading was equal to one).\\nOverall, the goodness-of-fit indices for the measurement model were evaluated to show a sufficiently reasonable fit, χ 2 (df = 231, N = 6588) = 986.42, p < 0.001; χ 2 /df = 4.27, and RMSEA = .022. As shown in Table 3 , the estimated correlations among variables varied widely from -.04 to .87, most of which were statistically significant.',\n       'The use of technology to aide instruction is an increasingly viable option for educators, as computer use becomes more prevalent in classrooms. For educators charged with improving the skills of struggling adolescent readers, who often lack motivation to improve their skills, the use of technology in instruction holds promise as a possible motivating strategy. While use of technology in the classroom alone is sometimes motivating and reinforcing, teachers should keep in mind that the most effective technologies are those that allow students to have more opportunities to practice new and targeted skills and those that allow teachers to more easily customize instruction to meet individual learning needs (Biancarosa, 2005) .\\nThe Peabody Literacy Lab is one example of a technology-based intervention focused on multiple components of effective reading instruction that has been linked to positive results for adolescents (Hasselbring & Goin, 2004) . This program combines comprehension instruction with wordlevel decoding instruction and opportunities to practice spelling. The program allows for progress monitoring and makes adjustments to each student\\'s content based on the results of that progress monitoring. This kind of real-time tracking and adjustment to instruction is a further advantage of incorporating technology into reading instruction.\\nModeling fluent reading is another support that computer programs have the capacity to provide for students during practice. One-on-one modeling is time consuming and impractical for teachers; however, computer programs with speech features are a helpful option for offering individual attention to students (Hasselbring & Goin, 2004) .\\nAnother benefit that technology can bring to reading instruction is providing support to students through hypertext. Hypertext allows students to access additional individualized \"on-demand\" support through pop-ups that can include definitions and pronunciations of words, background information on new concepts, and, if the software allows, teacher comments and questions. Use of these enhanced hypertext features has been linked to positive effects on secondary students\\' comprehension of expository text (MacArthur & Haynes, 1995) .',\n       'In many under-resourced schools in South Africa, learners are failing to meet the required academic standards, according to the Trends in International Mathematics and Science Study (TIMSS) and the Progress in International Reading Literacy Study (PIRLS), as well as in terms of matriculation results (Mullis, Martin, Kennedy & Foy, 2007) . The South African Department of Basic Education (DBE) tracks learner performance through the Annual National Assessment (ANA), the results of which indicate that learner performance is unacceptably low (DBE, Republic of South Africa, 2016) , at least for the poorest and most disadvantaged children. Although the national pass rate was 70.7% in 2015 (DBE, Republic of South Africa, 2015) , the results were skewed in favour of better-resourced schools, which are not financially, geographically or linguistically accessible to the majority of children in South Africa.\\nOne of the reasons learners are performing so poorly is that teachers are struggling, for various reasons, to provide a quality teaching and learning experience (Van der Berg, 2008) . The difficulties of working in socioeconomically challenged communities, poor initial teacher preparation, lack of ongoing professional development opportunities, and poor school infrastructure all contribute to poor teacher performance, and increase the need for ongoing support and development of teachers through effective instructional leadership (Spaull, 2013; Wood & Olivier, 2008) . Although the literature overwhelmingly identifies the principal as the main instructional leader, and local research has suggested that schools where principals teach do show better academic results (Roberts & Roach, 2006) , the majority of principals in so-called \"township\" and rural schools are too occupied with the daily challenges of just keeping their schools functioning, to fulfil the role of main instructional leader. In such cases, it makes more sense for them to delegate their task to heads of department.\\nAccording to policy guidelines (Department of Education, Republic of South Africa, 2000 Africa, , 2002 , heads of department (HODs) are positioned as instructional leaders. They should (i) assist teachers in setting and achieving personal and professional goals related to improvement of school instruction, and should monitor that these goals are successfully achieved; (ii) do regular formal and informal classroom observations; (iii) do postclassroom observation conferences with teachers, with the focus on improving instruction; and (iv) provide constructive critical evaluations, making recommendations for personal and professional growth goals according to individual needs (McEwan, 2003) . However, in the first cycle of this larger action research project, where data was generated from two under-resourced schools, we found that involvement of the participating HODs in instruction tended to be limited to acting as \"final checkers\" of teachers\\' reports of work covered, where they adopted a task-oriented management role, rather than working with teachers on an ongoing basis to improve instruction (Seobi, 2016) . We found that the HODs struggled to interpret the prescriptions of what they should do, and to translate these prescriptions into a coherent and sustainable framework for instructional support. We also found that they adopted a hierarchical, transactional leadership style, which did not foster the trusting relationships necessary for effective mentoring, coaching, and teamwork, which are essential for the provision of quality instructional support (Wood, Seobi, Setlhare-Meltor & Waddington, 2015; Zuber-Skerritt, Wood & Louw, 2015) . The findings of the first cycle of this research project clearly pointed to the need to explore ways to help the HODs to reflect on and improve their instructional leadership practices. Since the HODs would need to sustain any improvement, it was imperative that they own the process and take responsibility for ongoing development of their practice. We therefore engaged them in a participatory process, to enable them to work as a team to develop an instructional support framework that suited their particular school ethos and context. As Bush, Glover, Bischoff, Moloi, Heystek and Joubert (2006) point out, South African education needs theories of leadership that are relevant to the context of the country. Any such theory needs to take into consideration the challenging contexts in which teachers work, and needs to be able to build human capacity, despite the sociohistorical disadvantages that still impact so negatively on teaching and learning.\\nThe question that guided the process in the cycle that we now report on was \\'how can heads of department in under-resourced schools improve their instructional leadership practices?\\' We first explain the theory that helped us to facilitate and make sense of the emerging process, before outlining the methodology used to explore the research question. Since the process of instructional leadership development is the focus of the study, we then offer a step-by-step explanation of how participating HODs came to learn how they could collaborate so as to improve the quality of teaching and learning at their school. We conclude with some suggestions on how the learning gained from this study help develop a more context-specific theory of how to develop instructional leadership. Although based on a South African case study, the findings of this research could be applicable for education globally -whether in emergent or developed economies -since they provide general knowledge about how to improve instructional leadership practice.',\n       'During the ARMS 2012 data collection period, field offices were asked to offer incentives to enumerators for each record they completed from any of the impact groups in the treatment sample. Enumerators were to be told of this incentive before the data collection period. Only those states that had records in the treatment group were given the money to provide the incentives. Only about half of the states used the incentives as planned; twelve offices used the incentives as planned while fourteen did not. In most of the states that did not use the incentive, office staff and supervisory enumerators said that they did not want to recognize individuals, but wanted to recognize the group effort of the survey data collection. Since only half of all field offices used the enumerator incentives, we cannot statistically test the comparison of overall response rates for the treatment and control group across the whole experimental sample. However, we can provide some descriptive information that may inform NASS moving forward with the enumerator incentives. We can provide summary information about the response rates for the treatment and control groups, but we have to subdivide the sample by those offices that used the incentives and those that did not. In addition, because response rates differ by state, we cannot directly compare the actual response rates. Instead, we compare the difference between the response rates for the control and treatment groups for those states that used the incentives and those states that did not use them. Table 5 shows the differences in response rates for the treatment and control group for offices that used the incentives and those that did not. Since the sample was not selected in a way that differentiates field offices that use/do not use incentives, we cannot provide any statistical test. However, we can see that the difference in the response rates between the treatment and control groups for the states that used the incentive is higher than the difference between the treatment and control groups in the states that did not use the incentives. It could be that the use of the enumerator incentives did help achieve slightly higher response rates. The use of enumerator incentives may have been inconsistent because NASS has not typically used enumerator incentives in this way in the past. As with the data collection procedures, RDD staff worked with the headquarters survey team to develop the procedures for the enumerator incentives, but had less direct contact with the field office staff who administered the incentives. Since we offered enumerators additional money to complete certain cases, we conducted a small quality control callback operation to confirm that these enumerators actually collected the survey data from respondents. We found no reason to expect falsification. The procedures used and results from the quality control callback operation are in Appendix D.',\n       'CSF biomarkers may be valuable in clinical trials in at least four different ways: as diagnostic markers, for patient stratification, as safety markers and to detect and monitor biochemical drug effects (Table 3 ). The first generation of MCI clinical drug trials, such as the donepzil and vitamin E trials , recruited unselected heterogeneous MCI cases, meaning, that probably around half of the cases did not have AD-type neurodegeneration. This may have seriously reduced the ability to identify potential efficacy of a drug candidate. There could be reduced costs and numbers of recruited subjects in future trials that are enriched and stratified for MCI subjects using clinically meaningful CSF biomarkers (Hansson et al., 2006) .\\nSince AD is a disorder with a slow progression of symptoms, identification of a change in the slope of deterioration due to intervention with a disease-modifying drug candidate will require very large patient cohorts and treatment duration of several years. Small, short-term clinical trials may be valuable to verify a biochemical effect also in patients with AD, before the expensive and time-consuming step is taken to larger phase II or III clinical trials. A summary of biomarkers as surrogate measures for treatment effects on Aβ-related mechanisms is presented in Table 4 .\\nPresently, it is uncertain how the Aβ1-42 concentration in CSF might respond to treatment with efficacious drugs that target pathways leading to Aβ, production, fibrillization and/or amyloidosis in man (Gilman et al., 2005; Siemers et al., 2006) . Studies in transgenic mice, however, provide evidence that reduced CSF Aβ1-42 levels are to be expected for short-term treatment with inhibitors of γ-secretase (Lanz et al., 2003 (Lanz et al., , 2004 . Similar results have recently been seen in a phase IIa study of the Aβ clearance-enhancing compound PBT2 . Based on longitudinal studies of conditions involving acute neuronal injury (Hesse et al., 2001; Zetterberg et al., 2006) and data from the interrupted phase IIa AN1792 trial (Gilman et al., 2005) , t-tau should decrease towards normal levels if a treatment is successful in inhibiting the neurodegenerative process in AD. The same may be expected for ptau, although there are still no studies backing this hypothesis. The usefulness of other Aβ-related biomarkers, e.g., BACE1 activity, as biomarkers for treatment efficacy remains to be investigated. Nevertheless, the low intra-individual variability of CSF tau proteins and Aβ42 in 6-month and 2-year studies of AD and MCI patients is an important prerequisite for the use of these biomarkers that may reflect the effects of disease-modifying AD therapies in clinical trials Zetterberg et al., 2007) .\\nBesides the use of CSF biomarkers to identify and monitor the biochemical effects of a disease-modifying drug, they may also be valuable tools for safety monitoring in trials with drugs with potential serious side effects, such as immunotherapy. The phase IIa AN1792 trial was interrupted since 6% of cases developed meningoencephalitis (Orgogozo et al., 2003) . In routine clinical practice, CSF analysis is the standard method to diagnose encephalitis. Typical findings are an increase in CSF mononuclear cells together with signs of blood-brain barrier damage and intrathecal immunoglobulin production (Table 5) . CSF may thus be a valuable tool as safety measures in this type of trials.',\n       'Students attending less-than-4-year, for-profit institutions in 1995-96 primarily were white (58 percent), age 23 or younger (46 percent), and female (67 percent). They were also independent (71 percent), delayed their enrollment for a year or more after high school (69 percent; figure A), attended full time for at least part of the academic year (80 percent), and worked while enrolled (61 percent;  Private for profit Compared to students at other less-than-4-year institutions in 1995-96, these students were more likely to be female, black, single parents, independent, and in the lowest income quartile (for both dependent and independent students). With respect to enrollment characteristics, students at lessthan-4-year, for-profit institutions in 1995-96 were more likely to have delayed their enrollment for a year or more after high school, have attended full time for at least part of the academic year, and have not worked while enrolled compared to students at other less-than-4-year institutions. Between 1992-93 and 1995-96, there was little change in the demographic and enrollment characteristics of students at less-than-4-year, for-profit institutions. The same is true for their counterparts at other less-than-4-year institutions.',\n       'Canada as a whole experienced near-average precipitation conditions in 2014. Based on preliminary data, it was the 22nd driest year since nationwide records began in 1948, with nationally averaged precipitation 98% of the 1961-90 average. Drier-thanaverage conditions were observed for much of the Yukon, Northwest Territories, and in the far north, whereas some areas in southern Saskatchewan, Manitoba, and Ontario experienced wetter-than-average conditions (Fig. 7.3). Seasonally, winter 2013/14 was the 15th driest since 1948 and the national average precipitation was 91% of the 1961-90 average. Most of the country experienced drier-than-average conditions but wetter-thanaverage conditions were observed in western Ontario, eastern Nunavut, and the Atlantic provinces. Spring 2014 was the 22nd wettest in the 67-year period of record with nationally averaged precipitation 104% of the average. Wetter-than-average conditions occurred across much of the southern regions but drier-than-average conditions were also observed in southern Northwest Territories, eastern Nunavut, and northeastern Quebec. Summer 2014 was the 14th wettest since 1948 and the national average precipitation was 106% of the 1961-90 average. Wetter-than-average conditions were mainly found in the central regions of the country whereas drier-than-average conditions occurred in British Columbia and in the north. Fall 2014 was the 30th driest since 1948 with nationally averaged precipitation 98% of average. Drier-than-average conditions in the northwest and wetter-than-average conditions in the south and east continued until the end of the year.\\nFor 2014, the northern half of the CONUS tended to be wetter than the 1971-2000 average, while the Southern Plains were drier than average. Most other locations had near-average annual precipitation. Michigan and Wisconsin each had one of their 10 wettest years on record (Fig. 7.5b). Over the course of the year, drought conditions improved across the Midwest and Central Plains, with both drought improvement and deterioration in parts of the Southern Plains, Southwest, and Southeast. Drought worsened for much of the far West. Parts of the Northeast and Southeast had drought develop and disappear during the year. At the beginning of 2014, the CONUS moderate to exceptional drought area footprint was 31%, peaking at 40% in May and ending the year at 29%. The end-of-year drought footprint was the smallest for the CONUS since 2011, and below the 2000-13 average CONUS drought footprint of 32%. Winter was the 17th driest for the CONUS, with a precipitation total 27.4 mm below average. Belowaverage winter precipitation was observed from the West Coast to the Southern Plains. The warm and dry winter in California, combined with multiyear precipitation deficits, drastically worsened drought conditions. Winter is normally the wet season for California. Above-average winter precipitation was observed from the Northern Rockies to the Midwest. The cool and wet northern U.S. contributed to the 10th largest winter snow cover extent for the CONUS. The CONUS spring precipitation total was near-average, with above-average precipitation across the Northwest, Midwest, Northeast, and Southeast. Below-average spring precipitation was observed across the Southwest, Central and Southern Plains, and into parts of the Ohio Valley, contributing to a continued expansion of the CONUS drought footprint. The summer precipitation total for the CONUS was 239.5 mm, 25.9 mm above average, and the ninth wettest summer on record. A majority of the summer precipitation occurred in June and August. Six states across the northern tier of the country had one of their 10 wettest summers on record. Minnesota observed 205 mm of precipitation during June, marking the wettest month of any month for the state. The CONUS fall precipitation total was slightly below average.\\nRainfall was near and above normal in the north, northeast, south-central, and east of the country in 2014 (Fig. 7.8b). At the national level, a total of 830.8 mm precipitation fell on average, 106.6% of the 1971-2010 mean and the 18th wettest year since 1941. September contributed the largest amount to the annual rainfall, 190 mm (22.9% of the total), due to Tropical Storm Dolly (1-3 September) and Hurricanes Norbert and Odile (2)(3)(4)(5)(6)(7)(8), which each brought heavy precipitation to the impacted regions. In contrast, February had the lowest contribution (6 mm). Morelos and Colima each observed their wettest year since 1941, with 1777.8 mm and 1920.4 mm, respectively, twice their normal annual totals. Baja California Sur had its fourth wettest year with 334.7 mm (twice its average annual total); 20% of the total precipitation was attributed to Hurricane Odile. In the north of the Baja Peninsula, rains were scarce, and Baja California had its third driest year since 1941. Oaxaca had its seventh driest year on record.\\nThe start of the rainy season is identified as two consecutive pentads with at least 25 mm of precipitation followed by a third pentad with measurable precipitation. A similar approach is used to compute the end of the rainy season, but from the end of the year backwards. Compared with the 1981-2010 period, 2014 was normal in terms of the start and end dates of the rainy season for nine stations in Central America, with the exception of Puerto Lempira (P3) which saw an early start to the rainy season (it could be considered in the lower tail of the distribution at the p = 0.05 level). The starting and ending pentads of the rainy season for the stations were: San José (34, 61), Puerto Limón (20, 73), Liberia (48, 62), Puerto Lempira (3, 73), David (25, 54), Choluteca (25, 60), Philip Goldson International Airport (40, 62), and Puerto Barrios (1, 69). The year began with drierthan-average conditions across the region (please see Notable Events section) followed by wetter-thanaverage conditions-resulting in near-normal annual precipitation totals for most stations analyzed (Fig. 7.9). Other variables such as the above-normal maximum 5-day wet-period magnitude, below-nor- \\nRainfall over the eastern Caribbean between January and March was normal to above normal in the south and normal to below normal in the north. Trinidad, Tobago, St. Vincent and the Grenadines, St. Lucia, and Anguilla experienced normal rainfall; Grenada and Barbados were moderately wet. Dominica, St. Maarten, Antigua, and St. Croix were moderately dry, and St. Kitts was severely dry. Over the northern Caribbean, Puerto Rico and Jamaica had normal precipitation, while Grand Cayman was abnormally wet. Conditions over Cuba ranged from normal to extremely wet. January was the seventh driest for St. Thomas (25.9 mm), while San Juan recorded its fifth driest March (13.0 mm). During the second quarter of 2014, drier-thanaverage conditions extended across the eastern Caribbean to include Anguilla, Trinidad, Tobago, St. Lucia, Dominica, and St. Vincent. Aruba, Grenada, and Barbados were severely dry. In contrast, St. Kitts and St. Maarten experienced normal rainfall, while St. Croix was abnormally wet. Average rainfall was recorded over Puerto Rico and Grand Cayman, while rainfall in Jamaica ranged from moderately wet in the west to moderately dry in the east. Jamaica recorded its second lowest June rainfall (47 mm) on record, 9 mm above the driest June recorded in 1994. Western Cuba was abnormally dry compared to eastern Cuba, which was normal. St. Croix recorded its eighth wettest April (94.2 mm) and St. Thomas its eighth wettest May (169.7 mm). June was the third driest for both San Juan (19.6 mm) and St. Thomas (3.6 mm) and the fourth driest for St. Croix (11.9 mm). During July-September, normal rainfall helped alleviate dry conditions across Trinidad, Tobago, Grenada, and St. Vincent. St. Kitts, Aruba, and St. Croix also had normal precipitation, while Barbados and Antigua were moderately dry. Dry conditions persisted over Dominica and intensified over Anguilla and St. Maarten. While Puerto Rico was moderately wet, Jamaica ranged from normal in the west to moderately dry in the east. Western Cuba was dry while the east was normal. Grand Cayman recorded average rainfall. Summer (June-August) was the fourth driest since 1981 at Piarco, Trinidad (541.1 mm). July rainfall was the lowest on record for Jamaica (43 mm), second driest for St. Croix (22.9 mm), and fourth driest for St. Thomas (20.6 mm). In contrast, August was the fourth wettest for St. Thomas (197.1 mm), ninth wettest for St. Croix (154.4 mm), and tenth wettest for San Juan (248.7 mm). Dry conditions returned in September for St. Thomas (48.3 mm) and St. Croix (68.8 mm), where they observed their sixth and ninth lowest September values, respectively. During October-December, normal conditions continued across Aruba, Trinidad, Grenada, St. Lucia, and St. Kitts, while Anguilla, Tobago, St. Maarten, Barbados, and St. Croix were abnormally wet and St. Vincent and Dominica were extremely dry. Normal rainfall was observed for Puerto Rico while conditions in Jamaica ranged from moderately wet in the west to moderately dry in the east. Grand Cayman was abnormally dry. Apart from some abnormally dry western areas, Cuba had near-normal precipitation. Jamaica recorded its third driest October (121 mm) on record, while St. Croix and St. Thomas recorded their ninth (58.4 mm) and tenth (65.0 mm) lowest October values, respectively. November was the tenth wettest on record for St. Thomas (181.1 mm). December was record dry for the E. T. Joshua Airport (51.1 mm; located in St. Vincent and the Grenadines), while eighth wettest in St. Croix (143.0 mm). Overall, 2014 was record dry at Douglas-Charles Airport, Dominica (1908.5 mm) and 11th driest for Puerto Rico (1485.4 mm). Online Table S7.1 presents some extreme climate indices calculated annually for a number of Caribbean stations and compared with their respective median values.\\nIn Venezuela, drier-than-average conditions were present during JFM and October-December (OND). The lack of precipitation was associated with anomalous subsidence over the region. This pattern was particularly strong during JFM when precipitation was 40% of normal in the north and southeast regions. This feature eased during AMJ and JAS, favored by above-normal precipitation. AMJ precipitation in the northwest of the country remained below average, with as little as 60% of normal precipitation, while in north-central and eastern areas, precipitation was 120-130% of normal. In Colombia, precipitation was below normal most of the year, with deficits of up to 60% in the Andean, Caribbean, and Pacific regions. July was the driest month, with 20% of normal precipitation. During the first half of 2014, Ecuador\\'s In Peru, during JFM, extreme dry conditions were observed in the northwest and the southern Andes, while wet conditions prevailed in the southern and central Amazonia region. Precipitation over northern Bolivia and the Bolivian Altiplano was above normal early in the year. North of La Paz, anomalies reached nearly three times the normal, but were near to below normal the rest of the year. Over the valleys and lowlands, precipitation was below normal for the year, as little as 68% of normal. The eastern llanos (plains) observed above-normal precipitation most of the year.\\nPrecipitation deficits were observed between January and March over southeastern Brazil (150-200 mm month −1 below normal) and between January and May over northeastern Brazil (50-150 mm month −1 below normal), continuing the region\\'s severe drought that started in 2012. An atmospheric blocking and a high pressure system in large parts of tropical Brazil and the tropical South Atlantic, together with an absence of the South Atlantic convergence zone (SACZ) during summer, were responsible for the lack of precipitation over most of subtropical South America east of the Andes. As a consequence, a record dry spell of 45 consecutive days occurred during the December-February peak of the rainy season. The warm temperatures and dry conditions in southeastern Brazil led to positive 500-hPa height anomalies during the peak of the rainy season. This feature has been detected during previous dry episodes (Fig. 7.13a). Meanwhile, exceptional positive rainfall anomalies were observed in January and February 2014 in southwestern Amazonia (see \"Notable events\"). Rainfall extremes during March-May in parts of northern Northeast Brazil were due to an anomalously southward position of the intertropical convergence zone. In June, two frontal systems brought copious rain (100 mm above normal) to parts of Santa Catarina and Rio Grande do Sul states, affecting nearly 400 000 people (ht t p://rel ief web.i nt /map/colu mbia /16 -Ju ne -2014-south-america-severe-weather-and-floods). Wet conditions prevailed all year in southern Brazil (50-100 mm above normal), while above-normal precipitation was observed over Bolivia, Paraguay, and northern Argentina between March and July. A September frontal system produced intense rainfall in southern Brazil. In November two weak SACZ episodes produced beneficial rainfall in southeastern Brazil. The rainfall totals associated with these systems were not sufficient to end drought conditions in the region. Above-normal precipitation observed in March-July, caused the Parana and Paraguay Rivers to overflow in rural and urban locations in Paraguay, affecting almost 160 000 people, with the national capital Asunción among the worst impacted. Bolivia\\'s Santa Cruz department was impacted by an October dry spell, affecting 51 180 people and 20 344 hectares of crops/farmland (http://reliefweb.int/disaster /dr-2014-000147-bol). During 2014, the tropical upper atmosphere was significantly modulated by intraseasonal variability. As a result of MJO activity in February and April, three strong westerly wind bursts (WWB) were generated triggering Kelvin waves, which transported heat through the Pacific Ocean. From April to June, positive heat content anomalies up to +4°C were observed in central and eastern Pacific between 0 and 150-m depth. As a direct consequence, positive SST anomalies >0.5°C predominated along the equatorial eastern Pacific (120°-80°W) during May-December. The peak occurred after the arrival of a first Kelvin wave from May to mid-August, with SST anomalies between +2°C and +3°C. This warming extended from Peru to Central America and Mexico (see Fig. 7.11). The atmospheric response to positive SST anomalies was evident in the eastern equatorial Pacific, where positive anomalies of zonal wind at 850 hPa and 200 hPa were observed from March to August, suggesting a sustained coupling with the warmer oceanic conditions in the region (Fig. SB7.1). All these features configured a positive phase of the ENSO cycle with development on the eastern Pacific (as described by Wyrtki 1973Wyrtki , 1975. The effect of this first warm pulse in May and its atmospheric response on regional climate was evidenced in northern South America (Caribbean coast of Colombia and Venezuela), Central America, and the Caribbean. The anomalies at 200 hPa in the eastern Pacific influenced the medium levels and the behavior of the subtropical jet stream in central South America, which partially explained the heavy precipitation over Paraguay in June (Fig. SB7.2). Two moderate WWBs occurred near the dateline in mid-October. Two subsequent weak Kelvin waves reached the South American coast, producing SST anomalies between +0.5°C and +1.5°C. Meanwhile, positive zonal wind anomalies dominated the central and eastern Pacific Ocean (170°-130°W). During this period, the equatorial upper ocean heat anomalies continued to increase and a new accumulation of warm water took place at depths of 100-250 m. As a consequence, the equatorial eastern Pacific Ocean experienced SST anomalies above +0.5°C up to November. During the second half of December, SST anomalies over the eastern equatorial Pacific decreased due to the upwelling phase of a Kelvin wave, which caused a shift to weak cold conditions in El Niño 1+2 region, but with anomalies near +0.5°C between 90°-140°W. Therefore, the year finished with neutral conditions in Niño 1+2 and Niño 3 areas. For more detailed information on ENSO, see section 4b. \\nDuring 2014, rainfall was above normal across most of SSA, with the exception of Chile, which recorded its sixth consecutive dry year, with some locations recording 30-70% of average precipitation. This resulted in the nation\\'s driest 10-year period (2005-14) on record. Most of SSA, including the broad arid Pampa region, recorded several unusually heavy rainfalls in 2014. Some locations in northern Patagonia and southeastern wet Pampa region observed their wettest year on record (Fig. 7.15). Frequent heavy rainfall episodes produced devastating floods in Buenos Aires, Misiones, and Corrientes provinces in northeastern Argentina many times during 2014. The 2014 annual rainfall anomaly values for Argentina and Uruguay were estimated to be 122% and 143% of average, respectively. Argentina experienced its wettest year since 2003 and seventh wettest since national records began in 1961. After a lack of rainfall and unusual warmth at the start of the year, atmospheric conditions changed abruptly and heavy rainfall episodes occurred from February onward. The most significant events occurred across arid regions, where annual precipitation normally ranges between 80 mm and 250 mm-these regions accumulated more than 200 mm in just a few days, breaking historical records. Severe flooding affected large areas of the wet Pampa region of Argentina and Uruguay. Numerous daily and monthly records were broken across these regions, placing 2014 as one of the wettest years on record for most locations. September was characterized by major precipitation events, which mainly affected coastal Buenos Aires province. Many stations exceeded the normal September precipitation during the first 10 days of the month. Misiones province in northeast Argentina recorded between 350 and 400 mm. On 18 September, the station at Bernardo de Irigoyen set a new September 24-hour precipitation record of 129 mm. Exceptional rainfall accumulations during the first half of September were observed in south-central Uruguay, with amounts 5-7 times the average value for the same period.\\nOverall, the region experienced an annual rainfall deficit, associated with anomalous anticyclonic circulation that persisted over the region, especially during the first half of the year. However, the NWSR was also characterized by a strong geographical variation in precipitation ranging from 52% of normal at Nador, Morocco to 239% of normal El Jadida, also in Morocco. Large regional variations in precipitation were observed during winter (December−February) over the NWSR, with seasonal totals ranging from 43% of normal at Tetouan to 225% of normal at Smara, both in Morocco. In January, successive rainstorms and snowfall events occurred; strong winds accompanied the storms, exceeding 28 m s −1 at Mascara, Algeria and 29 m s −1 at Larache, Morocco. While winter precipitation over Egypt was generally below average (as low as 36% of average), some places in the eastern region of the country received above-normal rainfall (e.g., 155% of normal in Port Said). Rainfall in spring was below average for the region, with deficits as low as 24% of normal in Ouarzazate in southern Morocco. In May, the dry conditions improved across parts of the country. The region experienced cyclonic disturbances associated with humid air from the south, leading to rainfall that was 184% of the seasonal average at Laayoune, Morocco. During 17−19 May, 59 mm of precipitation was reported at Hassi Messaoud in Algeria. This 3-day record total is equivalent to the annual total rainfall at this location. Generally dry conditions persisted in northern Africa during summer, except at the end of August when convection contributed to heavy rainfall in some areas of Algeria. The monthly average for August is only 11.3 mm. On 20−21 September, 66 mm of rain was reported at Ouarzazate, Morocco, while on 27 September, 93 mm of precipitation fell in just two hours in Fnideq on the Moroccan Mediterranean coast. Successive extreme rain events, associated with a deep trough with surface pressure as low as 985 hPa, were reported in November and December in southern Morocco and Algeria. Snow was also reported in Algeria at the end of the year, with depths reaching 38 cm on 31 December in the northeastern city of Sétif.\\nWetter-than-average conditions associated with the warm northern tropical Atlantic phase of the Atlantic multidecadal oscillation persisted over most of the Sahel region. Rainfall totals for June− September, the period during which the West African monsoon provides much of the annual precipitation, are shown in Fig. 7.19a. Relatively dry conditions prevailed in some regions of the Sahel (12°-15°N), including northern Senegal and western Gambia. Much drier-than-normal conditions were observed near Lake Chad, with near-record dry conditions over eastern Niger and western Chad. Maiduguri, a station near the lake, recorded only 0.9 mm during June. In contrast, precipitation along much of the Coast of Guinea was above average with some locations in Ivory Coast and Ghana reporting much wetter-than-normal conditions. A sharp decrease in precipitation from the southern coast northward in the Sahel regions is evident in Fig. 7.19b. This often occurs due to warmer-than-average SST conditions over the Gulf of Guinea (Nicholson 2013). This was the case throughout the summer, especially during June when the Gulf of Guinea SSTs were much warmer than normal.\\nBurundi, the western half of Tanzania, and southern Kenya received above-normal precipitation while eastern Tanzania south of 5°S experienced below-normal rainfall during DJF (Fig. 7.22a). The MAM seasonal total rainfall was above normal north of the Rift Valley over Ethiopia, South Sudan, southern Somalia, eastern Kenya, and Tanzania. The Ethiopian Rift Valley and adjoining highlands, southeastern Ethiopia, northern Somalia, parts of Kenya, Uganda, Tanzania, Rwanda, and Burundi stretching along the Great Rift Valley and its escarpments received belownormal rainfall (Fig. 7.22b). Wet anomalies of up to +100 mm in total seasonal rainfall of 2014 ( Fig. 7.22c) between June and September (JJAS) were observed over Sudan, southwestern Ethiopia and adjoining South Sudan, and southwestern Kenya. Dry anomalies were observed over western South Sudan, and over Ethiopia, northward of 6°N with the exception of the Eastern Highlands and southwestern Ethiopia. The September-December (SOND) total rainfall was above average over Ethiopia, South Sudan, and southern Sudan (Fig. 7.22d). SOND rainfall was below normal over the rest of eastern Africa, with the exception of above-normal rainfall over small areas in northwestern Tanzania.   Figure 7.23 shows that the mean annual temperatures of the past 18 years were all above normal. A warming trend of 0.13°C decade −1 over the region is indicated by the data of these particular climate stations, statistically significant at the 5% level. Figure 7.24 presents the annual rainfall anomalies for 2014 compared to the 1971-2000 base period. Near-normal conditions prevailed over most of the country, but dry conditions were observed over the eastern parts, particularly in central and eastern KwaZulu-Natal province, for most of the year, resulting in below-average annual rainfall.\\nIn January western and far northeastern South Africa were wetter than normal, with normal to below-normal conditions over the central region. By February the central region became wetter, while the western area experienced below-normal rainfall. Substantial rain fell in March in the northeast, and in April dry conditions were experienced in North-West province, with above-normal rainfall in Eastern Cape province. These dry conditions spread east in May, with most of the central and eastern half of South Africa dry in June and July. In August the western interior experienced some relief from the dry conditions, but it remained dry in the east until October when wetter conditions spread eastward from the west; however, parts of KwaZulu-Natal province remained somewhat dry.  (iii) Notable events The first part of was characterized by flooding over parts of the Western and Eastern Cape in January and Gauteng and parts of Limpopo and North-West in February, as reported by the South African Weather Service. Continuous rain fell across five provinces during the first two weeks of March, resulting in extensive flooding over vast areas in the Gauteng, Mpumalanga, Limpopo, North-West, and KwaZulu-Natal provinces. Many people lost their lives and there was extensive damage to infrastructure, including houses, bridges, roads, and culverts. Farmers suffered losses due to the continuous rainfall because they could not harvest. The Western Cape province experienced localized flooding again in June. Due to drought conditions in 2013, farmers in parts of the Northern Cape and North-West provinces were still struggling to feed their livestock in early 2014, as reported by the South African Weather Service. A drought-relief fund was set up by the National Disaster Management Centre. In the second half of 2014, a prolonged dry winter season over the KwaZulu-Natal north coast resulted in water restrictions in Richards Bay and the surrounding areas. The uMfolozi River, a source of water for many communities, dried up. The combination of rainfall deficits and above-normal temperatures adversely affected sugar cane production in several areas in KwaZulu-Natal. In October a large dust storm traveled more than 800 km from central South Africa to the northeastern area of the country bordering Zimbabwe, as reported by the South African Weather Service. Dust storms of this size are rare in South Africa and further, it is uncommon to have a dust storm move such a great distance. Ambient air quality monitoring stations in the Vaal Triangle Area, south of Johannesburg, registered a tenfold increase in PM10 (particulate matter <10 microns) mass concentration within a timeframe of ~20-30 minutes, at which point the instrument range settings were exceeded. In November two tornadoes were observed, as reported by the South African Weather Service. This is significant, as only four tornado events are reported in South Africa on average per year. One tornado touched down on a farm in Goedgeloof, between Dundee and Vryheid in northern KwaZulu-Natal. According to reports, it lasted about 15 minutes and little damage occurred. The second tornado hit part of Soweto, in the Gauteng province, causing severe damage to a number of houses. Rough sea conditions along with a spring tide in December caused flood damage to some roads and houses along the coastal areas of KwaZulu-Natal, according to the South African Weather Service. A number of beaches were closed as a result of the rough sea conditions and strong rip currents, with a number of drownings reported.\\nThe annual rainfall amount over Réunion island was about 84% of average. January was the wettest month, owing to Tropical Cyclone Bejisa, whereas June was the driest on record.\\nAnnual precipitation was close to normal for most of the central and western European region. However, locations in southeastern England and eastern Scotland recorded wetter-than-normal conditions (up to 150% of normal). Continuous southwesterly air masses from the Atlantic led to frequent foehn situations in 2014 with well-above normal rainfall totals in the southern Alpine areas. Switzerland reported 145% of normal precipitation totals for the Ticino region and Austria recorded totals of 175% of normal in places. Winter 2013/14 was wetter than normal, particularly over the British Isles and Ireland, at the Atlantic coast of France, and in the southern Alps. Ireland experienced its wettest winter since records began in 1866. While places in Ireland, southern UK, and France (near the Alps) registered totals up to more than 200% of normal, most of central Europe except the Alps had below-average precipitation totals. The southern Alps in Austria had exceptionally heavy snowfall. Spring was wetter than normal in much of western Europe and eastern central Europe, whereas western central Europe saw below-average precipitation. In France, Germany, and the Netherlands some totals were <40% of normal, locally even <20%. During summer rainfall was generally above average, except in western Ireland, central Great Britain, and parts of eastern central Europe. Up to more than 150% of normal were recorded in France and Switzerland. However, below-average totals in June across nearly all areas contrasted with above-average totals in August. Ireland reported <50% of normal in S193 JULY 2015 STATE OF THE CLIMATE IN 2014 | some places during June and July; Luxembourg had its wettest August on record. Fall was dry in northern and western areas of the region, but wet near the Alps. Southeast of the Alps precipitation totals up to 150% of normal were recorded, whereas some continental coastal regions and parts of England received well-below-average totals (60-80% of normal). In September when high pressure conditions dominated over the UK, parts of the country received <20% of normal precipitation, making this the driest September since 1910 (but not an all-time record). Atmospheric conditions were cyclonic in October and part of November with well-above-average precipitation totals over much of western and central Europe. Switzerland reported the highest November totals in the Ticino region. In December below-normal or normal precipitation affected most of the region, though some major cyclonic storms caused heavy rainfall, especially in northern Germany and northern Poland.\\nAnnual precipitation was close to normal across nearly the entire Nordic and Baltic region. Only Estonia and central Norway/Sweden experienced 60-80% of the normal precipitation totals. Winter 2013/14 was dominated by an Icelandic low that extended far into the European continent. The positive NAO during the season reflected the low pressure in the north associated with strong westerlies. The result was wetter-than-normal conditions in eastern Iceland, Denmark, southern Scandinavia, northern Sweden, and central Finland. Some stations in southern Norway received more than 300% of normal precipitation, while only 50% of normal was recorded in central and northern parts of the country because a southerly airflow dominated and northern coasts were on the downwind lee side. Precipitation in spring ranged from near-normal in southern areas to above average in northern parts.\\nAnnual mean precipitation for the entire Iberian Peninsula was above normal as a consequence of the wet winter and fall. Spatial mean annual rainfall was ~680 mm in Spain and ~1100 mm in Portugal, 5% and 24% above normal, respectively, in large part due to a wet fall in western Iberia.\\nAnnual precipitation was mostly close to normal, except for the Balkans, southern Italy, and the southern Alpine region. More than 150% of normal totals were recorded in these regions. Croatia reported precipitation up to 175% of normal, making 2014 an extremely wet year for the country. During winter 2013/14 the Balkan Peninsula and Turkey were affected by above-average sea level pressure that led to drier-than-normal conditions (40-60% of normal). In contrast the southern Alpine region experienced frequent topographically induced precipitation mainly during January and February; seasonal totals were well above average (200% of normal). Spring was exceptionally wet in the Balkan countries and southern Italy, but drier than average in the Alpine region and eastern Turkey (locally 40-60% of normal). Most of the Balkan States received anomalous rainfalls up to 250% of normal (see Sidebar 7.2). Summer was wetter than normal in most areas except southernmost Italy and eastern Turkey (where totals as low as 40% of normal were recorded). The highest precipitation was measured in the Bosporus area, up to 200% of normal. The season was dominated by below-average sea level pressure associated with repeated westerlies that brought Atlantic cyclones into the area. Precipitation totals in fall were below average in some western parts of the region (Corsica, Sicily, central Italy), while the Balkan States and Turkey experienced an exceptionally wet season; more than 180% of normal precipitation was recorded in places. These conditions continued in December 2014, though eastern Turkey became drier.\\nAnnual precipitation totals in eastern Europe were mostly near-normal in 2014. Limited areas in northeastern European Russia recorded above-normal rainfall (up to 150%) and some western and southern areas of European Russia had below-average values down to 60% of normal. In winter 2013/14 precipitation was above normal in eastern areas, whereas the southwestern parts experienced dry conditions. The Black Sea region recorded seasonal precipitation totals 40-60% of normal, mainly due to rain deficit in December and February. Spring was characterized by low sea level pressure associated with frequent cyclones that brought wetterthan-average conditions to western and northeastern areas, whereas central parts of the region had belowaverage precipitation. The Ukraine, Moldova, and Romania received above-average rainfall of up to 150%. May contributed greatly to these high totals, owing to a trough centered over eastern Europe with monthly precipitation totals of up to 250%. Precipitation totals in summer were slightly above average, except for western and southern Russia and central Ukraine, where locally <40% of normal was recorded. In July nearly the entire area was under the inf luence of high pressure, which led to dry conditions and rainfall <20% of normal in part of southern European Russia. Romania and Moldova were affected by a persistent trough and received up to more than 150% of normal precipitation.\\nAnnual precipitation for the Middle East was below normal. Westernmost Kazakhstan, Azerbaijan, and parts of the eastern Mediterranean countries had dry conditions, with totals 40-80% of normal. Winter 2013/14 was drier than the long-term mean. Most of Syria received only ~40% of normal precipitation. Israel had a record dry winter due to an exceptionally dry January and February, with each month receiving <25% of normal totals. The season was dominated by significant above-average sea level pressure over southeastern Europe and the eastern Mediterranean region, accompanied by the influence of sinking motion (stippled and hatched in Fig. 7.30). Generally, spring continued to be drier than normal in most areas. Between the Black and Caspian Seas ~80% of the normal precipitation was recorded, while the eastern Mediterranean region was drier than normal in the north, but wetter than normal in the south. During May the eastern Mediterranean countries (except northern Syria) experienced exceptionally wetter-than-normal conditions (>250% of normal). Cyprus measured precipitation totals of more than 320% of normal and Israel reported its wettest May since records began in the 1920s. The wet May was followed by a dry summer. Precipitation totals were well below average (~40% of the normal) in eastern areas. The deficit occurred primarily in August, when <20% of normal rainfall was recorded in these areas. In fall Atlantic cyclones reached far to the south and influenced rainfall in the Middle East. The east Mediterranean countries and parts of Armenia and Azerbaijan recorded up to 150% of normal precipitation. In October Israel reported 1.5-3 times its monthly precipitation followed by another wet month in November (up to 150% of normal). On the other hand, Kazakhstan was under the influence of high pressure during fall that led to <40% of the normal totals in westernmost areas. During December 2014 much of the region received 60-80% of normal precipitation, with even less in some places, except western Kazakhstan, which had close to normal conditions.\\nPrecipitation over Russia was generally close to normal (80-120%) for the year as a whole. A relatively wet January in Russia (especially in the south) gave way to dry conditions in February in European Russia and above-normal precipitation in the east. Moscow received 19 mm of precipitation in February (about half of normal), while central Yakutia to the northern coast of the Sea of Okhotsk received more 300% of their normal monthly precipitation. Spring was generally dry in European Russia, with variable conditions elsewhere. In March the Southern Urals and central Western Siberia had more than 300% of normal monthly precipitation in places (more than 400% of normal in the city of Chelyabinsk). In May, northwestern European Russia received significant precipitation, with more than 200% of normal in individual regions. In late May, heavy rains in the Altai (53-184 mm in ten days) resulted in a 3.2-7.4 m \\nBoth annual precipitation amounts and annual sunshine durations were above normal in northern and eastern Japan. Migratory anticyclones brought sunny weather to these regions mainly in the spring\\nThe south Asian monsoon set in over Kerala (southern parts of peninsular India) on 6 June, five days later than its climatological normal date of 1 June, and covered the entire country by 17 July (two days later than normal). The advance of the monsoon over different parts of the country was not smooth and there was a hiatus of about 10 days (20-30 June) in the advancement. Indian summer monsoon rainfall (ISMR) during the 2014 monsoon season was below normal. For India, the long-term average (LTA) of the ISMR (1951-2000 base period) is 890 mm. During 2014, the ISMR averaged over the country as a whole was 88% of the LTA and was characterized by marked spatial and temporal variability. Central, peninsular, and eastern/northeastern parts of the country received normal rainfall (Fig. 7.39), while northern/northwestern parts of the country received substantially below-normal rainfall. During the season, rainfall activity was not well distributed in time. In the first half of the season (June-July), the country received substantially below-normal rainfall (78% of its LTA), while during the second half of the season (August-September), rainfall activity was near-normal (97% of its LTA). Rainfall in June was just 57% of its LTA, a record low. During the monsoon season, only 1 of 36 meteorological subdivisions received above-average rainfall, 23 received normal rainfall, and the remaining 12 subdivisions received below-normal rainfall. Of the 615 districts, 223 were affected by moderate meteorological drought (seasonal rainfall deficiency in the range of 26%-50%), while 56 were affected by severe meteorological drought (seasonal rainfall deficiency in the range of 51%-99%). Rainfall averaged over the country as a whole was below normal on most days until the second week of July. National average rainfall was nearly half its normal value during 22 June to 11 July (Fig.7.40). Rainfall over India was 114% of its LTA during the winter season (January-February), normal (100% of LTA) during the pre-monsoon season (March-May), and below normal (67% of LTA) during the post-monsoon season (October-December). The northeast monsoon (NEM) contributes 30%-50% of the annual rainfall over southern peninsular India and Sri Lanka as a whole. In 2014, NEM seasonal rainfall over south peninsular India was below normal (88% of LTA). Pakistan, at the western edge of the pluvial region of the south Asian monsoon, receives 60%-70% of its annual rainfall during the summer monsoon season (July-September). In 2014, the summer monsoon was generally subdued over the country during most of the season, and most parts of Pakistan had substantially below-normal rainfall during July (60% of LTA) and August (53% of LTA). However, rainfall during September was intense (218% of LTA) and contributed significantly to make seasonal rainfall for Pakistan slightly above normal overall. Bangladesh received abovenormal rainfall during its 2014 summer monsoon season (June-September). Sri Lanka received below-normal rainfall during its summer monsoon season (May-September); however, northeast monsoon rainfall during October-December 2014 was above normal. \\nIran experienced drier-than-normal conditions during 2014. During winter and spring, 26 of the 31 Iranian provinces experienced critical drought conditions, as did 23 provinces during summer. The affected provinces were mostly located in agricultural areas. Provinces receiving higher-than-normal precipitation were located in the northeast and central regions; this includes several normally arid provinces. The only relatively wet season overall was fall, with 750 mm precipitation in the north and 300 mm in the west and in the Zagross mountain area; however, 12 provinces still received below-normal precipitation during this period. Figure 7.42a,b shows the winter and fall 2014 spatial patterns of the standardized precipitation index (SPI), respectively. The red shading denotes dry conditions while the green shading indicates wet conditions. The severity of precipitation anomalies in the winter over the northern half of the country is clear, as is the recovery during fall.\\nDuring 2014, large month-to-month differences in rainfall were observed at many locations in the U.S.-Affiliated Pacific Islands (US-API), with all islands following a similar general temporal distribution of wet and dry months (Fig. 7.43). Higher-than-average rainfall (and in some cases, much higher-than-average rainfall) was observed at most islands during January-April. Rainfall in the RMI during April was extreme: Majuro set a monthly rainfall record; Kwajalein fell just short of its April record (although a lesser total in February set the record for that month); Kwajalein set a 24-hr rainfall record with a total of 300 mm; and Mili had over 750 mm of rain during the month. This heavy rainfall was consistent with an anticipated onset of El Niño; however, May and June were particularly dry. Following these two dry months, a seesaw of wet and dry months commenced, with a widespread pattern of a very wet July, a dry August, and a wet September/early October followed by moderate dryness again in the last two months of the year. Likewise, concurrent large fluctuations were observed in tropical cyclone activity in the western North Pacific. The 2014 annual rainfall was above normal at a majority of reporting locations, with only Woleai Atoll in Yap State reporting less than 75% of annual average rainfall. The 6-month and annual rainfall values for selected locations across Micronesia are summarized (along with the temperature) in Table 7.1.\\nRainfall variability at seasonal and longer time scales in the southwest Pacific is strongly associated with ENSO and the position and intensity of the  \\nRainfall averaged across Australia was 483.5 mm, or 104% of the 1961-90 normal, making 2014 the 38th wettest in the 115-year period of record (national rainfall records began in 1900). Notable areas with below-average rainfall include southeast South Australia, much of Victoria, all of Tasmania, an area covering southeast Queensland and northeast New South Wales, parts of inland northern Queensland, and from the south coast of Western Australia up to the coastal Pilbara. Above-average totals were measured in a broad band extending from the Cape York Peninsula in Queensland, across the Northern Territory, and down the interior of Western Australia to the south coast ( Fig. 7.49). The Northern Territory had its 12th wettest year on record while Tasmania had its 12th driest. All other states were outside the 20 wettest/driest years and within 20% of average rainfall. In Victoria, including 2014, 15 of the last 18 years (since 1997) have been drier than average. January and February were active months in the tropics, with Tropical Cyclone Christine and a tropical low providing heavy rainfall in January for Western Australia and western parts of the Northern Territory. Tropical Cyclone Fletcher brought several days of heavy precipitation to the region surrounding the Gulf of Carpentaria in February. Fletcher helped bring Kowanyama in Queensland a February precipitation total of 1470.6 mm, making this the wettest month at the station since records began in 1913. Another tropical low passed through the interior of Western Australia in February, providing more precipitation for the region. A low-level trough brought heavy rain and thunderstorms for eastern New South Wales and Queensland in late March, resulting in widespread flash flooding. The last significant tropical precipitation for the season occurred in April, with a trough in early April pulling in tropical moisture and producing heavy rainfall from northwest to southeast Australia, resulting in the second wettest April on record for South Australia. El Niño-like conditions in the tropical Pacific Ocean had some effect on the Australian region in 2014, with rainfall patterns broadly consistent with El Niño conditions. During July-November, rainfall was below average across much of the eastern twothirds of the continent. For southeastern Australia, August-October 2014 was the ninth driest such period on record. Meanwhile, much of Western Australia had above-average rainfall during September-November. December saw a break-up of the drier weather across the eastern parts of the country, with aboveaverage rainfall in the Northern Territory, extending through southern Queensland and across much of the east coast of Australia.\\nAnnual rainfall totals for 2014 were largely near-normal (80-119% of normal) for New Zealand (Fig. 7.50b). The exceptions were parts of the central North Island and Central Otago where rainfall was 50-79% of normal, and isolated parts of Northland near Kaikohe where 120-149% of normal rainfall was recorded. In addition, well-above-normal rainfall (>149% of normal) occurred near the far southwest of the South Island. It was the second driest year on record for Turangi (central North Island) and Dannevirke (southern Hawkes Bay), with these locations recording 69% and 78% of normal annual rainfall, respectively. February was a particularly dry month for New Zealand with a number of locations from as far north as Kaitaia (Northland) to as far south as Wanaka (Central Otago) recording <20% of normal February rainfall. In contrast, March and April were relatively wet months for the eastern South Island, where some locations received >400% of their normal rainfall, respectively, for those months. Christchurch observed its wettest March and second wettest April since records began in 1863, with the city receiving 71% of its normal annual rainfall during this two-month period alone. Of the regularly reporting rainfall gauges, the wettest location in 2014 was Cropp River, in the Hokitika River catchment (West Coast, South Island), with an annual rainfall total of 11 866 mm. The driest of the regularly reporting rainfall sites in 2014 was Alexandra (Central Otago), which recorded 305 mm of rainfall. North Egmont (Taranaki) experienced the highest 1-day rainfall total in 2014 (311 mm), recorded on 2 August.',\n       \"Genetic variation within the clusterin (CLU, also previously called apolipoprotein J, ApoJ) gene has been associated with risk of Alzheimer's disease (AD) in multiple independent genome-wide association studies of diverse ethnic groups [1] . In addition, among healthy participants in the Baltimore Longitudinal Study of Aging, carriers of the protective CLU allele showed slower rates of decline in memory performance relative to carriers of the risk allele [2] . However, our knowledge is very limited regarding the mechanisms through which genetic variation in the clusterin gene modifies the risk of AD. Some evidence suggests that the clusterin protective variant is associated with elevated gene expression and higher plasma clusterin levels [3] [4] [5] [6] . In turn, there is substantial evidence suggesting neuroprotective roles for clusterin in AD pathogenesis. For instance, clusterin acts as a chaperone to alter amyloid beta (ab) aggregation and toxicity, it has a role in ab clearance as well as in lipid metabolism in the brain, and it modulates inflammation and inhibits apoptosis [7] [8] [9] . In contrast, clusterin mRNA and protein levels have been shown to be higher in AD [5, 6, 10, 11] and mild cognitive impairment [11, 12] in some but not all [4, 6, 13] studies. Clusterin protein levels in plasma are also increased with increasing severity of the disease [6, 10] as well as in persons showing a more rapid decline [6] . Even in nondemented individuals, clusterin levels are negatively correlated with performance on tests of global cognition and attention/processing speed [12] , with greater brain atrophy [6, 12] and with a greater decline in white-matter volume [12] . These findings have led to the hypothesis that clusterin is elevated in response to brain pathology to exert its neuroprotective effect. In contrast, in vitro studies suggest that clusterin can promote amyloid aggregation and toxicity when the clusterin/ab ratio is low [14, 15] . Furthermore, in a longitudinal study of 241 old individuals, clusterin levels in the cerebrospinal fluid (CSF) have been related to a greater entorhinal cortex atrophy rate, and this relationship was observed only among CSF ab1-42-positive individuals but not among CSF ab1-42-negative individuals [16] . We postulate that one reason for these apparently contradictory findings may be variation in the role of circulating clusterin with increasing age, inflammation, and amyloid pathology.\\nIt is not clear whether clusterin levels in plasma can serve as an early predictor of dementia and AD among cognitively healthy individuals. The discovery of a well-established peripheral AD biomarker that is easily accessible and cost effective is of great importance because AD incidence increases as the population ages, and the ability to identify high-risk populations is thought to be crucial for the success of current and future therapies. We tested the hypothesis that elevated plasma levels of clusterin are associated with a higher risk of new-onset dementia and AD in cognitively healthy participants. We then examined whether age and sex modify this association. An interaction of clusterin levels with plasma ab was also tested, as the impact of clusterin may depend on ab burden [16] . Furthermore, because clusterin has been shown to play a significant role in inflammation and immune responses [17] , we assessed whether its relationships with dementia/AD risk differ in individuals with low compared to high serum C-reactive protein (CRP) levels. Finally, we have explored the relationship between clusterin levels and the risk for stroke in the current analysis, as both stroke and dementia share common risk factors and etiologies [18] . Moreover, clusterin also has been shown to alter the risk of cardiovascular and metabolic diseases [19] .\",\n       \"The finding that children from disadvantaged families start kindergarten with lower language and cognitive skills than those from more advantaged families is old news, emerging repeatedly in studies since the 1950s (e.g. Bereiter & Englemann, 1966; Deutsch, Katz, & Jensen, 1968) . The robustness of such differences is confirmed in more recent research such as the Early Childhood Longitudinal Study, Kindergarten Cohort (ECLS-K), a comprehensive analysis of young children's achievement scores in literacy and mathematics based on a large and nationally representative sample (Lee & Burkam, 2002) . Even before they entered kindergarten, children in the highest SES-quintile group had scores that were 60% above those in the lowest group. In terms of effect size, children in the highest SES-quintile scored .7 standard deviation (SD) units above middle-SES children in reading achievement, while children in the lowest SESquintile scored almost .5 SD units below the middle-SES mean. Moreover, the disparities in children's cognitive performance at kindergarten entry that were attributable to SES differences were significantly greater than those associated with race/ethnicity. Another recent study found that 65% of low-SES preschoolers in Head Start programs had clinically significant language delays (Nelson, Welsh, Vance Trup, & Greenberg, 2011) . This research revealed a systematic relation between degree of language delay and other weaknesses in academic and socio-emotional skills that were well established by 4 years of age. Socioeconomic gradients in language proficiency are also found within populations living in extreme poverty (L. Fernald, Weber, Galasso, & Ratsifandrihamanana, 2011) .\\nA challenging and controversial question: when do SES differences begin to emerge?\\nResults showing that SES differences in verbal abilities are already evident in the preschool years suggest that these disparities must start to develop in the first years of life, setting children on particular trajectories with farreaching consequences for later academic success. How early do such differences begin to emerge? Research on this important developmental question has been limited for a variety of reasons -ranging from methodological challenges in evaluating language proficiency in young children, to the complexities of engaging in debate about politically sensitive issues related to social stratification. The methodological problem is easy to characterize: Until recently, measures available for assessing language and cognitive proficiency in children younger than 3 years have not been high in predictive validity, limiting their effectiveness in linking characteristics in infancy to long-term outcomes. But with the refinement of more sensitive methods for evaluating early language, recent studies have revealed considerable variability in verbal skills among very young children -to be reviewed in the following section. Another set of issues that has discouraged research on early origins of cognitive differences among children from different backgrounds is more difficult to characterize. The legacy of a prolonged and bitter debate about the nature of racial and SES differences in the US has reinforced the reluctance of researchers to pursue the question of early origins of SES-related disparities in cognitive skills that are relevant to school success.\\nA brief history of this complex debate is relevant to the issues raised in the current study. The scientific consensus in the early 20th century was that cognitive abilities were entirely genetically determined, a view that changed gradually with mounting evidence that experiential factors were also influential (see Fernald & Weisleder, 2011) . By the 1960s, when the Civil Rights movement focused national attention on inequities in educational opportunities for Black children, there was intense interest in eliminating achievement gaps that could no longer be ignored. Riessman (1962) argued that SES disparities in school success resulted from cultural differences in minority children's early experience with parents in the home, rather than from immutable genetic differences. This 'cultural deprivation' argument appeared to offer hope for solutions through appropriate intervention, although characterizing the home environment of minority children as deficient in cognitive stimulation clearly had negative implications. While this idea rallied political support for new programs such as Operation Head Start, what came to be known as the 'deficit model' also generated intense controversy among educators who objected that parents should not be blamed for their children's difficulties in school. By the 1970s, politically motivated backlash to the deficit model converged with the rise of nativist theories of language development, which focused on modal patterns of development presumed to be universal rather than on differences among children. Fernald and Weisleder (2011) argue that this convergence was influential in curtailing debate on questions that had generated extensive research over the previous two decadesnamely, whether SES differences in children's verbal abilities are rooted to some extent in differences in their early language experience at home, and if so, whether these experiential differences contribute to the substantial disparities observed among children in their later academic success.\\nAlthough interest in variability in language learning had declined substantially by the 1980s, a few researchers began to explore in greater depth the potential contributions of early parent-child interaction to differences in language development (e.g. Hart & Risley, 1995; Hoff-Ginsberg, 1998; Huttenlocher, Haight, Bryk, & Seltzer, 1991) . Based on detailed analyses of mothers' speech to infants at home, these studies used longitudinal designs to identify features of maternal speech that predict language outcome measures. Hart and Risley found that by 36 months, the higher-SES children in their sample spoke twice as many words as the lower-SES children. But their most remarkable finding was the extreme variation in amounts of child-directed speech among families at different SES levels, differences that were correlated with children's early vocabulary and were also predictive of later school performance (Walker, Greenwood, Hart, & Carta, 1994) . According to Hoff (2003) , it was the quality of infants' early language environment that actually mediated the link between SES and children's vocabulary knowledge.\",\n       \"To examine whether inquiry-based science content courses promote a positive change in attitudes towards science for pre-service elementary teachers, we begin by comparing the means on the pre-and post-measures of our four attitudinal scales for HoS students. Results of paired t-tests indicate a statistically significant improvement over time for each scale. Specifically, for confidence, the mean increased from 2.61 to 2.98 (p<.001), reflecting a 0.37 5 To further test the reliability of the four scales, we calculated Cronbach's alphas separately for a) treatment and reference groups; b) first year cohort and second year cohorts (for treatment group), and c) survey responses at time 1 and time 2 (for different cohorts and for different treatment groups). In all we calculated alpha reliabilities for each of our four scales for fifteen different sub-samples with remarkably consistent results, with results ranging from a low of .77 to a high of .88. (Exploratory factor analyses using each of these groups also yielded the same four factor model). point increase or almost half of a standard deviation change from pre to post. HoS students' science enjoyment increased by about a fourth of point (and about a third of a standard deviation), from the pre-survey mean of 3.24 to a post-survey mean of 3.51 (p<.001). The largest change was observed for the anxiety scale, where the average decreased (meaning that students became less anxious over time) from a pre-survey mean of 3.11 to a postsurvey mean of 2.63 (p<.001), a difference of almost half of a point and approximately twothirds of a standard deviation. Finally, as discussed earlier, students view science as a relevant domain, as evidenced by a relatively high pre-survey mean of 3.66 on the utility scale. Nevertheless they slightly increased their views of the relevance of science after taking HoS courses (p<.05) to a post-survey mean of 3.74, a change of about a tenth of a standard deviation. Subsequently we turn to an examination of how the changes we observe for HoS students compare to changes in attitudes towards science among students taking more traditional lecture-based science content courses. Here we utilize multilevel mixed-effects models, an extension of regression analysis that is similar to a mixed-design ANOVA and appropriate when data are nested. For this study, repeated measures of attitudes are nested within individuals with time treated as a random effect (Rabe-Hesketh & Skrondal, 2008).The goal of this analysis was to determine whether the change in different dimensions of science attitudes observed for HoS students was similar or different than that observed for non-HoS students while controlling for students' background characteristics (which is particularly important as our two groups of students differed by gender and math SAT scores, both factors which likely predict attitudes). Table 2 displays the results of separate analyses for each dependent variable. The first row displays the coefficient comparing HoS and non-HoS students on the pre-survey (or time 1) for each attitudinal dimension. The second row displays the average change over time between the pre-and post-survey, while the third row displays the interaction between student group (HoS or non-HoS) and time. The change in attitudes from pre-to post-survey for HoS students is calculated as the sum of the main effect of time and the interaction term, while change for non-HoS students is captured by the main effect of time only. To simplify the presentation of results we include figures for each attitudinal outcome (Figures 1-4) that display the changes over time for each group, adjusted for the social and academic characteristics discussed above.6 Beginning with the first column predicting confidence, the results reveal that compared to their peers in traditional science classes, HoS students reported significantly lower science confidence on the pre-survey (-.391***). The coefficient for time is negative and significant; yet the interaction between time and student group is positive and significant, indicating that HoS students increased their confidence over time relative to non-HoS students. To help clarify the patterns for the two groups, Figure 1 displays the trends for each group. Here we see clearly that changes in attitudes occurred for both groups, yet in opposite directions. While HoS students initially had lower confidence than their non-HoS peers, they significantly increased their confidence over time. In contrast, non-HoS students significantly decreased their science confidence (-.127***), such that their confidence was slightly lower than non-HoS students on the post-survey. Returning to table 2, we see a similar pattern when predicting change in science enjoyment. HoS students initially reported significantly lower levels of enjoyment than their peers in more traditional classes (-.163**). However, once again we see a negative main effect of time but a positive and significant interaction between student group and time, indicating opposite directions of change for each group. As displayed graphically in Figure 2, HoS students significantly increase their enjoyment over time, while on average, non-HoS students report a decrease in their affect towards science (-.131***), and end their course reporting lower enjoyment than HoS students. Regarding changes in anxiety, we note that HoS and non-HoS students do not differ significantly on the pre-survey. The main effect of time is positive and borderline significant, yet the interaction term reveals a statistically significant difference between the two groups' average change in anxiety. Figure 3 clearly show the marked decrease in anxiety from the pre-to the post-survey for HoS students. For their non-HoS peers, however, the figure shows a slight increase in anxiety (.074∼). Finally, Table 2 displays the results for models predicting change in attitudes towards the relevance of science. HoS and non-HoS students do not differ significantly on the presurvey. But once again the interaction term reveals a statistically significant difference between groups in change over time, and the sign of the coefficient is positive in contrast to negative main effect of time. Figure 4 displays the disordinal patterns for the groups. HoS students significantly increase a small amount from the pre-to the post-survey while their peers' perceptions of the relevance of science decreases (-.116**). Finally, while the main focus of our analyses was to assess differences between HoS and non-HoS students regarding changes in their science attitudes, our multivariate analyses revealed patterns consistent with prior research, namely that females are significantly less confident in their science ability, and report significantly less enjoyment and more science anxiety than their male peers (Correll, 2001;Eccles, 1994). Our models also indicate some evidence of racial/ethnic differences in attitudes (see the models predicting confidence and anxiety), as well as differences by prior math achievement, as those with higher scores on the math portion of the SAT report significantly higher levels of confidence in their science ability and less anxiety. Given the differential distribution of HoS and non-HoS students on several of these covariates (see Table 1), including them in our models generally decreased the size of the difference between groups on the pre-survey (for example, the more male composition of the non-HoS group helped to account for their initially stronger math confidence), and also revealed larger differences between the groups on the post-survey than could be detected in simpler models that did not account for these factors.\",\n       'This section proposes a probabilistic formulation for the model. The conditional probability is given by p(y = 1|x G ,\\n-For each region i ∈ I and gene G , W i,G ∼ M-Laplace(0, λ W ) (M-Laplace stands for \"Multi-Laplacian prior\"). In other words: \\nThe maximum a posteriori estimation is given by:\\n( W, β I , β G , β 0 ) ∈ argmax W,β I ,β G ,β0\\np(W, β I , β G , β 0 |Y, X I , X G ; λ W , λ I , λ G , G, θ G ) ∈ argmax W,β I ,β G ,β0\\np(W, β I , β G , β 0 , Y, X I , X G ; λ W , λ I , λ G , G, θ G )\\nIt is equivalent to minimize the function S defined by:\\nS(W, β I , β G , β 0 ) = − log p(Y, W, β I , β G , β 0 , X I , X G ; λ W , λ I , λ G , G, θ G ) = R N (W, β I , β G , β 0 ) + Ω(W, β I , β G )',\n       'All the authors declared no competing interests. SMS and RP. Each author contributed important intellectual content during the drafting or revision of the manuscript and accepts accountability for the overall work by ensuring that questions pertaining to the accuracy or integrity of any portion of the work are appropriately investigated and resolved. EDK and SMS take responsibility that this study has been reported honestly, accurately, and transparently; that no important aspects of the study have been omitted, and that any discrepancies from the study as planned (and, if relevant, registered) have been explained. Table S1 . Comparison of baseline characteristics between participants in the study analyses who completed a followup visit and participants who did not complete a follow-up visit among those who survived to 1 year after the first study clinic visit. Table S2 . Follow-up association of pulse wave velocity, augmentation index, and central pulse pressure with global cognitive impairment using additional modeling. Table S3 . Stratified analysis of the follow-up associations by baseline global cognitive impairment status and interactions of pulse wave velocity, augmentation index, and central pulse pressure with baseline cognitive impairment status. Table S4 . Stratified analysis of the follow-up associations by race and interactions of pulse wave velocity, augmentation index, and central pulse pressure with race. Supplementary material is linked to the online version of the paper at www.kireports.org.',\n       \"Early Graduate Supplement NELS:88 participants who graduated from high school prior to data collection in the spring term of 1992 completed the second follow-up early graduate supplement to the student questionnaire. The intent of this supplement was to document the reasons for and the circumstances of early graduation, the adjustments required to finish early, and respondents' activities compared with those of other school survey members. The items for the second follow-up early graduate supplement were modeled on those used in the HS&B sophomore cohort early graduate supplement administered in the HS&B first follow-up in 1982.\",\n       'Hallway offenses included inappropriate behavior, fighting, skipping class, and tardiness (referred to as tardies). Incidences of all four categories decreased from year one to year three: Inappropriate behavior referrals from decreased from 110 in year one to 49 in year three. Skipping class dropped from 130 in year one to 55 in year three. However, tardy referrals decreased only slightly from 36 in year one to 34 in year three, as did referrals for fighting (41 in year one to 38 in year three). See Figure 3 . Researchers analyzed 3 years (2005) (2006) (2007) (2008) of student discipline referral data for grades 7 and 8 and examined what these results meant in terms of school-wide discipline and student behavior. Continuous progress monitoring of student behavior through office referral data helped to guide systemic reform efforts. Implementation resulted in a significant reduction in the number of discipline referrals. Data revealed a decrease of 23% in the total number (2239 vs. 1723) of discipline referrals from year one to year two and a decrease of 22% in the number (1723 vs.1340) of discipline referrals from year two to year three. Results obtained from the analysis indicate that the implementing the Texas Behavior Support Initiative (TBSI) was effective in reducing the overall number of offenses in student behavior in a rural middle school (See Figure 4) .\\nResults of the Chi-square analyses revealed statistically significant decreases in offenses. There was a significant X 2 (1)= 117.29, p<.017 decrease in offenses from year 1 to year 2 (2239 vs. 1723). There was a significant X 2 (1)= 85.14, p<.017 decrease in offenses from year 2 to year 3 (1723 vs. 1340). Obviously, this also resulted in a significant X 2 (1) = 358.40, p<.017 decrease in offenses from year 1 to year 3 (2235 vs. 1340).\\nStrategies and interventions were put into practice at every level to encourage positive behavior. The majority of the interventions were implemented at the primary (school-wide) level. These interventions included creating a set of schoolwide rules, revising the master schedule to include an advisory period, reducing the passing period from five to four minutes, universal monitoring of hallways between classes, visual reminders of the school-wide rules, and increased communication via radios for the behavior support team. At the secondary level, the behavior team concentrated their efforts on selected groups of students and targeted settings. Successful interventions at this level included daily administrative walk-throughs for targeted settings, initiating a partnership with Communities in Schools, improving interactions between students and staff, building relationships, and collecting and utilizing discipline data to guide decision-making.\\nStrategies and interventions at level three focused on the individual student. Interventions included hand scheduling of \"frequent flyers\" to 5 avoid an accumulation of problem behaviors in a single classroom, lunch detention, intent to cite warning notifications, civil citations, reintegration conferences for students returning from a disciplinary alternative campus, and continued collection and analysis of data to drive decision-making process. Figure 4 . The total number of offenses prior to TBSI to year 1, year 2, and year 3 of study. A reduction of 53% was seen since the inception of the Texas Behavior Support Initiative.\\nResults of the data analysis prompted the researchers to investigate academic achievement as measured by the Texas Assessment of Knowledge and Skills (TAKS) in relation to the decrease in discipline referrals. Data from the Texas Assessment of Knowledge and Skills (TAKS) revealed that scores improved from prior to TBSI implementation when compared to year three of implementation. The sample included all 7 th and 8 th grade students who were continually enrolled at the middle school during each of the data collection years (2005) (2006) (2007) (2008) . Improvement in scores could be attributed in part to the decrease in discipline referrals (See Table 1 ). 6 Some common themes that data revealed were 1. Most referrals occurred in the first semester of school. The lack of a smooth transition from one campus to another may be the reason for the greater number of referrals generated in the first semester. Infractions of the rules occur when the students are learning the new rules; 2. Most referrals occurred on Tuesdays. Tuesday was the day of the week in which school attendance was the highest; 3. Sixty-nine percent of all referrals came from male students. There were more boys than girls in the sample population resulting in a greater number of referrals generated by the male population; 4. Fifty-three percent of all referrals came from 7 th grade students. Seventh grade students are in transition from one campus to another; 5. Ninety percent of all referrals came from the classroom. Most infractions occur while they are learning the expectations of their new campus. That most referrals occur in a classroom is not surprising considering students spend most of the school day in this setting; 6. Eighty-five percent of all referrals were from Hispanic students. School demographics indicated that 81% of the student population was Hispanic. Therefore, the largest student population generated the most discipline referrals; 7. TAKS scores improved overall from prior to TBSI implementation to year three of implementation. Time previously spent correcting disruptive behavior was applied to direct instruction. This managed instruction resulted in an improvement in academic achievement. Implementation of these school-wide strategies and interventions have resulted in a reduction in the number of behavioral disruptions in both classroom and non-classroom settings such as hallways and cafeterias. Although academic achievement outcomes were not the primary focus of this study, the initial data suggests that there is a strong empirical connection between school-wide behavior support and academic achievement as measured by the Texas Assessment of Knowledge and Skills. Researchers compared the TAKS scores from the years prior to implementation of TBSI to year three, the state assessment data revealed an improvement in scores over a three year period (See Table 1 ). In conclusion, implementation of the Texas Behavior Support Initiative (TBSI) was an effective program in a rural middle school in reducing the number of offenses from Year 1 to Year 3. Senate Bill 1196 was successful in decreasing the number of referrals among seventh and eighth grade students in a rural school district. This type of program may perhaps be effective in other rural areas of the state, as well as other metropolitan and urban areas.',\n       \"Using automatic evaluation metrics such as BLEU and chrF allows MT system developers to rapidly test the performance of their engines, experimenting with the data sets available and fine-tuning the parameters to achieve the best-performing set-up. However, it is well-known (Way, 2018) that where possible, human evaluation ought to be conducted in order to verify that the level of quality globally indicated by the automatic metrics is broadly reliable.\\nAccordingly, for all four 'into-English' language pairs, we performed a human evaluation on 100 de-en it-en es-en fr-en better than both 19% 5% 5% 4% same 50% 69% 50% 68% worse than either 15% 26% 18% 17% not fully clear 16% 15% 27% 11% Table 8 : Percentage of sentences translated by our system which are adjudged to be better, worse or of the same quality compared to Google Translate and Bing Translator sentences from the test set, comparing our system against Google Translate and Bing Translator; we also inspected Amazon's output, but its quality was generally slightly lower, so in the interest of clarity it was not included in the comparisons discussed here.\",\n       'CSF levels of soluble A␤ 1-40 , soluble A␤ 1-42 , and oligomeric A␤ were analyzed from spinal tap samples taken at Baseline and Day60 (end of 2month treatment period). Two months of daily TEMT administration induced a 12% clinically important and significant increase in CSF soluble A␤ 1-40 levels compared to Baseline (10,234 ± 1,103 versus 11,458 ± 1,184 pg/ml; ES = 2.10; p < 0.001). Similarly, a 12% clinically important increase in CSF soluble A␤ 1-42 levels was induced by TEMT administration compared to BL (272 ± 34 versus 304 ± 28 , and oligomeric A␤ (C) following 2 months of TEMT (Day60) compared to levels at Baseline for all individual subjects. D) For subjects that showed a decrease in CSF A␤ oligomers following 2 months of TEMT (n = 3), combined performance on the ADAS-cog was stable on Day60 compared to Baseline. For subjects that showed an increase in CSF A␤ oligomer levels after 2 months of TEMT (n = 5), a sizable 5+ point improvement in their combined ADAS-cog score was present on Day60 versus Baseline. pg/ml; ES = 1.06; p < 0.05). For soluble A␤ 1-40 and A␤ 1-42 , BL and Day60 values for individual subjects are shown in Fig. 6A and B . By contrast, CSF levels of A␤ oligomers were not significantly changed by 2 months of TEMT administration (2163 ± 283 pg/ml) compared to BL (2203 ± 339 pg/ml). BL and Day60 values of CSF oligomeric A␤ levels for individual subjects are shown in Fig. 6C . When subjects were assigned to one of two groups based on whether 2 months of TEMT resulted in a decrease (n = 3) or increase (n = 5) in CSF oligomeric A␤, decreased oligomeric A␤ (averaging ↓28%) was associated with stable ADAS-cog performance between Day60 and BL (Fig. 6D) . By contrast, a treatment-induced increase in CSF oligomeric A␤ (averaging ↑25%) was associated with over a 5-point improvement in ADAS-cog for Day60 versus BL (Fig. 6E) .\\nMeasurement of t-tau and p-tau in CSF at BL and following 2 months of TEMT indicated no effect of treatment on either measure (Fig. 7A, B ). As well, there was no change in the p-tau to t-tau ratio in CSF following 2 months of daily TEMT administration ( Fig. 7C ). However, there was a modest, but clinically important, reduction in both the CSF p-tau to A␤ 1-42 ratio (ES = 0.61; p = 0.161) ( Fig. 7D ) and the CSF t-tau to A␤ 1-42 ratio (ES = 0.52; p = 0.202) following TEMT. Similarly, measurement of t-tau levels in plasma revealed modest, but clinically important, increases in t-tau at both Day60 (ES = 0.56; p = 0.161) and 14D Post (ES = 0.51) (Fig. 7E) .\\nTwo months of TEMT induced a 24% reduction in plasma soluble A␤ 1-40 versus BL (ES = 0.54; p = 0.241) ( Fig. 8A) , with the percent reduction in individual subjects positively correlated with their improvement in ADAS-cog performance (r = 0.72; p < 0.05). Modest increases in plasma A␤ were present at both Day 60 (ES = 0.60; p = 0.166) and 14D Post (ES = 0.60; p = 0.205) compared to BL Fig. 7 . Following 2 months of daily TEMT (D60), CSF levels of p-tau (A) and total tau (B) were unchanged compared to their Baseline (BL) levels. As well, there was no change in the p-tau to t-tau ratio in CSF as a result of TEMT administration (C). However, the p-tau to A␤ 1-42 ratio was reduced (D) and total tau in plasma was increased (E) following 2 months of daily TEMT. Means ± SEMs are presented. Post = 14Days Post-treatment. *ES significant at > 0.5 level versus Baseline. ( Fig. 8B) . A prominent effect of TEMT on plasma A␤ oligomers was evident, wherein a large clinically important 33% reduction was observed after 2 months of TEMT administration (ES = 1.16; p < 0.05) and maintained 14 days after treatment termination (ES = 0.73; p = 0.177).\\nSeparate from the above TEMT-induced effects within CSF and plasma were the presence of large concentration differences between CSF versus plasma for the various A␤ isoforms-this, irrespective of whether evaluated at BL or Day60. For soluble A␤ , approximately 50-fold higher levels were present in CSF versus plasma. Similarly, a 6-fold higher level of soluble A␤ was evident in CSF versus plasma. By contrast, oligomeric A␤ levels were twice as high in plasma compared to CSF. For p-tau, levels were always higher in CSF because p-tau levels were essentially zero in plasma. Subjects exhibited no clear concentration difference for t-tau, with half of them having higher levels in CSF and the other half having higher levels in plasma.',\n       'In 2017, the southern British Columbia interior experienced its longest and most severe wildfire season in the province\\'s history. After a wet spring, the region had its driest summer on record. One of the earliest and largest fires ever recorded in Canada burned west of Kamloops in the Ashcroft-Cache Creek-Clinton area. The towns of Ashcroft, Kamloops, and Kelowna each received less than 10 mm of total precipitation during the entire summer. A province-wide state of emergency, the first in 15 years and the province\\'s longest one, began on 7 July and lasted until 15 September. In total, the British Columbia Wildfire Service reported 1265 fires that destroyed 1.2 million hectares of timber, bush, and grassland, exceeding the previous record for burned land by 30%. Total firefighting costs exceeded half a billion Canadian dollars and insured property losses reached close to $130 million Canadian dollars ($103 million U.S.  (Kochtubajda et al. 2017). In May, eastern Ontario and southern Quebec each experienced one of their worst spring flooding events on record. Several rivers exceeded the maximum amount of water released in the past and overflowed from Gananoque to Gaspésie. In Montréal, April rainfall totaled 156.2 mm-its second wettest April in 147 years. Both Ottawa and Montréal had their wettest spring on record-with 400 mm or more at each location (records date back to the 1870s). Spring flooding forced 4000 people to evacuate their homes from the Ottawa region to near Quebec City. Many towns and cities declared states of emergency, including Gatineau, Laval, and Montréal. According to the Insurance Bureau of Canada, spring flooding in April and May resulted in 15 750 claims and $223 million Canadian dollars ($177 million U.S. dollars) in property damages. In total, more than 5000 residences were flooded, 550 roads were washed or swept away by floods or landslides, and-tragically-on 6 May, two people were swept away by the swollen Sainte-Anne River in the Gaspé region. 2) United StateS-J. Crouch, A. Smith, C. Fenimore, and R. R. Heim Jr. The annual average temperature in 2017 for the contiguous United States (CONUS) was 12.5°C or 1.0°C above the 1981-2010 average-its third warmest year since records began in 1895, 0.2°C cooler than 2016 and 0.4°C cooler than 2012 ( Fig. 7.3). The annual CONUS temperature over the 123-year period of record is increasing at an average rate of 0.1°C decade −1 , with the trend increasing since 1970 to 0.3°C decade −1 . The nationally averaged precipitation total during 2017 was 104% of average, the 20th wettest year in the historical record. The annual CONUS precipitation total is increasing at an average rate of 4.3 mm decade −1 . Outside the CONUS, Alaska had its seventh warmest year (+1.2°C departure) since statewide records began in 1925, and near-median precipitation (104% of average). Complete U.S. temperature and precipitation maps are available at www.ncdc.noaa.gov/cag/.\\nThere were 16 weather and climate events with losses exceeding $1 billion (U.S. dollars) each across the United States (Fig. 7.5) in 2017, including three tropical cyclones, eight severe storms, two inland floods, a crop freeze, drought, and wildfires. The 2017 total tied with 2011 as highest annual number of U.S. billion-dollar disasters (adjusted for inflation) since records began in 1980. Cumulatively, these events led to 362 fatalities and caused $306 billion U.S. dollars in total, direct costs-a new U.S. annual cost record. The previous costliest year for the U.S. was 2005 with losses of $215 billion. One of the more noteworthy events included the western wildfire season, with total costs of $18 billion, tripling the previous U.S. annual wildfire cost record set in 1991. Overall, wildfires burned over 4.0 million hectares across the United States during 2017, which is well above the 2000-10 average of 2.7 million hectares. Hurricane Harvey had total costs of $125 billion, second only to Hurricane Katrina in the 38-year period of record for billion-dollar disasters. Hurricanes María and Irma had total costs of $90 billion and $50 billion, respectively. Hurricane María now ranks as the third AUGUST 2018 | S196 costliest weather and climate disaster on record for the nation, and Irma ranks as the fifth costliest. Tornado activity during was above average for the first time since 2011 with 1400 tornadoes confirmed, compared to the 1991-2010 annual average of approximately 1250. There were 34 tornado-related fatalities, well below the 30-year average of 110. 3) MexiCo-R. Pascual Ramírez and A. Albanil The 2017 mean temperature for Mexico was the highest since national temperature records began in 1971, marking the fourth consecutive year that a new national annual temperature has been tied or broken. Precipitation during 2017 varied greatly across the country; however, the 2017 national precipitation total was near average at 99.4% of normal.\\nTen tropical cyclones affected Mexico in 2017, five fewer than the 1971-2012 average of fifteen. Six tropical cyclones were near land or made landfall from the Pacific basin, and four from the Caribbean basin/ Gulf of Mexico. The Pacific number was fewer than the average of ten, and the Caribbean/Gulf of Mexico number was near the average of five. Of note, Caribbean Hurricane Franklin (Category 1 on the Saffir-Simpson scale) produced the year\\'s highest 24-hour precipitation total for Mexico when 404 mm fell in Veracruz upon landfall on 9 August. This value ranks among the top 20 highest daily precipitation totals recorded in the country, according to the Mexican National Meteorological Service. Drought conditions, which commenced during spring (March-May) 2016, continued to affect southern Mexico in 2017, in particular the Isthmus of Tehuantepec in Oaxaca. Drought conditions deteriorated during the first five months of 2017 due to the warmer-and drier-than-average conditions affecting the area. However, heavy precipitation associated with Tropical Storms Beatriz and Calvin, which made landfall in the affected area, helped ameliorate the long-term drought. These two storms impacted the same area within two weeks of each other (1 June and 12 June, respectively), producing much-needed precipitation and relief for the agriculture sector, but causing damage to infrastructure, such as damaged roads and bridges due to landslides. Drought also af- fected southern Sinaloa, in northwest Mexico, causing agricultural and livestock losses, and a shortage of drinking water in more than 400 rural communities. Several heat waves affected eastern Mexico, notably the Huastecas (an area that encompasses the states of San Luis Potosi, Hidalgo, and Veracruz) from 26-30 April and again from 5-8 June. During both heat waves, the maximum temperature reached 50°C, breaking the previous record of 49°C in Huejutla, Hidalgo, set in April 2013. These heat waves were produced by a broad high pressure system located over northeastern Mexico, inhibiting cloudiness and thus increasing temperature. Another major heat wave affected the municipality of Aldama, Chihuahua, during 11-20 June. c. Central America and the Caribbean 1) Central ameriCa-J. A. Amador, H. G. Hidalgo, E. J. Alfaro, B. Calderón, and N. Mora For this region, nine stations from five countries were analyzed (Fig. 7.9). Stations on the Caribbean slope are: Philip Goldson International Airport, Belize; Puerto Barrios, Guatemala; Puerto Lempira, Honduras; and Puerto Limón, Costa Rica. Stations located on the Pacific slope are: Tocumen International Airport and David, Panamá; Liberia, Costa Rica; Choluteca, Honduras; and Puerto San José, Guatemala. The station distribution covers the relevant precipitation regimes located on the Caribbean and Pacific slopes of Central America (Magaña et al. 1999). Precipitation and temperature records for the stations analyzed were provided by Central American National Weather Services (CA-NWS) or by NOAA. Anomalies are reported using a 1981-2010 base period and were calculated using CA-NWS data. The methodologies used for all variables can be found in Amador et al. (2011).\\nTropical storms were very active in the Caribbean basin (6°-24°N, 92°-60°W) during 2017. There were eight named storms: five tropical storms (Bret, Franklin, Harvey, Nate, and Phillipe) and three major hurricanes (Irma, José, and María). Tropical Storm Nate made landfall in Nicaragua and crossed Honduras on 5-6 October. Nate induced indirect cyclonic circulations (Peña and Douglas 2002) over the isthmus, impacting the Pacific slope of Costa Rica. According to the Costa Rica National Emergency Commission (CNE, its Spanish acronym), Nate caused more than $540 million U.S. dollars in damages, the highest amount in the country\\'s documented history of natural disasters since 1996. This information is based on a CNE study (Hidalgo 2017) of economic loses including Tropical Storms Alma (2008) and Nate 2017and Hurricanes Cesar (1996), Mitch (1998), Tomas 2010 (Fig. 7.10a). Some locations in the northern Caribbean (including southern Cuba and Bahamas) experienced below-normal surface temperatures during January-June. In the latter half of the year, above-normal surface temperatures (+0.2° to +1.0°C) were spread across the entire region. Trinidad reported its tenth warmest annual mean temperature (28.0°C) since records began in 1946; second highest mean maximum temperature in August (33.6°C), which tied with August 2015 and 2016; and highest daily maximum temperature for August (35.8°C) set on 23 August. San Juan, Puerto Rico, had its third warmest mean temperature in both February (26.4°C) and September (29.2°C) since records began in 1898. Grenada had its highest May mean maximum temperature on record as temperatures soared to 31.1°C in Point Salines. Several locations across the Caribbean had annual maximum temperatures among their nine highest on record (Table 7.1).\\nCategory 5 Hurricane Irma severely impacted the Caribbean during 5-8 September. Some of the impacts of Irma on the islands included: 14 deaths and over 50 000 residents without electrical power in the Turks and Caicos; one death and total destruction in Barbuda; several deaths reported in St. Martin; one death and severe damage in Anguilla; damage to property in St. Kitts; five deaths and extensive damage in the U.S. Virgin Islands; four deaths and severe impacts in the British Virgin Islands; major power outages over eastern Puerto Rico; more than 2000 homes damaged in the Dominican Republic; and flooding in some northern coastal areas in Cuba. See Sidebars 4.1 and 7.1 for more detailed information about Irma.\\nHeavy rains that fell in Peru during January-May (Fig. 7.14) were triggered by the coastal El Niño present in the eastern tropical Pacific Ocean (see Sidebar 7.2). Torrential rainfall triggered flash floods and landslides that affected over 625 000 people in the regions of Tumbes, Piura, Lambayeque, La Libertad, Ancash, Ica, and Arequipa and claimed nearly 100 lives. Losses include 242 bridges, 13 227 km of rural and main roads (1.5% of the national road system), 45 335 km of agricultural irrigation channels, and 60 400 ha of crops. In the suburbs of Lima, landslides (\"huaycos\") destroyed houses, and the highway between Lima and the Andean cities was inaccessible for several days. The January 2017 precipitation total for the city of São Paulo was 453.8 mm, 179% of normal for the month, and its wettest January since 2011. The copious rain prompted flash floods in several locations across the city. In the city of Maceio, located on the coast in the state of Alagoas in northeast Brazil, a state of emergency was declared due to torrential rains that produced landslides and flash flooding on 27 May, killing three people. By 29 May, over 8400 families were affected, and more than 16 500 people were left homeless. Total rainfall in May 2017 was 742.4 mm (more than twice the monthly normal of 344.7 mm), with 169.6 mm recorded on the 27th. During the first two weeks of June, well-abovenormal rainfall was observed in the eastern portion of the state of Santa Catarina (southern Brazil) due to the passage of a cold front. Torrential rains affected more than 28 800 people and, in some districts, a state of emergency was declared due to floods. The same cold front caused heavy rainfall and flash floods in Rio de Janeiro, and the total rainfall measured on 20 June was almost 247 mm (June climatology is 461.8 mm). This event affected public transportation in the city and flooded some neighborhoods. West-central Brazil, particularly Brasilia (Distrito Federal), has been affected by dry conditions since 2015. The drought conditions, which continued into 2017, were the worst in the last 57 years. In April 2017, Brasilia received only 20% of its normal April precipitation, which is 125 mm; in fact, during the peak of the rainy season (October 2016-April 2017), only February had above-normal monthly rainfall (Fig. 7.15). This prompted a state of emergency and mandatory water restrictions. The most intense cold episode during austral winter 2017 occurred during 17-19 July. A polar air mass affected the Andes, bringing cooler-thannormal conditions to the western Amazonia regions of Brazil, Peru, and Bolivia. On 17 July, minimum temperatures as low as 10°C were recorded in the Bolivian Amazon and in Puerto Maldonado, Peru (July climatology of 18°C), while on 18 July the western Brazilian Amazon saw temperatures drop to 7.2°C in Campo Verde (located in the state of Mato Grosso; climatology of 21.2°C), 11.3°C in Epitaciolândia (located in the state of Acre; climatology of 19.0°C), and 11.1°C in Guajará-Mirim (located in the state of Rondonia; climatology of 20.0°C). In the city of São Paulo, the maximum temperature was 8°C (climatology of 11.7°C) on 18 July, and one person died due to exposure to the cold temperatures. From mid-July to mid-August, a cold front in Peru produced temperatures as low as −20°C at 4000 meters above sea level (the record-coldest value is −25°C set on 6 July 1968 at Macusani station in Puno region), and snow fell in the Andes of Peru and Altiplano. 3) SoUthern SoUth ameriCa-J. L. Stella and L. S. Aldeco This region includes Argentina, Chile, and Uruguay. (i) Temperature Above-normal temperatures were observed across southern South America (SSA) during 2017, with annual mean temperatures 0.5°-1.5°C above normal. The national mean temperature anomaly for Argentina and Uruguay was +0.68°C and +1.0°C, respectively, placing 2017 as the warmest year on record since 1961 for both countries. The five warmest years on record for Argentina have all occurred since 2012 ( Fig. 7.16). The mean temperature anomaly by decade since the 1960s (Fig. 7.17) shows an increase across central and northern Argentina during 2001-2010 and a significant rise across the country as a whole during the decade to date (2011-2017). Su mmer (December-Febr ua r y) 2016/17 was particularly warm over most of SSA, with mean temperatures 1°-2°C above normal. Chile had its second warmest summer since 1964. At the end of the season, a heat wave affected a large area in central Argentina. The maximum duration of extreme heat, defined here as minimum and maximum temperatures surpassing the 90th percentile, ranged between five and eight days, and for some locations these conditions extended into the beginning of March, resulting in one of the latest heat waves recorded in that area. Below-normal maximum temperatures and above-normal minimu m temp er at u re s during austral autumn (March-May) resulted in near-normal mean temperatures across much of SSA (±0.5°C). Winter (June-August) 2017 was extremely warm over much of the region, with temperatures 1°-3°C above normal across the eastern and northern parts of SSA. This was the warmest winter on record for Uruguay and second warmest for Argentina in their 46-year records. Several individual locations in eastern Argentina reported their warmest winter on record. A new national maximum temperature record was set on 17 June when the temperature soared to 40°C at Tinogasta (northwestern Argentina), marking the first time on record the temperature reached 40°C between May and July. Meanwhile, a few cold outbreaks during June and July also impacted the region. Bariloche (northwestern Patagonia, Argentina) broke its absolute minimum temperature record when temperatures dropped to −25.4°C on 16 July. The previous record was −21.1°C set on 30 June 1963. These cold spells also caused heavy snowfalls over southern Argentina and Chile and broke several minimum temperature records across Uruguay during June and July. Spring (September-November) was characterized by below-normal temperatures. The cooling in the Pacific Ocean during spring contributed to the change in temperature pattern across SSA.\\nDuring January, an extraordinary heat wave affected central Chile and Argentina. The Chilean cities of Antofagasta and Curicó recorded the most prolonged warm periods with extreme high temperatures for 14 and 17 days, respectively. The temperature at Santiago de Chile rose to 37.4°C, the highest value recorded in the 104-year record. The locations of Chillán (41.5°C), Los Angeles (42.2°C), and Curicó (37.3°C) also broke their maximum temperature records. In Argentina, the temperature reached 43.4°C on 27 January at Puerto Madryn, the highest temperature ever recorded so far south (43°S) anywhere in the world. Drought, combined with high temperatures, triggered devastating forest fires in large areas of central and southern Chile in January. More than 600 000 hectares were burned, with thousands of people affected. Central Argentina had a similar situation with forest fires affecting La Pampa province, leading to more than 1 million hectares burned and cattle and crops losses. On 30 March, Comodoro Rivadavia reported an impressive daily rainfall amount of 232.4 mm, close to the city\\'s annual normal precipitation total. The heavy rain produced severe flash floods that affected the region. A few days later, the city was impacted once again by heavy rainfall (more than 60 mm in a few hours), leaving most of the city destroyed. On 15 July, Santiago, Chile\\'s capital, experienced its heaviest snowfall since 1922, with 3-10 cm of snow. Meanwhile, the same synoptic system produced 40 cm of snow over the city of Bariloche, Argentinaits heaviest snowfall in 20 years. SIDEBAR 7.2: THE 2017 COASTAL EL NIÑO-K. TAKAHASHI, V. ALIAGA-NESTARES, G. AVALOS, M. BOUCHON, A. CASTRO, L. CRUZADO, B. DEWITTE, D. GUTIÉRREZ, W. LAVADO-CASIMIRO, J. MARENGO, A. G. MARTÍNEZ, K. MOSQUERA-VÁSQUEZ, AND N. QUISPE located at the core of the ITCZ extension, had a February-March precipitation total of 723 mm, which is nearly seven times its normal amount of 106 mm. The largest precipitation anomalies were observed at low and medium elevations on the western slope of the Andes, triggering several floods and mudslides along the Peruvian coast. Mean January-March 2017 river discharge was around 250% of normal in the Santa (9.01°S, 77.76°W), Rímac (11.77°S,76.46°W), and Cañete (12.77°S, 75.83°W) River basins. Impacts along the coast were severe. In the northern regions, a total of 50 927 houses were damaged with close to 1.2 million people affected by flooding, and over 76 000 ha of crops were damaged. As is common with El Niño, this event affected marine resources, primarily the anchovies (Ñiquen and Bouchon 2004;Ñiquen et al. 1999), resulting in decreased fat content and early spawning as a reproductive strategy (IMARPE 2017). The estimated growth of the Peruvian gross domestic product in 2017 was 1.3% lower than expected (BCRP 2017). The coastal El Niño appears to have been initiated by westerly anomalies in the equatorial far-eastern Pacific in January, the largest for that month since 1981, with a northerly component near the coast (Fig. SB7.3a). At upper levels, the Bolivian high (Lenters and Cook 1996) was located west of its normal position, and a subtropical ridge spread from the Northern Hemisphere, resulting in easterly anomalies and divergence favorable for convection over northwestern Peru (Kousky and Kayano 1994;Vuille et al. 2000). The Madden-Julian oscillation (MJO) had its highest amplitudes in the second half of January and was dominated by the MJO phases 1 to 3, which feature westerly anomalies in this region, according to the Real-time Multivariate MJO index (RMM; Wheeler and Hendon 2004; see Section 4c). The northerly component was probably associated with the negative mean sea The original concept of El Niño consisted of anomalously high sea surface temperature and heavy rainfall along the arid northern coast of Peru (Carranza 1891;Carrillo 1893). The concept evolved into the El Niño-Southern Oscillation (ENSO; Bjerknes 1969), although the original El Niño and the Southern Oscillation do not necessarily have the same variability (Deser and Wallace 1987), and the strong El Niño episode in early 1925 coincided with cold-to-neutral ENSO conditions (Takahashi and Martínez 2017). To distinguish the near-coastal El Niño from the warm ENSO phase, Peru operationally defines the \"coastal El Niño\" based on the seasonal Niño 1+2 SST anomaly (ENFEN 2012;L\\'Heureux et al. 2017). While recent attention has been brought to the concept of ENSO diversity (e.g., \"central Pacific\" vs \"eastern Pacific\" events; Capotondi et al. 2015), the coastal El Niño represents another facet of ENSO that requires further study in terms of its mechanisms and predictability. A strong coastal El Niño developed off the coast of Peru from January to April 2017 (ENFEN 2017;WMO 2017a,b;Takahashi and Martínez 2017;Ramírez and Briones 2017;Garreaud 2018). The changes were dramatic within the cool coastal upwelling region, as daily SST at Puerto Chicama (7.8°S, 79.1°W) increased abruptly from ~17°C by mid-January to a peak of 26.9°C in early February (ENFEN 2017). The mean maximum/minimum air temperature anomalies along the coast ranged between +1.0°C and +2.3°C across the north, central, and southern regions during February-March. Convective precipitation is activated in the eastern Pacific when SST exceeds a threshold of ~26°-27°C (Takahashi and Dewitte 2016;Jauregui and Takahashi 2017). With SST well in excess of 27°C, the southern ITCZ branch (Huaman and Takahashi 2016;Fig. SB7.2a) was very strong between February and March 2017 and extended into the South American continent ( Fig. SB7.2b). The coastal city of Piura (5.2°S, 80.6°W), level pressure anomalies in the southeast Pacific (Fig. SB7.3a). The latter could have been associated with Rossby-wave teleconnections from the western Pacific (Garreaud 2018), but the SLP anomalies also extended zonally uniformly across the subtropical South Pacific (Fig. SB7.3a), consistent with the negative phase of Antarctic Oscillation (Mo 2000a), while the subtropical anomalies closer to the coast of South America were probably partly a response to preexisting positive SST anomalies in that region. In early February, rainfall in the southern ITCZ became active, and the subsequent growth and maintenance of the event was consistent with the ocean-atmosphere mechanisms proposed for the 1925 coastal El Niño (Takahashi and Martínez 2017), that is, positive feedback between surface warming to the south of the equator, enhanced southern branch of the ITCZ, and reinforced near-equatorial northerly surface wind anomalies (Figs. SB7.3b,c; e.g., Xie and Philander 1994). The strong coastal ocean warming off northern Peru (> +2°C) was limited to a shallow layer of about 30 m until the end of February, consistent with local surface forcing (Garreaud 2018). This, jointly with the smaller regional basin scale, explains the much faster timescale of this event (Takahashi and Martínez 2017). The termination of the event in April (Fig. SB7.3d) was also abrupt, as the insolation-driven seasonal sea surface cooling (Takahashi 2005) deactivated the southern branch of the ITCZ, shutting down the feedback mechanism. We should note that toward the end of March, the subsurface warming off northern Peru became deeper (down to 180 m; ENFEN 2017) and persisted until May, probably associated with local ocean-atmospheric Bjerknes feedback (Takahashi and Martínez 2017; Dewitte and Takahashi 2017), although warm ENSO conditions did not materialize (L\\'Heureux et al. 2017; also see Section 4b), similar to 1925. The knowledge of the basic mechanism of the 1925 coastal El Niño guided the official Peruvian forecasts in 2017, but only once the event started in late January, since international climate models provided little indication that such an event would occur (ENFEN 2017). Extending the lead time and accuracy of the prediction of coastal El Niño events is a critical challenge for Peru and requires increased understanding and improved models for this region.\\nFlooding in both February and December 2017 caused loss of life and damage to property in Morocco. Cold spells, ranging from 0.3° to 7.0°C below normal, were reported in January and December in Morocco. An all-time heavy rainfall of 119.2 mm was recorded on 23 February at Rabat, Morocco. Extended heat waves occurred over the region during May and June with maximum temperatures exceeding 40°C. These were associated with eastern continental winds and caused significant forest fires, especially in Morocco and Algeria. About 325 forest fires were reported in Morocco, causing the destruction of about 2056 hectares of forested land. 2) WeSt afriCa-S. Hagos, I. A. Ijampy, F. Sima, S. D. Francis and Z. Feng In this section, West Africa refers to the region between 17.5°W (eastern Atlantic coast) and ~15°E (the western border of Chad), and from 5°N (near the Guinean coast) to 20°N. It is typically divided into two climatically distinct subregions; the semiarid Sahel region (north of about 12°N) and the relatively wet Coast of Guinea region to the south. The rainy period over the region is associated with the latitudinal movement of the convective zone referred to as the West African monsoon which typically occurs during June through September.\\nOn 6 July, flooding and wind storms occurred at Jarra Bureng and Jasobo, in The Gambia\\'s Lower River region, destroying hundreds of homes and farms, and affecting 20 000 individuals; wells and latrines were also affected. The event lasted for 4 hours. The event affected 94 households and 857 people across 5 communities. On 12 July, a wind storm at Kerewan, North Bank region, caused one casualty and affected 222 households. A wind storm claimed two lives the same day in The Gambia\\'s West Coast region. In Nigeria, heavy rain during August and September caused the Niger and Benue Rivers to overflow, causing flooding in the Benue and Kogi States. In Benue State, it was reported that 100 000 people were displaced by flooding, 12 local governments within the state were affected, and around 4000 homes damaged. No fatalities were reported. Flooding in Kogi State came just days after thousands of people were displaced by floods in Benue. The Kogi flood displaced over 10 000 people. The worst affected area was the state capital, Lokoja, which lies at the confluence of the two rivers. Other affected areas included Ibaji, Igalamela-Odolu, Ajaokuta, Bassa, and Koton-Karfe. A bridge at Tatabu village along Mokwa-Jebba road, in the Kwara State, collapsed after a heavy rainfall. The road is the major link between the northern and southern parts of the country; motorists were advised to use alternative routes. The popular Ahmadu Bello way in Victoria Island, Lagos State, was temporarily closed on 7 September by the Lagos State Police Command due to flooding. On 27 August, several hours of rainfall caused floods in Churchill Town, Bakoteh, and Ebou town in The Gambia\\'s west coast region, and Tabanani and Sare Molo in its Central River region. Two lives were lost, more than a thousand homes were damaged, and about 4000 people were displaced. On 12 and 29 August, flooding at Kuntaur Niani, in the Central River region, caused significant internal displacements, damage to public and private properties, including a bridge, and submerging of farmlands. There were five fatalities and around 8000 people affected. In Niger, heavy rain on 26 August caused flooding in the capital city of Niamey and surrounding areas; around 100 mm of rain fell in Niamey. The UN Office for the Coordination of Humanitarian Affairs (UNOCHA) reported that two people had died and four were injured. According to Niger\\'s government, 219 houses were destroyed and over 1000 people were left homeless in Gabagoura and other villages around Niamey. 3 ) e a S t e r n a f r i C a -G. Mengistu Tsidu Eastern Africa, also known as the Greater Horn of Africa (GHA), is a region comprised of South Sudan, Sudan Republic, Ethiopia, Somalia, Eritrea, Kenya, Uganda, Rwanda, Burundi, and Tanzania. Despite its location across the equator, the region has a relatively cool climate due to its generally high altitude. Some parts of the region are also characterized by bimodal seasonal rainfalls. In general, the GHA experienced above-average temperatures in 2017.\\nHeavy rainfall recorded in most parts of the region throughout the 2017 rainy seasons caused flooding. For example, eastern Kenya and Tanzania had steady torrential rain in May. As a result, Mombasa recorded 235 mm on 9 May, which led to flash flooding. According to news outlets, at least nine people perished. There were also heavy rains in mid-May, with a number of stations in western and central Ethiopia recording 49 mm and above (e.g., Gore: 59.9 mm; Jimma: 53 mm; Addis Ababa Bole: 49 mm). The subsequent flooding led to a death in the Gambella region of Ethiopia on 18 May. JJAS rainfall was also notably heavy over Sudan and Ethiopia. Resulting f loods affected more than 53 000 people in the Gambella and Oromia regions during August and September, according to a UNICEF humanitarian report.  and Mozambique from December 2016 to February 2017. In contrast, northeastern Namibia, all of Botswana, Southern Zimbabwe, Zambia, southern Mozambique, and northeastern South Africa remained lower t ha n nor m a l (Fig. 7. 2 6 a). Normal to below-normal mean temperat u res ex pa nded to include much of Zimbabwe in MAM (Fig. 7.26b). During JJA, warm conditions dominated across the region, with the exceptions of isolated pockets in Botswana, Zimbabwe, Zambia, Mozambique, and southern strips of South Africa, where nor ma l mea n temperat u re cond it ions were obs er ve d (Fig. 7.26c). However, during SON, southern Mozambique and adjoining areas in Zambia e x p e r i e nc e d t e mp e r a t u r e anoma lies exceeding −1°C, with normal mean temperature p r e v a i l i n g o v e r m o s t o f Zimbabwe, Zambia, northern Botswana, northeastern South Africa, and Swaziland. The rest of the region remained moderately warmer than normal (Fig. 7.26d). For the year as a whole, warmer-than-normal conditions throughout 2017 prevailed across the southern pa r t of t he reg ion (spat ia l representation not shown), as evident from the annual mean temperature anomalies of 26 climate stations in South Africa, which averaged 0.48°C above average (Fig. 7.27).\\nTropical cyclone Carlos passed 130 km off the western coast of Réunion Island on 7 February. A maximum wind gust of 37 m s −1 was recorded at Bellecombe, and 934 mm of rain was recorded at Grand-Ilet within a 72-hr period. The agricultural sector sustained losses up to 4.9 million U.S. dollars. In Madagascar, March 2017 was marked by Cyclone Enawo which formed in the Indian Ocean. It tracked across the island from 7 to 9 March, producing three-day precipitation totals of 224 mm at Sambava, 210 mm at Antsohihy, 184 mm at Antananarivo, 159 mm at Fianarantsoa, 291 mm at Mananjary, and 178 mm at Taolagnaro which were, respectively, 82%, 86%, 112%, 113%, 81%, and 101% of the normal monthly precipitation for March at each location. Enawo led to 81 fatalities, injured 250 people, and caused significant flooding. Eastern Madagascar was the most affected. For this section, 1961-90 is used as the base period for temperature, and 1981-2010 is used as the base period for precipitation, as described in Figs. 7.33-7.37, unless otherwise specified. European countries conform to different standards applied by their individual national weather services, and their specific base periods are noted throughout the subsections as needed. All seasons mentioned in this section refer to the Northern Hemisphere. More detailed information can be found in the Monthly and Annual Bulletin on the Climate in RA VI -European and the Middle East, provided by WMO RA VI Regional Climate Centre on Climate Monitoring (RCC-CM; www.dwd.de/rcc-cm). Anomaly information has been taken from Figs. 7.34-7.37 when national reports are not available.\\nA severe storm affected France on 6-7 March, with peak gusts reaching 54 m s −1 in Brittany. On 18-19 May, thunderstorms over Germany (low \"Dankmar\"), accompanied by hail and heavy rainfall of more than 36.3 mm (Bad Bibra) within 1 hour, led to flooding in several cities. Local intense rainstorms occurred in France during 29-31 May, with 24-hr totals often exceeding 20 mm: 53.9 mm in Genouillax, 58.9 mm in Muret, 72.1 mm in Castelnau-Magnoac, and 80 mm in Chateauponsac. During the same period, in Switzerland a violent storm with heavy rain and hail the size of golf balls was reported at the station in Thun; the storm brought the highest daily precipitation amount (59.6 mm) there since the start of measurements in 1875. In summer, severe hailstorms impacted western and central Europe. Perhaps the most remarkable was cyclone Zlatan, which developed over England on 19 July. Moving eastward, it affected the eastern half of France, Switzerland, Austria, Germany, the Czech Republic, and Poland with heavy rain and hail causing damage, especially in Germany (North Rhine-Westphalia, Station Cologne measured 48.8 mm h −1 and gusts of 26 m s −1 ) with closed roads and traffic delays. The airport in Cologne was closed for 90 minutes. In Austria, between 4 and 6 August, intense precipitation in Lungau (state of Salzburg) and Obersteiermark (state of Styria) led to several landslides, causing an estimated damage of more than 20 million Euros ($25 million U.S. dollars) to the local road network. In October, three storms (ex-Hurricane Ophelia, Storm Brian, and Storm Herwart), with extreme wind speeds of 40 m s −1 or more, brought much damage to the United Kingdom, Ireland, Germany, the Netherlands, France, Austria, Czech Republic, Poland, and Slovakia, with falling trees killing at least seven people in Germany and three in the UK, as well as road and railway blockings affecting traffic for days during clean up. During Ophelia, an individual wave height record of 26.1 m was set at the Kinsale gas platform off the Cork coast (Ireland). Several storms affected central Europe from 17 to 18 November, with wind gusts of more than 48 m s −1 , impacting traffic and causing major damage to trees and buildings. 3) the nordiC and the baltiC CoUntrieS This region includes the Nordic countries Iceland, Norway, Denmark, Sweden, and Finland, and the Baltic countries Estonia, Latvia, and Lithuania (i) Temperature Temperatures across the Nordic and Baltic area in 2017 were mostly higher than normal, between +1° and +2°C. Estonia, Finland, and Denmark had positive anomalies of +1.5°, +1.3° and +1.2°C, respectively. Winter 2016/17 was exceptionally mild due to the inf luence of above-average 500-hPa heights (Fig. 7.36a), with +2°C anomalies in the south and up to +5°C in northern Sweden and Finland. Denmark recorded its fifth and fourth highest daily minimum and maximum temperatures in December, respectively, since 1953. During long-lasting foehn winds in January, a daily mean temperature of 13.8°C was measured at station Sunndalsøra (Norway) on 25 January, which was the highest daily mean temperature ever recorded in January by a weather station in Norway and is a value commonly measured around the beginning of July. During February, 12 stations in Norway observed new daily maximum temperature records as well as extreme anomalies at the Arctic station of Svalbard Lufthavn, with a monthly mean temperature of +9.3°C. On 26 March, under the influence of southwesterly flows, daily maximum temperatures of 20°C or more were measured at stations Akershus, Oslo, Hedmark, Buskerud, and Telemark in Norway, the first known occurrence of such high temperatures in March. In May, the northeastern part of the North suffered from a cold wave which led to several record below-average anomalies in Latvia (e.g., station Rezekne −4.7°C, Mersrags −6.1°C). Finland was also affected, with anomalies between −1° and −3°C for May across the whole country. Temperatures for Lithuania in May also were slightly below normal, and frost days were even recorded. In contrast, a new record high maximum temperature of 31.8°C was measured at Sigdal -Nedre Eggedal (Buskerud) in Norway on 27 May. Under the influence of below-average 500-hPa anomalies, summer in the Baltic States on average had slightly below-normal temperatures with anomalies ranging from −0.5° to −2.5°C in the east of Finland. Lithuania observed an unusually cold July (−1.4°C), while the rest of the summer was closer to average. Overall, the summer was rather cold without any hot spells, which was apparent in the maximum temperatures; for example, Sweden, with a daily maximum temperature of only 28.0°C, had its coolest summer since 1922. During autumn, temperatures were higher than normal in all Nordic and Baltic countries, with around +1°C anomaly. The entire north was under the influence of above-average 500-hPa height anomalies, which also led to a new record high sea level air pressure of 1044.1 hPa at Lycksele and Åsele in northern Sweden. November and December were especially mild. It was the warmest autumn in Denmark since 1984, with a new maximum temperature record of 17.1°C on 2 November at Kjevik (Kristiansand, Vest-Agder, Norway), as well as at station Yngør Lighthouse (Tvedestrand, Norway) with a reading of 14.4°C. With temperatures up to +5°C above normal in eastern Finland and between +1°C and +4°C for most of the Baltic States, 2017 ended rather warm.\\nOn 12 August, widespread thunderstorms left 50 000 households without power in southern Finland. At the end of September, following heavy precipitation and warmth, major damage was caused by floods and landslides in southeastern and eastern Iceland. In Lithuania, three microscale extreme heavy rain events were observed in summer. On 12 July, precipitation totals exceeded 80 mm in 12 hours. Between 22 and 24 November, Storm Ylva caused wind gusts as high as 47.5 m s −1 at station Narvik-Fagernesfjellet (Nordland) in Norway. Another storm, \"Birk\", brought heavy precipitation to Hordaland and Rogaland counties, with a maximum daily precipitation total of 127.5 mm at Gullfjellet (highest mountain in Bergen, Hordaland) measured on 23 December.\\nA heat wave lasting 17-18 days in the northern and central regions of Portugal and 11-12 days in the remainder of the country occurred between 7 and 24 June. During summer, high temperatures and severe precipitation deficits in Portugal enhanced extensive wildfires (> 1000 ha), with more than 60 fatalities; the fires were so large they were visible from space. Due to ex-Hurricane Ophelia, strong winds prevented the extinguishing of fires in Portugal and Spain (9-21 October). At least 41 people were killed in wildfires across the region. Additionally, ashes from the fires were transported as far as the UK, where yellow skies and a red sun were reported, and Switzerland, where ashes were detected at air monitoring sites in Payerne and on Jungfraujoch (3580 m a.s.l.). According to the Portuguese Institute for Nature Conservation and Forests, burned areas exceeded 440 000 ha, a new record. Central regions of mainland Portugal were the most affected by very large fires (> 1000 ha) during several periods: [16][17][18][19][20][21][16][17][18][23][24][25][26][9][10][11][12][13][14][15][16][17][18][19][23][24][25][26][27][5][6][7][8][9][12][13][14][15] October. During these periods, the associated meteorological conditions were extremely favorable to fire propagation and adverse to fire combat; fire weather index values were higher than the 90th percentile in the majority of the regions. \\nIn Bulgaria, for the first time in the past 60 years, the coastal waters of the Black Sea were frozen-an occurrence observed only three times since the beginning of the 20th century. On 21-22 April, a severe frost event in Slovenia caused catastrophic damage to crops. In Greece, extensive and long-lasting snowfall during several days in January caused severe traffic problems, trapping hundreds of vehicles, disturbing public transport in Thessaloniki, and suspending f lights. After serious power failures, the Aegean islands of Skopelos, Alonnisos, and Evia declared a state of emergency. A severe hailstorm during 7-9 May caused heavy damage in the agricultural areas in northern Greece. At the beginning of June, Bulgaria was hit by a series of severe thunderstorms accompanied by heavy rainfall and hail causing floods and damage to crops. Further local storms with hail were reported in Slovenia and Italy with hourly precipitation totals reaching as high as 46.5 mm in Salsomaggiore (Italy). At station Vojsko in Slovenia, on 6 June, a new 24-hr daily precipitation record of 200 mm was set. Two intense hailstorms were registered on 7 and 14 June in North of Macedonia. On 3 July, northwestern and north-central Bulgaria reported severe convective storms accompanied by strong winds and extreme hail, with stones measuring up to 8 cm diameter in Mezdra and Levski. During the first half of July, an intense heat wave hit Croatia, drying out the plant cover, which led to the outbreak of a wildfire on 17 July near Split. Approximately 4300 ha of forest, brush, olive groves, and vineyards were burned. With a 40-km long fire front at its maximum, it was one of the biggest wildfires in Croatian history. Turkey recorded a severe hailstorm on 27 July, with hailstones up to 9 cm in diameter observed in Istanbul. In Naples, Italy, a heavy thunderstorm on 5 September brought hail up to 11.5 cm in diameter and weights up to 350 g, injuring several people and animals, as well as causing damage to vehicles, houses, trees, and crops. The slow-moving cyclone \"Quasimodo\", approaching Italy from the Ligurian Sea, reached the city of Livorno on 9 September, with heavy precipitation causing flooding and damage, along with six fatalities. After passing over Toscana and the city of Pisa, the cyclone reached the Balkans. The Adriatic coast and the islands of the Adriatic Sea received more than 500 mm precipitation, causing floods in Croatia that damaged houses and cars. A total of 135 million Euros (around 160 million US dollars) in damages in the aftermath of the flood was estimated just for the Croatian county Zadar. Very heavy precipitation on 1 December led to extensive flooding and landslides in Greece. Between 8 and 12 December, an exceptional meteorological event occurred in Italy with intense rain at some locations (more than 300 mm in 48 hours). At Cabanne, Genoa province (Italy), an overall total of 507.0 mm was measured. Strong winds as high as 49.5 m s −1 were measured at Loiano.\\nOn 20 April, the Kirov region (Russia) reported exceptionally heavy snowfalls and freezing rain, leading to power failures due to damage to transmission lines for 44 settlements and to extensive damage to forests and agriculture. In April, Moldova reported extreme weather conditions, with rain, snow, and sleet depositing on wires and trees, as well as strong wind and frost with disastrous consequences for the country\\'s economy. Likewise, reports were made in the Ukraine of unusually high numbers of frost days that damaged fruits, vegetables, and other crops. A severe thunderstorm (\"Falk\") hit Moscow (Russia) and the surrounding area on 30 May. For the first time since the beginning of instrumental observations in Moscow for more than 100 years, wind gusts of 30 m s −1 were recorded, resulting in structural damage to buildings. Also in Moscow, 11 people were killed and 70 injured on 29 July during a storm where wind gusts reached 29 m s −1 . Heavy precipitation of 100-120 mm within a 24-hr period in the region of Bucharest (Romania) caused flooding in July.\\nThree men died after being swept away by strong easterly winds of 14-18 m s −1 accompanied by gusts of 22-25 m s −1 in northern Israel on 12 April. One day later, at the station Neot Smadar in the southern Negev, heavy rainfall of 27 mm, of which 10 mm fell within only 5 minutes, was measured, resulting in flooding and the closing of two main routes to Eilat. On 18 May, severe sandstorms were advected to southern Israel from the Sinai Peninsula, where they were created by downdraft winds related to welldeveloped clouds. These Haboob-type sandstorms are uncommon in Israel. As a result, the Eilat Airport was closed for several hours. On 16 October, a heavy rainfall event in Nahariyya (northwest coast of Israel) brought more than 70 mm of precipitation within two hours. During the morning hours of 30 October more than 50 mm within one hour were measured in Haifa. Both events were followed by flooding and subsequent road closures.\\nEight typhoons landed in China, the same number as 2016, and near the average of 7.2. However, the impact period was longer than normal, with the first typhoon, Merbok, landing in Shenzhen, Guangdong, on 12 June, 13 days earlier than normal, and the last typhoon, Khanun, landing in Zhanjiang, Guangdong, on 16 October, 10 days later than normal. Typhoon impacts varied strongly by time and region. For instance, Typhoons Nesat and Haitang landed successively on the coast of Fuqing city in Fujian Province during 30-31 July, and four typhoons hit the Grand Bay Area of Guangdong-Hong Kong-Macao in June, July, August, and October. As Tropical Cyclone Hato headed toward Hong Kong, the subsidence effect AUGUST 2018 | S238 ahead of its circulation brought oppressive heat to the territory on 22 August as the temperature at the Hong Kong Observatory soared to an all-time high of 36.6°C. Stormy weather with hurricane-force winds battered the city during the passage of Hato on the following morning. With Hato\\'s approach coinciding with the astronomical high tide, its storm surge resulted in serious sea water flooding and damage in many low-lying areas in Hong Kong. In 2017, meteorological disasters caused by rainstorms and floods in China were prominent and brought major losses, especially in southern China. Rainstorms occurred often and frequently in succession. Eleven days of persistent heavy rainfall occurred over southern China from 22 June to 2 July, associated with a rain belt across the provinces of Hunan, Jiangxi, Guizhou, and Guangxi, where local accumulations exceeded 500 mm. During summer, the high temperature events hit China earlier in northern areas but were more intense in southern areas, which resulted in a record number of days with high temperatures (daily maximum temperature ≥35°C) since the beginning of the record in 1961. On 21 July, the maximum temperature at Xujiahui, in central Shanghai, was 40.9°C, setting a record for its 145-year period of observation (since 1873). In the west during mid-July, 53 high temperatures were recorded, which tied or broke records in counties (or cities) in Xinjiang, Gansu, Inner Mongolia, Shaanxi (44.7°C in Xunyang), Ningxia, and Shanxi. South Korea experienced above-normal temperatures and slightly below-normal rainfall during summer. The summer mean temperature over South Korea was 24.5°C, which was +0.9°C above normal. In particular, extreme temperatures were observed from late-June through late-July. During this period, South Korea was strongly influenced by the western North Pacific subtropical high that extended more to the northwest compared to its normal position brought hot, moist air by the southwesterlies along its flank. The summer rainfall (609.7 mm) over South Korea was 84% of normal (723.2 mm). The ratios of monthly rainfall amount to the normal value in June, July, and August were 38%, 103%, and 88%, respectively. The 2017 Changma (early summer rainy period) started on 24 June and ended on 29 July. The Changma rainfall total was below normal (291.7 mm; normal: 356.1mm). The 2017 Changma was notable for the following: 1) onset and retreat were later than normal; 2) heavy rainfall events were concentrated on the central part of the Korean Peninsula; and 3) large spatial differences of rainfall between the southern and central regions of South Korea were observed. In northern Japan, on 5-6 July, record-breaking heavy rain associated with the active Baiu front fell in Kyushu region, with 129.5 mm h −1 and 545.5 mm (24-h) −1 observed at Asakura in Fukuoka prefecture. The heavy rain caused serious damage, including landslides and river overflows. In Okinawa/ Amami, monthly mean temperatures were record high in August (+1.4°C above normal) and recordtying (since 1946) high in September (+1.3°C above normal) due to a stronger-than-normal subtropical high over south of Japan. In western Japan, the monthly precipitation total was record high, at 333% of normal for October. In Mongolia, a total of 76 extreme weather events were observed, including episodes of heavy snow and flash flooding. Together, these events caused about $1.9 million (U.S. dollars) in economic loss.\\nDuring 2017, three cyclonic storms (one each in April, May, and November) formed over the north Indian Ocean. The first storm, Maarutha, formed over the east central Bay of Bengal on 15 April. However, it moved northeastward away from the Indian region and crossed the Myanmar coast on 16 April. Though the system did not cause significant weather over the mainland of India, it caused light to moderate rain over the Andaman and Nicobar islands during its formative stage. The second severe storm, Mora, formed over the Bay of Bengal during 28-31 May. It made landfall over the Bangladesh coast on 30 May and dissipated over the northeastern parts of the country on 31 May. This system caused moderate to heavy rain over many parts of the northeastern region of India after making landfall. The last severe storm, Ockhi, (29 November-5 December) formed over south Bay of Bengal and moved to the Arabian Sea. The storm, while moving across southern parts of India, caused severe damages in Kerala, Tamil Nadu, and Lakshadweep. The storm also claimed the lives of many fishermen (18 from Tamil Nadu and 74 from Kerala). Ockhi ultimately recurved and moved towards the south Gujarat coast. Heavy rain and flood-related incidents during the monsoon season claimed around 800 lives from different parts of India (see Table 7.2 for 24-hr rainfall records over India). Around 150 people reportedly died in the state of Assam from 13 June to 11 September in two spells of floods. More than a hundred people were reported dead in Uttar Pradesh due to heavy rain and floods of the Ghaghara, Gomati, and Rapti Rivers during 4-10 September. About 120 deaths were reported from the western industrial state Gujarat during the month of July and 107 from Bihar during 13-23 August. On 13 August, 46 deaths were reported due to massive landslides at Kotrupi on the Mandi-Pathankot National Highway near Jogindernagar in Himachal Pradesh (India). Similarly, 15 people died in Papum Pare, Arunachal Pradesh on 11 July due to a landslide. Heat wave conditions prevailed mainly over peninsular parts of India during the second fortnight of May, which claimed the lives of about 100 people in the state of Telangana. However, the loss of lives in 2017 was much less than in the previous years due to timely heat wave warnings and heat wave action plans initiated by government. In April 2017, Larkana, a city in the southern province of Sindh of Pakistan, experienced a record maximum temperature of 51.0°C on 20 April. On 28 May, Turbat, in western Pakistan, recorded a temperature of 53.5°C, tying the all-time highest temperature recorded in Moen Jo Daro, Pakistan, on 26 May 2010. In August, Bangladesh suffered one of its worst floods in the past four decades, which affected approximately one-third of the country, primarily in the northern, northeastern, and central regions. Rangpur district in the northeast experienced a month\\'s worth of rain-360 mm-in just two days (11-12 August). Around 140 deaths from the floods were reported, over fifty thousand people were displaced, and approximately six million were affected. Fifteen districts of Sri Lanka were affected by severe floods during the last ten days of May. Parts of Sri Lanka received 300-500 mm of heavy monsoon rain in a 24-hour period around 25 May, resulting in widespread flooding. The highest recorded rainfall was 533 mm in Kukuleganga. Galle, a coastal city, received 223 mm and Ratnapura experienced 453 mm of rainfall during 27-30 May, leading to severe inland flooding. Around 150 people were killed and around 450 000 were affected.\\nA maximum temperature of 53.7°С was observed in Ahvaz in summer. In spring, an extreme warm temperature of 51.7°С was recorded in Sistan and Balochestan. While the entire country experienced dry conditions during 2017, an extreme rainfall event, with 264 mm in 24 hours, was observed in summer at Station Lahijan. Table 7.3 lists measured extreme events in each season over Iran. The frequency and duration of dust storms in 2017 became higher in some parts of the country, especially in the southwest and southeast during winter and spring. AUGUST 2018 | S242  27°-35°N, 105°-114°E) during September and October is referred to as West China Autumn Rainfall (WCAR). It is the final stage of the rainy season in mainland China. WCAR can have severe impacts on agricultural production-including the harvesting and sowing of winter crops-and reservoir levels. Due to the fragile ecological environment of West China, above-normal WCAR often results in landslides and debris flows, which threaten lives and economic development in the region. WCAR in 2017 was 170% of normal ( Fig. SB7.4a), the second highest total since records began in 1979 ( Fig. SB7.4b). The event greatly affected 6 million people over seven Chinese provinces. Over 480 000 hectares of crops were damaged, and the total economic loss was over $20 billion (U.S. dollars). In the lower troposphere during September and October 2017, an anticyclonic anomaly at 850 hPa covered a large domain of southern China, with southwesterly anomalies at its western flank. Another anticyclonic anomaly (high pressure system) existed to the north of the anticyclonic anomaly. These two south-north anticyclonic anomalies led to a horizontal trough over West China. The southwesterly anomalies in the southern portions of the trough transported moisture from the northern Indian Ocean and South China Sea into West China while the horizontal trough kept the moisture stationary in the region, resulting in enhanced WCAR. The unusualness of the rainy season was the result of the combination of various atmospheric patterns. Typically, the enhanced WCAR is associated with positive tropical Indian Ocean SST anomalies (Fig. SB7.4c). Positive tropical Indian Ocean SST anomalies could induce a Kelvin wave response in terms of easterly anomalies in the lower troposphere. The Kelvin wave easterlies generated anticyclonic shear and resulted in anticyclonic anomalies over the western North Pacific, leading to anomalous moisture transport as seen in the 2017 event. Divergence at 200 hPa (Fig. SB7.4d was predominant over West China and appeared to be part of the circumglobal wave train over midlatitudes. Divergence aloft and convergence (associated with the horizontal trough) at lower levels over the region provided a favorable dynamical condition to the enhanced rainfall in the region. From early to mid-August, convective activity was particularly inactive over and around the Philippines. During the same period, the North Pacific subtropical high (NPSH) did not extend to mainland Japan as usual but shifted southward from its normal position, corresponding to the Pacific-Japan (PJ) pattern (Nitta 1987;Kosaka and Nakamura 2010;Fig. SB7.5a), with suppressed convective activity over and around the Philippines. Furthermore, the Tibetan high in the upper troposphere extended southward to cover Okinawa/Amami. Meanwhile, the Okhotsk high, which brought cool wet northeasterly flows to the Pacific side of northern and eastern Japan, had persisted since late July. The persistence of the Okhotsk high was presumed to be mainly due to blocking-high development over the Sea of Okhotsk, in association with the meandering westerly jet stream over northern Eurasia (Fig. SB7.5b). Corresponding to the PJ pattern with suppressed convective activity over and around the Philippines, this anomalous atmospheric circulation in the lower troposphere brought longer-than-normal sunshine durations, adiabatic heating associated with stronger-than-normal subsidence, and westerly warm air inflow over Okinawa/Aamami. These factors contributed to significantly warm conditions over Okinawa/ Amami, and monthly mean temperature over Okinawa/Amami in August 2017 was the highest on record for August since 1946. A southward extension of the Tibetan high presumably contributed to the warm condition over the area. At the same time, the low-level anticyclonic circulation anomalies brought considerable moisture to the middle and lower Yangtze River basin where above-normal precipitation was observed. Meanwhile, due to both the Okhotsk high and the PJ pattern, the Pacific side of northern and eastern Japan experienced significantly below-normal sunshine duration. The condition of enhanced NPSH over the south of Japan persisted from August to October. It is considered to have been caused by active convection over the Maritime Continent due to positive SST anomalies in the western tropical Pacific, which were related to the development of weak La Niña conditions in the eastern equatorial Pacific (see Fig. 3.2). AUGUST 2018 | S244 h. Oceania 1) overvieW-C. Ganter The climate of Oceania experienced a neutral ENSO state for most of 2017, which then transitioned to weak La Niña conditions late in the year. The Indian Ocean dipole (IOD) was neutral during its typical active period (May-November), while several countries were influenced by a positive southern annular mode during the austral winter. Persistent strong stationary high pressure systems in the Tasman Sea during November and December contributed to warmer weather across southeast Australia and New Zealand, and they also warmed surface waters of the Tasman Sea, causing a notable marine heatwave which persisted into 2018 (see Sidebar 7.4). 2) northWeSt paCifiC and miCroneSia-M. A. Lander and C. P. Guard This assessment covers the area from the international dateline west to 130°E, between the equator and 20°N. It includes the U.S.-affiliated Islands of Micronesia but excludes the western islands of Kiribati and nearby northeastern islands of Indonesia. Temperature and precipitation anomalies in this section are relative to a 1981-2010 period. Weather conditions across Micronesia during 2017 were mostly unremarkable. Annual rainfall was near to above average at most locations, and tropical cyclone activity was much lower than average. The western North Pacific summer monsoon system was displaced to the west and north of Micronesia, accompanying a similar westward and northward displacement of the basin\\'s tropical cyclones. These patterns of rainfall, wind, and typhoon distribution were typical for an ongoing La Niña. The regional oceanic response to La Niña climate conditions (e.g., increased trade wind strength) was sustained higherthan-average mean sea level.\\nLate in December 2016, and again in October 2017, a landfalling waterspout caused damage on an atoll of Micronesia. In the former case, a large and intense waterspout swept across Falalop (one of the islets of the Ulithi Atoll). Eyewitnesses described a surge of high wind that blasted across the islet, filling the air with lofted debris that appeared to be rotating. The Ulithi waterspout/tornado occurred in association with deep convection in a near-core rainband of Tropical Storm Nock-ten. As reported in the 23 January 2017 issue of the Khaselehlia Press: \"On December 22, 2016, a water spout turned tornado ripped through the island, tearing apart over 20 newly repaired homes and cook houses along its path. \"It sounded like a jet was flying low over the island. Luckily, we had been warned that Typhoon Nock-ten could be headed in our direction so we were prepared for a potential disaster. If we hadn\\'t received warning about Nock-ten, this tornado would have claimed lives on Falalop,\" said local resident Jon Rumal Jr.\" … It was not officially verified that this event was a tornado, but eyewitness accounts are convincing. The next incident of a landfalling waterspout occurred on 14 October when waterspouts were observed at Nomwin Atoll in the Hall Islands of Chuuk State. One of the waterspouts went ashore on Nomwin where \"it was strong enough to topple banana trees, and weak infrastructure houses were down and damaged\" as reported to the Chuuk Weather Service Office. A boat was found capsized in Nomwin waters on 15 October. It is thought by islanders that the boat was capsized by a waterspout. The Nomwin incident of waterspout formation occurred in association with a large area of heavy convective showers comprising the monsoon depression that would become Tropical Storm Lan two days later. The sea level across Micronesia exhibits large fluctuations related to ENSO. During the 2015 El Niño, the sea level dramatically fell across the region. During 2016, with the demise of El Niño, the sea level began a dramatic climb to become 6-8 cm above average by the end of the year. The sea level remained well above average throughout Micronesia for all of 2017, as La Niña became established in the second half of the year. Interannual variations of sea level across Micronesia are almost entirely a result of forcing by the Pacific trade wind system (blue line in Fig. 7.49). Fortunately, due to a general lack of high surf and swell, the high sea levels  (e.g., 1983, 1997, and 2015). There is a sharp rise of sea level in 2016 that remained high during 2017. A 12-month moving average has been applied to the raw monthly values of each time series. in 2017 only resulted in mostly nuisance inundations at times of unusually high astronomical tides. However, two incidents of moderate inundation (both related to brief episodes of high wind and waves) occurred on the lagoon side of Majuro and on the northeast coast of Kosrae. Table 7.4. Temperature (°C) and rainfall (mm) anomalies for selected Micronesia locations during 2017. Average (AVG) values are for the 1981-2010 base period. Latitudes and longitudes are approximate. \"Kapinga\" stands for Kapingamarangi Atoll in Pohnpei State, Federated States of Micronesia. Shading of the boxes indicates: red for above-average temperature and blue for below average; green for above-average rainfall and yellow for below average.\\nOn 7 February, a trough over the Solomon Islands and a slow-moving tropical low pressure system located in the Coral Sea caused heavy rain in and around Honiara, the capital city, situated on the northwestern coast of Guadalcanal. The rainfall total for that day was 208 mm, the sixth highest since records began in 1949 (Source: Global Historical Climate Network -Daily). The heavy downpour triggered flash flooding in the city. Major roads were submerged and flooding was reported in the main hospital and in many residences. All of the Guadalcanal plains were covered with water, and farmers who supply vegetables to Honiara lost around 70%-80% of their production. New Caledonia experienced its driest winter on record. Cold fronts that usually bring precipitation to the island passed farther south than usual from mid-June to mid-July as a consequence of a positive southern annular mode. While the southern annular mode returned to neutral by August and frontal systems returned to their near-average position, most frontal systems rapidly dissipated while approaching New Caledonia. In Noumea, the capital city, the winter precipitation total was 33% of normal, the lowest on record (since 1951). Spring (SON) is usually the driest season in New Caledonia, and 2017 was no exception. The rain showers in early December ended the longest sequence of daily rainfall totaling 5 mm or less: 139 days for Noumea. As a result, New Caledonia experienced agricultural and hydrological droughts in the second half of 2017. The main consequences were restrictions on drinking water and fires that destroyed vast areas of vegetation, including primary forests, despite a territory-wide fire ban. 4) aUStralia-S. Tobin and S. J. Jacobs The base period for this section is 1981-2010. Nationwide monthly average temperatures are based on the ACORN-SAT dataset (Trewin 2013), which extends to 1911. The rainfall and daily temperatures are based on the AWAP dataset (Jones et al. 2009), which extends to 1910.\\nExceptional warmth affected large parts of eastern Australia from late December 2016 into February 2017. Records were set in southeastern Australia and southern Queensland for consecutive warm days or nights, or for total number of warm days or nights during January. Five separate locations in Queensland broke previous state records for hottest February day on the 12th. The McArthur Forest Fire Danger Index (FFDI) reached catastrophic levels across much of New South Wales on 12 February. A fire in the Warrumbungle Shire destroyed most of the small township of Uarbry. Slow-moving tropical lows brought heavy rain over much of northern and western Australia between late January and early February. Cumulative rainfall resulted in flooding in the Kimberley and in parts of southwest Western Australia, the latter of which typically has low summer rainfall. Flooding affected large areas of the east coast during March, resulting from thunderstorms in New South Wales around mid-month, thunderstorms in Victoria on 20 and 21 March, and Severe Tropical Cyclone Debbie at the end of the month. Debbie caused flooding and widespread wind damage in Queensland and northeastern New South Wales, with flooding continuing into April in some rivers. An exceptional period of warm weather during the last week of September saw many records for high temperatures or early season warmth set in eastern Australia (see Sidebar 7.4). In early October, heavy rainfall associated with surface and upper-level troughs affected southeastern Queensland and northeastern New South Wales, with flooding around Bundaberg. Further heavy rain midmonth affected the same region, as well as areas of Queensland\\'s tropical coast, with flooding around Tully. After a cool and frosty start to November an extended period of very warm weather affected Victoria and Tasmania, driven by long-lived blocking highs over the Tasman Sea during both November and December (see also Sidebar 7.4). November monthly mean temperatures were the highest on record for Tasmania and second highest for Victoria. Warmth was more widespread in December, affecting all states and the Northern Territory. These high pressure systems also contributed to record high sea surface temperatures for Bass Strait and the Tasman Sea as clear skies allowed more solar radiation absorption, and light winds limited mixing of surface waters. An influx of tropical moisture between 1 and 3 December brought two to three times the monthly average rainfall to parts of northern Victoria and southern New South Wales. Flooding resulted in central to northeastern Victoria, with some flash flooding around Melbourne. For further detail on these and other significant events please see Monthly Weather Reviews, Special Climate Statements, and the Annual Climate Statement-all available from www.bom.gov.au /climate/current/. 5) neW Zealand-B. E. Noll In the following discussion, the base period is 1981-2010, unless otherwise noted. The nationwide average temperature is based upon the National Institute of Water and Atmospheric Research (NIWA) seven-station temperature series that began in 1909 (see www.niwa.co.nz/our-science/climate /information-and-resources/nz-temp-record /seven-station-series-temperature-data). All statistics are based on data available as of 9 January 2018.\\nOamaru (Otago) had its second wettest year on record (813 mm of rain). On 21 July, 161 mm of rain fell, leading to flooding and making it the wettest day in the town since records began in 1950; thereafter, Oamaru recorded just 163 mm during the remainder of the year (August-December 2017). AUGUST 2018',\n       'Quebec Province, Can.',\n       'A vulnerability assessment was conducted to facilitate development of adaptation strategies and actions for Olympic National Forest (ONF) and Olympic National Park (ONP). The authors first reviewed available climate model projections to determine likely levels of exposure to climate change on the Olympic Peninsula. The second step involved working with regional scientists and resource specialists to review relevant literature on the effects of climate change and on available projections to identify likely climate change sensitivities in each of four focus areas on the Olympic Peninsula (hydrology and roads, fish, vegetation, and wildlife). The final step was a review of current management activities at ONF and ONP and identification of management constraints in order to evaluate some aspects of institutional capacity to implement adaptation actions. Citation: Halofsky et al. (2011) http://www.fs.fed.us/pnw/pubs/pnw_gtr844.pdf',\n       \"shows the two-stage least squares estimates, using the assignment to small class as an instrument for the average class size during Project STAR.\\nColumn 1 shows highly significant (p < 0.01) differences between small and regular classes. On average, students in smaller classes were 0.142 standard deviations higher in grit than their peers in regular classes. When we control for student and fourth grade characteristics (columns 2 and 3), the effect slightly decreases but remains highly significant. According to our preferred estimate (column 3), students assigned to smaller classes during Project STAR are 0.123 standard deviations higher in grit in fourth grade, compared to students in regular classes. 7 This effect is both statistically significant 7 Using the factor loadings extracted from the factor analysis as dependent variable does not change the results. Results are and relevant in magnitude. The grit effect is twice as large as the test score effect in the same year (Schanzenbach, 2006) and also slightly larger than the effect of class size on other non-cognitive skills found in the related literature. For example, Dee and West (2011) use the National Education\\nLongitudinal Study to study the impact of class size on student engagement in school, estimating effect sizes ranging between 0.05 and 0.09 standard deviations. We find that the impact of class size on grit is larger than the effects that Dee and West found for engagement, a finding that underlines the importance of grit relative to other non-cognitive skills.\\nThe first stage of the TSLS model (column 4) shows that the number of students in small classes is about seven students fewer than in regular classes, confirming the descriptive statistics of the average class size in Table 1 . The estimates in column 5 corroborate the findings in columns 1 to 3. When the average class size increases by one student, students' grit decreases by 0.017 standard deviations (p < 0.01). If the class size increases by seven students, as in Project STAR, grit decreases by 0.119 standard deviations. This estimate is very close to the one we obtain with the OLS models. This closeness is due to the high implementation fidelity of the treatments in Project STAR, as also shown by the R-squared of the first stage in column 4.\\nTo understand how each grit attribute is affected by class size reduction, we estimate the relation between class size reduction and the single items on the grit scale. Appendix When pupils face difficult problems (e.g. in take-home assignments), smaller classes increase pupils' persistence by 0.144 standard deviations (p < 0.01). In addition, pupils' initiative increases by 0.101 standard deviations in small classes (p < 0.05). Students' engagement in doing more than merely the assigned work increases by 0.112 standard deviations (p < 0.05), showing the increase in effort spent on both in-class work and homework. When we use the TSLS approach (panel B), the estimates are in line with the findings in panel A, and the significance level remains the same relative to OLS.\",\n       nan, nan,\n       \"Based upon the conclusions drawn from the literature review, particularly the uncertain contribution of different amounts, types, and sources of financial aid to students' choice of institution to attend, the most important question to be addressed is: What is the contribution of financial aid to the price of the institution attended by 1989/90 dependent freshmen after controlling for other student and institutional characteristics?\",\n       'Critical-thinking skills (CTS) \\nunderstanding, and measuring skills rather than rote learning. Educators unanimously agree that, CTS can be taught using different teaching methods including concept mapping, critical questioning workshops, and systematic literature reviews and to problem based learning (PBL). PBL is a student-centered, task-based instructional method in which the teacher serves as a facilitator. The paper adapted a library based work; the data were collected using textbooks, journals articles and internet search. The literatures reveal that; PBL is the best approach to build CTS, integrating it into all areas of learnings and to the',\n       'The Mapping Marsh Migration feature can be accessed by clicking on the \"Marsh\" button. The user can use a slider bar to see how various SLR scenarios may impact marsh distribution. Maps represent the potential distribution of 12 marsh and wetland types (Cowardin et al., 1979) based on their elevation and how frequently they may be inundated under each scenario (Figure 4). As sea levels increase, some marshes may migrate into neighboring low-lying areas, while other sections of marsh will be lost to open water. There are advanced options where users can select an appropriate accretion rate and time interval for their area of interest. The wetland data portrayed as the initial condition within the viewer are derived from NOAA\\'s Coastal Change Analysis Program (C-CAP) (http://www.csc.noaa.gov/landcover). The mapping processes employed to create the layers in the marsh migration tool are similar to those discussed above in the mapping SLR section. The same assumptions are made in both, and both use a linear superposition method and incorporate tidal variability using the methods in NOAA (2007NOAA ( , 2010. Additional assumptions are made that marshes that cannot maintain their elevation relative to sea level will gradually become submerged and be converted to an intertidal mudflat or open water over a period of many decades (Morris et al., 2002). Titus (1988) states that because periodic flooding is the essential characteristic of salt marshes, increases in the frequency and duration of floods can substantially alter these ecosystems. Titus (1988) also states that salt marshes extend seaward to roughly the elevation that is flooded at mean tide, and landward to roughly the area that is flooded by spring tide. We make the same assumptions in this part of the tool. Our method assumes that specific wetland types exist within an established tidal elevation range, based on accepted understanding of what types of vegetation can exist given varying frequency and time of inundation, as well as salinity impacts from such inundation. The marsh migration mapping procedure uses four tidal surfaces from the VDATUM model instead of just MHHW: mean high water spring (MHWS), MHHW, mean tide level (MTL), and mean lower low water (MLLW). The MHWS surface is a modified form of the MHHW surface but has been shifted upwards in relation to the highest tide levels in the spring. For example, if during the May-June period the highest predicted tides are 5.5 feet (1.7 meters) and the MHHW datum is 5 feet (1.5 meters), the entire MHHW surface would be adjusted upwards by 0.5 feet (0.2 meters) to generate a MHWS surface. The fifth surface that must be derived prior to modeling the impacts from SLR is the boundary between the upper elevation of freshwater wetlands and uplands (FWUB). This boundary is determined by comparing existing wetlands to current elevation values for the study area and determining whether each elevation increment is primarily wetland or upland. The threshold of 66% is used to establish a conservative estimate for where this transition takes place. Therefore, whatever elevation is made up of at least a 66% majority of wetlands would be considered wetland, but less than 66% wetland would be considered a primarily upland elevation value. Once established, this elevation is treated like the tidal surfaces created above. The detailed mapping process is as follows: 1. Add desired SLR amount to each of the five surface grids. This is done using a look-up table ( Figure 5) based on A1B rates of SLR (see IPCC, 2007). . Look-up table that has amounts of SLR (in feet) and years, based on the IPCC A1B rates. Highlighted example shows that 1.75 feet ( 0.5 meters) of sea level would be added to the present-day tidal surface to simulate a condition at year 2050. Increments of 0.25 ft. (0.1m) were used. 2. Add desired amount of accretion to the DEM surface. Accretion is the vertical rise of the marsh\\'s surface caused by buildup of organic and inorganic matter. The amount of accretion is a result of sediment delivery and deposition dynamics that occur at an individual site. The net amount of accretion for a given year is determined by the accretion rate multiplied by the number of years being modeled. In the SLR and Coastal Flooding Impacts Viewer, the user can select from four predetermined rates of accretion. These rates are presented as high, at 6 millimeters (mm) per year, medium (4mm per year), low (2mm per year), and no accretion (0mm per year). These rates were determined from rates generally used in previous studies. 3. Subtract the resulting five surfaces from step from the resulting DEM in step 2. The result is a new surface layer for each of the MLLW, MTL, MHHW, MHWS, and FWUB. 4. Model the land cover class transition rule set. Using surfaces created in step 3 above, a simple rule set can be instituted to model wetland habitat class transitions, based on these new tidally adjusted elevation surfaces. Within the viewer, wetland categories are assumed to exist within the boundaries between the tidal thresholds. As the amount of SLR causes an area to move down within the tidal spectrum (based on the modeled surfaces derived above), wetland categories will change from one type to the next, according to their new location. For example, as sea level rises, and the tidal threshold locations move up in elevation relative to the land, upland categories may transition into fresh marsh, and freshwater marsh areas may transition into brackish, salt marsh, or unconsolidated shore habitats. In the viewer, classes are not allowed to transition in the opposite direction (i.e., saltwater marshes are not allowed to become freshwater marshes). This would only be the case if the amount of accretion would exceed the amount of SLR being modeled, and it is assumed that this would not be likely given that the accretion rate on the upper end of the tidal spectrum would be less than optimal, and that areas not flooded regularly would have no accretion at all.',\n       'A third strategy is to adopt a more expansive notion of food system planning that addresses the root causes of food system inequities: siting food production and distribution infrastructure to reduce poverty as well as improve effi ciency; focusing economic development plans on sectors of the food industry that offer pathways to higher-wage jobs; protecting food businesses that cater to low-income residents as neighbourhoods are rezoned; and changing planning processes to more effectively involve community stakeholders with knowledge of social determinants and hard-toreach demographic groups, like recent immigrants. Integrating upstream determinants of health into the types of issues that food planners address requires interdisciplinarity, and successful planning processes can break down barriers among administrative agencies and advocacy groups and foster interdisciplinary approaches to problem-solving (Corburn, 2009 ). Moving beyond the food metrics typically tracked to monitor progress in addressing the health, social, economic, and environmental impacts of the food system requires identifying the root causes of downstream outcomes and fi guring out ways to aggregate, organize, and analyse this information so it is useful to various stakeholders and city government. This can seem daunting to food planners with neither the resources nor the power to aggregate, organize, and analyse such data. Fortunately, integrating upstream and downstream metrics can be carried out iteratively, by starting with existing relevant data, using lessons from other big data projects, and engaging in a food system planning process that brings multiple stakeholders together to track a broader range of food metrics, spanning issues from poverty to social wages (housing, healthcare, education) to economic and environmental trends. ',\n       'Carries genetic instruction to host cells to produce RNA which in true stimulates immune system to produce antibodies.',\n       'Two NCES longitudinal studies will be collecting institution data and transcripts in 2017, beginning in February. The Beginning Postsecondary Students Longitudinal Study (BPS) surveys \"first-time\" college students at three points in time: at the end of their first year, in their 3 rd academic, and in their 6 th academic year after first starting in postsecondary education. BPS collects data on student demographic characteristics, school and work experiences, enrollment persistence, transfers, degree attainment, and transition to employment. In 2017, data will be collected for the BPS:12 student cohort who started college, for the first time after leaving high school, during the 2011-12 academic year. The High School Longitudinal Study of 2009 (HSLS:09) is a nationally representative, longitudinal study following a cohort of students who were in ninth grade during the 2009-2010 school year. The study focuses on understanding how high school experiences affect young adults\\' learning and their education and career choices over time. The study also explores young adults\\' transitions from high school to college, to the labor force, or to adult roles. Of special interest is course-taking and career preparation in science, technology, engineering, and math (STEM).',\n       'Participants underwent florbetaben PET using a Discovery STe PET/CT scanner (General Electric Medical Systems, Milwaukee, WI, USA) or a Biograph mCT PET/CT scanner (Siemens Medical Solutions, Malvern, PA, USA) in a 3D scanning mode that examined 35 slices of 4.25-mm thickness spanning the entire brain. A bolus mean dose of 381 MBq was injected into an antecubital vein. Ninety minutes after injection, a 20-minute emission PET scan in a dynamic mode (consisting of 4×5 min frames) was performed. The florbetaben PET data were visually assessed by trained experts. Regional cortical tracer uptake (RCTU) (1=no uptake, 2=moderate uptake, 3=pronounced uptake) in the frontal, lateral temporal, posterior cingulate/precuneus, and parietal regions were assessed. Cases with an RCTU score of 1 in all four brain regions were classified as a brain amyloid plaque load (BAPL) score of 1, while cases with an RCTU score of 2 in any brain region and no regions with a score of 3 were classified as a BAPL score of 2. An RCTU score of 3 in any region was classified as a BAPL score of 3. Cases with a BAPL score of 1 were considered to be negative for amyloid, and those with a score of 2 or 3 were considered to be positive for amyloid. ',\n       'Please write \"unavailable\" or \"unknown\" in data cells as applicable. On this form, \"N/A\" means \"not applicable.\" Data cells left blank are presumed to be zeroes.',\n       'Agritourism has received a lot of attention in recent years both among researchers and also state policy makers. According to the U.S. Travel Association, travel and tourism is a $947 billion industry in the United States that has directly generated more than 8.1 million jobs. Travel and tourism generates $147.9 billion in tax revenue for federal, state, and local governments, with the restaurant industry accounting for the majority of economic activity. An increasing popular and growing opportunity for agricultural producers is agritourism (Agricultural Marketing Resource Center, 2016) . Research has been conducted to identify farm and farm operator characteristics that are associated with the adoption of agritourism, a term which has been used to describe activities ranging from U-pick activities, field rides, cultural or historic exhibits, festivals, paid or customized hunting tours to wildlife observations and holiday-related activities. Bagi and Reeder (2012) hypothesized that if successful, such activities might be beneficial to the agricultural economy and have positive environmental and health-related objectives. They further observed and noted that among those who might benefit, most are low-income, undereducated, and older farmers, as well as small family farms. The purpose of this paper is to determine the extent to which some of these findings Although many do not realize it, agritourism has a long history in the United States. Holland and Wolfe (2000) provide a historical narrative of agritourism in the United States. Agritainment (agritourism and entertainment farming enterprises) dates back to the 1800s, when families would visit their relatives in the country to escape from the city\\'s summer heat. The advent of the automobile in the 1920s and the stresses of the Great Depression of the 1930s and 40\"s generated renewed interest in rural/farm recreation. These demands for rural recreation continued through the 1970s into the 1990s as manifest by the popularity of horseback riding, farm petting zoos, farm vacations, bed and breakfasts, and commercial farm tours during those decades (Holland and Wolfe, 2000) .\\nAgritourism has been defined and labeled in various ways in the literature. Philips et al. (2010) provide a typology of definitions of Agritourism. The term agritourism has often been used interchangeably with agrotourism, farm tourism, farm-based tourism, and rural tourism (McGehee and Kim, 2004; Clarke, 1999; Ilbery and Bowler, 1998; Roberts and Hall, 2001 ; Barbieri and Mshenga, 2008) . Agritourism may be defined as \"rural enterprises which incorporate both a working farm environment and a commercial tourism component\" (Weaver and Fennel, 1997; McGehee et al., 2007) . Barbieri and Mshenga (2008) referred to agritourism as \"any practice developed on a working farm with the purpose of attracting visitors.\"\\nExamples of agritourism may include farm stays, bed and breakfasts, pick-your-own produce, agricultural festivals, and farm tours for children, or hay rides (Clarke, 1999; McGehee et al., 2007) . Farm/ranch recreation refers to activities conducted on private agricultural lands, which might include fee-hunting and fishing, overnight stays, educational activities, etc. This category of tourism is a subset of a larger industry known as agritourism. Agritourism in turn is a subset of a larger industry called rural tourism that includes resorts, off-site farmers\\' markets, non-profit agricultural tours, and other leisure and hospitality businesses that attract visitors to the countryside. Rural Tourism differs from agritourism in two ways. First, rural tourism enterprises do not necessarily occur on a farm or ranch, or at an agricultural plant, and secondly, they do not generate supplemental income for the agricultural enterprise. Agritourism and naturetourism enterprises might include outdoor recreation (fishing, hunting, wildlife study, horseback riding), educational experiences (cannery tours, cooking classes, or wine tasting), entertainment (harvest festivals or barn dances), hospitality services (farm stays, guided tours or outfitter services) and on-farm direct sales (u-pick operations or roadside stands).\\nFarm enterprise diversification has become a strategy for small farms to remain viable especially in the face of Yeboah et al. 85 high risks facing modern day farming. McGehee et al. (2007) have identified agritourism as a form of enterprise diversification. Ilbery and Bowler (1998) describe seven pathways to agricultural diversification, of which on-farm recreational activities are one survival strategy for farm businesses. Incorporating agritourism as an alternative enterprise has the potential to contribute to agricultural sustainability, broaden farmers\\' economic base, provide educational opportunities to tourists, and engender a strong communal cohesion (Ilbery and Bowler, 1998) . Beus (2008) describes agritourism as a possible strategy for many U.S. farmers to expand their incomes and stay in business. This practice, referred to as the \"cultivation of tourists on the farm in addition to crops\" is already well established in countries like Switzerland, Italy, New Zealand and other European countries.\\nAs pressure increases on farmers to diversify their enterprises in order to remain competitive, agritourism has emerged as one viable alternative. In an exploratory study of agritourism development in Nova Scotia, Colton and Bissix (2005) identified a number of issues and challenges. Chief among the issues and challenges identified by stakeholders as critical to the development of successful agritourism include marketing, product development, government support, education and training, and partnership and communication. There was consensus among stakeholders that farmers going into agritourism need to be able to define the product that they are offering consumers and be able to communicate this to the potential visitors. Also, fostering linkages with other farmers, business community, educational and governmental agencies, as well as, researchers can significantly impact the success of agritourism ventures.\\nHowever, successful operation of agritourism depends on certain factors both within and beyond the control of the farmer. Industrialization and globalization provide opportunities as well as challenges and threats to the survival of small farms in this ever-changing agricultural landscape. While agritourism may provide a way to diversify small farms, there are challenges to successful operation of an agritourism farm. Barbieri and Mshenga (2008) investigated the role of owner and firm characteristics on the performance of agritourism farms. They found out that the length of time in operation, number of employees, and farm acreage tended to have a positive impact on agritourism performance as measured by annual gross sales. In other words, larger farms tend to be more successful as agritourism sites. Their hypothesis is that larger farms, as measured by larger acreages and large number of employees, are able to offer a great variety of tourism products and services that ultimately attract more tourists. Other characteristics such as location of the farm, whether it is a working farm, whether the operator has a business or marketing plan, source of start-up capital and the farmer\\'s educational level did not appear to have a significant relationship with the success of agritourism.\\nIn a more recent study, Bagi and Reeder (2012) conducted a national survey to investigate the factors affecting U.S. farmers\\' participation in agritourism. Their results revealed a slew of factors that either promote or hinder the successful operation of an agritourism business. Among the factors that have positive impact are: public access to the farm; proximity to central cities; farms in Rocky Mountains and southern plains, and farms enrolled in conservation programs. Other characteristics that impinge upon farmers\\' decision to participate in agritourism include age, educational level of the farmer, number of acres of farm, whether the farmer pays for advice, and whether the farm is organized as a partnership or corporation. The data showed that nationally over 84 million acres (representing 10% of farm land) is engaged in agritourism, employing 17 million fulltime-equivalent days of family labor. Figures from the Agricultural Resource Management Survey (USDA-ERS, 2007) showed that the gross income from agritourism operations was in excess of $16,000 per annum, while national total income from agritourism activities was $554 million in 2007. An additional $258 million was generated from direct sale of farm produce to tourists.\\nMost of the above cited studies focused on established large farms that are already practicing agritourism. Those that dealt with issues and challenges focused exclusively on existing agritourism operations as opposed to new entrants. There are no studies identifying the challenges that prevent farmers, especially small and sociallydisadvantaged ones from adopting or incorporating agritourism into their farms. A number of relevant questions remain unanswered: For example, what factors constrain the likelihood that small farmers will adopt agritourism on their farms? Are those practicing agritourism doing better economically than those that do not? The present research seeks to provide answers to some of these and other questions that have not been tackled in the literature, particularly as they relate to agritourism development among small and sociallydisadvantaged farmers in North Carolina. However, the question of whether agritourism does enhance farm profitability is not addressed. As noted by Schilling et al. (2014) \"parsing out the effects of agritourism on farm income is challenging for several reasons.\" Reasons cited included the lack of consistent definition for \"agritourism\"; variation in reasons for farmers to develop agritourism enterprises and the strong likelihood of selfselection. While these studies provided a broad overview of the current state of agritourism in North Carolina and elsewhere, they do not provide any demographic information about the farm operators that may be useful for other operators, specifically Small and Socially Disadvantaged Farmers (SSDFs), to use in planning their own agritourism operation. In addition to other objectives, this research helps bridge this gap by analyzing the opportunities for agritourism enhancement among SSDFs and the factors that may influence the decision to add this enterprise to their farm operations. It is hoped that the findings of this research will help provide the foundation for proposing recommendations for addressing the needs of small and socially-disadvantaged farmers in North Carolina who are either involved with agritourism or have interest in adding this enterprise to their farm operations.\\nResearch among small farmers in North Carolina indicates that profit maximization was not a priority reason for farming and farmers cite a \"love of farming\" and \"desire to keep the family farm in the family\" as the primary reason for farming (Yeboah et al., 2009) . Given recent economic conditions, small farms that do not operate efficiently can exacerbate loss of farm ownership especially for socially disadvantaged farmers. The concept of \"family farm\" is changing dramatically and small farmers increasingly see themselves as entrepreneurs. Many farms, especially those in eastern North Carolina, will have to continue to change in size and structure to remain viable in the 21 st Century agricultural environment. Farmers must focus much of their energies on diversification as a means to stay competitive and agritourism can provide the diversification and additional income to make the small farm profitable.',\n       'FA, MD and HSC statistical group differences were created using the track-based spatial statistic tool (TBSS) [36] . Firstly, all FA images were aligned to the MNI 152 standard space [37] . Then a skeleton was created from the mean FA > 0.2. Local maxima of FA images of each subject were then projected onto the skeleton. At each voxel in the skeleton statistical group differences were determined using permutations tests (FSL randomize) [38] . MD and HSC maps were projected onto the same skeletons to determine statistical group differences. The statistical results were thickened to allow better visualization of group differences. Correction for multiple corrections was estimated using family wise error (FWE) and the threshold-free cluster enhancement (TFCE) option [38] .\\nGlobal averages for FA, MD and HSC were calculated for each subject using the WM skeleton as a region of interest (ROI). In brief, for each statistical contrast comparing two groups, an FA TBSS skeleton was generated by using voxels greater than 0.2. The volume of FA skeleton was then calculated. The proportion of abnormal voxels was computed as the ratio between the volume of voxels showing statistical differences between groups and the total skeleton volume. This strategy intended to avoid the contamination grey matter signal. Regional values of FA, MD and HSC were calculated for each subject using masks based on statistical significant voxels derived from various contrast of interest. To estimate the normalized MD or FA volumes, we computed the ratio between the volume of abnormal voxels when compared to CN as defined by TBSS (corrected p <0.05) and the total WM skeleton volume.\\nIn addition, in order to compare the magnitude of change across patient populations, FA, MD and HSC absolute scores were transformed as z-scores. Transformed FA, MD and HSC values were compared across groups using one-way-ANOVA.',\n       'An application to the SIRD data is presented next. To examine the performance of t he proposed methods for th e SIRD, a simulation with a population generated using the SIRD data is presented in the end. We have implemented the proposed imputation methods in R (R Development Core Team 2009). To fit the required nonparametric regressions, we use the R function loess with default settings, which fits a local polynomial surface in one or more regressor variables. The required linear regressions are easily fit in R usi ng the Statistics Canada, Catalogue No. 12-001-X function lm. Our implementations of the proposed methods include error checking; (such as ensuring that there are sufficient points for regression fitting at each stage) which is particularly important in bootstrap and simulation settings where the imputation methods are replicated many times, and each iteration cannot be examined manually. We defaulted to an overall mean imputation in cases where there were not enough data points to fit a regression.',\n       'The human brain is a complex multi-scale structure in space and time, and produces fine molecular, cellular and neuronal phenomena [56, 57] . Neuroimaging can provide brain images with high temporal and spatial resolution, but dynamic information for the brain is lacking. Therefore, in brain science research, simulation tools, brain models, and connectomes have been developed gradually and built to provide simulation information of neurons, brain structures, and networks. Simulation tools focus on individual neurons and the corresponding models of ion channels [58] . For example, the GEneral NEtwork SImulation System (GENESIS) was designed to simulate neural networks using standard and flexible methods to obtain detailed and realistic models [59] . A model of the human brain is a \\'\\'reference brain\" that provides important biological details. It outlines the spatial framework and brain composition from a macroscopic perspective and helps researchers extract and analyze microscopic data from molecular processes to various behaviors for modeling and simulation. The brain connectome is the \\'\\'Google TM Maps\" for brain models. It provides precise human brain coordinates and helps researchers transform detailed neural connections with human brain cognition and behavior. Connectomes map elements to human brain networks dynamically [60] , where circuit abnormalities presumably reflect a complex interplay between genes and the environment [61] . Large-scale models of the human brain and connectomes not only provide basic insights of brain structure [62] but also serve as biomarkers of brain diseases to help researchers explain diseases such as AD and PD [63] [64] [65] [66] [67] and even help researchers understand the sex-based differences in human behavior [68] .\\nVast human brain structures and high-resolution imaging technology determine the essence of brain models and connectomes to be a big data set. The brain model of the HBP consists of 100 neocortical columns [69] . Defense Advanced Research Projects Agency (DARPA)\\'s Synapse project 500 billion neurons [70] [71] [72] . Also, Izhikevich et al. published a detailed large-scale thalamocortical model that simulates one million multicompartmental spiking neurons [73] . On a microscopic scale, the number of neurons and synapses contained in brain connectomes is approximately 10 10 -10 11 and 10 14 -10 15 [74] . At the macroscopic scale, the cortical hypothalamic junction contains hundreds of brain regions and thousands of comprehensive pathways data sets [75] . Currently, the Human Connectome Project (HCP) already has 7-T diffusion magnetic resonance imaging (dMRI) and 3-T resting-state fMRI (R-fMRI) data [76, 77, 78] . Not only structure but also highresolution imaging. When building a brain model, an optical microscope sufficient to track a single neuron has a resolution of 0.25-0.5 microns, and an electron microscope capable of displaying synaptic or chemical signals has a resolution of nanometers [79] . Diffusion tensor imaging [80] and four main MRI modes (structural MRI, task fMRI, dMRI, R-fMRI) can be used to measure connectivity in the brain [61] with resolution of 1-3 mm or even smaller [81] . Brain models and connectomes as big data sets provide abundant information and knowledge to drive the development of brain-research programs. The Izhikevich neuron model has influenced more than 3000 academic studies by 2019. Based on the 500-subject release of the HCP, the Budapest Reference Connectome Server v2.0 and v3.0 has generated the common sides of connectomes in 96 and 477 different cortical layers [82, 83] . Also, disease research by the HCP applies HCP-style data to people at risk or suffering from brain disease (e.g., anxiety, depression, epilepsy) [84] .\\nThe human brain is not only a simple big data set, but also a complex mathematical object. Hence, building models of the human brain and connectomes requires powerful platforms for data storage and processing. To meet the HPC requirements for brain models, the Izhikevich neuron model [85] used the Beowolf Cluster [86] with 60 processors of 3-GHz each, the HBP and Synapse project used the IBM Blue Gene supercomputer, and Spaun used eight Core Xeon processors (2.53 GHz) [71] . The HCP infrastructure uses IBM HPCS from the WU Center for High Performance Computing to execute pipelines and user-submitted jobs to meet the high-throughput dataprocessing requirements of approximately 200,000 inputs and outputs per second [71, 87] . In addition, the HCP has established a set of informatics tools, including ConnectomeDB [88] and Connectome Workbench [89] , to collect, process, share and visualize high-throughput data [90] . Not only brain models and connectomes, workflows and simulation tools are moving toward high-performance computing and distribution. Simulation tools such as GENESIS [59] , NEURON [91] , NEST [84] , Network and Cache Simulator (NCS) [92] , Neosim [93] , and SpikeNET [94] have also been extended to support parallel processing systems to improve performance. After parallelization, NEURON achieved almost linear acceleration. It requires an integration time of 9.8 seconds and communication time of 1.3 seconds if running 40,000 realistic cells on 2000 processors on the IBM Blue Gene supercomputer [95] . Also, the BigBrain-based 3D-PLI workflow uses the JUDGE and JUR-OPA supercomputers from the JSC to meet the requirements for data reading, analysis, and calculation [96] . The JUROPA supercomputer owns 26,304 cores, and its Rpeak reaches 308.3 TP/s. With HICANN from Heidelberg University, brain activity can be simulated at 10,000 times the normal speed to compress one day into ten seconds [72] . The HBP estimates that supercomputers with exaflop computing speed and exabyte computing memory can simulate a complete human brain (1000 times that of the rodent brain) [69] . With the development of ultrahigh-performance computers and computing environments, a model of the whole brain and dynamic brain connectomes will be completed eventually.',\n       'A ∈ R I1×I2×···×I N and an integer R, if it can be expressed as\\nwe call it CP factorization (see Figure 2 for graphical representations). For convenience, in the following we write',\n       'The segmentation of the hippocampus was performed using a fully automatic method we previously developed (Chupin et al., 2009; Chupin et al., 2007; Colliot et al., 2008) . This approach segments both the hippocampus and the amygdala simultaneously based on competitive region-growing between these two structures. It includes prior knowledge on the location of the hippocampus and the amygdala derived from a probabilistic atlas and on the relative positions of these structures with respect to anatomical landmarks which are automatically identified.',\n       'In this section, we introduce our proposed multi-stage attentive transfer learning framework and network settings.',\n       'When sharing dwMRI-derived CMs on the UMCD, the user should specify the scan type as DTI, High Angular Resolution Diffusion Imaging (HARDI), or Diffusion Spectrum Imaging (DSI). In the dwMRI-specific fields, the user should note the number of gradient directions included in the scan sequence along with the maximum b-value, whether eddy correction was performed, and the tractography method (deterministic or probabilistic). In the Preprocessing Notes, the user should describe the software package used and preprocessing details including whether multiple dwMRI scans were acquired and averaged, how diffusion tensors/Orientation Distribution Functions (ODFs) were calculated, the tractography algorithm, any voxelwise masking criteria, and the maximal angular threshold allowed for fibers to turn between adjacent voxels.',\n       \"The CAHSEE is a two-part exam of mathematics and English language arts (ELA) skills. The math section assesses students' mastery of the California math content standards for 6th and 7th grade and their Algebra 1 skills using a multiple-choice format. The ELA section is aligned with state content standards through 10th grade and utilizes a multiple-choice format along with one essay. Both tests are administered in English, regardless of a student's primary language.\\n6 Students must pass both parts (with a minimum score of 350) to earn a high school diploma.\\nThe test is first administered to students in the spring of 10th grade, and students have at least five subsequent opportunities to retake the sections they have not yet passed (twice in 11th grade and 12th grade and at least once following the end of the 12th grade school year).\\n7 Districts notify students and their parents of their CAHSEE performance about 7 weeks after the exam is administered. Because students are told their exact score, not simply whether they passed or failed, students who fail have some sense of how close they came to scoring the requisite 350 they need to meet the CAHSEE requirement.\",\n       'The AortaGen Consortium includes 9 cohort studies that completed genome-wide genotyping and had measured CFPWV, plus 2 cohort studies that had measured CFPWV and collected DNA for replication genotyping. Each study adopted collaboration guidelines and the consortium established a consensus on phenotype harmonization, covariate selection, and an analytic plan for within-study genomewide association and prospective meta-analysis of results across studies. Each study received institutional review board approval of its consent procedures, examination and surveillance components, data security measures, and DNA collection and its use for genetic research. All participants in each study gave written informed consent for participation in the study and the conduct of genetic research. Details of study cohort, CFPWV measurement protocols and inclusion and exclusion criteria are provided in the online-only Data Supplement Methods and Table 1 .',\n       'Wetland water volumes and ponded areas were simulated using a mass balance approach. For each wetland, the volume was updated on a daily time step using the following equation:\\nIn this equation, the contributions to the water volume are denoted by P t for direct rainfall onto the ponded area, R t for nonsnowmelt runoff from the catchment, S t for snowmelt runoff, and D gain t for incoming water from an upstream drained wetland. The loss terms are G t for groundwater losses, E t for evaporation from the water surface, and D loss t for water, which is drained out of the wetland. The calculation of all nondrainage terms is followed precisely as laid out in Huang et al. (2013) and is repeated below.\\nThe contribution of direct precipitation to water surface as a function of the ponded area Aa t and the rainfall rate ρ t is the product ρ t · a t . This quantity is dependent on temperature; any precipitation during periods when the air temperature is below zero is instead added to the snowpack. Total evaporation from the ponded area of the wetland is represented in equation (6) in terms of the potential evapotranspiration, the ponded area, and an air temperature indicator I E . I E = 1 if the trailing mean of the air temperature over the previous 10 days exceeds 3°C and is 0 otherwise.\\nAny precipitation occurring when the air temperature falls below zero is accumulated as snowpack (SP t ) over the entire catchment basin, including the ponded area which is assumed to freeze over. On days when the temperature exceeds zero with existing snowpack on the ground, the snowmelt is calculated in (7) via the degree-day method and this is subtracted from the remaining snowpack. The snowmelt term is nonzero only if the temperature is above zero, corresponding to the condition that the indicator variable I T = 1. To accommodate variability in catchment response to snowmelt runoff, we assigned each catchment a snowmelt capture ratio (CR) at random from 85% to 100%. This parameter controls the fraction of snowmelt that accumulates in the wetland.\\nDeep groundwater loss through wetland bottoms was not considered in this model. All subsurface loss was assumed to be lateral as hydraulic conductivities through wetland bottoms are sufficiently low to preclude substantial vertical loss (van der Kamp & Hayashi, 2009 ). Lateral loss is represented in equation (8) as a function of a reference recession slope of 0.04 m 2 /day S RR , the daily potential evapotranspiration PET t , a reference evaporation E R of 4.7 mm/day, and the wetland perimeter p t . The value of the factor ω was set to be 0.90 as in Huang et al. (2013) .\\nRainfall runoff was calculated using the curve number approach (U.S. Soil Conservation Service, 1954) modified to incorporate antecedent moisture. The full details of this derivation are covered in (Huang et al., 2013) and were implemented exactly to that specification. Each watershed was assigned a curve number CN i at random from 49 to 89 to allow for a range of runoff responses representative of grasslands (CN = 49) to cultivated fields (CN = 89). To allow for the consolidation of wetlands, we incorporated the possibility of drainage. A period of drainage was assumed to run from 1 January 1996 to 1 January 1997 in all simulations. During this period, we considered two cases. In the first, all wetlands had identical probability of being drained, while in the second case, only wetlands with ponded area less than 10 ha were allowed to be drained with equal probability, and all larger wetlands had zero probability of drainage. The second case represents a scenario in which smaller wetlands are more likely to be drained. If a wetland was selected for drainage, a downstream wetland was selected at random from the remaining set of wetlands. The entire\\nIn our regression analysis, we adopted a two stage procedure. We first enumerated all possible models with no interaction terms. Then, the best performing model was selected and we added an interaction term between the significant covariates to investigate the correlation structure for that model. For the first stage a set of Bayesian linear regression models was constructed with the covariates {ΔWheat, ΔCorn, ΔSoy, Δ Drainage, and Area 1985 } and response variable S C defined for each county. All possible models controlling for Area 1985 and including one or more of the covariate set {ΔWheat, ΔCorn, ΔSoy, and ΔDrainage} with no interactions were considered, for a total of 2 4 À 1 = 15 models.\\nEach variable was standardized to have zero mean and unit standard deviation. These were used as predictors of S C on a per-county level. We found that ΔWheat, ΔCorn,and ΔSoy all covaried ( Figure S3) ; this was not surprising given the ongoing transition from small grains into soy and corn across the northern Great Plains (Johnston, 2014) . We placed a half-Cauchy prior on the error variance and fixed its β hyperparameter to be 0.25. A flat prior distribution was assumed for the linear regression coefficients and the intercept. Since this model has a continuous likelihood function unlike the change point model in the previous section, it can be efficiently estimated with MCMC using the No-U-Turn Sampler (NUTS; Hoffman & Gelman, 2014) and we employed NUTS as implemented in PyMC3 to estimate the parameters of this model. For each model, we drew 500 samples per chain after discarding an initial 500 samples. Four chains were computed per model, and these were used to calculate the convergence diagnostic b R as in the previous section. We then ranked the models (Table S3) according to the Watanabe-Akaike Information Criterion (WAIC; Watanabe, 2013) , which penalizes models with more parameters and favors models with an estimated higher model likelihood.\\nThe WAIC-optimal model with no interaction term included ΔWheat and ΔDrainage as predictors in addition to Area 1985 . The response variable and all predictor variables were centered and standardized to have zero mean and unit standard deviation. In the second stage of our analysis, we refit the optimal model of ΔLS including ΔWheat and ΔDrainage and Area 1985 , while also adding the interaction term ΔWheat × ΔDrainage as a predictor. This term was included to identify whether or not locations undergoing land use change were sufficiently heterogeneous with regard to drainage application to warrant a more complicated model. We applied the same estimation procedure as before to calculate parameter estimates for this interaction model.\\nSummaries of the posterior distribution for the model parameters are shown in Table 1 . For the model parameter indicating the change point year, posterior probability mass was concentrated on the years 1998 and 1999 with a smaller portion showing support for 2000. These results indicate strong support for the hypothesis that structural changes to ND PPR area-perimeter relations took place around the year 1999. Furthermore, the low residual variance suggests that this is an appropriate representation of the data. We repeated this analysis with the intersection of the GSW-derived water polygons with all NWI polygons buffered by 100 m to assess whether this result was still valid when considering only locations previously identified as wetlands in the National Wetland Inventory. With this added constraint, we found that the most likely year for change was again 1999, though the 95% credible interval also included the year 1997 as well.',\n       'According to MSN evacuation plans, by L-53 (or 53 hours to hurricane landfall) National Guard and other pre-contracted assets will begin assembling at airfields, with other medical teams ready to receive patients at designated Medical Marshalling Points (MMPs). By L-51, hospital decision makers are supposed to communicate their decisions to either: shelter in place, partially evacuate, or completely evacuate. From preliminary plans, the first load of MSN patients are set to depart from airfields at L-48 (LA DHH ex 9). Medical response will need to be very coordinated and stay within schedule to avoid hurricane risks to patients from early surge and wind effects. coastal Louisiana may face storm surge and/or wind risks. Since hurricane track and intensity cannot be certain at 48 hours to landfall, the safety of sheltering in place will remain uncertain. ',\n       \"The best combination of manifold learning technique and parameters is Locally Linear Embedding with a manifold dimension of d~11, a neighbourhood size k D~2 3 and combining the top k d~7 matches in STAPLE, giving a mean (SD) Dice's similarity index DS max of 0.9077 (0.0211). In contrast, Isomap and Laplacian Eigenmaps resulted in Dice's similarity indexes of 0.8995 (0.0228) and 0.8971 (0.0245) with d~21, k D~2 3 and k d~9 and d~13, k D~2 1 and k d~1 9 respectively. Each graph in Figure 1 shows the mean Dice's similarity index for each manifold learning technique when d, k D and k d are fixed to their respective optimal parameters. It is interesting to note that all 3 manifold learning techniques result in a very high mean Dice's similarity index (.0.89). Using a 2-tailed paired t-test, Locally Linear Embedding gives a significantly (p~0:0216v0:05 and p~0:0275v0:05) higher average Dice's similarity index compared to Isomap and Laplacian Eigenmaps, whereas the difference between Isomap and Laplacian Eigenmaps is not statistically significant (p~0:3250w0:05). The accuracy achieved by fusing multiple segmentations quickly rises to a maximum and then gradually declines as the number of segmentations increases. This is in line with results published in [5] and [7] : the gradual decline corresponds to adding dissimilar images into the combination process, resulting in segmentation errors. The accuracy also flattens out for manifolds of 3 or more dimensions. This suggests that our data set of hippocampi can be described mostly by 3 main modes of variation, and this is consistent across all manifold learning techniques presented. The number of neighbours k D used to build the connected graph has little effect on the accuracy when using Isomap and Laplacian Eigenmaps. In contrast, increasing k d increases the accuracy achieved with Locally Linear Embedding. Table 3 compares the mean Dice's similarity index (SD) obtained by selecting atlases with manifold learning and using the BASE method. The results show that all 3 manifold learning selection methods significantly outperform (pv0:05) the plain selection method. Table 4 shows the mean (SD) of the manual and automated hippocampal volumes. The automated volumes were computed using Locally Linear Embedding with the optimized parameters. Table 5 . Mean (SD) of the volumes (in mm 3 ) in the left hippocampus in the baseline images of the labelled ADNI data set of 30 images for method validation.\\nControl (n = 10) MCI (n = 10) AD (n = 10) The mean (SD) of differences between the manual and automated hippocampal volumes by baseline diagnostic group was 27 (129) mm 3 (automated,manual) for controls and 212 (150) mm 3 (automated.manual) for AD subjects. In order to test the validity of our method, we compare the proposed method to a state-of-theart method for hippocampus segmentation based on a similar atlas library approach [7] . Using the same library of 110 hippocampus images and optimal parameters defined in [7] , a similar leave-oneout method is performed. The mean Dice's similarity index was 0.8955 (0.0172) compared to 0.9077 (0.0211) in our method. Even though these values differ by 0.01 point only, the difference is statistically significant (p,0.001). Figure 2 plots the volume correlation between the manual segmentation and our automatic segmentation method. The volume differences between manual segmentation and automatic segmentation are similar to zeromean random noise. Figure 3 shows an example of segmentation obtained with our method. Overall, these results show that registering atlases that have been selected by manifold learning (i.e. selection in the lowerdimensional space) produces accurate and robust segmentation in the framework of multi-atlas based segmentation and gives better results compared to atlas selection without manifold learning (i.e. selection in the high-dimensional space). Also, given our data set of atlases, Locally Linear Embedding gives significantly better results than Isomap and Laplacian Eigenmaps.\",\n       'As phrased, the subject of the conference-Hw will we recognize a shortage or surplus in the scientific and technical workforce: Improving the data system for decisionmaking?-suggests that determining whether or not a labor shortage (or surplus) exists can be highly problematic. This paper agrees with this implicit assumption and argues that the difficulties result from both the traditional empirical approach taken to measuring shortages (surpluses) and to the substantial gaps in knowledge about unfilled vacancies by occupation. An alternative, admittedly more cautious, approach is developed and is applied in a case study format to six different types of engineering occupations: mechanical engineers, aerospace engineers, industrial engineers, electrical and electronic engineers, civil engineers, and chemical engineers. In what follows, labor shortages will be used as the anchor for the analysis, leaving the discussion of labor surpluses as the implied analogous case. The classic theoretical definition of a labor shortage, the persistence of vacancies even in the face of rising wages, implicitly describes a labor market in which the demand for labor is rising faster than the supply that is forthcoming in response to higher wage offers. The traditional approach to determining whether a labor shortage exists is to model the demand for and the supply of labor separately and use the difference between the two as a measure of current or, as relevant, future labor shortfalls. Such a seemingly straightforward task immediately poses a number of challenges. First, what is the unit of observation? Are vacancies being defined on the basis of detailed occupations, or are vacancies being defined on some other dimension, such as worker skills? Second, what is the unit of time reference? Is the policy concern over current or future labor market conditions? If the latter, then is the perspective one of a relatively short-term nature, such as the next three years, or is it long term, say ten years or more? If one could confidently measure the current or future demand for and the supply of workers by detailed occupation-much less by skill level-then one could also inform policymakers as to the potential for worker shortages by these same dimensions. However, the difficulty of measuring current or projected demand and supply by occupation or by worker skills presents a formidable obstacle. Currently, no federal statistical survey provides measures of vacancies and employer wage offers by detailed occupational group. 2 Measures of the supply and demand for worker skills are even more problematic. ____________ Another obstacle is the difficulty of modeling, for each detailed occupation, the dynamic labor market responses to shortage conditions. In addition to changing wage offers, there are numerous other possible adjustments that employers may adopt when they are having a difficult time finding workers. How industries manage their human resource requirements is influenced by a great many factors, including, but not limited to, the available labor supply, including immigration; the skill levels of prospective job seekers; the use of technology in the production process and the required capital-labor ratio consistent with the technology used for production; how work is organized; the use of employees from the personnel supply services industry; the hiring of self-employed contractors; the use of flextime and \"flexiplace\"; the use of overtime or mandatory shift coverage; and the hiring of offshore labor in other countries. This paper argues for an equilibrium approach to determining whether current labor market conditions for detailed occupations are consistent with evidence of a labor market shortage. In particular, is there a pattern of substantial increases in nominal wage offers that elicit or are accompanied by fairly small increases in net employment levels-indicating difficulties in finding sufficient supplies of skilled workers in the occupation? 3 Certainly, this is not the only circumstance in which an occupation may be in shortage. For example, a shortage can exist in the short run but be effectively eliminated if rising wages result in substantial increases in employment over longer periods. Alternatively, an occupation can be thought of as being in a shortage condition even when wages are not rising in an attempt to restore labor market equilibrium. In particular, it is possible that employers are adjusting at other margins, such as hiring immigrants, offering or requiring overtime, hiring temporary workers, or moving jobs offshore, to name a few. While such pure nonwage adjustments may be observed in response to hiring difficulties, the approach taken in this paper to identifying shortages is stricter and admittedly more traditional. Substantial increases in nominal wages that are accompanied by weak employment growth are used to initially screen occupations for the possible existence of long-term shortage conditions. A second screen that is applied asks whether the fortunes of a given occupation are closely tied to the fortunes of particular industries or a set of industries. And if so, how have these particular industries fared over the period under analysis? Does the profile of output in these industries suggest there has been continued strong demand for the services of employees in the occupation? This triage approach narrows the list of occupations being considered as having longterm shortage conditions. For the remaining occupations, other complementary pieces of evidence are examined to determine whether, if combined, they paint of portrait of a labor market for detailed occupations that are facing long-term shortages. This examination includes asking whether the experienced unemployment rate for these occupations was low (or whether it got significantly lower), indicating a relatively small untapped labor force willing and ready to take a job. Did employers seek to expand their base of employees, reaching out to various demographic, immigrant, or armed service veteran groups that may not be typical employees in the occupations? Did employers offer more overtime or increase the use of part-time workers? Did educational attainment requirements for the occupation ____________ decline over the period, as evidenced by a significant leftward shift in the educational distribution of the occupation? While this approach provides evidence on possible shortage occupations based on past data, what can be said about prospects for future labor market shortages? It is recommended that analysts look at a number of indicators. What is known about projections of net employment growth in occupations? For example, the BLS program of long-term employment projections provides estimates of net employment change for 10 years in the future. Is the occupation projected to grow faster than average? And are there significant anticipated replacement needs for the occupation? As mentioned above, the market for six different engineering occupations will be examined. The purpose of this analysis is to demonstrate, in a case-study format, the use of existing data to build a prima facie case as to whether there are substantial current or anticipated difficulties in hiring workers from selected occupations. Data from the Current Population Survey are used to examine the pattern of nominal wages and employment. In particular, the 1994-2000 period is used to compare labor markets at the early stage of a job market recovery (1994) to those at the end of an expansion (2000). As necessary, to guarantee sufficient weighted cell sizes for estimation, data for 1994-1996 are combined and compared to data for 1998-2000. Otherwise, comparisons are made between 1994 and 2000. Finally, in addition to examining the historical employment and wage trends, BLS projections of occupational employment for the 2002-2012 period are used to examine the anticipated demand for these occupations in the long run. Table 10.1 provides the evidence on employment and wage trends over the 1994-2000 period. Three occupations-mechanical engineers, aerospace engineers, and industrial engineers-experienced substantial increases in nominal wages over the period, but the change in labor supply in each of these occupations associated with these higher wages, while positive, was extremely weak. Wages for mechanical engineers and aerospace engineers rose faster than the average increase for all occupations, while wage increases for industrial engineers were only slightly less than the overall average. The labor market in all three of these occupations had a very sluggish labor supply response associated with the significant increases in wages, indicating labor markets in which there may be long-term difficulties finding workers. In contrast, three occupations-electrical and electronic engineers, civil engineers, and chemical engineers-each experienced substantial increases in employment in association with wage increases. In particular, for electrical and electronic engineers and for civil engineers, if shortage conditions existed, they were temporary and were effectively erased by the pattern of significant increases in labor supply in response to the rising wages. The sixth engineering occupation, chemical engineers, experienced a highly elastic labor supply response in association with higher wages. In this case, a 4.6 percent nominal wage change between 1994 and 2000 was accompanied by a 65.3 percent net supply increase of full-time workers in this occupation between 1994 and 2000. Of the six occupations discussed, only three pass the initial screen for possible longterm difficulties in finding workers: mechanical engineers, aerospace engineers, and industrial engineers. The next screening test assesses whether the fortunes of any of these occupations are tied to the fortunes of a particular industry or set of industries-and how have these industries fared? One of these occupations, aerospace engineers, is concentrated in two aero- space industries. In 2000, 61 percent of aerospace engineering employment was in the aircraft and parts and the guided missiles, space vehicles, and parts industries. Real output in these two industries declined over the 1990-2000 period, from $142 billion to 136 billion (measured in 1996 chain-weighted dollars). The other two occupations are spread out over more than 100 industries. In the case of mechanical engineers, no one industry employed more than 8 percent of employees in this occupation group. One industry, engineeringarchitectural-surveying, employed 15 percent of industrial engineers, while no other industry employed more than 6.5 percent. The concentration of aerospace engineers in a declining industry makes it difficult to establish that the small employment gain in this occupation from 1994 to 2000 was the result of an occupation in a long-term shortage condition. Indeed, the declining fortunes of the aerospace industry may have discouraged entry into the aerospace engineering occupation over this period and may quite possibly have made it difficult to find workers in this occupation-leading to the observed increases in nominal wages. In addition, the 2002-2012 BLS employment projections estimate a net employment decline of 5 percent for this occupation. Taken together, these pieces of evidence suggest ruling out this occupation from further consideration. For the remaining two occupations-mechanical and industrial engineers-a natural next step is to examine whether there were other labor market responses that would reinforce the view that there have been long-term difficulties in finding adequate supplies of workers. Table 10.2 provides evidence on the experienced unemployment rate 4 for these two occupations. The experienced unemployment rates declined steadily for both engineering occupations and were consistently below the rate for all workers. A relatively low experienced unemployment rate is consistent with the idea that workers in these fields might become difficult to find if the demand for their services were to expand significantly in the short run. Table 10.3 provides the evidence on a variety of other characteristics. As the table indicates, both engineering occupations have extremely small shares of female employees, a The experienced unemployment rate for a specific occupation is based on the concept of experienced unemployment in the occupation-that is, the number of unemployed individuals whose last job was in that occupation. The experienced unemployment rate is the number of experienced unemployed in the occupation divided by the sum of employed and experienced unemployed in the occupation.  1994-1996 and 1998-2000. However, for both mechanical and industrial engineers, the overall proportions of individuals who work 40 hours a week or more was relatively stable over the period. Nor was there any significant shift in the proportion of workers who work full or part time, the latter being a very small percentage of the total for both occupations over the period. Reliance on employees who have served in the military actually decreased in relative terms, and the proportions of employees who are immigrants was virtually unchanged for both occupations over the period. And finally, the evidence on educational attainment shows a trend toward the preferred hiring of 4-year college graduates for both of these occupations, since the percentage of those with some college fell and the percentage with 4 years of college or more rose over the period. The lowering of relative skill requirements needed to gain entry into each occupation does not appear to have been a deliberate strategy on the part of employers. Overall, for both occupations, the evidence does not support the idea that firms, in responding to long-term shortage conditions, adopted any significant nonwage adjustments in their hiring or workplace practices to increase either the number of employees or the number of hours worked. What about future prospects for these three occupations? What is the expected increase in net employment for these occupations? Table 10.4 provides the BLS projections of occupational employment for the 2002-2012 period. Net employment in both of these occupations is projected to increase over the period, although the percentage change is less than the percentage change for all occupations. Another potential indicator of future shortages is the number of workers in the occupation who will need to be replaced owing to retirements. Table 10.3 provided the shares of employment in each occupation aged 55 and older. Overall, the percentage of employees aged 55 and older rose from 12.2 to 12.9 percent between 1994-1996 and 1998-2000. In contrast, the percentage of mechanical engineers aged 55 and older fell from 15.0 to 12.8 percent, and the percentage for industrial engineers also fell, but only slightly, from 12.9 to 12.3 percent. What does the overall evidence suggest in terms of long-term difficulties in finding sufficient supplies of mechanical and industrial engineers? There does appear to have been a deliberate rising wage strategy that was relatively unsuccessful in attracting new workers into these occupations over the 1994-2000 period. In addition, BLS projections for 2002-2012 indicate continued increases in demand for the services of these workers. In contrast, however, the evidence does not indicate that employers were adopting various alternative nonwage strategies for hiring workers in these two occupations. And perhaps the single most important piece of evidence missing to bolster the view that these occupations suffered longterm difficulties in finding skilled workers over the 1994-2000 period is information on the number of vacancies and the wage offers accompanying these vacancies. Overall, the evidence points to possible long-term shortage conditions based on the wage-employment results, but the lack of complementary evidence on nonwage adjustments suggests viewing this conclusion with appropriate caution. ',\n       'The growth in the representation of foreign students among doctorate recipients from U.S. universities captures changes on both sides of the market for graduate education. In particular, the growth reflects some combination of the following circumstances: 1) shifts in demand for graduate study among foreign born arising from changes in the sending country, 2) shifts in demand arising from changes in institutions that affect the \"costs\" of matching students with U.S. graduate programs, including the development of international networks, and 3) adjustments in the supply-side or offerings of U.S. universities that differentially affect foreign students. The forces affecting the representation of foreign students in U.S. doctorate education are presented through a basic supply-demand framework. \"Demand shocks\" generated by increases in the number of undergraduates (potentially) prepared for graduate study from abroad are one dimension of change. Those countries with relatively high BA growth might be expected to expand in the share of PhDs received from U.S. institutions. Growth in the size of cohorts prepared for graduate study (for simplicity, those with the BA) is the most obvious type of demand shift varying across countries. Such shifts may include growth in the fraction of college graduates or shifts in cohort size, varying in magnitude and timing across countries. Over the course of the last half century, a number of political transformations such as the fall of communism in the Soviet bloc or the normalization of relations with China have dramatically altered the demand for graduate study in the U.S. among foreign students. Beyond changes in the number of students prepared for graduate work in a country, a related change in demand comes from the development of networks that reduce the costs of foreign study. Following dynamic models similar to the Carrington et al. (1996) of the South-North migration of blacks in the first half of the 20 th century, successful experiences of initial migrants lead to dramatic reductions in information costs among those in later cohorts. Students from specific regions or foreign universities may establish links with U.S. programs; in turn, U.S. universities may use past experience in recruiting and selecting students. Such network effects have the long term result of increasing the relative benefits of pursuing doctorate study and the share of students from abroad pursuing graduate study in the U.S. By lowering the costs and increasing the value of graduate education in the U.S., such networks serve to shift the demand for graduate education in the U.S. The supply-side of the U.S. market for graduate education is by no means fixed over time. Because doctorate-level students do not pay full tuition for their studies, the availability of opportunities is likely to be determined by research funding and other institutional sources of support, including state funding and demand for teaching assistants. These sources of support have varied over time, with federal funding for science stagnant from the 1970s through the mid-1980s. Then, beginning in the mid 1980s, there were quite substantial increases in federal research funding to colleges and universities in both the physical sciences and the health sciences. As a result, we expect supply shocks affect the doctorate education market. Increases in research funding or direct public support for graduate programs in the U.S. have the effect of increasing the number of opportunities for study in U.S. graduate programs. If the elasticity of demand for graduate study among those from abroad is greater than for the U.S. (perhaps because the opportunity cost is study in another country rather than a different career), funding shifts will yield relatively larger responses in degree attainment among foreign students, resulting in increasing share with positive shocks and decreasing share with adverse shocks. When the fraction of a country\\'s potential doctorate students choosing to study in the U.S. is initially small (or when there is excess demand among foreign students for U.S. programs), expansions in U.S. opportunities could plausibly have proportionately larger effects on the number of individuals pursuing a degree in the U.S. than when the share pursuing degrees is already quite large. A second explanation is that when foreigners considering studying in the U.S. have alternatives that are close substitutes (e.g., studying in Australia) elasticity of demand will be much higher. For those from the U.S., the alternative to pursuing a PhD at a U.S. university is unlikely to be a close substitute, demand will likely be more inelastic, and the change in graduate study in response to a supply shock somewhat more limited.',\n       'Flag indicating whether any of the inputs to X1MOMEDU were statistically imputed. F-32 HSLS:09 Base-Year Data File Documentation',\n       nan,\n       \"For example, in a comparison of the percentages of males and females who enrolled in postsecondary education, only one comparison is possible (males v. females). In this family, k=1, and the comparison can be evaluated with a Student's t test. When students are divided into five racialethnic groups and all possible comparisons are made, then k=10 and the significance level of each test must be p .05/10, or .005. The formula for calculating family size (k) is as follows: k=j * (j -1)12, where j is the number of categories for the variable being tested. In the case of raceethnicity, there are five racialethnic groups (American Indian/Alaskan Native; Asian/Pacific Islander; black, non-Hispanic; Hispanic; and white, non-Hispanic), so k=5*(5-1)/2=10. Trends. In some instances pair-wise comparisons proved too cumbersome. For example, one would like to say something about the general relationship between the percentage of first-time beginners who attained a degree and their number of risk factors when they began postsecondary education. In many cases not all of the six possible comparisons are statistically significant, even though the data appear to suggest clear trends. In such cases, a weighted least squares regression formula was used to test whether the inverse trend between the number of risk factors and the percentage of students with a postsecondary degree was significant, even if all of the pair-wise comparisons were not. This regression test for linearity was done in this analysis using the data manipulation and regression capabilities of the Microsoft EXCEL spreadsheet program. The input data for the regressions were the estimates and standard errors in the output tables created by the Data Analysis System. All of the variables included in the regression equations were transformed by dividing them by the standard error of the relevant proportion. An intercept variable was also created by dividing a column of Is by the standard error of the corresponding proportion. The new dependent variable was then regressed on the new independent variable and the intercept variable. The statistical significance of beta for the independent variable was then evaluated in relation p 5_0.05, or t .?..1.96. One important limitation of this test is that it can only be used to assess trends across interval variables or variable categories.  c-bo 2'j:4 \",\n       'Explanation of Weights. Eight weights were developed for inclusion on the data files. They',\n       'Medium-sales farms. Gross sales between $100,000 and $249,999.',\n       'The execution of the Information Interaction Method on the Wellcome Trust Breast Cancer dataset takes around 45 minutes in a 2.3 GHz AMD Opteron Processor. The execution of 1000 permutations in a single processor would take about a month, or 3 days if we use 10 processors. We also made some experiments with an Alzheimer dataset from the Alzheimers Disease Neuroimaging Initiative (ADNI) with 600000 SNPs and the execution after data quality procedures took around four days. We did not complete the execution of the 1000 permutation tests because we could conclude in advance that the results would not be statistically significant. The complete set of 1000 permutation tests would take several years to complete in a single processor, although it could be parallelized. Since most of the time it is not practical to have access to a large number of processors, there is still the need for methods that need less permutation tests in order to measure statistical significance. We are currently working on methods that obtain the estimates of the p-values with a much smaller number of permutation tests.',\n       'Identifying the boundaries of an urban area can be challenging [142, 143] . This is mainly because urban areas tend to be strongly connected to their neighboring urban areas, rural hinterlands, and distant areas for their economic and hydrological functions. Also, the spatial extent of urban areas changes in time, cities can grow or shrink, independently of their defined administrative and political boundaries [144] and potable water supply zone. The changing form of urban areas and their connectivity with other distant areas makes any definition of hydro-economic decision boundaries relative to context. Hence, the dynamics and flows of virtual and real water in an urban area will crucially depend on the definition of the boundary. It thus seems necessary to simultaneously compute WFs at multiple boundary conditions when dealing with flows in urban areas, for example the potable water supply boundary, the watershed or aquifer boundary, the state or regional government boundary, national boundary, and planetary boundary.\\nIn practice, for research purposes, the definition of the urban scale tends to be arbitrarily set by the dataset employed and the agency in charge of data collection. For instance, in the U.S., the Census Bureau uses several definitions of urban areas: metropolitan statistical area (MSA), microstatistical area, and combined statistical area (CSA). The MSA has a rural to urban gradient as it consists of one or more counties around an urban core that has a population greater than 50,000 while the microstatistical area is a densely populated region with more than 10,000 residents but less than 50,000 [145] . The CSA is a grouping of adjacent MSAs. Furthermore, river basins, population, and population density have all been used to define boundaries for urban areas [17, 56, 132] . There are multiple definitions of an urban area and this should be taken into consideration in urban WF analysis, because per the logic of ERA these different boundaries imply different points of view and different accounting of footprints and values.\\nBringing the urban WF to the parcel/individual establishment and residential level will create the most opportunity for urban citizens, stakeholder groups, and water managers to operationalize the urban WF metric, and it will provide the most reliable information for informing urban planning and policy decisions. Detailed urban WF studies (i.e., at the parcel/individual establishment level) could help account for the spatial variability of WF components within each city and their unique interaction with the different direct and indirect water supplies, and help assess the implications of local urban impacts on the WF. However, economic data collection is generally not harmonized with the boundaries of an urban potable water provider-a limitation that should be addressed by the economic development and planning departments of cities that wish to employ WF analysis. More research into the effects of boundaries on urban WFs, sustainability benchmarks, and water decisions is needed. Specifically, economic statistics need to be developed by city governments for industrial and commercial (IC) establishments within their boundaries, and these statistics need to be linked with water consumption accounts and urban land use planning data. This three-way linkage between IC socio-economic productivity, IC water use, and land use planning will provide the foundation for urban water planning and decision-making utilizing WFs.',\n       \"Background. Age-related olfactory loss (presbyosmia) is a prevalent sensory impairment with a large public health impact. In cross-sectional analyses, we found striking health disparities in olfactory function among older U.S. adults. Here, we report a 5-year follow-up to determine the magnitude of within-person olfactory decline. Methods. The National Social Life, Health, and Aging Project (NSHAP) interviewed a probability sample of home-dwelling older U.S. adults (57-85 years) in 2005-2006 (Wave 1) and reinterviewed them in 2010-2011 (Wave 2), assessing demographics, social life, and health, including olfaction. Odor identification was measured with a 5-item version of the Sniffin' Sticks (0-5 correct). Fourteen hundred and thirty-six respondents provided olfaction data in both waves. Multivariate linear and logistic regression were used to model the association between change in olfactory performance and demographic, health, and psychosocial factors. Results. Odor identification declined most rapidly among older individuals (0.25 additional errors per 5 years for each decade of age, p < .001) and in men (0.17 additional errors per 5 years compared to women, p = .005). Among those with perfect scores in Wave 1, African Americans declined more rapidly than Whites (p = .04). Neither socioeconomic status, health conditions, cognition, mental health, alcohol use nor smoking was associated with change in olfaction (p > .05, all). Conclusions. The rate of olfactory decline increases with age and is greater among men than women despite adjusting for differences in psychosocial and health conditions, indicating physiologic factors as drivers. African Americans are more likely to experience initial olfactory decline, consistent with an earlier onset of aging among this subgroup.\",\n       'Cells grown as monolayers in a T-25 flask (growing surface 25 cm 2 ) were inoculated when they were at 80% of confluency. First, aliquots (100 µL) of the concentrated air sampler collection media were filtered through a sterile 0.45 µm pore-size PVDV syringe-tip filter to remove bacterial and fungal cells and spores. Next, the spent LLC-MK2 and Vero E6 cell culture medium was removed and replaced with 1 mL of cell culture medium, and the cells inoculated with 50 μL of cell filtrate. When virus-induced cytopathic effects (CPE) were evident, the presence of SARS-CoV-2 was determined by rRT-PCR.',\n       'Step 1 -Training model: find λ * and p(w)',\n       'We used county-level estimates of nitrogen fertilizer use to compile state statistics for farm use of nitrogen fertilizer between the years 1992 and 2006. The selection protocol can be found in [29] .\\nState-level estimates of total food and industrial use of maize were gathered from data published by the University of Nebraska-Lincoln [30] . The data was derived from the \"Feed Grain Database: Yearbook Tables,  Corn: Food, Seed, and Industrial Use,\" ',\n       'Evidence on the role of TAMs in neurodegenerative/neuroinflammatory diseases is rapidly growing. At present, there is a significant bulk of data in animal models of AD, PD, and MS, while very few data are reported in patients. Chronic neuroinflammation, mediated by microglia and astrocytes, is a crucial player in neurodegeneration [122] . The pathological hallmark of many neurodegenerative diseases is a specific protein deposit; it is the case of betaamyloid and tau accumulation in Alzheimer\\'s disease (AD) and alpha-synuclein in Parkinson\\'s disease (PD). In vitro and animal model studies showed that pathological protein deposits can stimulate a chronic neuroimmune response, with in turn releases proinflammatory cytokines and reactive oxygen species, contributing to degeneration. On the other hand, an ineffective microglial phagocytosis is an early finding in the disease process, impairing clearance of abnormal proteins [123] .\\nThe role of microglia has been extensively studied in animal models of neurodegenerative disease. The development of mouse models of amyloid deposition allowed testing the effects of amyloid-activated microglia in AD in vivo [124] . The polarization of microglia into a proinflammatory phenotype is likely to be a key step in neurodegeneration. In an AD model, a pathogenic stimulus such as hypoxia, able to promote amyloid deposition and neurodegeneration, triggered the polarization of microglia into an activated phenotype [125] ; consistently, the suppression of proinflammatory responses produced protective effects in a lipopolysaccharide inflammation-induced AD model [126] . Results of studies in PD mouse models confirmed the role of activated microglia in neurodegeneration: MPTP administered to mice induced a consistent gliosis in the substantia nigra pars compacta associated with significant upregulation of inducible nitric oxide synthase [127] . In MS, proinflammatory T helper lymphocytes are classically considered the main players in lesion generation. Nevertheless, it is proved that MS development is associated with microglial activation and, notably, this was observed both in active demyelinating lesions and inflammatory nondemyelinating areas [128] . Other studies in AD models, however, showed that the experimental increase of microglial activation could also enhance clearance of the amyloid deposits [124] . This effect could be maximum in the very early stage of neurodegeneration, when a \"protective\" inflammation develops with the aim of contrasting and clearing the pathological process [109] . Differently, in other conditions microglial activation may be detrimental. A recent study showed that microglia-mediated phagocytosis can be activated by phosphatidylserine, which is externalised by live neurons containing tau deposits, and an analogous phagocytic signal exists in human tauopathies [129] . These different results suggest that the classification of microglia into an activated and a resting phenotype is only a simplification, since various microglial populations exist with a specific role, detrimental or beneficial in different stages of disease.\\nPET imaging for neuroinflammation is a valid approach for in vivo quantification of dynamic changes in neuroinflammatory processes. Increasing data provided by several PET studies, especially in AD, confirm that microglial activation accompanies neurodegeneration, particularly in the early phase, where a therapeutic approach might be beneficial [130] .\\nStudies were performed to investigate the role of TAMs in AD, overall point to a protective effect against progression, probably acting on both neuronal survival and amyloid deposition. It was showed that the nerve growth factor, which may counteract AD-related neurodegeneration of cholinergic neurons [131] , induced both Tyro3 and Axl expression in differentiating embryogenic cells and protecting them against apoptosis [132] . Moreover, a study on the role of Tyro3 in amyloid precursor protein (APP) processing and amyloid deposition in the hippocampus of AD models, showed that the overexpression of Tyro3 significantly decreased amyloid beta plaques burden from cell lines, while in Tyro3 knockdown transgenic AD mice the number of amyloid plaques increased in the hippocampus [133] . Zhang and colleagues recently analysed the effects of Jujuboside A, a molecule with antioxidant, anti-inflammatory, and neuroprotective properties, in an APP/PS1 mouse model. They found that Jujuboside A exerted its activities through Axl-mediated pathways, and it was efficient in facilitating amyloid plaque clearance and ameliorating cognitive deficits, thus suggesting that Axl could stimulate microglial phagocytic activity promoting amyloid clearance [134] . In line with these findings, a very recent study showed that melatonin administration was able to ameliorate cognitive functions both in healthy nontransgenic (NoTg) and AD transgenic (3xTg-AD) mice [135] . Authors detected not only a decrease of proinflammatory cytokine expression but also the modulation of Gas6 and its receptors and upregulation of proteasome activity, which is an important mechanism involved in both neurodegenerative and neuroinflammatory disorders [136, 137] .\\nIn human, few studies are aimed at investigating the relationship between TAM expression and amyloid pathology both in normal aging and in AD. Mattsson and colleagues analysed the baseline levels of CSF proteins involved in microglial activity and amyloid metabolism, assessing the longitudinal CSF levels of the peptide amyloid-β1-42 (Aβ42) decrease in cognitively healthy people [138] . Axl, chromogranin A, and angiotensin-converting enzyme were the most significant proteins associated with longitudinal Aβ42 decrease, suggesting that they might predict the development of amyloid pathology at the earliest stages of AD [138] . Axl plasma levels, along with other analytes involved in amyloid metabolism such as matrix metalloproteinase-9 and apolipoprotein E, were associated with amyloid burden measured by [ 11 C]-PiB PET imaging in AD subjects from the Alzheimer\\'s Disease Neuroimaging Initiative (ADNI) cohort [139] . Sainaghi and colleagues reported the first evidence of a significantly increased CSF level of Gas6 in AD patients compared to controls [140] . The higher levels of Gas6, particularly in the early stage of disease, suggested a compensatory role of Gas6, with the aim of downregulating proinflammatory cytokine production and promoting amyloid clearance [140] . A very recent report investigated the genetic regionspecific expression changes in AD and control brain homogenates, through a series of biochemical, molecular, and bioinformatics analyses [141] . The study shows an upregulation of genes related to the toll-like receptor signalling, usually involved in amplifying immune responses in the CNS, along with the upregulation of Pros1 in moderate stages of AD and an increase of Gas6 expression from normal cognition through AD-type pathology. Considering the role of TAMs in modulating toll-like receptor signalling, these findings suggest again that a dysregulation of TAMs may contribute to AD pathology [141] .\\nPD is a progressive neurological disorder that affects both motor and nonmotor systems [142] . Widespread aggregation of the α-synuclein protein into inclusions called Lewy bodies, which can be detected in both the central and peripheral nervous systems, is the pathological hallmark of the disease [142, 143] . It is currently believed that a higher α-synuclein burden is associated to a more severe PD phenotype [144] . TAMs may play a role in PD pathogenesis, influencing microglial activation and phagocytosis and regulating alphasynuclein deposition. Studies in animal models support this hypothesis. Indeed, in early-stage PD, deficiency of the transcriptional factor Nrf2, which regulates Axl and Mertk in microglial phagocytosis and inflammatory gene expression, exacerbated protein deposition, neuroinflammation, and neuronal loss [145] . Moreover, a work on a transgenic mouse model of hereditary PD, characterised by a deposition of alpha-synuclein predominantly in the spinal cord, showed an increased expression of Axl, mainly in the spinal cord but also in the brain, which was age correlated [31] .\\nMS is a progressive autoimmune disease of the CNS, characterised by inflammation, demyelination, and neurodegeneration. Demyelination derives from cell infiltrates of proinflammatory T-helper lymphocytes [146] ; oligodendrocyte loss and microglial activation strongly contribute to the pathological process, leading to axonal damage, which can also be present independently of lymphocyte infiltration and myelin damage [147] . Whether neurodegeneration is a primary or secondary event is not completely clear; nonetheless, it represents the major contributor to clinical disability [148] . TAMs have been extensively studied in animal models of MS and to a lesser extent in human [74] .\\nSeveral studies underlined the protective role of TAMs in the \"cuprizone model,\" which is particularly useful in studying factors which influence myelin damage and repair. The administration of cuprizone, a copper chelator, to adult mice, induces a toxic demyelination without affecting the bloodbrain barrier; a spontaneous remyelination can be observed in the first week [149] . Gas6, Axl, and Mertk are upregulated in mice after cuprizone-induced demyelination, in parallel with microglial activation [117] . The absence of Axl in the mouse model delays recovery from cuprizone toxicity due to a deficit in phagocytosis of myelin debris and extends axonal damage [60] . Gas6 knockout mice show a more severe cuprizone-induced demyelination, a delayed remyelination, an increased microglial activation, and a greater oligodendrocyte loss [117] . Furthermore, the administration of Gas6 improves recovery from cuprizone-induced injury, favouring remyelination and cellular and myelin debris clearance [118] . EAE is a model of CNS severe inflammation, with demyelination and axonal damage, induced by immunization with myelin antigens or myelin-specific T lymphocyte transfer. In EAE, as showed in the cuprizone model, Gas6, Axl, and Mertk are upregulated. Gas6 knockout mice have more severe demyelination and axonal damage linked to the EAE, and the Gas6 delivery protects against demyelination and accelerates repair [113] . Axl deficiency increases inflammatory response and hinders cellular and myelin debris clearance in EAE [120] .\\nOverall, these findings in MS and EAE models suggest a protective role of the TAM system, especially of Gas6 and Axl, stimulating recovery of myelin and axons, favouring remyelination, regulating microglial activation, and accelerating myelin debris clearance.\\nTAMs are involved in MS lesion formation in human and play a role in disease progression. Analysing brain homogenates from chronic active and chronic silent MS lesions, Weinger and colleagues found the elevated levels of membrane-bound Mertk and soluble Axl and Mertk, with an inverse correlation with the Gas6 levels in lesions. These findings indirectly confirmed the protective role of Gas6, whose reduction in chronic lesions, due to the bond with soluble receptors, contributed to sustain pathology [150] .\\nSainaghi and colleagues measured both the CSF and plasma levels of Gas6 in sixty-five MS patients comparing them with forty controls. CSF Gas6 concentration was significantly higher in patients than in controls, with an inverse correlation with the severity of the relapses [151] . This study confirmed again the primary role of Gas6 in favouring myelin repair and recovery from damage. Another study evaluated plasma concentration of total and free Pros1 in sixty-five MS patients and fifteen controls. Plasma levels of total Pros1 were decreased in MS patients compared with controls, with very low levels of plasma free Pros1 in patients with higher disease severity, suggesting ProS1 dosage as a potential marker of disease progression [152] .\\nFinally, genome-wide association studies identified the Mertk as a novel risk gene for MS susceptibility, with several single-nucleotide polymorphisms within the gene suggestive for association with MS ( [153] ; Ma et al. 2011) . Mertk is important in mediating myelin phagocytosis by myeloid cells, which in MS lesions are represented by microglia and also macrophages derived from circulating monocytes. A recent study confirmed, in MS-derived macrophages, an impaired phagocytosis relatively selective to myelin and linked to an abnormal reduction on expression of Mertk [154] . Treatment with TGFβ could restore phagocytosis and expression of the receptor and the ligand [154] . These results encourage the development of new molecular immunomodulation therapies which may have an impact on disease progression.',\n       \"This report is based on final data from two Federal surveys. The first is the U.S. Department of Education's Integrated Postsecondary Education Data System (IPEDS) Completions Survey conducted annually by the National Center for Education Statistics (NCES). The second is the Survey of Earned Doctorates, conducted annually for the National Science Foundation (NSF) and four other Federal agencies. In addition, population data on various age groups Table   Page 19. Doctoral degrees awarded, by major field group:     8 198,805 104,993 34.6 50,899 16,498 24.5 14,775 3,233 18.0 195,888 107,667 35.5 50,034 17,230 25.6 14,199 3,454 19.6 193,247 109,915 36.3 46,614 17,612 27.4 14,128 3,744 20.9 191,215 113,480 37 196 24,541 11,657 47,257 175,191 1968 359,747 165,200 37,464 14,602 2,105 15,243 27,175 13,791 54,820 194,547 1969 412,865 189,272 41,270 15,962 2,633 17,915 31,190 16,758 63,544 223,593 1970 9,673  2,707  32,112  24,550  13,584  49,548  289,687   1989   487,566  189,338  56,759  9,777  2,380  29,682  23,852  14,291  52,597  298,228  1990   .   495,867  189,082  54,732  9,106   2,001   27,184  24,050  15,399  56,610  306,785   1991   508,952  189,328  52,522  9,253  1,946  25,700  25,007  16,155  58,745  319,624  1992  525,395  195,779  52,305  9,289  2,177  25,693  27,473  17,130  61,712  329,616  1993   537,536  200,315  52,724  9,424  2,453  25,483  30,439  18,029  61,763   337,221  1994  537,061  202,284  52,609  9,588  2,665  25,397  33,347  18,749  59,929  334,777  1995  531,146  202,217   52,421   9,605  2,954  25,066  35,915  19,638  56,618  328,929  1996  528,000  203,341  51,798  9,694  2,972  24,857  39,100  19,965  54,955  324,659  1997  525,282  201,471  50,882  9,382  2,924  25,324  39,967   19,491  53,501  323,811  1998  525,714  200,221   49,575  9,279  2,722  26,670  40,235  19,057  52, ,595  21,679  34,663  45,186  409,122   566,284  140,012  9,973  4,319  775  15,185  22,401  38,619  48,740  426,272   1991   599,045  148,347  9,665  4,425  782  14,494  23,776  42,738  52,467  450,698   1992   624,677  159,486  9,636  4,586  1,024  14,196  26,720  46,903   56,421  465,191  1993   641,742  165,720   9,981   4,764  1,050  13,950  29,182  49,222   57,571   476,022   1994   646,080  170,977  10,403  5,067  1,203  13,788  31,921  51,019  57,576  475,103   1995   643,290   175,931  10,950  5,292  1,524  13,554  35,555  52,963  56,093  467,359   1996  651,815  181,333  11,316  5,702  1,485  12,764  39,369  53,863  56,834  470,482   1997   661,307   187,011  11,470  5,882  1,542  12,792  42,533  55,243  57,549  474,296   1998   673,865  190,397  11,339  5,994  1,599  13,098  44,844  55,400  58,123  483,468 NOTE: See section C for specific fields that are included in each category.                                       The Foundation provides awards for research and education in the sciences and engineering. The awardee is wholly responsible for the conduct of such research and preparation of the results for publication. The Foundation, therefore, does not assume responsibility for the research findings or their interpretation. The Foundation welcomes proposals from all qualified scientists and engineers and strongly encourages women, minorities, and persons with disabilities to compete fully in any of the research and education related programs described here. In accordance with Federal statutes, regulations, and NSF policies, no person on grounds of race, color, age, sex, national origin, or disability shall be excluded from participation in, be denied the benefits of, or be subject to discrimination under any program or activity receiving financial assistance from the National Science Foundation. Facilitation Awards for Scientists and Engineers with Disabilities (FASED) provide funding for special assistance or equipment to enable persons with disabilities (investigators and other staff, including student research assistants) to work on NSF projects. See the program announcement or contact the program coordinator at 703-292-8636. The National Science Foundation has TDD (Telephonic Device for the Deaf) capability, which enables individuals with hearing impairment to communicate with the Foundation about NSF programs, employment, or general information. To access NSF TDD dial 703-292-5090; for FIRS, 1-800-877-8339.\",\n       '[36] In this paper, we document the experiment design and results of simulations using an eddy-permitting ocean general circulation model -HYCOM, examining the upper-ocean impacts in the BoB caused by two consecutive TCs during Figure 5. The buoy and cruise sampling locations from October to December, 1999. Red and blue dots represent good quality temperature data by drifting buoys, while brown dots are \"unchecked\" temperature data. Black diamonds represent cruise campaigns that collect not only surface but also subsurface temperature data. Blue dots are the drifting buoys measurements in the region of 80 E-82.7 E, 0 N-5 N after 11/16 in 1999. See text for more description for the purpose of separating blue from red. Figure 6. SST comparisons between the MR results and drifting buoy observations. Blue triangles are for the data in the region of 80 E-82.7 E, 0 N-5 N after 11/16 in 1999, red crosses are for the data when measurements have good quality and not classified as blue (see text for explanations), and brown crosses for \"unchecked\" measurements. The numbers shown on the lower right portion of the Figure are standard errors. The RcWIND results are very similar to those of the MR. October-November 1999. The implications of these results for the BoB heat budget will be examined in the companion paper (part 2). The HYCOM simulations are designed and performed using 0.25 Â 0.25 grids and 30 vertical layers, which is driven by satellite-observed CCMP winds, TRMM precipitation, and ERA-Interim reanalysis radiative flux and other forcing variables (section 2.2). Freshwater input from medium to large rivers surrounding the BoB are also included. Figure 7. Temperature profiles from cruise observations (red) and from HYCOM RcWIND (black). The MR and RcWIND have almost identical temperature profiles from the equator to 10 N. Only 12 out of 47 temperature profiles are shown to demonstrate the model skill from 76 E to 96 E at every 2 interval ( Figure 5, black diamonds) during October-November, 1999. The above forcing fields are able to capture the observed TC1 and TC2 identified by the IBTrACS data, although the CCMP winds significantly underestimate the maximum wind speeds associated with the TCs (section 2.1 and Figure 1). To overcome this underestimation, one additional experiment is performed using reconstructed TC-wind velocities based on the modified Rankine vortex as in Holland [1980] (section 2 and Figure 2). [37] Solutions from the HYCOM MR and RcWIND run with enhanced TC winds reasonably reproduce the upper ocean thermal structure and SSH in the BoB compared with cruise and satellite observations, albeit with quantitative model/data differences. HYCOM simulations, however, exhibit weaker vertical temperature gradients in the thermocline layer, suggesting a more diffusive thermocline in the model than in the observations. The strong SST reduction ($À3 C) near the Orissa seashore along the tracks and on the right is shown in RcWIND. Compared to the TMI observations, the contrast between the high SST in the southwestern BoB and low SST in the south central BoB during TC1 is less evident in RcWIND and results in weaker cold advection from the south central BoB to the southwestern BoB. Also, RcWIND produces weaker SST reduction in the eastern and southwestern BoB during TC2, possibly due to the lack of enough information for TC wind reconstruction in the two regions. TC2 (category 5) generally cools the \"Bay-wide\" SST less than TC1 (category 4) likely due to the initial SST depression by TC1. On the other hand, the onshore Ekman transport and mass convergence induced by TC2 cyclonic wind is more prominent because of the stronger winds and longer lifetime than TC1 over the ocean. [38] Acknowledgments. We would like to thank Luc Bujold for providing buoy and cruise observation data, Allan Wallcraft for HYCOM consultation, and anonymous reviewers for their precious comments. We also thank CISL of NCAR for providing computer resources and technical support. Jih-Wang Wang and Weiqing Han are supported by NSF CAREER award 0847605, NASA OSTST award NNX08AR62G, and NOAA NA11OAR4310100.',\n       \"Science teachers' PD learning is dependent on their positive attitudes and meaningful learning experiences, as well as how they position themselves as learners during PD (Halai, 2005; Nadeem et al., 2013) . PD programs in Pakistan are shifting from the non-reflective attitudes to more reflective stance. Iqbal and Shayer (2000) found that a favorable teaching environment with access to proper resources was connected to the motivation of science teachers and their willingness to improve their professional skills. Aslam (2013) concluded that where there was a lack of teachers' involvement in PD planning and training process, there was a lack of interest in and weak attitudes toward PD programs. Similarly, Tahir (2010) observed that those PD activities in which teachers were involved in developing, practicing, and critiquing materials for field experiences were valued more than those that provided ordinary materials. He claimed that success of PD learning depends on the participants' attitudes toward PD and access to experiences in which teachers could actively participate.\",\n       'Career expectation\\nCovariate groups oversampling of some ethnic and minority groups and (ii) the effect of multistage cluster sampling on standard error estimation. We followed the NCES guidelines by using sampling weights for statistical analyses (3). We accounted for the complex sampling design by using the STATA 9.0 statistical software package (3, 7).',\n       'A quarter century ago, an important finding in stratification research showed that the intergenerational occupational association was much weaker among college graduates than among those with lower levels of education. This article provides a comprehensive assessment of the \"meritocratic power\" of a college degree. Drawing on five longitudinal data sets, the author analyzes intergenerational mobility in terms of class, occupational status, earnings, and household income for men and women. Findings indicate that the intergenerational association is strong among those with low educational attainment; it weakens or disappears among bachelor\\'s degree holders but reemerges among those with advanced degrees, leading to a U-shaped pattern of parental influence. Educational and labor market factors explain these differences in mobility: parental resources influence college selectivity, field of study, and earnings more strongly for advanced-degree holders than for those with a bachelor\\'s degree alone.\\nhigh school graduate counterparts, a premium that has increased dramatically over the last quarter century (Autor, Katz, and Kearney 2008) . College attainment is also related to better health, longevity, happiness, and a host of extraeconomic outcomes (Ross and Mirowsky 1999; Pallas 2000; Rowley and Hurtado 2003; Attawell and Levin 2007; Stevens, Armstrong, and Arum 2008) . But college attainment is related to more than economic and extraeconomic well-being. An important finding in stratification research shows that the direct influence of parental resources on the economic position of adult children is much weaker-virtually zeroamong college graduates than among those with less schooling (Hout 1984 (Hout , 1988 . The virtually null intergenerational association among college graduates does not naturally mean the elimination of social inequality. Access to college is strongly dependent on parental resources (Hout, Raftery, and Bell 1993; Ellwood and Kane 2000; Haveman and Smeeding 2008) , and the socioeconomic gap in access appears to have increased over time (Kane 2004) . The finding means, however, that for those who attain a college degree, their socioeconomic standing is independent of their socioeconomic background. In other words, a college degree fulfills the promise of meritocracy-it offers equal opportunity for economic success regardless of the advantages of origins. This finding is not a U.S. anomaly. Research has shown a weaker intergenerational association at higher levels of schooling in other industrialized countries such as France, Sweden, and Germany (Vallet 2004; Breen and Jonsson 2007; Breen and Luijkx 2007) . The United States is, however, the clearest case in which the intergenerational socioeconomic association fully disappears among college graduates, providing \"a new answer to the old question about overcoming disadvantaged origins: A college degree can do it\" (Hout 1988 (Hout , p. 1391 .\\nThese findings describe the state of affairs in the 1970s. They were replicated for the 1980s (Hauser and Logan 1992, table 4 ), but no evaluation exists since then. The higher education system has undergone substantial change over the last quarter century. College expansion and differentiation, and the increase of postbaccalaureate advanced degrees define a new educational landscape that may have altered mobility patterns of college graduates. In addition, the original findings refer specifically to the intergenerational occupational association. Recent developments in mobility research show that measures such as class, occupational status, individual earnings, and total family income capture distinct dimensions of economic well-being and suggest that mobility findings may Barron\\'s selectivity scores to higher education institutions identified in the Baccalaureate and Beyond data set. Emily Rauscher and Robert Taylor provided excellent research assistance. Direct correspondence to Florencia Torche, Department of Sociology, New York University, 295 Lafayette Street, No. 4129, New York, New York 10012. E-mail: florencia.torche@nyu.edu be contingent on the measure used (Bjorklund and Jantti 2000; Beller and Hout 2006; Erikson and Goldthorpe 2008) . A comprehensive test of the meritocratic power of a college degree requires, then, considering distinct indicators of economic well-being.\\nFurthermore, in spite of its empirical relevance, the factors accounting for the weak intergenerational association among college graduates have not been examined or theorized. Researchers have hypothesized that labor markets for college graduates are highly meritocratic and thus blind to the advantages associated with social origins (Breen and Jonsson 2007) . However, no testable definition of meritocracy, embedded in the operation of the educational system and the labor market, has been elaborated or examined.\\nThis article addresses these questions and provides a comprehensive assessment of intergenerational mobility across levels of schooling. First, I evaluate historical changes in the higher education system and discuss their implications for intergenerational association among college graduates. I also formulate a testable theoretical account of the \"meritocratic power\" of a college degree by drawing on the literatures on educational stratification and labor market inequality. Second, I introduce the variables, data, and analytical strategy. I describe the four measures of economic well-being used in the analysis-social class, occupational status, individual earnings, and total family income-and explain why it is necessary to consider all of them in the study of social mobility. Third, I present the main findings of intergenerational mobility across levels of schooling. I also investigate whether these findings represent change or stability over time and examine educational and labor market mechanisms accounting for variation in mobility across levels of schooling. Finally, I offer the discussion and implications.',\n       \"stantial impact on Alzheimer disease (AD) research. The first selective radioligand developed for this purpose, 11 CPittsburgh compound B (PiB), has played a critical role in this research, contributing to the increasing understanding of disease development and the presence of cortical Ab in the cognitively normal older population. Because the 20-min half-life of 11 C-labeled radioligands such as PiB restricts their use to centers with a cyclotron on-site, the recent development of 18 F-labeled radioligands such as florbetapir promises to facilitate the accessibility of Ab imaging in AD research. The longer half-life of 18 F (110 min) makes it possible to transport these radioligands from production site to PET scanner.\\nValidation studies have established that cortical Ab measured with florbetapir can be detected in cognitively normal subjects, in patients with mild cognitive impairment (MCI), and in patients with Alzheimer disease (AD) (1, 2) in proportions comparable to those reported for PiB (3) . Furthermore, autopsy studies in which individuals were scanned with PiB (4) and florbetapir (5,6) before death have provided additional evidence that both PET ligands bind to fibrillar amyloid in cortex. However, data directly comparing PiB and florbetapir in the same individuals are still limited (7) .\\nThe goal of this study was to compare measurements of cortical retention ratios of PiB and florbetapir in a subset of Alzheimer's Disease Neuroimaging Initiative (ADNI) participants who underwent 2 consecutive PiB imaging sessions followed by 1 florbetapir scanning session at approximately 1-to 2-y intervals. Because the precise quantification of cortical Ab may be influenced by various image processing and analysis methods, we examined the influence of several of these factors. Finally, we examined the feasibility of transforming cutoff thresholds for Ab positivity, with the goal of working toward standardization in Ab imaging quantification.\",\n       'In conclusion, we have shown in this paper that it is possible to efficiently obtain high-quality brain maps by exploiting locally linear discriminative analysis and analytic approximations of permutation tests. We experimentally demonstrated that MIDAS bears important advantages compared to commonly used brain mapping techniques, underlining its potential value in neuroimaging studies.\\nThe second moment is also approximated, up to the first order term, using the delta method: The optimal weight vector in LS-SVM is a product of the aforementioned C matrix, which solely depends on the data samples X and the non-imaging variables y (e.g., clinical diagnosis). Therefore, multiple discriminative model weights, W 2 R dÂr , can be obtained if multiple non-imaging variables, Y 2 R nÂr (e.g., diagnosis, age, sex, etc.), are used for training:\\nAs explored in Haufe et al. (2014) , these models can be adjusted for the underlying noise patterns, as well as the interdependent effects between the non-imaging variables, by left and right multiplying the weight vectors W with the data covariance matrix and the inverse label covariance matrix, respectively. This results in activation patterns A, where the effect captured by each weight vector is independent of the underlying noise and possible imbalances in the non-imaging variable distributions:\\nThe expectation of the multiple activations is still zero, which results in the corresponding MIDAS statistic for the qth non-imaging variable, s q i to also have an expectation of zero: , and H q is the qth column of H.',\n       'Okula başlama yaşının düşürülmesi ile ilgili hükümetler tarafından farklı gerekçeler ileri sürülmektedir. 2008 yılında Polonya\\'da okula başlama yaşının altıya düşürülmesi ile ilgili Polonya hükümeti ve eğitim bakanlığı tarafından ileri sürülen argümanlardan bazıları şu şekilde ifade edilebilir: Bunlardan birincisi, ailelerin daha iyi eğitimli olmaları, daha güçlü sosyalleşmenin olması, çocukların geçmişe göre zihinsel gelişimlerinin ve okula hazır olmalarının daha erken yaşta olmasıdır. Ayrıca gelişmiş ülkelerin çoğunluğunun 6 yaşında okula başlaması da bir etken olarak gösterilmiştir. Özellikle yoksul ailelerin çocukları için okula erken başlamanın pedagojik destek sunması ve akranların etkisiyle olumlu etkisinin olabileceği belirtilmiştir (Herbst & Strawiński, 2016) . Milli Eğitim Bakanlığı\\'nca Türkiye\\'de konu ile ilgili eleştiri ve sorulara cevap olarak (MEB, 2012) \"Bu konuda dünya genelindeki uygulamalarla paralellik sağlanmış, erken yaşta eğitime başlayan bireyin bir yıl erken hayata adım atması sağlanmıştır. Günümüz dünyasında istenilen her türlü teknolojik ve fizikî şartların uygunluğu göz önüne alındığında bireyin okula bir yıl erken başlaması çok önemlidir.\" denilmiştir. Gerek Türkiye gerekse Polonya örneğinde olduğu gibi okula başlama yaşının düşürülmesinde günümüz çocuklarının daha erken yaşta olgunlaşması ile okula daha erken yaşlarda hazır hale geldikleri düşüncesinin etkili olduğunu söylemek mümkündür.\\n4+4+4 eğitim sisteminde en çok tartışılan konulardan birisi muhakkak ki okula erken yaşlarda başlama olmuştur. Bu konuda verilen ilk tepkiler daha çok sendikalar, kurumlar ve sivil toplum kuruluşlarının (AÇEV, 2012; Eğitim İş, 2012; Boğaziçi Üniversitesi, 2012; ODTÜ, 2012; Türk Tabipleri Birliği-TTB, 2012 vb.) doğrudan kurumsal değerlendirmelere dayanmaktadır. Yapılan bu değerlendirmelerde okula başlama yaşının 60 aya düşürülmesinin sakıncalarına ve olumsuzluklarına dikkat çekilmiştir.\\nOkula erken yaşta başlama ile ilgili farklı araştırmalar yapılmıştır. Cerit, Akgün, Yıldız, ve Soysal (2014) gerçekleştirdikleri çalışmada yeni eğitim sisteminin (4+4+4) başlangıcında yaşanan sorunlara ilişkin okul yöneticilerinin ve öğretmenlerin görüşleri doğrultusunda en önemli sorunun okula başlama yaşının küçük olması olduğunu ifade etmişlerdir. Araştırmacılar bu sorunları uyum sorunu, farklı yaş gruplarının aynı ortamda eğitim alması sonucu bireysel farlılıkların artması, programın uygulanmasında yaşanan sıkıntılar, kalem tutma, dinleme-okuma-yazma-anlama problemi, dikkat eksikliği, verilen görevi yapamama, fiziksel yetersizlikler, kendisini ifade edememe, tuvalet ihtiyaçlarını karşılayamama, istenilen kazanımların uygulanmasında yaşanan güçlükler vb. olarak sıralamışlardır. Duran (2013) yaptığı araştırmada 60 aylık öğrencilerin yazma becerisinin gelişiminde sorunların olduğunu ortaya koymuştur. Benzer bir araştırmayı yapan Kapçı, Artar, Avşar, Daşcı ve Çelik (2015)\\'te genel olarak küçük yaşta okula başlayan öğrencilerin, altı yaşında okula başlayanlara göre sosyal, duygusal, bilişsel açıdan eğitim-öğretim ortamında pek çok sorun yaşadıklarına işaret etmektedir. Diğer taraftan Başar (2013) yaşları 60-66 ay olan öğrenciler arasında farklı zamanlarda incelemelerde bulunmuştur. Elde ettikleri sonuçlara göre 60-66 ay aralığında bulunan öğrencilerin dönemin başında okula uyum, tuvaleti kullanma, tuvaletten sonra ellerini yıkayabilme ve elbisesini giyme, kendi başına yiyip içebilme gibi kişisel öz bakım becerilerinde yetersiz oldukları tespit edilmiştir. Yine bu araştırmanın bulgularına göre aynı öğretim yılının sonu itibari ile bu becerileri kazanmada sorunların pek görülmediği ifade edilmiştir. Benzer bir şekilde öğrencilerin ilkokula başladıkları dönemde kalem tutma, satır aralığına yazma, sesleri yazım yönüne göre doğru yazma, tahtayı kullanma becerilerinde sorunlar yaşadıkları tespit edilmiş iken eğitim-öğretim yılı sonunda bu yaş grubu öğrencilerinin okuma ve yazma sorunlarının aşıldığı belirlenmiştir. Öztürk ve Uysal (2013) okula takvim yaşı olarak erken başlayan ile geç başlayan öğrenci gruplarının yaşadıkları sorunları öğretmen görüşlerine dayalı olarak karşılaştırdıkları çalışmalarında, her iki öğrenci grubunun da benzer sorunları yaşadıklarını ancak sorunların çözüm sürecinde ayrıştıklarını ortaya koymuşlardır. Buna göre yaşça büyük olanlar öğrenmeye ilişkin sorunları daha hızlı çözerken, küçük olanlar daha uzun sürede sorunların üstesinden gelebilmişlerdir.\\n1930-1970 yılları arasında okula başlama yaşıyla ilgili olarak farklı araştırma sonuçlarını karşılaştıran Güneş (2013) önceleri çocukların 6 yaşından önce okula başlamasının okuma yazma öğretimi açısından başarısızlıkla sonuçlanabildiği düşünüldüğünü, daha sonraları yapılan özellikle Delogne (1968) nin çalışmasına dayanarak okuma yazma öğretimi için en uygun 5 yaş 5 aylık ile 6 yaş 6 ay arası dönemin olduğuna vurgu yapmaktadır. Güneş (2013) okuma yazma için 7 yaşın çok geç olacağını ve çocukların erken yaşlarda eğitilmesinin gelecekteki başarılarında belirleyici rol oynayacağını ifade etmektedir.\\nOkula başlama yaşının öğrencilerin akademik başarılarına etkisine ilişkin yurt dışı kaynaklarda da farklı sonuçların olduğu görülmektedir. Araştırmalarda (Elder & Lubotsky, 2009; Sakic, Burusic, & Babarovic, 2013; Stipek, 2002) genellikle okula daha büyük yaşta başlayan çocukların okul yaşamında daha iyi performans gösterdiği iddia edilmektedir. Sakic ve arkadaşları (2013) tarafından 4. sınıf öğrencileri arasında yapılan çalışmada okula başlama yaşına göre akademik başarı açısından farklılıklar ortaya konmuştur. Buna göre okula daha büyük yaşta başlayanların, daha küçük yaşta başlayanlara göre akademik başarıları daha yüksek düzeydedir. Araştırmacılar aradaki bu farkın oldukça düşük olduğuna yani okula başlama yaşı ile başarı arasındaki ilişkinin zayıf olduğuna dikkat çekmektedirler. NICHD (2007)\\'e göre okula başlamada ve başarıda temel belirleyici yaş değildir. Hatta Furlong ve Quirk (2011) öğrencilerin okula hazır oluşlarının daha sonraki başarılarında, yaştan daha fazla ilişkili olduğuna vurgu yapmaktadırlar. Shah (1971) okula başlama yaşının matematik başarısını etkilemediğini belirtmektedir. İfade edilen bu sonuçlardan farklı olarak ülkemizde PISA (2009) testinden elde edilen sonuçlara göre ise öğrencilerin okula başlama yaşı ile onların PISA puanları arasında negatif yönlü ve anlamlı bir ilişki bulunmuştur. Buna göre okula başlama yaşı arttıkça öğrencilerin başarıları düşmektedir (Gürsakal, 2012) .\\nYukarıda bazı örnekleri verilen literatürde, okula başlama yaşı ve okul başarısı arasındaki ilişkiyi açıklamaya çalışan gerek yurt içi gerekse yurt dışı çalışmaların farklı sonuçlarının olduğu görülmektedir. Özellikle ülkemiz açısından bakıldığında 4+4+4 eğitim sisteminin üzerinden 4 yıl geçmesine rağmen, öğrencilerin akademik başarılarının okula başlama yaşı ile karşılaştırıldığı çalışmaların sayısı oldukça azdır. Sınırlı sayıda yapılan çalışmalar ise yoğun olarak görüşme sonuçlarına ya da kurumların görüşlerine dayanmaktadır. Bu durum ülkemizde sıklıkla tartışılan okula başlama yaşının öğrencilerin akademik başarıları üzerindeki etkisini açıklamada yetersiz kalmaktadır. Bu nedenle yapılan bu çalışmayla okula başlama yaşı ile öğrencilerin akademik başarıları arasında bir ilişkinin olup olmadığı araştırılmıştır. Araştırma sonuçlarının ülkemizde sıklıkla tartışılan okula başlama yaşının akademik başarıdaki rolünün anlaşılmasına yönelik somut veriler sağlaması beklenmektedir. Çalışma kapsamında öğrencilerin okula başlama yaşı ile akademik başarıları arasında bir ilişkinin olup olmadığını incelemek amaçlanmıştır. Bu kapsamda öğrencilerin ilkokuldaki akademik ortalamaları okula başlama yaşlarına göre ele alınmış ve aşağıda belirtilen alt problemlere cevap aranmıştır: ',\n       \"The Coleman Report (Coleman et. al. 1966) was the first national study to describe ethnic to school with Blacks have baseline test scores upon entering kindergarten that are similar to those who are in all-White classes (Humphreys, 1988, documents a similar finding among high school students). When we eliminate from the sample Whites who have no Black children in their class (more than 60 percent of all White children fall into this category), we obtain similar results. 4 7 differences in academic achievement among children at various stages of schooling. It documented that substantial differences in educational achievement between blacks and whites not only existed at every grade level, but increased with student age. Since then, substantial effort has been devoted to understanding what variables account for the gap, as well as how and why the magnitude of the gap has changed over time.' A number of stylized facts have emerged. Socio-economic status and the effects of poverty are important factors in explaining racial differences in educational achievement (Brooks-Gunn and Duncan 1997, Mayer 1997, Brooks-Gunn et. al (1994, 2000. Even after controlling for socio-economic status in conventional regression analysis, a substantial gap still remains. That gap has generally been declining over time, although for high school students today, the gap is slightly larger than it was in the late 1980's (Grissmer et. al. 1998, Hedges and Nowell 1998, and Humphreys 1998. Finally, the gap in test scores between Blacks and Whites historically emerges before children enter kindergarten and tends to widen over time (Phillips et. al. 1998, Carneiro, Heckman, andManoli 2002).\",\n       'Problem solving is an ongoing important topic for mathematics education because learners can apply formal mathematical knowledge and skills via them (Common Core State Standards Initiative [CCSSI] , 2010; Ministry of National Education [MoNE] , 2013; National Council of Teachers of Mathematics [NCTM], 2000) . Without a problem situation, what does -7 x 42 =?‖ mean? It requires only one operation as a ready-made algorithm. At this point, word problems serve as a context to learn mathematical concepts since they involve verbal descriptions of problem situations with basic mathematical operations (De Corte, Verschaffel, & De Win, 1989; Verschaffel, De Corte, & Greer, 2000) . Further, word problems can help teachers and researchers to evaluate students\\' mathematical skills, to train them to think creatively, to motivate them, and to help them to learn new mathematical concepts (Dewolf, Dooren, Hermens, & Verschaffel, 2015; Wang, Fuchs, & Fuchs, 2016) . Thus, students are often confronted with word problems as a major component of the mathematics curriculum across kindergarten through high school, and many high-stakes standardized tests such as PISA (OECD Program for International Student Assessment) and TIMSS (The Trends in International Mathematics and Science Study).\\nWord problems in mathematics can be classified according to arithmetic, algebra, time, velocity, and geometry (Reed, 1999; Verschaffel et al., 2000; Wong, Hsu, Wu, Lee, & Hsu, 2007) . Among the types of the problems, arithmetic and algebraic word problems have been most extensively studied to understand students\\' procedural and conceptual understanding and difficulties in word problem solving. The results of both national and international studies showed that linguistic (vocabulary, syntactic and semantic factors) and non-linguistic factors (concept format and cultural differences, teaching and learning techniques, attitudes and student perceptions) influence the difficulty of word problems and students\\' understanding (e.g. Björn, Aunola, & Nurmi, 2014; Boonen, Van Wesel, Jolles, & Van Der Schoot, 2014; Erdem, 2016; Gürsoy, Güler, Bülbül, & Güven, 2015; Mellone, Verschaffel, & Dooren, 2014; Varol & Kubanç, 2015; Vilenius-Tuohimoa, Aunola, & Nurmi, 2008) . On the other hand, there is limited number of studies focusing on geometric word problems. However, geometric word problems might be more complicated than other types of word problems due to their complex cognitive nature. Specifically, in order to solve a geometric word problem, learners must be skillful in solving arithmetic word problems and they must have conceptual and procedural understanding of geometric concepts and properties (Mesquita, 1998; Wong et al., 2007) . Consider the geometric problem of -a rectangle has a perimeter of 50. If one of its sides is 5 more than another one, what is the area of the rectangle?‖ In order to solve this geometric word problem, students need to know how to find area of rectangle and to calculate the length of sides of the rectangle. In other words, comprehending geometric word problems require linguistic, geometric, and schematic knowledge (Wong et al., 2007) . At this point, multiple representations have been considered as an important theme to understand students\\' constructions of geometric concepts and problem solving process (Font, Godino & D\\'Amore, 2010; Goldin, 1998; Pape & Tchoshanov, 2001; Tchoshanov, 2002) . Concordantly, many researchers have a consensus on the central role of representations in word problem solving process (Boonen et al., 2014; Dewolf et al., 2015; Cifarelli, 1998) . In particular, some researchers argue that representations have an important role in mathematical problem solving especially for difficult problems or verbal problems as problem solvers commonly externalize verbal expressions in problem situation by using symbols or visual representations (Gagatsis & Shiakalli, 2004; Hitt, 2002; Lesh, Post & Behr, 1987) . Furthermore, some researchers and standards of school mathematics point out the importance of developing students\\' abilities to use appropriate representations and to make correct and robust translations among them in learning and understanding of mathematics (CCSSI, 2010; Hiebert & Carpenter, 1992; Janvier, 1987; MoNE, 2013; NCTM, 2000; Van de Walle, Karp, & Bay-Williams, 2010 ).\\nIn the examination of students\\' representations, it would be found a remedy for students\\' misunderstandings, errors, and difficulties on word problems by obtaining a deep understanding about their comprehension process. However, in case students do not produce suitable correct representations while solving word problems, they can give up solving the problems. At this point, it is necessary to try new approaches that provide opportunities to the students in terms of producing new representations. To achieve this, it is proposed that teachers/researchers may generate a variety of useful questions by presenting an idea in one representational mode and asking the student to illustrate, describe, or represent the same idea in another mode to diagnose a student\\'s learning difficulties (e.g. Gagatsis & Shiakalli, 2004) . At this point, researchers recommend asking students open-ended questions in order to attend their mathematical thinking, after they solve a problem. Thus, these questions can give opportunities to understand how they solve problems and why they propose certain strategies and representations (Franke, Webb, Chan, Ing, Freund, & Battey, 2009) . Additionally, students must become more precise and clearer as long as they describe their thinking, especially providing enough detail and making referents clear (Franke et al., 2009; Nathan & Knuth, 2003; Sfard & Kieran, 2001) . In this regard, clinical interviews have been accepted as a strong qualitative research method by the mathematics educators due to providing -a window into the learner\\'s mind‖ as well as developing an understanding about individual differences among the learners in terms of mathematical thinking and problem solving strategies (Clement, 2000; Ginsburg, 2016; Goldin, 1997; Groth, Bergner, & Burgess, 2016; Heng & Sudarshan, 2013; Koichu & Harel, 2007; Newel & Simon, 1972; Zazkis & Hazzan, 1999; Weiland, Gudson, & Amador, 2014) .\\nVarious clinical interview techniques such as think-aloud interviews (TAIs) and open-ended prompting interviews (OEPIs) are utilized to obtain knowledge about an individual\\'s or a group of students\\' existing and developing mathematical knowledge and problem-solving behaviors (Goldin, 1997; Maher & Sigley, 2014) . In TAIs, learners continually speak aloud what they think in their mind. In this process, interviewer makes limited prompting such as -tell me what you were thinking or keep talking‖ to facilitate interviewee\\'s verbalization of their thoughts in a laboratory settings if they remain silent for an extended of a time (Ericsson & Simon, 1993; Willis, 2005) . As an alternative to the think aloud, in OEPIs, interviewer asks some prompting questions to the interviewee to obtain deeper understanding about his/her thinking. Educational researchers have different perspectives on which type of clinical interviews can be the best to understand students\\' thinking. For instance, some researchers advocated the purity of implementation of \"think-aloud techniques‖ because they emphasize that in case an interviewer ask a question to the learner, s/he often conceives the question as a request to revise his or her previous answer and to change it a new one (Aronsson & Hundeide, 2002; Ericsson & Simon, 1993; van Someren, Barnard, & Sandberg, 1994) . On the contrary, many practitioners of cognitive interviewing emphasize the necessity of examining learners\\' thinking processes by -prompting questions‖ rather than focusing -pure‖ think-aloud (DeMaio & Rothgeb, 1996; Willis, 1994) . In related literature, although there are many studies conducted by using think-aloud protocols to understand students\\' word problem solving processes (e.g. Montague & Applegate, 1993) there a few studies to examine students\\' performance by comparing different interviewing techniques. For instance, Bannert and Mengelkamp (2008) conducted an experimental study to understand the influence of verbalization method on students\\' learning and metacognitive abilities through both clinical interviewing and prompting interviewing methods. They reached that prompting to the students for metacognitive reflection should affect their learning performance positively compared with the performance of the control group and the thinking-aloud group. In sum, there is no study that specifically focuses on the changes/developments in learners\\' representations when solving word problems through different clinical interviewing process. Instead, studies generally identify learners\\' difficulties that they encountered in word problem solving process such as linguistic (vocabulary, syntactic and semantic structures of statements in problems) and non-linguistic factors (concept format and cultural differences, teaching and learning techniques, attitudes and student perceptions). Furthermore, there are a few studies about geometric word problems (e.g. Wong et al., 2007) . This situation indicates the presence of a gap in the literature in terms of understanding students\\' representations and problem solving process in geometric word problems. In the current study, rather than only identifying learners\\' difficulties and their reasons related to word problems, we aimed to investigate how secondary school students\\' produced and changed their representations for solving word problems in two different clinical interviewing processes. Therefore, we think that examination of the nature and changes in students\\' representations for solving geometric word problems in different interviewing processes would contribute to the literature in terms of methodological and theoretical aspects as a lens of understanding students\\' mathematical thinking from an alternative point of view. Considering the purpose, we answered the following research questions:',\n       \"As shown in [2.43-3 .98]). Younger age and female sex predicted progression to diabetes from both stages. In all models, each kilogram per squared meter of BMI increased risk of progression by 3-4%, and HDL cholesterol was also a strong predictor. Higher systolic blood pressure and higher triglycerides were significant predictors of hyperglycemic progression in all models.\\nCONCLUSIONS -In this retrospective cohort study of real-world patients with incident IFG, we found that 8.1% who met the added portion of the ADA's 2003 criterion for IFG (100 -109 mg/dl) progressed to diabetes over a mean follow-up of 6.3 years, an annual rate of 1.34%. Among subjects with incident IFG under the old ADA definition (110 -125 mg/dl), we observed an annual rate of progression to diabetes of 5.56%. This \",\n       'Unravelling the mechanism of SARS-CoV-2 entry into semen and the testes would help us assess the early impact on male reproductive function. Equally, the impact of SARS-CoV-2 related stress and fever manifesting in depressed semen quality can not be underestimated and also possible intergenerational effects (Chan et al., 2020) . The limited and poorly reported small studies in seminal plasma and the testes lack adequate controls to establish evidence-based guidelines for the public nor are they able to produce guidance on sexual practices or reproduction, given the lack of social distancing appears a major risk. The main weaknesses of reports relate to poorly validated diagnostic procedures, lack of controls, mildly infected males tested several weeks later along with the fact that vital respiratory tissue most likely to harbour the viral RNA has never been reported alongside as well as the lack of control analyses.',\n       nan,\n       'The distance from the mainland decreased for all reef types over time. Over the entire study period, sand-oyster reefs were generally further ',\n       \"A typical record used in a traditional harmonic analysis would have a water-level sample at least every hour. Thus, there would be ~7200 observations for a 10-month record, which is the time span of the data from the Princess of Acadia GPS project that was used in this research (7 December 2003to 25 September 2004. Because it takes the Princess of Acadia ferry 3 hours to transit between Saint John, NB and Digby, NS and the ferry makes 1 to 3 round trip crossings a day, the period of time between crossings of a VTGZ not only exceeds the sample interval for a traditional water-level record but it varies from crossing-to-crossing. However, the ferry's route is along a co-amplitude line with similar tidal characteristics at each end. Thus, it is safe to assume that the tidal characteristics along the ferry's route are similar and a priori knowledge of the tidal constituents contributing to the tidal signals at Saint John, NB and Digby, NS are used as a priori knowledge to model the tides in each of the VTGZs. The non-uniform sampling interval of the water-level record in each VTGZ allows for tidal frequencies higher than the Nyquist frequency to be resolved (Scargle, 1982). In the 2008 specifications and deliverables for water-level stations the CO-OPS recommends that water-level measurements should be computed from an average of at least 180 one-second water-level measurements (CO-OPS, 2008). Because the ferry follows different paths at different speeds through each of the VTGZs each water-level estimate {h wl ) in each VTGZ is computed from a different number of h m ; x solutions. The water-level estimates in VTGZs 2 through 61 were computed differently than the water-level estimates in VTGZ 1 and 62. This is done because VTGZ 1 and 62 encompass the ferry terminals and to avoid averaging over long time spans (the ferry could be moored for anywhere between 1 to 24 hours depending on the ferry schedule), the averaging period in VTGZ 1 and 62 is held constant at 181 sec. Because the amount of time the ferry spent in a VTGZ varied from crossing-to-crossing minimum and maximum time constraints were established to restrict which crossings are used to characterize the height of the water-  and VTGZ 35 (Figure 36-d). These two distributions show that the ferry usually crosses these zones at an average speed of 7.5 m/s, however there is a group of crossings of each VTGZ at 9.8 m/s. The two different peaks in the distributions of the average crossing speed is a result of the ferry using 2 engines under most conditions, but when the ferry is behind schedule two more engines are engaged, increasing the maximum cruising speed. The time when the water-level was sampled in each VTGZ was controlled by the ferry crossing schedule. Consequently, the time series used in the harmonic analyses were not equally spaced. Figure 37 shows the distribution of sample intervals for the time series of water-level estimates in VTGZs 2 through 61. The average sample interval between water-level estimates (ferry crossings) in each VTGZ was 7.26 hrs +/-9.5 hrs (1 a). There are some samples that are separated by more than 4.5 days as a result of the ferry traveling outside the VTGZs (Figure 31). Sixty-five percent of the samples are separated by a period of time smaller than the Nyquist equal sampling interval for the M2 tide. The other 35% exceed the Nyquist sampling interval for M2. \",\n       'Most of the studies [12] , [67] , [68] , [97] , [101] , [125] , [137] have been using quantitative MRI measures within the area of MTL to determine if a subject will develop the disease, and the results agree that entorhinal cortex is a better predictor compared to other structures such as the hippocampus. The best discrimination accuracy between normal and patients with memory difficulty, who eventually developed the disease, was achieved by two volume studies of deToledo-Morrell et al. [97] and Killiany et al. [95] with an overall predictive accuracy of 93.5% and 93%, respectively.\\nIn the prediction of progression of the disease, the highest accuracies were achieved when both entorhinal cortex and hippocampus were combined in the analysis. VBM methods and cortical thickness gave lower accuracy compared to the other methods, and there is \"lack of research\" regarding the use of texture analysis in the prediction of progression from MCI to AD.',\n       'Effects of attitudes on team effectiveness can manifest in one of two ways. Campion defined direct effect as \"preferences towards teamwork\" that reflect desire of team members to be in team environment (Campion, Medsker, & Higgs, 1993). Irrespective of team presence in organizations, not everyone likes to work in teams and certain employees prefer individual work. Teamwork may bring frustrations due to social loafing, conflicts and other problems. Campion et al., (1993) concluded that teams are less effective in general, looking at several criteria, when members have low preferences towards teamwork (Campion, Medsker, & Higgs, 1993). Other authors came to the same conclusion that preferences towards teamwork increase effectiveness (Bell, 2007;Jung & Sosik, 1999;Watson, Johnson, & Merritt, 1998), quality of teamwork (Eby & Dobbins, 1997;Harris & Barnes-Farrell, 1997) and team sustainability (Harris & Barnes-Farrell, 1997). Second way of influence attitudes have on effectiveness is through similarities in them. Taking into consideration the findings of social psychologists that people like people similar to themselves (Byrne, Clore, & Smeaton, 1986), teams function better when there is moderate similarity of attitudes on significant matters like team leader effectiveness or organizational support (Bliese & Britt, 2001). Despite potential value of similarities, there are considerable dangers in situations where team members are much alike. When team members agree on every issue, necessary debate in work activities lacks and team members become resilient to changes. Janis (1982) defined such dynamics as group thinking and described it as \"model of thinking individuals adopt in cohesive groups in situation when team unity has prevalence over realistic consideration of alternative ways of action... Group thinking is related to deterioration of mental efficiency, reality checks and moral judgment as a result of group pressure\" (Leana, 1985).',\n       'Quercetin is a polyphenol derived from fruit (apples) and vegetables and contributes to cardiovascular health. The effects of dietary quercetin on endothelial function and atherosclerosis were studied in mice fed an HFD. Quercetin protected HFD-fed mice against oxidant-induced endothelial dysfunction and atherosclerosis. Indeed, quercetin activates genes involved in mitochondrial biogenesis and oxidative metabolism in HFD-fed obese mice [130] . The mode of action was found to be via hemeoxygenase-1 (HO-1). HO-1 is an antioxidant enzyme that protects against oxidative stress, inflammation, and metabolic dysregulation. Indeed, quercetin stimulates hepatic mitochondrial oxidative metabolism through HO-1 induction via the Nrf2 pathway [130] .',\n       'At the start of the South African (SA) pandemic in March 2020, the radiological literature was dominated by descriptions of the chest CT findings in patients with COVID-19 pneumonia. It was reported that CT was the preferred imaging examination, with greater sensitivity and specificity than CXR (33, 34) and since the sensitivity of the nasal swab test for PCR was low, a chest CT examination could serve as a substitute in screening patients for COVID-19 infection (35). However, these reports originated in I n P r e s s settings with an adequate number of CT scanners, usually between 6 and 11 CT scanners per million people (36) . By comparison, countries in sub-Saharan Africa have only 0.08 -1.7 CT scanners per million people in the public healthcare sector (37, 38, 39, 40) . Thus, the greatest initial challenge of the pandemic was managing the expectations of referring physicians that CT was needed for the early diagnosis and management of COVID-19 patients. The situation was addressed by the Radiological Society of South Africa (RSSA) which endorsed the ACR recommendations for patients suspected of COVID-19 infection, limiting the use of CT in the screening or first-line diagnosis setting (41) . PCR was relied upon for diagnosis of infection.',\n       'The results in the previous section show quite significant differences between what maximum water level predicted by eSURF and that observed in the field. The question is: are these errors mainly caused by the data that is being used for the validation or are these caused by the way eSURF interpolates and predicts the water levels? With this in mind the following can be said: 1) It seems unlikely that vertical datum differences between observed and predicted water levels are the main factor contributing to large errors. Measured water levels are relatively low compared to the peaks in errors occurred at some eSURF prediction points. Differences vertical datum cannot explain those high observed errors. 2) Also stations that had missing water level data during hurricane landfall were excluded from the validation. Only stations that captured a peak water level during the full course of a hurricane are included. Therefore missing data (or missing peak water level data) cannot be used as an explanation for errors. 3) Only 2.4 % of the eSURF prediction points have been used in the final validation. This is mainly caused by the limit number of observation stations that performed measurements during a specific hurricane.',\n       'Indicates the relationship of the first parent to the sample member; that is, the parent to whom all \"parent 1\" variables (e.g., X2P1RELATION, X2PAR1EMP, P2YRBORN1, P2USYR1) refer. X2P1RELATION is pulled from the first follow-up parent questionnaire, and if missing it is imputed from the base-year parent questionnaire and the first follow-up student questionnaire. X2P1RELATION is statistically imputed for first follow-up student sample members when all sources of parent data are missing (imputed values in X2P1RELATION can be identified using X2P1RELAT_IM = 2).',\n       'Routine sequence management and analysis was carried out using DNAStar. The sequence alignment of complete genome sequences was performed using MAFFT (v.7.307) with default parameters. The codon alignments of full-length S and RdRp gene sequences were converted from the corresponding protein alignments by PAL2NAL (v.14); the protein alignments were created by Clustal Omega (v.1.2.4) using default parameters. Maximum likelihood phylogenetic trees were generated using RAxML (v.0.9.0) with GTR+G substitution model and 1,000 bootstrap replicates.',\n       \"During and after the period of Matrix II evaluation, an independent search was conducted through the scientific literature for basic habitat requirements of the assemblage species. Particular attention was given to those that had received no input or had received ranics that indicated high importance but with no substantiation. This search was conducted primarily through a computerized query (using the Compact Cambridge retrieval system) of the Aquatic Sciences and Fisheries Abstracts (ASFA) and Life Sciences Collection (LSC) installed on compact disk and available in the University of Washington library system. Retrievals were made using the scientific binomial and common names of the assemblage species. The literature citations were printed as a bibliography and the titles and abstracts were scanned to select those citations specific to estuarine habitats. Citations that were relevant were then examined in toto to extract any information that identified attributes of estuarine habitats as important determinants of the assemblage species' use of that habitat. Those selected citations (Supplement 3, Simenstad et al. 1990) were added to the Mathx II database. ATTRIBUTE DATA QUESTIONNAIRE All attributes receiving moderate or high ranks in the Matrix II survey were chosen as the basis of the Wetland Attribute Assessment Questionnaire. Together with the information gathered from the parallel literature search (above), these attributes formed the criteria upon which the Protocol was subsequently structured. The questionnaire was sent to more than 200 habitat specialists; the mailing list was compiled using predominantly the Resource Guide to Wetland Scientists of the Pacific Northwest (Washington Department of Ecology 1988), and was augmented by a review of the contemporary literature on estuarine ecology in the Pacific Northwest and by our own familiarity with the researchers working in the region. It requested documented information about these attributes, and it also solicited information on (1) procedures and sampling designs the investigators had used to measure theattributes, (2) descriptions and comments on idealized sampling designs, (3) expected values, (4) data formats, and (5) published sources of these data. The survey was conducted from the perspective of natural estuarine habitats common to the Pacific Northwest, and especially to the sub-estuaries of Puget Sound that were the primary focus of the UEMWG discussions. Despite the fact that the estuarine habitat mitigation and restoration being considered by this group was occurring predominantly in disturbed, highly modified habi tats, the UEMWG determined that the functional relationships identified in the protocol develop ment process should represent the optimum function of estuarine habitats. While it was recognized that habitat function in an urbanized estuary might be highly contingent upon external factors such as water quality or fish and wildlife population levels, the UEMWG considered the maximum potential function of a created or restored habitat to be the primary objective. This was one of the principal reasons for basing the Protocol on characteristics of the habitat that would enhance fish and wildlife utilization, rather than on an assessment of fish and wildlife occurrence and abundance per Se. The results of the questionnaire are included as Supplement 4 in Simenstad et al. (1990).\",\n       'Despite their relatively brief duration, westerly wind events have been shown to be an important factor in the onset of El Niño surface warming over the equatorial Pacific cold tongue region. While corresponding events of easterly wind are not as obvious in the wind time series, we have shown here that counterparts to the WWEs exist as surges in the easterly wind stress field; we call these easterly wind surges (EWSs). We have introduced an objective definition for EWSs that identifies 340 events in the 1986-2012 period. On average, then, there is roughly 1 EWS per month. The average duration, amplitude (in wind stress anomaly), and zonal extent of EWSs are roughly similar to those of WWEs.\\nA key motivator of this work was to find out if EWSs also play a role in the onset of cold tongue cooling in La Niña events comparable to the role of WWEs in the onset of warming in El Niño events. We find that statistically significant decreases in SSTA follow the EWS events that occur in ENSO-neutral conditions during the months of the year associated with La Niña onset and growth. This is based on composites of the ;150 events found at these times in the 1986-2012 period, which show that cooling amplitudes up to 0.48C persist in the equatorial cold tongue region for 2-3 months after the EWSs subside. This amount of cooling is comparable in magnitude and zonal extent to the warming that similar studies indicate follows a typical WWE.\\nThe application of (a single) EWS composite wind stress anomaly to an ocean model produces cooling anomalies that resemble the cooling patterns seen in the observations following a single EWS event. Notwithstanding some model deficiencies seen along the far eastern Pacific coast, the cooling amplitudes in the model following the application of a single EWS are at least as large as those in the observations. This holds even if the set of EWSs considered is restricted to just those that occur just in warm-neutral ENSO conditions: that is, before La Niña-type SSTA cooling is evident (i.e., 08C , Niño-3 , 0.758C). This strongly suggests EWSs provide a mechanism for La Niña onset.\\nWe find that distribution of EWSs varies with ENSO state (e.g., Niño-3 or Niño-3.4) such that their average frequency over conditions cooler than 218C (;2 per month) is roughly double the full study period average. This suggests to us that much of the strengthening of the easterly trade winds during La Niña results from EWSs becoming more frequent as ENSO SSTA cools. On the other hand, if this strengthening of the easterly trades is best thought of as a fundamentally lower-frequency (interannual) process, then our methods of identifying EWS in La Niña would be biased high. We suggest that the hypothesis that the La Niña strengthening of the trades is accomplished episodically is one that deserves more attention than it has received up to this point.\\nModel experiments show that the application of a series of EWSs that includes the observed ramp up in EWS frequency is sufficient to drive a La Niña-type SSTA pattern in the model (e.g., end of calendar year Niño-3.4 SSTA , 218C, lasting for 3 months). In this scenario, starting from near-neutral, springtime conditions, the occurrence of an EWS typically drives waveguide cooling that, in turn, makes EWSs more likely to occur (and WWEs less likely to occur; VH00; Gebbie and Tziperman 2009a) . The coupled Gill (1980) mechanism is then carried out in this phase of La Niña by EWSs.\\nEven without the ramp in frequency, however, a series of ;1 per month EWSs is sufficient to drive the model about halfway to a La Niña-type anomaly state (e.g., Niño-3.4 SSTA in the range from 20.58 to 20.758C). This strongly suggests that EWS events can make an important contribution to both the initial onset and subsequent growth of La Niña events.\\nThese results provide an initial step in understanding the role that EWSs play in ENSO, but much remains to be learned. One obvious question motivated by these results is whether indices tracking early ENSO stage EWS activity can be found that add to the existing strategies for bridging the springtime prediction barrier (cf. Clarke and Van Gorder 2003; Kug et al. 2010) . Also, the role that EWSs play as La Niña events mature and decay (e.g., WWEs have been shown by VH00 to maintain waveguide warmth but not necessarily add to it in warm ENSO conditions) or occur in other anomaly states, perhaps in close succession to WWEs, or even at the mature stages of El Niño, is deserving of attention. Although, at first glance, EWSs and WWEs appear to have similar (though oppositely signed) wind stress characteristics, whether or not there are asymmetries in their distribution or frequency that contribute to asymmetry in the longer-term average zonal wind stress characteristics associated with El Niño and La Niña (e.g., Ohba and Ueda 2009; Okumura et al. 2011; Dommenget et al. 2013 ) may also be deserving of further attention. Also, as discussed above, the reasons for the observed tendency for EWSs to increase in frequency as the tropical Pacific moves from moderately warm to cool anomaly states is deserving of future study. We suggest that improving our understanding of the sources and predictability of EWSs offers a means to improve our understanding of these aspects of La Niña onset.\\nAcknowledgments. This manuscript benefitted from the considerable proof reading skills of S. Bigley. The authors thank M. Lengaigne and an anonymous reviewer for their thoughtful reviews and helpful comments, as well as M. Cronin for help obtaining (and efforts to compute make publicly available) the wind stress anomalies based on the TAO/TRITON buoys. This publication is (partially) funded by the Joint Institute for the Study of the Atmosphere and Ocean (JISAO) under NOAA Cooperative Agreement NA10OAR4320148 and funded by the Climate Observations and Monitoring Program, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. Support was also provided by the NOAA/Pacific Marine Environmental Laboratory (PMEL).',\n       nan, 'Hiring procedures for benefits eligible employees; and',\n       \"Dorothy: While I was at ONR, I was nominated for the Federal Women's Award. At that time, it was given to the five top women in government each year, and I received it. I went to a very elegant dinner in honor of the awardees, and the Assistant Secretary for R&D (research and development) was my escort. He was a very famous man and is still around, but I cannot remember his name. That was a very nice honor. A group of recipients of the award continue to meet twice a year. After ONR, I went to the Office of Education and ran the National Center for Education Statistics. I did a lot of new things there. I started a series of longitudinal studies, which are continuing to this day. I started a fast response survey, so we could get data on policy issues on a timely basis. I did a lot of federal and state relations work because it was extremely important to get the good will of the state people so they would respond to our surveys. It was really hard work. (I remember a man from Texas saying he was sent to meetings to kill as many surveys as possible.) I also went to a lot of meetings of the chief state school officers, because I thought I could be more effective working at that level than working with the people who were responding to the surveys. That was a lot of work because many of the chiefs were political appointees, so every year I had about a dozen new ones to convince of the importance of the surveys. I enjoyed NCES very much. That was a very demanding job, actually. It was hard for my husband, because I traveled so much.\\nEd: What was your next move after the National Center?\\nDorothy: I got caught in some nasty politics at the National Center for Education Statistics. In any event, I decided I would resign. I was eligible to retire from government service, so I retired.\\nEd: That would have been roughly when? Dorothy: That was 1974. It was really nasty politics. It involved enmity between people in the Office of the Commissioner and the Department, and I was caught in the middle. I went to work at the National Academy of Sciences. I knew about the job at the Academy in an odd way. I was at one point the Chair of the Conference Board of Mathematical Sciences. At that time, Truman Botts was the secretary for the group, and he had an office in the Academy building. I went to see him occasionally before meetings of the Board.\\nWendy: Wasn't this called the Board of Mathematical Sciences (BOMS)? Dorothy: No, this was not an Academy entity. This was the organization of all of the mathematics societies. I served on that for the ASA (American Statistical Association) for a while, and then I became an officer. Then I became the Chair. I was the Chair for two years. That's when I interacted with Truman Botts and learned about the job at the Academy. I thought this was a likely place for me to go. I had administered contracts for the National Center for Education Statistics, so many avenues were closed to me. This included Westat, which is a place I would like to have gone, but I had just signed a big contract with Westat. I did, however, get a job at the National Academy as Director of their Human Resources Studies. I worked on that for a while. Later I moved to the Committee on National Statistics and did several studies for the Committee.\\nEd: So, you were a staff person for CNSTAT? Dorothy: Yes, they are under the Commission of Behavioral and Social Sciences.\\nEd: The other is the Board of Mathematical Sciences and Applications. When it was first set up it was BOMS. Dorothy: I was always interested in statistics and education. I read a report of a state governors' meeting. They expressed concern about mathematics and science education in this country. They cited studies done by the International Association for Education (IEA). I had seen those studies. I was upset, because they were very flawed studies. The IEA hadn't done anything about nonresponse. They hadn't done anything to ensure the tests that were given to the kids were administered in an equal manner across countries. They hadn't done anything to ascertain whether the questions on the test fairly represented the curricula in the countries, and so forth. It concerned me that the governors were making national policy based on flawed data. I talked to people in the Department of Education and people in the National Science Foundation and came up with the idea of having a Board on International Comparative Studies in Education, which would monitor those studies. The Board would also see to it that future studies were of better quality. The funding was forthcoming, and I ran that Board for around 12 years.\\nEd: Was that based in the Connecticut Avenue office?\\nDorothy: No, we were still in Georgetown. The Academy convened a large meeting a few months ago for the dissolution of the Board that I founded. It really wasn't needed anymore, since the Board had done its work. The IEA now has high standards for their studies. Also, the management of the U.S. part was moved to Boston University, where Al Beaton and some other very competent people are involved. The Board didn't need to monitor study quality any longer.\\nWendy: So, you saw a need, did something about it, and saw it come to fruition? Dorothy: Yes, we monitored the big study called TIMSS from the beginning to the end. That is the Trends in International Mathematics and Science Study. It is widely quoted now. At least one can have confidence in the data.\\nEd: So you went to the National Academy in 1975. And you were there 19 years. Dorothy: Yes, I decided to retire to spend more time with my husband. He had some health problems and I wanted to have time with him. I'm really glad I did. I did some consulting for a while, and I did some writing.\\nWendy: I suppose your husband has been very supportive of you-with all of the traveling? Dorothy: He certainly has. Leon is a very patient man, in that regard.\\nWendy: Well, we've taken up enough of your time. Thank you very much for describing your long and fruitful career in statistics.\\nEd: Yes, thank you. You can be very proud of your many accomplishments.\",\n       'Though the general dearth of policies regarding parenting students has been fairly established, the literature indicates that the trend is shifting. In separate studies, Mason (2004) and Kuperberg (2009) studied the rate at which schools implemented maternity related policies. In her 2007 survey of 62 top-ranked graduate schools, Mason (2004) found that 26 percent offered maternity leave to students; 10 percent offered paid maternity leave; 50 percent offered childcare subsidies; and 10 percent offered programs for emergency backup care. In a five-year follow-up study, Kuperberg (2009) found that in most areas, the accommodations top tier research universities are extending to parenting students have grown: Table 1 Comparison of Mason (2004) and Kuperberg\\'s (2009)  Gender, motherhood, and cumulative disadvantage. The identity, role, work-family conflicts, and organizational barriers PhD mothers face influence their ability to accumulate necessary career-related resources in graduate school and to compete in the ALM for tenuretrack jobs. Because of the various challenges PhD mothers face, they tend to be at a cumulative disadvantage in graduate school and in their early careers, leading to their underrepresentation in the primary labor market of the ALM (e.g., Morrison et al., 2011;Wolfinger et al., 2008). If PhD mothers struggle to obtain important career-related resources in graduate school, they may not have the ability to develop an advantage that will compound over time, and ultimately, they may be less able to compete for jobs in the ALM (Merton, 1973;DiPrete & Eirich, 2006;Kennelly & Spalter-Roth, 2006). PhD mothers who are unable to find a tenure-track position early on may face an increased disadvantage when it comes to penetrating the barrier to the primary academic market, resulting in a similar type of \"second-class\" stigma as contingent faculty (e.g., Kezar & Sam, 2010). Indeed, these limitations may directly lead PhD mothers to become disproportionately concentrated in the secondary labor market in the ALM, though their movements in and among the primary and secondary labor markets has been largely unexplored (Hudson, 2007;Rhoades, 2013). The effects of PhD mothers being at a cumulative disadvantage may be observed in their \"leaking out\" of the academic pipeline, or the pathway to the professoriate (e.g., Biggs, et al., 2014;van Anders, 2004;Wolfinger et al., 2008). This \"leaking out\" occurs when PhD mothers (and in some cases, women in general) become effectively filtered out of the academic pipeline (e.g., Biggs, et al., 2014;van Anders, 2004;Wolfinger et al., 2008). The literature on the academic pipeline suggests that mothers specifically exit the pipeline at three distinct phases: during graduate school, after obtaining a PhD but prior to seeking a tenure-track position, and after obtaining a tenure-track position but prior to earning tenure (Morrison et al., 2011;Wolfinger et al., 2008). Women exit the pipeline at all three stages in larger numbers than men, and parenthood is related to their exiting (Mason, 2004(Mason, , 2013. At the first point in the pipeline, women are more likely to exit during graduate school or prior to seeking a tenure-track position in part because there are systemic barriers associated with parenting that discourage women from pursuing academic careers (van Anders, 2004). Women experience greater challenges with mobility, which may be required upon entering the professoriate (Wolf-Wendel, Twombly, & Rice, 2004). Women in STEM fields in particular may perceive incompatibilities between the \"greedy\" cultures of science and motherhood (Etzkowitz, Kemelgor, & Uzzi, 2000;Kelly & Grant, 2012;Rosser, 2004;Grant, Kennelly & Ward, 2000). Women in science tend to strongly associate with their professional fields and thus may consider business and industry to accommodate their family lives better than academe (Monosson, 2008). Though men tend to be just as interested in having children as women, women seem to selfselect away from academia in response to perceived gendered barriers related to parenthood (Ginther & Kahn, 2006b;van Anders, 2004). At the second point in the pipeline, women tend to exit before obtaining tenure track positions in greater numbers than men (Wolfinger et al., 2008). Women face persistent gender equity issues in the academy at all levels, especially because disciplines like science, technology, and engineering fields have low proportions of female PhD graduates and faculty members. Even though there are much higher rates of female PhD\\'s graduating from areas like the humanities, psychology, and the fine arts, there are still proportionally lower numbers of women faculty members in those fields (Mason, 2004). Women obtain fewer tenure track positions than men in part because of the direct effects of motherhood and timing on the female career path (Miller, 2009). In her study of more than 1,400 academic and non-academic women, Miller (2009) found that delaying motherhood leads to a substantial increase in career earnings. Women earn 9 percent more income per year of motherhood delay, as well as a 3 percent increase in wages, and a 6 percent increase in work hours (Miller, 2009). College-educated women gain the largest advantages with motherhood delay. Mothers experience fixed wage penalties and lower income returns as compared to nonmothers, which suggests a \"mommy track\" effect, where earlier family formation accounts for the lower rate at which women obtain tenure-track jobs (Miller, 2009;Wolfinger et al., 2008). At the third point in the pipeline, women who have tenure-track positions exit prior to obtaining tenure at higher rates than single women or men do (Mason, 2004). An estimated 53 percent of women leave tenure-track positions before achieving tenure (Marschke, Laursen, McCarl Nielsen, & Rankin, 2007). Despite the \"mommy track\" observed early on in women\\'s academic careers, neither family formation nor marriage accounts for women\\'s difficulties in obtaining tenure and promotion (Wolfinger et al., 2008). Women do tend to carry heavier service and teaching loads, which may be related to their leaving tenure-track jobs prior to tenure at higher rates than men (Misra et al., 2011). Regardless of whether they were married or had children, women are more likely than men to leave tenure-track positions prior to obtaining tenure (Mason, 2004). Women with children also tend to faces challenges in promotion up the academic ladder, and advancement into senior faculty ranks or administrative positions continues to be a challenge (DiFuccia, Pelton, & Sica, 2007;Kulis, Sicotte & Collins, 2002;Ginther & Kahn, 2004). Finally, women on the tenure-track also experience negative familial outcomes, such as lower rates of marriage and family formation, and higher rates of divorce. As Mason (2004) describes it, \"the gap in marriage, family formation, children, and divorce is as large as the occupational gender gap\" (p. 86). Despite its prominence in the literature, it is important to note that the academic pipeline metaphor has received recent criticism, in part because of its inability to fully explain the career paths of women in the professoriate (e.g., Cannady, Greenwald, & Harris, 2014;Pawley & Hoegh, 2011). Researchers have criticized the academic pipeline as being too linear and too restrictive to accurately describe the path of women and other underrepresented groups, like ethnic minority groups, to the professoriate. The conceptualization of the pipeline as \"a career trajectory with one inlet, one outlet, and one direction of flow\" fails to accurately capture the experiences of academic mothers, who often pursue part-time or temporary positions immediately after graduate school, yet sometimes go on to become full-time, tenured professors later on (Cannady et al., 2014, p. 456). Another shortcoming of the pipeline metaphor is that as a supply side model, it focuses mostly on those who fail to meet the benchmark criteria of the academic gatekeepers at each juncture of the pipeline, or those who \"leak out,\" without adequate attention to those who join the professoriate through different paths. The pipeline does little to illuminate those who do not take the traditional path from graduate school directly to the professoriate, and thus is conceptually ill-fitting for some groups. Researchers have suggested other less linear concepts to describe the paths of women and ethnic minority students to the professoriate, such as \"ecosystem,\" \"lattice,\" or \"labyrinth,\" which offer movement in a general direction with points of divergence along the way (Cannady et al., 2014;Pawley & Hoegh, 2011). This study includes analyses of employment patterns that represent the entry, exit, and reentry movements of PhD mothers into and out of the ALM within the first eight to thirteen years post-PhD graduation. The employment pattern analyses may reveal whether PhD mothers tend to \"leak out\" of the academic pipeline in a linear fashion, or whether PhD mothers\\' employment patterns are better described in more of a non-linear or lattice-like fashion.',\n       'We have also developed a highly sensitive Simoa method for the axonal protein neurofilament light (NFL) protein [127] . This assay has many-fold higher analytical sensitivity than assays using the same anti-NFL antibodies based on the electrochemiluminescence (ECL) Meso Scale Diagnostics (MSD) technique or standard ELISA [128] , meaning that NFL can be measured also in blood samples from normal individuals who have plasma NFL concentrations that are below the level for accurate quantification when using ECL-MSD or ELISA. In contrast to tau protein, the correlation between plasma and CSF levels of NFL protein is tight [127] .\\nA recent study on the ADNI cohort showed a marked increase in plasma NFL in AD cases (149% of control levels), with a ROC AUC value of 0.87, which is comparable to the core AD CSF biomarkers [129] . While the change in the MCI group was less pronounced, plasma NFL was highest MCI cases with positive amyloid PET scans, and predicted faster cognitive deterioration, higher rate of future both brain atrophy (measured by MRI) and hypometabolism as measured by FDG-PET [129] . Importantly, in a study on 48 familial AD mutation carriers and non-carriers, blood NFL was increased in symptomatic familial FAD cases, but also in pre-symptomatic mutation carriers, with levels correlating with expected estimated year of symptom onset as well as both cognitive and MRI measures of disease stage [130] . These results indicate that blood NFL detects neurodegeneration also in the preclinical stage of AD.\\nIn this context, an important piece of knowledge is that high plasma (or CSF) NFL is not a feature that is specific for AD. Instead, increased levels are found in many neurodegenerative disorders, such as frontotemporal dementia, progressive supranuclear palsy and corticobasal syndrome [131, 132] . Thus, a possible future application for plasma NFL is as a screening test at the first clinical evaluation of patients with cognitive disturbances, e.g., at the primary care unit. Here, plasma NFL might serve as simple, non-invasive, and cheap screening tool, primarily to rule out neurodegeneration.',\n       nan,\n       'The Conservation Reserve Program (CRP) is a voluntary set-aside program in the United States designed to ameliorate soil erosion, control crop overproduction, enhance water quality, and provide wildlife habitat by replacing crops with other forms of land cover. Because CRP includes primarily grass habitats, it has great potential to benefit declining North American grassland bird populations. We looked at the change in national and state population trends of grassland birds and related changes to cover-specific CRP variables (previous research grouped all CRP practices). Changes in national trends after the initiation of the CRP were inconclusive, but we observed signficant bird-CRP relations at the state level. Most bird-CRP relations were positive, except for some species associated with habitats that CRP replaced. Practice-and configuration-specific CRP variables were related to grassland bird trends, rather than a generic measure of all CRP types combined. Considering all CRP land as a single, distinct habitat type may obscure actual relations between birds and set-aside characteristics. Understanding and predicting the effects of set-aside programs (like CRP or agri-environment schemes) on grassland birds is complex and difficult. Because available broad-scale datasets are less than adequate, studies should be conducted at a variety of spatial and temporal scales.',\n       'Across all model specifications, irrigation has a negative effect on the share of fallowed cropland. This is understandable because irrigation obviates the need for water management through fallow. Years of farming experience and farming occupation are found to have positive effects on the share of fallow but the effect of farming occupation is statistically insignificant. These results show that experienced farmers and full-time farmers see the need to better manage soil moisture.\\nPercentage of cropland under CRP and WRP programs has a significant negative effect on the share of fallow, reflecting that wheat farms are more likely to enroll less productive cropland and thus reduce the amount of cropland in fallow. Soil variables, including sand and clay contents and soil erodibility factors, affect the share of fallow negatively due to differences in the soil water retaining and restoration capacities. This also reflects a larger opportunity cost of fallowing cropland with fertile soils. For example, soils with higher sand contents likely produce lower crop yields [26, 27] , and thus farmers are more likely to fallow cropland with soils of higher sand contents.',\n       \"Metropolitan farms have exploited traditional DTC marketing outlets due to the relatively low transportation costs of accessing many proximate customers (Castle, Wu, and Weber, 2011; O'Hara and Lin, 2019) . However, the use of online marketplaces by farmers has not been extensively researched. In this study, we examine a specific marketing mechanism by which the internet may have offered a comparative advantage to rural farmers. We provide new evidence that online marketplaces may be strategically important for rural farms that are new to DTC marketing and lack cost-effective access to densely populated urban DTC marketplaces.\\nOur data do not allow us to establish a direct linkage between broadband expansion policies and online marketplace use by DTC farms. We do not know when the internet became available to respondent farms due to the large aerial scale of the Federal Communications Commission (FCC) data, and the LFMPS did not ask DTC farms the year they adopted an online marketplace. Also, other pertinent conditions besides broadband availability have changed over time, such as improvements in online transaction technology. Nonetheless, broadband policies have improved rural internet availability, which is a precondition for developing an online marketplace. So, a potential barrier to having an online marketplace was being partially addressed by rural development policy aimed at increasing high-speed internet availability, reliability, affordability, and adoption in rural America. While broadband policies could have assisted DTC farms with online marketing, panel data would be valuable in refining the linkage between changes in broadband access and the decision of a DTC farm to develop an online marketplace.\\nWhile our research focuses on how online marketing can impact producers, consumer impacts could be further investigated. One drawback of online purchases is that consumers do not experience the in-person interactions that are important in traditional DTC marketplaces (Hunt, 2007) . However, consumers may benefit from online marketplaces because online purchases may take less time to execute than purchases at traditional DTC marketplaces. For instance, consumers with scheduling constraints that prevent them from attending a weekly farmers' market could still make food purchases directly from farmers by doing so online. Further, the availability of online products could result in increased competition and, consequently, lower prices and different products.\",\n       \"FO identify what they regard as the three main 'bottlenecks' to automation of tasks performed by labour. One proposed bottleneck, 'perception and manipulation', is argued to derive from robots not being as able as humans in the manual manipulation of odd-shaped objects. The second and third bottlenecks are respectively 'creative intelligence' and 'social intelligence'. These bottlenecks are suggested to exist due to AI being still unable to deal with many creative and social tasks. FO estimate models to establish the association between their hand-labelling of whether an occupation can be fully automated (from stage 1) and the nine O*NET job characteristics for the 70 6-digit occupations. 8 Three alternative models are estimated, all of which are essentially non-linear regression (Logit) models of the indicator of whether an occupation can be fully automated on the nine O*NET variables. The base model is the standard Logit, 6 The O*NET also has information on the specific tasks undertaken within each occupation, as well as on the education, training, experience and licensing requirements of each occupation. 7 Autor (2013) has noted that '…researchers who wish to use these databases [O*NET or DOT] as sources for task measures are essentially required to pick and choose among the plethora of scales available…While I have found that task measures distilled from DOT and O*NET can serve as powerful proxies for occupational tasks, I am at best only moderately comfortable with these tools because their complexity and opacity places little discipline on how they are applied and interpreted. Learning (GPML) toolbox of Rasmussen and Nickisch (2010) during estimation and try two different ways of characterising the covariance matrix when defining the Gaussian process:\\nthe exponentiated quadratic and rational quadratic. The exponentiated quadratic is found to perform the best of the three models using the criteria of log likelihood and area under the receiving operator curve (AUC), narrowly outperforming the rational quadratic. Both nonparametric methods considerably outperform the base Logit.\\nThe non-linear (Logit) estimation method results in the predicted probabilities of automation for occupations clustering near zero and one. This is evident from Figure 1 , which shows the unweighted distribution of predicted probabilities for the 70 hand-labelled occupations. probabilities are also clustered around zero and one, with 57 per cent of occupations having probabilities greater than one-half of being fully automated.\",\n       \"The USDA's Agricultural Resource Management Survey (ARMS) data on apple production were used for this study. This survey contains information on production practices, inputs and costs, and financial performance of the U.S. farm households. Data on most direct inputs and farm characteristics come from the Phase II part of the survey, whereas other variables such as yields and area harvested come from the Phase III part of the survey. Data from the latest commodity survey of apple production in 2007 were used in the analysis. The ARMS data have unique characteristics that make it well suited for this research. First, the data set covers more than 90% of the acreage of targeted commodities. Second, it uses a stratified random sample in which each farm represents a known number of similar farms in the population based on their probability of being selected (weights). Using this statistic, the ARMS sample can be expanded to reflect the targeted population. Lastly, the enterprise costs-of-production data contain sufficient detail about specific inputs to isolate the seed and pest control costs used to produce a given commodity.\\nSeven states were represented in the survey: Michigan, Oregon, New York, Pennsylvania, North Carolina, California, and Washington. Washington was used as the base (benchmark) for its continuous and successful production history and because it is the state with the highest total production (Economic Research Service, U.S. Apple Statistics, ERS-USDA, 2012).\\nOnly conventional (nonorganic) farmers are considered in this study as the intent is to estimate the supplemental effect of MI on pesticide use.\\n2 The use of biological control is 1 For example, AZM has been the pesticide most used by Washington State apple growers since the late 1960s and, in 2008, 80% of Washington apple growers used AZM primarily to control codling moth (Cassey, Galinato, and Taylor, 2012).\\n2 Pesticides refer to chemical insecticides and fungicides. Although insecticides and fungicides have different impacts on outputs, we pool them as MIs because they have the potential to substitute both. defined as ''1'' if the farmer was using the technology and ''0'' otherwise in the ''Pest Management Practices'' section of Phase II of the survey. Although we would prefer to use a quantitative measure of the MI applications, the small percentage of farmers using this technology in 2007 makes a dummy variable more appropriate. In the sample of 547 conventional farms, 197 farms were using on average three MI products, from which the main ingredient included one of the following: Granulovirus, Bt, Bacillus subtilis, Bacillus pumilus, and Thricoderma sp. All of these products are strictly used for biological control. Figure 1 shows the percentage represented by each biological agent, from which 96% fall into the MI definition.\\nMI provide good resistance to different varieties of insects and diseases, caused by either bacteria or fungus, in apples. The most common microbial pesticide is the Granulovirus used against codling moth (Cydia pomonella) and Bt proven to work against many insects (Ohlendorf, 1999) . Regarding the others, Bacillus subtilis and Bacillus pumilus provide mild resistance against fireblight and some other diseases (Peighamy-Ashnaei et al., 2008; Sundin et al., 2009) . It is important to mention that the MI technology does not completely eliminate the need for chemical pesticides because it is ineffective against some insects and diseases. Table 1 presents summary statistics for adopters and nonadopters. Pesticide includes insecticide and fungicide applications net of any biological control product. Contrary to the previous findings based on experimental data (Cross et al., 1999; Peighami-Ashnaei et al., 2009) , the use of pesticides on plots with MIs is 25% higher than on plots without it. This positive relationship could be explained by more intensive pest management practices of the adopters and corroborated by the adopters' higher sales volumes. The difference in sales between adopters and nonadopters (38%) is much more pronounced than the difference in yields suggesting possible higher quality attributes including visual appearance, which, for apples, is achieved by increased chemical application rates. The data also show that the adopters have less experience, which fits some of the paradigms about biological control adoption constrained by institutional and social barriers (Peshin and Dhawan, 2009 ).\",\n       \"Comparisons between MCI subjects and healthy controls showed that node strength was the only metric to be significantly different between the two groups (mrd: +61%; p-value = 0.03), while all the other tests showed no significant differences. Similarly, the inferior parietal cortex showed the highest difference in strength between AD and controls, while the mid-occipital cortex showed the highest increase in terms of clustering coefficient (Fig. 6B) .\\nComparisons between MCI subjects and AD patients showed no significant differences for any test or metric, although there was a trend of increase in entropy in AD patients (+19%; p-value = 0.076) and the principal component eigenvalues were different at a trend level (Krzanowski's test p-value = 0.064). (Fig. 5A,B) . Full results for all the cross-sectional comparisons are reported in the supplementary materials.\",\n       'We first calculated the number of urban and rural 100-patient-waivered physicians, 30-patient-waivered physicians, opioid treatment programs, and substance abuse treatment facilities in each state and year from 2004 through 2011 and matched this to information regarding the total grams of buprenorphine dispensed in each state and year. Using the state-year as the unit of analysis, we specified a multivariate weighted ordinary least squares regression model of a state\\'s annual total buprenorphine dispensed per 10,000 state residents (hereafter referred to as \"per capita\") as a function of the state\\'s number of buprenorphine providers per capita, by type of provider (100-patient-waivered physicians, 30-patient-waivered physicians, opioid treatment programs, and substance abuse treatment facilities) per capita. We normalized the data by 10,000 state residents to account for the fact that the population totals by state differed substantially. We estimated weighted least squares with state-level population weights to account for possible differences in the variance of the error terms (heteroskedasticity) across states that might be caused by state-level population size. The state\\'s annual total buprenorphine dispensed is the sum of buprenorphine dispensed across all providers in that year. By exploiting the variation in the number of buprenorphine providers over time and within states, we used the model to estimate the amount of buprenorphine that each provider type contributed to the total. The substantial differences in the availability of opioid treatment programs and substance abuse treatment facilities by urban-rural status suggest that the role of 100-patient-waivered physicians and 30-patientwaivered physicians may be different in these settings. Accordingly, we specified the model with interactions of provider type by urban-rural status. We excluded rural opioid treatment programs and rural substance abuse treatment facilities, however, because there were very few such facilities, their estimates were imprecise, and they affected neither the substantive nor the statistical significance of other included variables. Because buprenorphine was approved in 2002 and its use grew substantially thereafter, we allowed for time trends by admitting a set of time indicator main effects as well as time indicator interactions with each provider-type variable. Thus, the intensities of treatment are estimated separately for each year in the study period. We also included state indicators to account for time-invariant, idiosyncratic differences across states. To verify the robustness of our analytic approach, we estimated several different alternative models, including models with and without normalization for state population size, weighted and unweighted by the state\\'s population size, and GEE models with various assumptions about the correlation structures or the error terms. We found that the main results were substantively unchanged and were very robust for alternative specifications and modeling assumptions.',\n       nan,\n       'The College Entrance Examination in China, called \"Gao-Kao\" in Chinese, is the most high stakes assessment in China and parallels the most competitive examinations globally. Two days each year, millions of high school graduates and people with equivalent educational qualifications take the test. Students with higher Gao-Kao scores get into better universities. They can get better jobs after graduation, and eventually, become winners in the thriving economy. Hence, the test is considered to be the most critical turning point in every student\\'s life, and studying for it can never be over emphasized. This type of high stakes examination has parallels with other national college entrance examinations, such as the ACT and SAT in the U.S., but the U.S. examinations do not carry the weight that Gao-Kao does. The U.S. system considers other factors within the college placement process. Under such high pressure, over use of the Gao-Kao test score seems inevitable. In many places, high schools are ranked by their average Gao-Kao scores and teachers are rewarded by their class averages. As a result, many people blame Gao-Kao for causing bad educational practices such as teaching to the test and social problems such as creating students who are test-taking \"machines\" with limited creativity. Thus, the Gao-Kao examination manifests symbolic meanings that correlate with stress and intense competition. Ironically, the Gao-Kao data has seldom been used for important educational policy making decisions. China\\'s NEEA (National Education Examinations Authority) is directly under the MOE (Ministry of Education). The NEEA exerts great effort to ensure the quality of the test questions and the reliability of the tests. There is not examination continuity from year to year. That is, unlike the SAT or ACT exams in the U.S., Gao-Kao scores from different years cannot be compared directly. It is not possible to tell whether the difference in Gao-Kao scores from one year to another is the result of changes in student proficiency or a shift in item difficulty.\\nFindings from investigations on trends regarding education quality in individual schools, regionally, or on national levels remain untapped. Modern score equating techniques (Kolen & Brennan, 2004) provide the NEEA with a tool to make better use of the Gao-Kao data. The goal of equating is to produce a linkage between different test forms so that the scores from each test have the same meaning and can be compared directly. Analysis of student growth becomes possible when Gao-Kao tests over successive years can be equated. Moreover, it has been widely acknowledged that one-time assessment scores are not a fair way to compare teachers or schools since students come to school with different backgrounds (Doran, 2003) . This is significantly different, in contrast with the U.S., where multiple factors are considered for college entrance decisions. Such factors include aptitude testing, high school GPA (grade point average), extra-curricular activities and unique life experiences. Over the last decade, value-added analysis in China has become the most promising tool to evaluate school effectiveness. The idea behind the value-added approach is simple. School quality is determined by the increase in student knowledge and skills, extracting the impact of non-school factors such as the student\\'s family SES (socio-economic status), etc. (Ballou & Sanders, 2004) . It is strongly desired that the Gao-Kao data be used in this way since valueadded accountability models can greatly motivate teachers and schools (Doran & Fleischman, 2005) . In this study, we took data from Hainan province as an example and implemented two technical modifications to the Gao-Kao English test. First, we linked the tests from 2010 and 2011, and aligned them onto a Rasch scale (Kolen & Brennan, 2004) . Secondly, we collected background information from the examinees via a background survey. The results were used KOME − An International Journal of Pure Communication Inquiry Volume 1 Issue 2 pp 55-63.\\nto examine student growth and were further applied to construct a value-added school accountability model. All of the aforementioned, taken together, offer relevant themes for interpretation using Kenneth Burkes Dramatistic Pentad (Golden, Berquist, Coleman & Sproule, 2011) as a foundation for framing the varied communicative elements. The Dramatistic Pentad stresses the act, agents, scene, agency and purpose (which will be explained later in this report).',\n       'Among all who met entry criteria, with or without complete follow-up 400 m test data, results of the bidirectional relationships of slower usual gait speed with poorer DSST and TMT-B remained significant after Bonferroni correction (see Supplementary data, Appendix 5, available in Age and Ageing online). (2) 17 (2) 17 (2) ',\n       'Student and employment status (EMPSTAT) variable and subcategory',\n       'Far-field subaerial landslide (CVV). Fig. 16a shows the surface elevation from the superposition of the incident CVV tsunami wave train with the calibrated M2 tide elevation at the SE corner of the 154 m grid, for the TT1 phase situation; as expected, the maximum tsunami and tide elevations have been synchronized. The computed time series of surface elevation at the Sewells Pt. reference station (NOAA tide gauge #2; Table 5), plotted on the same figure, shows a strong reduction of the CVV tsunami elevation across the wide shelf and in the shallow entrance of the Chesapeake Bay, due to both directional spreading and dissipation of the larger incident waves mostly by bottom friction (and some breaking). From a maximum elevation of 8 m on the offshore boundary of the 154 m grid, the tsunami elevation at Sewells Pt. is reduced to 1.7 m. Many of the smaller oscillations in the incident wavetrain have disappeared, being damped out. Computations are pursued by one-way coupling in the 39 m grid. Fig. 16b first shows that there is a good agreement of the tsunami surface elevation computed at Sewells Pt. in both grids, with expectedly more higher frequency oscillations occurring in the 39 m grid, owing to the better resolution. (a) (b)  in 154 m grid (Fig. 2); (b) at NOAA station #2 Sewells Pt, in 154 m grid (solid) and in 39 m grid (Fig. 3) (dashed). Time is from the start of the CVV event  Fig. 17a then compares the surface elevations computed at the so-called \"river station\" (Fig. 3), located upstream and in the middle of the James River (-76.64 E, 37.15 N), in the 154 m and 39 m grids. The agreement is good, but elevations in the finer grid are up to 0.15 m higher than in the coarser grid, which justifies using the 39 m grid to compute tsunami inundation levels in the James River. Compared to Fig. 16 at Station #2, we also see that during its propagation up the James River, the tsunami wave train has lost all of its higher-frequency oscillations and is reduced to three main oscillations of about 1.5 hour period; also, unlike in Fig. 16, the larger elevations occur later in the wave train. Fig. 17b then shows results computed in the 39 m grid at NOAA Stations #3 and #4, with the \"river station\" used as a reference (Fig. 3), compared to surface elevations obtained for the calibrated M2 tide only. As expected for TT1, the leading tsunami and tide elevations are almost synchronized. However, higher surface elevations are seen to occur for later times in the wave train, likely due to an enhancement of smaller incident tsunami waves by the ebbing tidal currents. (a) (b) Finally, Fig. 18 shows the envelope of maximum surface elevation computed for this case in both the 154 m and 39 m grids. While at and near the James River mouth, maximum tsunami inundation reaches 2-2.5 m, in the river, we see a significant decrease in maximum inundation, in the 1.1-1.5 m range. Nevertheless, Fig. 18b shows that many low lying areas of the river banks would be flooded.  Point (Fig. 3) Near-field Submarine Mass Failure (CRT). Fig. 19a shows computed time series at Sewells Point (NOAA station #2; Table 5) of the CRT tsunami elevation combined with the calibrated M2 tide, in both the 154 m and 39 m grids. As expected, the maximum tsunami and tide elevations have been synchronized. Comparing to the large incident tsunami elevation (without tide) at the offshore boundary of the 154 m grid shown in Fig. 7, similar to the CVV case, there has been a strong reduction of the tsunami elevation across the wide shelf and the shallow entrance of the Bay, due to both directional spreading and dissipation of the large incident waves by bottom friction and some breaking. From a maximum elevation of 4-9 m along the offshore boundary of the 154 m grid, the tsunami elevation at Sewells Point is reduced to 1.45 m. Similar to the CVV case, Fig. 19b shows results computed in the 39 m grid at NOAA Stations #3 and #4, with the \"river station\" used as a reference (Fig. 3), compared to surface elevations obtained for the calibrated M2 tide only. The leading tsunami and tide elevations are again almost synchronized but this time the highest combined surface elevations occur for the leading crest in the wave train, with about 0.9-1 m above the MHW+SLR reference level. Compared to Fig. 19a at Station #2, we again see that during its propagation up the James River, the tsunami wave train has lost all of its higher-frequency oscillations and as for CVV is reduced to three main oscillations of about 1.5 hour period Finally, Figure 20 shows the envelope of maximum surface elevation computed for this case in both the 154 m and 39 m grids. While at and near the James River mouth, maximum tsunami inundation reaches 1.5-2 m, in the river, however, we see a significant decrease in maximum inundation, in the 0.9-1.1 m range. Although less than for CVV, Fig. 20b shows that some low lying areas of the river banks would be flooded for this case as well. ',\n       'By objectively tracking tropical cyclones (Strachan et al. 2013 ) that are directly simulated by the models, we found a substantial decrease in annual storm counts from the historical to future both globally (120 vs 98) and in the Pacific Ocean (91 vs 75; 1008E-608W, which includes all of the Pacific Ocean as well as the southeastern Indian Ocean, Gulf of Mexico, Caribbean Sea, 1 to 4 1 to 4 1 to 4 1 to 3 2 to 4 1 to 3 0 to 1 0 to 1 GFDL-CM3 3 to 6 1 to 4 3 to 7 0 to 3 1 to 3 1 to 4 2 to 4 1 to 3 MIROC5 1 to 7 0 to 3 1 to 7 0 to 4 0 to 3 0 to 2 0 to 2 0 to 2 MRI-CGM3 6 to 11 6 to 9 7 to 12 5 to 9 4 to 8 4 to 7 1 to 3 2 to 4 MRI-AGCM3.2 4 to 8 1 to 5 2 to 6 0 to 4 4 to 6 2 to 5 1 to 4 1 to 3 Multimodel mean 3 to 7 2 to 5 (4/6 models) 2 to 7 1 to 5 (5/6 models) 2 to 5 2 to 4 (4/6 models) 1 to 3 1 to 3 (4/6 models increased or unchanged) and northwestern Atlantic Ocean). Good intermodel agreement (4 out of 5 models) of such a decrease was found; however, inherent model biases (too many storms in the Pacific Ocean on average) and coarse spatial resolutions (ranging from ;100-to ;200-km grid spacing; see Table 1 ), which limit formation of intense cyclones, pose uncertainties especially when counting smaller or weaker storms (Hodges et al. 2017 ). To assess the sensitivity to model horizontal resolution, we repeated the tracking analysis with a global higherresolution atmospheric model (20 km; MRI-AGCM3.2) forced by observed SST, which lessens current-climate biases, or forced by warming calculated from the CMIP5 projections added to the observations (Mizuta et al. 2012) . The higher-resolution model also simulated fewer tropical cyclones in the future (61 vs 84 in the Pacific Ocean and neighbor basins; 1008E-608W). Our results are consistent with a recent study using a regional atmospheric model with similar resolution and prescribed warming that projected a decrease in storms in the northwestern Pacific, which was not fully offset by slightly more storms forming in the northcentral Pacific (Zhang and Wang 2017) . A somewhat different result-more future storms in the deep tropics of the northwestern Pacific and fewer in the subtropics-was obtained by Manganello et al. (2014) using a 16-km global atmospheric model, although there was small overall change in tropical cyclone occurrence.',\n       'Participants were recruited through purposeful, criterion sampling. 21 Fliers were distributed to KARRN partners and letters were sent to over 200 people with stroke who received rehabilitation at various sites under the organizational umbrella of 2 large regional medical centers. Participants had to meet the following inclusion criteria: diagnosis of stroke or caregiver of someone diagnosed with stroke, at least 18 years of age, able to participate in a 60-to 90-minute interview, native language of English, and rural Appalachian Kentucky county resident. According to the Appalachian Regional Commission, Appalachian Kentucky consists of 54 counties, 22 43 of which are considered economically distressed. 23 Counties were further identified as rural using the Rural-Urban Continuum Codes, also known as the Beale Codes, yielding a total of 50 out of 54 rural counties in Appalachian Kentucky. 24 Thirteen individuals with stroke and 12 caregivers who met the inclusion criteria volunteered to participate. Informed consent was obtained from each participant. Participants represented 10 rural counties in Appalachian Kentucky (Figure 1). County descriptions including population, rural code, economic status, and number of participants are provided in Table 2. The average population of the 10 counties was 25,152, and 90% of the counties are classified as distressed. Interviews took place at locations determined by participants (homes (9), regional hospital meeting centers (3), and residential nursing facilities (2)). A dyad was not required, but the majority of stroke survivors (85%) did have their caregivers join them in the interview. Additionally, one person living with stroke was not able to participate in the interview due to a decline in medical status, but her caregiver did participate. Stroke survivors included 9 females (69%) and 4 males (31%), with an average age of 63.4 years (range, 42-89 years) and an average of 3.6 years post-stroke (range, 1-14 years). None of these participants were employed at the time of the interviews. The majority of stroke survivors (69%) were in households with an annual income of $35,000 or less, while the remaining 31% had an income of $50,000 or more. Marital status included: married (54%), widowed (15%), separated (8%), and divorced (23%). As evidenced by a self-perceived overall rating of recovery (visual analog scale in which \"0\" indicated no recovery and \"100\" indicated full recovery), 92% of the stroke survivors perceived residual deficits at the time of the interviews. Self-perceived recovery ranged from 30% to 100%, with an average of 62%. Common secondary complications included falls (11 (85%), with at least one fall poststroke and as high as 7 falls reported for one person) and depression (10 (77%)). Caregiver participants included 7 females (58%) and 5 males (42%), with an average age of 55.9 years (range, 38-75 years). Caregiver relations to survivors were evenly split with half being spouses and half being children. Eleven of the caregivers were married (92%). Levels of educational attainment represented included: elementary education (8%), high school graduate (33%), and higher education (59%). Half of the caregivers were employed, 4 (33%) were retired, and 2 (17%) were unemployed. In contrast to the stroke survivors, the majority of caregivers (67%) reported an annual household income of $50,000 or more, while the remaining 33% reported $35,000 or less. All participants in this study were white, consistent with the 95.4% white demographic of Appalachian Kentucky. 25 A summary of all of the participant demographic data is provided in Table 3.',\n       \"Guided by the Hoover-Dempsey and Sandler model of parent involvement processes Walker et al., 2005) , we identified barriers and facilitators to SBPI among parents who lived in low-income urban communities and whose children attended three public middle schools. Our findings support previous research demonstrating links between motivational factors (i.e., motivational beliefs, invitations for involvement, and personal life context) and the frequency and types of parent involvement implemented (Deslandes & Bertrand, 2005; Green et al., 2007; Hoover-Dempsey et al., 2005 ). In the current study, motivational beliefs for parent involvement were expressed in the form of parents' positive role construction regarding their function to support their child's educational endeavors. However, the quality of invitations for involvement by teachers and the school as well as parents' personal life context presented a number of barriers. One important contribution of this study is the finding that students' aggressive behavior in the school not only contributed to some parents' negative perceptions of the school climate but in fact it also hindered their engagement in SBPI. Overall, the barriers identified help to explain why parents in the current study indicated infrequent involvement in SBPI activities.\",\n       'In addition to high-quality classroom instruction, teachers in this sample spoke of the need for high-quality intervention instruction. Many of the schools the teachers worked at offered additional support to struggling readers in the form of pull-out classes, for primary and upperelementary students, or support classes, in the form of an extra period of reading instruction, for the middle school students. During these pull-out classes, specialists often provided instruction with specialized programs such as Leveled Literacy Intervention or Wilson Reading System. Not all schools were able to offer such additional support; one specialist who worked for a large urban area lamented the fact that they were unable to provide additional services due to lack of funds: We don\\'t have that resource, we have no personnel, we have no person who does that. . . so they\\'re not receiving the intensive intervention. Overall, teachers viewed intervention as a key component to assisting struggling readers, and that like classroom instruction, the intervention needed to be targeted directly to student need. However, targeting individual student need was both time and resource intensive, requiring large amounts of small group instruction, one-on-one instruction, or requiring pull-out programs with specialized curriculum. Teachers in this sample also noted that the quality of intervention instruction was uneven as well, both in terms of interventions chosen and who was providing the intervention instruction. Teachers noted that for interventions to work they needed to be \"research-based\" and \"systematic\" and that students wouldn\\'t make progress if \"the interventions set in place just isn\\'t working.\" Thus, teachers see choosing the appropriate intervention to use with a struggling reader is of paramount importance; however, participants believed that not all teachers were capable of doing so. One reading specialist noted that at her school classroom teachers were not knowledgeable enough about interventions to choose appropriate instruction. Many of the upperelementary and middle school teachers echoed this sentiment, expressing a desire for more professional development and training for themselves as well as other teachers who work with students in older grade levels: Like many of the teachers on our team, we feel like we just really need to learn more about how to teach these kids that are like 10 years old that are still like so far behind. Like how do we teach them? And how do wehow do we work with them? In combination with choosing appropriate reading intervention, teachers spoke of issues with the particular teachers tasked with providing those interventions. For example, teachers spoke of how pull-out services for struggling readers were being provided by inexperienced teachers, teachers without reading endorsements, or teachers without Master\\'s degrees in reading, and that they felt the quality of intervention provided by these teachers was not as strong as it could be.',\n       \"In cognitively normal (CN) elderly individuals, white matter hyperintensities (WMH) are commonly viewed as a marker of cerebral small vessel disease (SVD). SVD is due to exposure to systemic vascular injury processes associated with highly prevalent vascular risk factors (VRFs) such as hypertension, high cholesterol, and diabetes. However, cerebral amyloid accumulation is also prevalent in this population and is associated with WMH accrual. Therefore, we examined the independent associations of amyloid burden and VRFs with WMH burden in CN elderly individuals with low to moderate vascular risk. Participants (n = 150) in the Alzheimer's Disease Neuroimaging Initiative (ADNI) received fluid attenuated inversion recovery (FLAIR) MRI at study entry. Total WMH volume was calculated from FLAIR images co-registered with structural MRI. Amyloid burden was determined by cerebrospinal fluid Aβ 1 42 levels. Clinical histories of VRFs,\",\n       'The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Fig. 12. Liner trend from 1981 to 2015 of (a) seawater pCO 2 , (b) DIC, (c) CO 2 flux, (d) ΔpCO 2 , (e) SST, (f) surface zonal wind, and (g) surface wind speed. Liner trend in a d shown is calculated from the optimal interpolation results with low estimation errors (square error ratios < 0.9). Dark red (blue) in a d shows positive (negative) regression coefficient calculated from all optimal interpolation results (see text). Hatched areas with diagonal lines indicate regression coefficients significant at p < 0.05. (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article).',\n       'Within the Carlsbad and Del Mar study areas, there were no commercial or industrial properties in the bluff erosion or chronic inundation hazard zones.',\n       'The regression analysis reveals a fairly consistent pattern in which the racial/ethnic gap diminishes as family environment and support, student behavior, and school factors are considered. This pattern is indicated by the change of the logistic regression coefficient associated with race/ethnicity from statistically significant in equation 1 to statistically insignificant in the subsequent three equations. Equation 2 examines the relationship between family environment and support variables and S&E entry. The estimates for the three family variables are positive and statistically significant (with logistic coefficients of 0.52, 0.89, and 0.26, respectively), suggesting that, holding race/ethnicity and gender constant, students would be more likely to enter S&E if their parents had a college education and expected their child to have a college education, and their families could provide some financial support for college. Note that as the three variables enter the equation, the logistic coefficient associated with race/ethnicity becomes smaller in magnitude (-0.16) and is no longer statistically significant. This finding implies that when comparing students with these similar family characteristics, the racial/ethnic gap in S&E entry tends to disappear. The gender gap, however, remains substantial (-0.75 in logit estimate). Equation 3 includes a set of student behavior variables, which concentrate on students\\' academic preparation during their secondary schooling. Note that the overall model fit improves when these variables are entered, as indicated by the larger Chi square of 1223.98 (relative to that of 427.16 in equation 2). Students\\' psychological characteristics relating to math and science learning are evidently strong predictors of postsecondary S&E enrollment. Given the same race/ethnicity, gender, and family environment and support, the variables students\\' motivation to learn science, aspiring for science and technology jobs, and self-confidence in math learning are positively related to S&E entry, with logit estimates of 0.99, 0.53, and 0.39, respectively. S&E entry is also clearly related to the two measures of math and science credits, holding other conditions constant. Both total math and science credits and advanced math and science credits are found to be positive and statistically significant predictors of S&E enrollment, with coefficients of 0.11 and 0.44. Gaining in science learning as indicated by the increasing proficiency level in NELS:88 science tests is not related to S&E entry, nor is the gifted or advanced program placement, as neither coefficient is statistically significant. After including the student behavior variables, the racial/ethnic gap further changes from negative to positive (0.24 in logit), though not to statistically significant, and the gender gap also narrows, while remaining substantial (-0.63 in logit). Conspicuously, the gender/race/ethnicity interaction effect disappears after the student behavior variables are entered into the equation. Moreover, the coefficients associated with parents\\' educational attainment and family financial support are reduced to virtually nil (not statistically significant logits of 0.05 and 0.17, respectively), and the coefficient for parents\\' expectations for their child to attain college education also becomes much smaller (0.25 in logit) and is not statistically significant. These findings highlight the critical importance of students\\' secondary academic preparation in relation to postsecondary S&E enrollment. They show that once students\\' attitudes regarding math and science learning and their coursework in the two subjects are taken into account, the chance to enter S&E programs tends to approach equity for students of different racial/ethnic and socioeconomic backgrounds. School variables are entered in equation 4. Teachers\\' having majored or minored in math or in science in their academic training is not a significant predictor of students\\' S&E entry (with a logit coefficient of 0.09). Neither is schools\\' science coursework requirement of 3 or more years (logit estimate -0.08). Including these two variables makes little difference to the estimates of other predictor variables or to the overall model goodness of fit.\\nTheoretically, the gender gap in S&E achievement and attainment was a result of psychological and sociocultural influences which discouraged women from reaching high in an area that has been traditionally dominated by men. Economic resources and material support were not a significant issue in dealing with the gender gap. The above analysis of the overall sample data found that a broad gender gap only narrowed to a limited extent after predictor variables were entered into the equations. In other words, those predictor variables did not account well for the low S&E entry among women. This finding led to questions as to whether there was some cultural value that backed girls\\' venture into S&E areas, or, alternatively, environmental factors that fostered girls\\' intellectual orientation in terms of postsecondary program choice. Perhaps there were unique joint effects among the family expectations, girls\\' academic preparation, and their school conditions. We tested a number of possible interaction effects to explore these questions. We hypothesized that traditional values that emphasize marriage, family, and children, in contrast to \"nontraditional\" views that stress individual success and independence, might make a difference in female students\\' career choice. To derive a measure of such value orientations, we conducted a principal component analysis with 15 items from the third follow-up student file that assessed the respondents\\' perceived importance of life goals. The resulting two factors, stable with different extraction and rotation methods, were labeled \"traditional value\" and \"nontraditional value,\" respectively, based on their high loading on specific items (see appendix IV for the data items and the loadings resulting from the factor analysis). When tested, only the factor labeled nontraditional value related to majoring in S&E among female students. Therefore, respondents\\' \"nontraditional\" value orientation in high school years and its interaction term with gender were tested to see how this value orientation specifically affects girls\\' S&E choice after high school. The model specifications were largely the same as those for racial/ethnic gaps, only with an additional value orientation indicator and its interaction term with female. The results, however, do not support the hypothesized relation between S&E selection and value orientation: neither the estimates for the \"nontraditional\" value orientation nor its interaction term with female are statistically significant. To further focus on girls\\' family environment and support and behavior variables in connection to S&E entry, another analysis was conducted with NELS:88 data for Asian and white cohort members only, because the observed gender difference took place primarily in this group. Table 34 presents the results of the analysis. The model specifications are essentially the same as those in the overall panel data analysis, except that the race/ethnicity variable and the gender/race/ethnicity interaction term were removed. The resulting estimate for virtually every predictor variable is similar to that in the overall panel data analysis. With the full model (equation 4), holding other conditions constant, parents\\' expectation for their child\\'s college education is related to greater likelihood of S&E entry; students\\' motivation to learn science, aspiring for science or technology jobs, self-confidence in math learning, and both the total credits and the advanced credits in math and science, are positively related to S&E entry. The gender gap-indicated by the coefficient associated with the variable gender-tends to narrow upon entering behavior items, but to a fairly small extent (from -0.74 to -0.63 in log odds ratio), and it remains substantial and highly statistically significant. On the other hand, teachers\\' major or minor in math or science and schools\\' science coursework requirement, again, do not relate to postsecondary S&E entry. In short, the analysis of data for Asian and white students reveals no different pattern of gender gap from that found in the overall analysis. Teacher major in S&E is a three-category variable with missing/not applicable cases coded -1 and counted 5,050 cases. NOTE: The sample size n changed across equations due to listwise deleting of missing cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study of 1988 (NELS:88), \"Base Year\" through \"Third Follow-Up\" panel data, weighted with normalized panel weight F3PNLWT. This page intentionally left blank.',\n       nan,\n       'Among the 93 early childhood teachers who responded, all but 11 were women and their average age was 36.5 years. 76% of them were qualified early childhood teachers, while 5% were qualified in childcare and youth work. Other qualifications were child welfare officer (3%), other social and health care workers (2%), Physical education teacher (one man), and undefined education (3%). Some of the early childhood teachers had advanced qualifications in preschool education (6%), in other education (5%) and in science and mathematics (one woman), while 3% were early childhood teacher students. One-third of the responses were from early childhood teachers only working with children 0-3 years, while to-third were from those working with children aged 3-6 or 5-6 or with mixed age groups.',\n       \"For each subject, the whole-brain time-varying connectivity matrices are computed based on M (M = 116) ROIs from the automated anatomical labeling (AAL) template using a sliding window approach [3, 4] . As shown in Fig. 1 , the averaged BOLD time-series S i in ROI i are first computed. Then, the window {W t } are generated and applied to S = {S i }, where T is the total number of sliding windows. Next, for each W t , an FC matrix R t of size M * M that includes FC strengths between all pairs of S it are calculated. Thus, for each subject, a set of R t (t = 1, 2, . . . , T ) are obtained, representing the subjects' whole-brain dFC. Due to the symmetry of each R t , all FC strengths in R t among M ROIs corresponding to a window t are converted to a vector x t with M (M − 1)/2 elements. Therefore, all the dFC time series from the k th subject can be represented by a matrix\\nwith a size of T * {M (M − 1)/2} and used as input to Full-BiLSTM classification model.\",\n       'IVs estimates do not provide the average treatment effect (ATE), which could be interpreted as the expected average causal effect of the treatment or condition. However, they do provide the local average treatment effect (LATE), which means that the IVs technique estimates the average causal effect on those affected by the instrument (Kleibergen and Zivot 2003, pp. 173-188) . The LATE therefore is informative about subjects \"who will take the treatment if assigned to the treatment group, but otherwise not take the treatment\" (Angrist and Krueger 2001, p. 77) .\\nFor example, in Angrist and Krueger\\'s (1991) study on returns to education, the estimated effect of schooling was informative for those deciding to quit school when they have the opportunity to do so. Research that uses the \"surprise\" cohort composition (Hoxby 2000 (Hoxby , 2002 found effects that were informative for schools that were affected by (2) Stage 2: y =x i β i + Zβ + e unexpected changes in cohort size but not for schools that would have strict regulations on class size and/or student allocation. In studies that used proximity to relevant educational institution, the LATE effect was only informative for participants whose decision to enroll was influenced by the distance to the institution, but it was not informative for those that were insensitive to distance while deciding whether to enroll.\\nRegarding the validity of an IV, the question is whether estimates based on the participants\\' responses to the instrument might be generalized to the entire population. Several questions could be posed to address this issue. The first question concerns whether there is reason to believe that the effects estimated using IVs might be heterogeneous, that is, differ among the groups of respondents. If the effects are assumed to be homogenous, LATE equals ATE, and generalizations based on IVs estimations are legitimate. If the heterogeneity of the effect could not be clearly rejected, additional questions could be asked. How large is the group that is affected by the instrument? If the group covers the majority of the population, the problem of generalizability might not to be important. Another question about representatives might be asked. If there is no reason to believe that the individuals affected by the instrument are substantially different from the entire population (i.e., biased the sensitivity to the instrument), the estimated LATE effect might be considered valid for the entire population.',\n       'Although a statistical analysis revealed that there was no difference between female and male participants in the mean scores of stereotype, female participants predominantly depicted their scientists wearing lab coats and eye glasses, working in the laboratory surrounded by symbols of research. In particular, for grade 3, more females than males drew their scientists in a laboratory setting with symbols of knowledge, such as books, charts, etc. Meanwhile, a majority of male participants portrayed their scientists as a male Caucasian. It is not surprising that males tended to draw their scientist as a male, while females drew both male and female scientists. For grade 7, female students described scientists with eyeglasses, while male students drew their scientists with symbols of technology and mythic stereotypes. For grade 10 students, the overall analysis confirms a predominantly \"manly\" image of a scientist irrespective of the country of origin of the students. ',\n       'The Hippocampus, as part of brain, is responsible for emotion, memory, and the autonomic nervous system. In [8] , we used a statistical analysis by VBM plus DARTEL to examine global and regional GM atrophy patterns in people with AD, according to sex. In more detail, VBM plus DARTEL analysis demonstrated greater global GM reduction in the female subjects (F-AD versus F-NC), compared to the male subjects (M-AD versus M-NC). In this study, we focused on HGM volumes in patients with AD compared to normal controls based on sex. The results of this study revealed that the reduction in HGM region is around 1.38 times more in female against male subjects. Our statistical analysis suggests that there may be a link between HGM atrophy and AD based on the sex. This funding supports our previous study that sex is an important factor in the AD [8] .',\n       \"In this section we estimate effects separately by expenditure sub-categories. This accomplishes two goals. First, it provides us with an alternative approach to examining whether teachers' unions favor spending state aid increases on class size reductions (i.e., teacher hiring) or on increasing teacher compensation. Specifically, note that instructional expenditures are primarily composed of expenditures on teacher compensation. Furthermore, recall that in Table   2 , we find that reform-induced increases in state aid have similar effects on class size in both strong and weak union states. Thus, if we find that reform-induced increases in state aid have a larger effect on instructional expenditures in strong union states than weak union states, this would suggest that the strong union states must be spending more of the marginal dollar of increased instructional spending on raising teacher compensation.\\nThe second reason we explore effects by expenditure subcategories is that while we focus our examination of the allocation of resources on teacher salary increases and class size reductions, other inputs to education production can be important as well. Thus, we examine how much of each dollar of SFR-induced state aid passes through to various subsets of expenditures, for example, current expenditures versus capital outlay, and among current expenditures, instructional versus non-instructional spending.\\nIn Table 6 We also find heterogeneity by teachers' union strength in the effects of SFR-induced increases in state aid on non-instructional expenditures (column 4) and on capital outlays (column 5), though the interaction of state aid and union power is statistically insignificant for the latter. Districts in strong union states see a 34 cent increase in non-instructional spending and 19 cent increase in capital outlays for every dollar increase in state aid compared to only a 23 cent and 14 cent increase, respectively, in weak union states. Thus, while there are important differences in how teachers' union power affects instructional spending, there are also important differences across these other spending categories. This suggests teachers' unions prefer not only higher teacher salaries, but also increases in items that may improve working conditions, such as curricular and administrative support, and infrastructure improvements.\",\n       'In addition to developing a better understanding of how the breach forms, we set out to better understand how it closes. There are several processes (long-shore transport, cross-shore transport, wind-blown transport, tidal exchange, runoff, etc.) that likely contribute to breach closure. Breach closure is dynamic occurring over a duration controlled by the ambient forcing. It is hypothesized that cross-shore transport is the predominant mechanism and have employed SBEACH as a tool to test that hypothesis. The SBEACH model only considers closure due to waves. Tide was included so that the waves acted on a variable water level. Considering the small tidal prism, waves are a more dominant forcing than tidal currents. Therefore, tidal currents in the breach were excluded, although they may act to change the length of time the breach is open. The model helped with examination of the long term wave-induced sediment transport into the breached section of the beach.',\n       'All analysis in this paper has been performed in R [20] , scripts are freely available at https://github.com/KHP-Informatics/TC.',\n       'In addition to the documented reduced risk of AD among asymptomatic older adults using NSAIDs, recent studies have investigated the relationship between use of antiinflammatory agents and cognitive function in this popula- tion. In a study of 13 153 individuals, between 48 and 67 years of age, who regularly utilized NSAIDs or asprin, no associations between NSAID use and any of the cognitive tests were observed, although a modest association was observed between aspirin use and better performance on delayed recall and verbal fluency tests. 201 Yet others observed no positive impact of prescription NSAID use on cognitive function in community-dwelling older adults. 202 However, as emphasized by Pasinetti, 76 daily doses of up to 1200 mg of NSAIDs such as ibuprofen are analgesic but not anti-inflammatory, and it typically requires daily doses of 2400 mg for a systemic anti-inflammatory effect. It is interesting to note that in an investigation of the impact of chronic NSAID use on cognitive decline in older adults, Rozzini et al 203 found a positive association between chronic NSAID use and reduction in cognitive decline over 3 years, as measured by the Short Portable Mental Status Questionnaire.As Karplus and Saag 204 point out, large-scale, randomized, controlled trials using NSAIDs in this population are needed before it is clear whether the known risks of NSAIDs are outweighed by their potential long-term benefits on cognition. There have been several investigations of the impact of G biloba on cognitive function in adults asymptomatic for dementia. Several of these studies found that G biloba appeared to improve speed of processing and memory function, particularly on measures of working memory. [205] [206] [207] However, these studies were typically short in duration, ranging from 6 hours to 12 weeks, and included middleaged rather than older adults. Several large-scale, multisite, randomized clinical trials of G biloba in older adults are ongoing and their results should further clarify the relationship between this agent and cognitive performance in this population. The influence of estrogen on cognition and memory in normal aging has also received considerable recent attention. [208] [209] [210] [211] [212] [213] [214] One of the most consistent findings to emerge from the above literature links estrogen to the maintenance of memory function in aging women. Several studies found that estrogen significantly improved performance on tasks of both the immediate and delayed recall of verbal and nonverbal material. [213] [214] [215] [216] [217] While several observational studies have shown that estrogen administration has a positive effect on attention span, concentration, and memory function, others have not observed an association between ERT and cognitive function. [218] [219] [220] Methodological differences among these investigations, including variation in the age of subjects and the cognitive tests employed, may account for the mixed results. In particular, there is often inconsistent use of estrogen over time in postmenopausal women, which may impact the outcomes from such observational investigations. However, recent evidence from imaging studies lends further support for a positive benefit of estrogen on cognitive functioning. In cortical regions typically hypometabolic in AD, Eberling et al 221 found that older women who had never taken estrogen exhibited metabolic ratios intermediate to those of AD patients and women on ERT. Similarly, a longitudinal assessment of regional cerebral blood flow changes observed increased flow over time in estrogen users compared with nonusers, particularly in the hippocampus and temporal lobes. 222 Since the decision to take ERT may be impacted by education and socioeconomic variables, randomized clinical trials are needed to systematically address the merits of estrogen for cognitive processing in older women. To date, there have been a limited number of randomized clinical trials of estrogen use in healthy individuals, with the majority short-term in duration and often investigating younger adults. 215, 223 Data from large, long-term, randomized clinical trials in this population are required before we can adequately assess the long-term benefits of estrogen use on cognition as well as its role in AD prevention.',\n       \"The atmospheric boundary layer and lower free atmosphere, or aerosphere, is increasingly important for human transportation, communication, environmental monitoring, and energy production. The impacts of anthropogenic encroachment into aerial habitats are not well understood. Insectivorous birds and bats are inherently valuable components of biodiversity and play an integral role in aerial trophic dynamics. Many of these insectivores are experiencing range-wide population declines. As a first step toward gaging the potential impacts of these declines on the aerosphere's trophic system, estimates of the biomass and energy consumed by aerial insectivores are needed. We developed a suite of energetics models for one of the largest and most common avian aerial insectivores in North America, the Purple Martin (Progne subis). The base model estimated that Purple Martins consumed 412 (± 104) billion insects*y is greater in North America during the breeding season than during other phases of the annual cycle, however the maximum daily insect consumption*km -3 occurs during fall migration. This analysis provides the first range-wide quantitative estimate of the magnitude of the trophic impact of this large and common aerial insectivore. Future studies could use a similar modeling approach to estimate impacts of the entire guild of aerial insectivores at a variety of temporal and spatial scales. These analyses would inform our understanding of the impact of population declines among aerial insectivores on the aerosphere's trophic dynamics.\",\n       'Of 3,300 physicians who received the survey, 1,675 responded (50.8% response rate).',\n       nan,\n       \"In this randomized comparison of web and paper survey response outcomes in a study of cancer survivors ascertained through a central cancer registry, the overall proportion responding was slightly lower among the web arm. However, this difference in response rates was not significant. Considering that a recently-updated metaanalysis demonstrated that web surveys continue to yield response rates that are 12 percentage-points lower than other modes [39] , the difference in response rates in this study (a seven percentage-point difference) was smaller than anticipated. This is a notable departure from longstanding trends showing web surveys obtaining much lower response rates than paper-based surveys [34] [35] [36] [37] [38] 56] except in limited instances with specialized populations wherein Internet use may be more prevalent, including college students [57] , physicians [58] [59] , and volunteer samples recruited online [60] .\\nFurthermore, unlike most prior research showing the demographic profile of web survey respondents is often much different and less representative of the target population than is found with paper surveys [61] [62] [63] [64] [65] [66] , in this study the demographic representativeness of the responding sample members was mostly similar across survey modes. However, we did find that the oldest age group (65 or above) was more likely to respond than the youngest among the paper arm, but we did not observe this for web. This is consistent with prior research finding older individuals overrepresented among responders to a paper survey while web respondents are on average younger [63, 65] and that responders to web surveys are typically younger than those to a paper survey [64] . Due to the continuing relationship between age and web survey response patterns, using web alone to survey cancer survivors may not yet be advisable, especially since the larger population of cancer survivors is on average older than those included in this study, and only 44% of adults age 80 or above use the internet [67] . We also observed that Hispanics were less likely than non-Hispanics to respond the web survey, but this was not the case for the paper survey. This further suggests that adding a paper response option would be beneficial.\\nOur web survey was slightly different than most in that it was not email-administered. The use of a primarily postal mail-based contact protocol to encourage response may have been advantageous in helping to establish legitimacy and trust in the surveyor [40] , but we did not test this directly. In the final phone call follow-up phase of this study, we offered nonrespondents in either study arm the option to respond over the telephone, but only 3 individuals completed the survey using this method. A similar approach that uses a paper response option for nonresponders to a web survey, the web-push design, has proven effective in samples of the general public as a way to collect a majority of responses online while also providing an option for those unable to respond using the internet [63, 64] . There is some evidence that this approach may even be more effective than paper-only in certain populations who are accustomed to receiving similar communications from the surveyor via email [68] . Further, in a study of Dutch childhood cancer survivors, various strategies of offering a paper-based alternative to a web survey produced similar response rates [69] . Thus, using a paper follow-up response option to a web survey administered by a cancer registry may be an effective approach to obtain responses from those reluctant to respond to a web survey.\\nOverall, we did not find evidence that sending a brochure describing the cancer registry encouraged significantly more people to respond. However, the results suggest it could have varying effects across survey modes; while response to the paper survey was unchanged with its introduction, response was slightly higher (but not significantly so) for the web survey, thereby reducing the gap in response between the two modes. This also could be related to establishing legitimacy and trust, which may be especially helpful when asking people to respond online. Responding online entailed manually typing an unknown web address and entering an access code, and could possibly have been viewed with more suspicion or hesitation. There has [70] [71] [72] . A recent analysis of multiple studies recruiting via the Utah Cancer Registry found that inclusion of a brochure describing details of the study in the recruitment packet decreased study cooperation [32] . However, study-specific brochures are very different in nature than the type used in this study, so it is unclear whether these past results are informative for cancer registry-specific informative brochure use. Due to the relatively small effect size of the brochure in this study, it would be worthwhile to retest this comparison in a larger sample size to further evaluate its effect across modes. It is worth noting that many registries require enclosure of such brochures in research recruitment mailings in order to explain how a person's name was obtained. While our registry does not require a brochure, for registries that do, future testing may be made more informative by concentrating on variations in brochure design and contents rather than whether it is included or not.\\nIn the comparison of individual demographics of respondents to nonrespondents, for the overall sample we found differences by time since diagnosis, age, and ethnicity. These differences are consistent with prior studies which have documented demographic factors that have influenced response to surveys or other studies administered via cancer registries include Hispanic ethnicity [73, 74] , age [6, 7, 11, 21, 22, 32, 73, 75, 76] , and time between diagnosis and recruitment [6, 11, 73, 77, 78] . Unlike prior studies, we did not observe overall differences according to sex [73, 74, 77] , race [6, 7, 32, [73] [74] [75] , or cancer stage at diagnosis [6, 7, 73] . However, we did see females overrepresented among respondents to paper but not web, and Hispanics and nonwhites underrepresented among web respondents. While we were unable to fully assess response by educational attainment, we did find that web and paper respondents reported similar levels of education.\\nThere are limitations worth noting for this study. First, we were unable to offer incentives for participation, which resulted in a lower response rate than similar studies that have been conducted out of the registry. We were also unable to assess whether educational attainment or other socioeconomic variables affected response across the study arms because registries do not collect this information. Additionally, because this was a pilot study, the sample size was relatively small, making it difficult to identify significant differences for small effect sizes or to draw conclusions about various demographic subgroups included in the study. We also did not evaluate the use of a paper response option delivered later in administration, as is done in most web-push designs.\\nAnother limitation is that due to the focus of this study on early-onset diagnoses of particular cancer sites, and the oversampling of some subgroups, our sample is not representative of cancer survivors generally. Most notably, in this study of early-onset cancer survivors, only 16.8% of the eligible sample was aged 60 or older; in contrast, 61.3% of all cancers of any site diagnosed in the same time period in Utah were among individuals aged 60 or above. Additionally, with the inclusion of two female-specific cancer sites and an oversample of Hispanics, compared to the entire registry of cancer diagnoses from 2001 to 2016, our sample had a higher percentage of females (56.5% vs. 48.4%) and Hispanics (15.5% vs. 5.8%) than are included in the database. Therefore, the results may not be generalizable to other registry-based study samples. In prior research, we found that recruitment outcomes from samples obtained from this registry vary according to cancer and demographic variables [32] , thus we expect response results would vary in studies of different cancer sites or age groups. Nevertheless, the differences observed across experimental arms and demographic subgroups offer informative evidence and warrant further experimentation with web surveying in other registry samples.\",\n       'Since most mammograms are 2-D and the number of data is large, AI images can be successfully analyzed using deep-learning in natural images. The discovery of breast cancer is the detection and classification of tumor lesions, the detection and classification of micro-calcifications, and risk-scoring work, which can be effectively analyzed by CNN or RBM methods. For the measurement of breast density, CNNs for feature extraction were used, 39) and a modified region proposal CNN (R-CNN) has been used for localization. It has been reported that U-net is used for segmentation breast and fibro-glandular tissue (FGT) in MRI in a dataset and accurate breast density calculation results are observed. 40) It has been reported that a short-term risk assessment model has been developed by achieving a predictive accuracy of 71.4% by calculating the risk score by implementing the mammographic X-ray image as a risk prediction module MLP (multiple layer perception) classifier. 41) Data augmentation using GAN is being applied in various areas of the medical field. 21, 49) In liver lesion classification using CT images, it was reported that the accuracy of 7.1% was increased when the number of data using GAN was increased. 21) In chest pathology classification using X-ray image, accuracy was reported to be increased by 21.23%. 49) It has been reported that synthesized images using GAN can be a method of data augmentation in medical image analy-sis. However, more research is needed to see if synthetic images can be used for artificial intelligence learning to determine clinical diagnostics that require rigorous accuracy.',\n       nan,\n       \"Our observation shows that patients with hypertension who had previously taken ACEI/ARB drugs for antihypertensive treatment have an increased tendency to develop severe pneumonia after infection with SARS-COV-2. Hypertensive patients may need to . CC-BY-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\\nis the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.04.06.20054825 doi: medRxiv preprint adjust medications to be eliminate ACEI/ARB drugs to reduce the risk of severe pneumonia.\\nMechanistically, the increase in the zinc metallopeptidase ACE2 might help SARS-COV-2 enter the cells and accelerate its replication. ACE2 is the only known human homologue of the key regulator of blood pressure, angiotensin-converting enzyme (ACE).\\nSince its discovery in 2000, ACE2 has been implicated to play an important role in the preservation of heart function, and the balance of blood pressure, with its effects being mediated, in part, through its ability to convert angiotensin II (Ang II) to angiotensin-(1-7).\\nUnexpectedly, ACE2 also serves as the cellular entry point for the severe acute respiratory syndrome (SARS) virus and the enzyme is therefore a prime target for pharmacological intervention on coronavirus-related diseases 9 . In 2003, ACE2 was identified as the receptor for severe acute respiratory syndrome coronavirus (SARS-CoV), which mediates the infection and transmission of SARS-CoV. This effect does not rely on the protease activity on ACE 10 . Structural analysis showed that the spinous protein of SARS-CoV was in contact with the head-end of the ACE2 catalytic domain subunit I, did not involve subunit II, and did not block the active region. Once SARS-CoV is connected to ACE2, the extracellular part of ACE2 will be lysed, and the transmembrane part will be transferred into the cell, which mediates further fusion of the virus particles and host cells 11 . Through next-generation sequencing technology, the entire genome sequence of COVID-19 has been successfully determined 12 . Compared with the genomic sequences of previous coronaviruses, it is found that SARS-COV-2 has 79% similarity with SARS-CoV 13 .\\nIt is worth noting that ACEI/ARB drugs that are administered to hypertensive patients have resulted in an increased expression of ACE2 in the lungs. The role of ACE2 is opposite . CC-BY-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\\nis the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.04.06.20054825 doi: medRxiv preprint to that of ACE. ACE2 can hydrolyze angiotensin I (Ang I) to produce angiotensin (1-7) peptides, thereby generating vasodilation and lowering blood pressure. ACEI/ARB drugs may increase the protein concentration of ACE2 in rat plasma, mRNA level of ACE2, and even the activity of ACE2 in cardiac tissue 14 . Thus, it is possible that those hypertensive patients who had been taking ACEI/ARB might have an increased level of mRNA and protein expression of ACE2 in the lungs, making them more vulnerable for SARS-COV-2 entry, as well as for development of severe COVID-19 pneumonia. These findings are worth investigating further.\\nHowever, some scholars believe that the down-regulation of ACE2 expression after a neo coronavirus challenge aggravates lung injuries. This opinion mainly comes from animal model studies in multiple laboratories and a small sample clinical study. Studies have shown that ACE2 expression decreases after viral infection, causing acute lung injury; exogenous supplementation with ACE2 could alleviate acute lung injury 15 . Human recombinant ACE 2 (rhuACE2) and angiotensin II type 2 receptors protect mice from severe acute lung injuries induced by acid aspiration or sepsis 16 . It was found that patients with acute respiratory distress syndrome (ARDS) who were treated with rhuACE2 injections had rapidly decreasing levels of Ang II and increasing levels of Ang1-7 17 . The down-regulation of ACE2 caused the imbalance of ACE/Ang II and ACE2 and the activity of Ang II in the renin-angiotensin-aldosterone system (RAAS) was enhanced because of the lack of antagonism 17 . Ang II mediates increased pulmonary vascular permeability by binding to receptor AT1, and might also cause pulmonary vascular contraction, resulting in hydrostatic pressure elevation and pulmonary edema 10, 17 . More clinical evidence is required to determine whether the expression of ACE2 is down-regulated in humans after a coronavirus (including . CC-BY-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. Nevertheless, the results of our study suggest that in the widespread COVID-19 pandemic, the hypertensive population may need to quickly consider their choice of antihypertensive medications to reduce the possibility of developing severe pneumonia. Our investigation found that COVID-19 patients taking ACEI/ARB agents are younger than patients taking non-ACEI/ARB agents; this may be related to their doctor's prescribing habits.\\nHowever, we cannot directly conclude that COVID-19 patients taking ACEI/ARB antihypertensive drugs will have more severe pneumonia than those not taking ACEI/ARB drugs because many confounding factors such as coexisting diseases have not yet been excluded.\\nOur survey shows that the population with hypertension does not seem to be more susceptible to COVID-19 comparing with the non-hypertension population. Previously, a large sample study of 1099 patients showed that hypertension is a coexisting disease in only . CC-BY-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\\nis the (which was not peer-reviewed) The copyright holder for this preprint Our research has the following limitations: First, our survey only focused on Hankou Hospital in Wuhan, and the number of patients included was small; larger sample size and multi-center clinical data will be needed to support our conclusions in a future study. Second, our investigation included all clinically confirmed cases. Many patients were not tested because of the lack of nucleic acid test kits at the beginning of the outbreak. Further, there were some patients were suspected to have COVID-19 but were excluded from our study because they tested negative in the hospital; however, based on the exact epidemiological history, the patient's signs and symptoms, and chest CT examination results, we trust the reliability of the COVID-19 diagnosis in these patients. Third, due to the urgent data collection, we had a shorter follow-up time for some patients. The shortest hospital stay for patients is only > 14 days. This may interfere with the final prognosis and lead to the failure to analyze survival times. Fourth, we did not analyze the comorbidities such as shock and ARDS, because our focus is to analyze the risk associated with hypertension to provide evidence for the issue of COVID-19 prevention and treatment guidelines.\",\n       \"TDR can be a highly effective and long-term solution to mitigate impacts of coastal flooding, as long as the development parcels are moved out of harm's way-to higher, further inland, and more protected areas.\\nMuch like TDR, PDR is an effective and long-term solution; however, because participation in a PDR program is voluntary for the land owner, the programs can take a long time to implement over a large area. The PDR programs that would be most effective for limiting damage from coastal flooding are those that create large and contiguous areas with permanently preserved open space to serve as a buffer between flooding and development.\\nWhen rolling easements lead to the removal of existing structures, they can prevent future flood damages in structure no longer subject to repeated or sustained inundation, but the value of the structure is also lost. Rolling easements are, however, a long-term strategy for potentially discouraging new housing developments along shorelines because they create the expectation that shoreline development will not be protected from flooding and erosion with engineered barriers or other types of coastal armoring. Thus, rolling easements have the potential long-term effect of mitigating future floodrelated damage. Rolling easements can be most effective when implemented along undeveloped stretches of shoreline.\\nFee simple acquisition is highly effective because the public owns all rights to the land and can restrict development for as long as it maintains ownership.\\nRelocation is effective if the infrastructure is moved out of the vulnerable area. If the infrastructure is not moved completely outside the vulnerable area, the overall effectiveness depends on the exposure and resilience of the infrastructure to coastal flooding.\\nSurge barriers prevent storm surges from entering interior areas for water-level increases up to what they are built to withstand. The technology effectively reduces the height of extreme water levels in the area behind the barrier. For movable barriers, a storm surge monitoring and forecasting system must be used to ensure that the barrier is moved into position before a storm surge arrives.\\nBeach nourishment is a short-term solution that protects people and property by decreasing the energy of waves and limiting how far inland storm surges travel. Beaches must be supplemented with additional quantities of sand every few years, however, for this measure to continue to be effective. Beach nourishment is very effective against water-level increases up to the beach height; for larger events that greatly exceed the beach height, however, beach nourishment will have a minimal effect on mitigating coastal flooding, and the flooding levels will be similar to what they would be without the measure.\\nLevees can provide a high degree of protection against flooding in low-lying coastal areas. Effective levees are built with a high volume of material to resist water pressure, sloping sides to reduce wave energy, and crest heights sufficient to prevent overtopping by flood waters. Levees prevent flooding until they are overtopped with water or are breached (i.e., broken or eroded away), at which point flood waters will rise about as high as they would without a levee.\\nSandbags are usually effective at temporarily mitigating flood damage. They are most effective when used to build barriers that are less than 4 feet tall. 14 Taller or poorly constructed barriers can be prone to sliding and overturning. Sandbags are also susceptible to water seeping through or beneath the bags. Sandbags should be used to provide a basic level of protection, but they are unlikely to hold back flood waters entirely.\\nElevated development is effective in protecting buildings and infrastructure from floods at water levels lower than the base of the first floor of the raised facility and is generally effective for the expected life expectancy of the structure, which might range from 25 to 50 years.\\nFloating development is an unproven technology for modern and dense development. Theoretically, it is effective for protecting buildings from floods at water levels lower than the maximum height to which the building is designed to float. Floating development is designed to be effective for the life cycle of the building.\\nFloodable buildings are designed to be effective at preventing structural damage from floods up to a specific height. Supporting floodable infrastructure, such as a water-holding parking garage or pond, can hold a specific amount of water, making it easy to calculate how well the structure will mitigate flooding.\\nThe ability to physically move buildings outside of an at-risk area eliminates damage to buildings from flooding. The logistics of moving a large number of buildings are complex, however, because of the time involved. Being able to move buildings requires advance flood warnings as well as sufficient transportation infrastructure to support the movement of both people and buildings.\\nWhile building, parcel-sized, and more localized drainage systems can be designed to accommodate specific-size flood events, large and more regional systems are hard to design, integrate, and maintain. As such, the overall effectiveness of these systems is not very predictable.\\nUsually floodproofing is designed to reduce but not completely eliminate the amount of damage for a given level of flooding. Floodproofing involves the recognition and acceptance of certain amounts of damage for specific flood levels and is only effective up to these predetermined levels. Dry floodproofing is effective up to a certain water-level increase, after which the force of the water acting on the structure can cause it to collapse.\\nFor as long as the open space is preserved, designating vulnerable areas as undevelopable is highly effective at preventing flood-related damage in those areas by reducing the amount of infrastructure that could be exposed. Effectiveness is most often evaluated by modeling the impacts of flooding under different open-space and development scenarios.\\nThe effectiveness is directly related to the regulations that the zoning implements and the associated enforcement. For example, preventing any development is highly effective, while requiring raised and flood-proofed structures is moderately effective at preventing future damage.\\nDevelopment fees are an effective way for local governments to pay for new infrastructure to mitigate potential future impacts of coastal flooding brought about by new development. Thus, the effectiveness of the development fees depends on the success of the chosen infrastructure at mitigating flooding damage.\\nWetlands reduce the risk of property damage and loss of life from flooding through a number of mechanisms. Wetlands act as natural reservoirs, storing flood waters and then slowly releasing them, delaying and attenuating peak flood flows. Wetlands also dissipate wave, wind, and storm surge energy through resistance provided by the wetland vegetation. Studies have found that a loss of one hectare (about 2.5 acres) of wetland corresponds to an average increase of $33,000 in storm damage from hurricanes. 26 However, wetlands are less likely to be effective in mitigating the effects of very large flood events, particularly regional floods of long duration.\\nMangroves can reduce storm surge levels by slowing the flow of water and reducing surface waves. Experts estimate that each kilometer of mangrove forest provides a 5-to 50-cm water level reduction. In addition, surface wind waves are expected to be reduced by more than 75 percent over 1 kilometer of mangroves. 29,30 However, some models indicate that mangrove forests may provide less protection against intense, slow-moving hurricanes.\\nReefs function as natural breakwaters that reduce the height and energy of waves hitting the shore. According to the Nature Conservancy, in Louisiana, two oyster reef restoration projects, with a total length of 3.6 miles, are expected to reduce wave height by 51 to 90 percent and wave energy by 76 to 99 percent. 32 This reduces both shoreline erosion and storm surge flooding. Reefs are particularly effective against smaller events but may provide limited protection against erosion and coastal flooding during major storms. 33\\nDune vegetation, which acts to stabilize loose sand, is the most reliable way to protect dunes from wind and waves. Dune plants are especially effective at trapping and holding windborne sand, promoting dune growth over time. Dune vegetation also decreases the wind velocity near the ground, reducing wind erosion at the sand surface. 37 Dune vegetation root networks can also help to stabilize the dune. Ultimately, the vegetation helps preserve dunes and enhances their ability to protect the coast from erosion and coastal flooding. Dunes are effective in preventing flooding up until the water-level increase exceeds the height of the dune, or until sufficient erosion occurs to cause the dune to collapse\\nBarrier islands protect coastal communities from damage caused by waves and storm surge. Studies have indicated that the loss of barrier islands can increase wave height by as much as 700 percent during fair-weather forecasts. 40 Recent observations following hurricanes have shown that areas behind restored barrier islands weather storms much better than nearby areas. 41\",\n       'The final core collected, WM050315-04, was extracted from the Isle of Hope, roughly 30m northwest of WM050315-03 (Fig. 12). The location of the core was part of the isle prior to the construction of Diamond Causeway; its location is not part of the upland hammock Mud (clay/silt) w/some sand, fine, color 2.5Y3/1 -10YR7/1 Mud (clay/silt) w/sand, fine, plant fragments, bioturbation, color 5Y4/1 -5Y7/1 Sand, fine, w/mud and shell fragments oxidized lamination at ~350 cm BLS dipping at ~20 degrees, but no other well defined strata are present. The color of the sand does gradually lighten with depth, starting at 10YR6/2 at 20 cm BLS and lightening to 10YR8/3 at ~385 BLS. The deposits in this core fit the description of Pleistocene \"mottled\" facies described by Howard and Scott, 1983, which they interpret to be beach dune deposits.',\n       nan,\n       'We implemented SparseVM using Keras [11] with a TensorFlow backend [1] . We used the ADAM optimizer [22] with an value of 5 × 10 −5 and a learning rate of 5 × 10 −4 . We used a modified version of the U-Net-like [30] architecture proposed in the original Voxel-Morph paper [7] , which is publicly available online. The outputs of this model are a flow field and the corresponding warped moving image. We modified the inputs to this architecture to include the masks for the two input images. We implement SLCC efficiently using sparse convolutional layers [14] . Our code is available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.',\n       'table A, as well as smaller proportions of other types of schools, do not appear in table 1 because no students were reported in membership for these schools. In the 1998-99 school year, the CCD began reporting schools operated by the U.S. Department of the Interior\\'s Bureau of Indian Affairs (BIA) and the domestic Department of Defense Dependents Schools as separate entries, and they are not included in the U.S. totals shown in the tables in this report. Some, but not all, of these BIA and Department of Defense schools previously were included in the states within whose boundaries they were located. Most local education agencies are those that are typically thought of as \"school districts.\" Operated by a local school board, they provide instructional services for students and comprised almost 89 percent of local agencies in 1999-2000 (table 2). A smaller proportion, about 8 percent, were supervisory unions or regional education service agencies whose major responsibility is to offer administrative, special program, testing, or other services to school districts. Finally, around 4 percent of the reported agencies were operated directly by a state or federal government or were other than any of the preceding categories. The number of regular school districts increased by 1 percent from the 14,772 reported in 1994 to a total of 14,928 in 1999-2000. The governance of charter schools varies from state to state. In some cases, they are not considered under the administration of the regular public school district within whose boundaries they operate and are reported on the CCD with       a separate education agency for each charter school. When this occurs, the districts are reported under the category of \"other education agency\" For example, although not all states designate a separate agency for each charter school, in the District of Columbia the establishment of 27 charter schools explains why the District is shown with 28 local education agencies in table 2.',\n       'Within the financial ratios, positive correlations or synergies dominate. This is contrast to other findings with regard to the economic dimension [71]; though, the contrasting results are based on the rather qualitative SAFA method. The mostly positive correlations between ratios and the observations that financial ratios continuously increase across quarters of the performance indicators Y, Z E and Z A suggest that a reduced set of indicators could be applied. Such a reduced set could simplify data acquisition and could increase the comprehensibility and acceptance of a sustainability assessment tool. The most extreme case would be to focus on just one financial ratio. The correlation analysis (Table 2) points to financial ratios on profitability. These ratios provide a clear measure of economic performance. P1, net profitability, suggests itself as single financial ratio, since it exhibits positive correlation coefficients with all other financial ratios. The concentration on only one ratio which is highly dependent on assumptions on imputed costs, however, is also risky. Alternatively, the income per family work unit could be used.',\n       'Yet existing theoretical and empirical work is contradictory as to whether minority children in the United States are disproportionately represented in special education. Some mechanisms have been theorized to result in overidentification (e.g., Harry et al., 2008) ; others in underidentification (e.g., Coll, Crnic, Lamberty, & Waski, 1996; . Some empirical studies find that minority children are overidentified as disabled (e.g., Oswald, et al., 1999; Sullivan & Bal, 2013) . Others find that minority children are underidentified (e.g., Hibel et al., 2010; Morgan, Farkas, Hillemeier, & Maczuga, 2012; Morgan, Staff, Hillemeier, Farkas, & Maczuga, 2013; Shifrer, Muller, & Callahan, 2011; Sullivan, 2013) as well as less likely to be referred for services (Delgado & Scott, 2006) . Below, we briefly survey this contradictory theoretical and empirical work. We then identify methodological and substantive limitations in the existing empirical studies to justify the current study.\\nMechanisms theorized to result in racial-, ethnic-, and language-minority children being overrepresented in special education include cultural, linguistic, and racial bias. For example, minority children are believed to experience systemic prejudice that results in their abilities and behaviors being unjustifiably characterized as problematic and atypical (Coutinho & Oswald, 2000; Harry et al., 2008; Hays, Prosek, & McLeod, 2010; Lorsen & Orfield, 2002; O\\'Connor & Fernandez, 2006) , resulting in segregation into special education (Blanchett, 2006; Reid & Knight, 2006) . 1 Prior to possibly becoming sensitive to minority overrepresentation, teachers may have been more likely to refer minority students for special education (e.g., Hosp & Reschly, 2003) , although early evidence of this tendency is also contradictory (Skiba et al., 2008; Tobias, Cole, Zibrin, & Bodlakova, 1982; Tobias, Zibrin, & Mennell, 1983) . Schools have been theorized to use academic and behavioral standards aligned more with White, English-speaking, middle-class, privileged populations (Blanchett, Klingner, & Harry, 2009 ). Racial-, ethnic-, and language-minority children may be less likely to meet these standards as a result of receiving lower-quality educations provided at underresourced schools (e.g., Peske & Haycock, 2006) and so more likely to be identified for special education. Minority children in the United States are also more likely to be exposed to biological and environmental factors in early childhood (e.g., fetal alcohol syndrome, low birth weight, lead exposure, poverty) that disproportionately increase their risk for impaired cognitive, academic, and behavioral functioning and disability (Annie E. Casey Foundation, 2014; Donovan & Cross, 2002; Mann, McCartney, & Park, 2007) .\\nConsistent with mechanisms theorized to result in overrepresentation, Skiba et al.\\'s (2005) analyses of cross-sectional statelevel data and statistical control for school-and district-level confounds indicated that Black and Hispanic children were overrepresented in special education. This was the case for multiple disability conditions, including intellectual disabilities, emotional disturbances, speech or language impairments, and learning disabilities. Oswald et al.\\'s (1999) analyses of cross-sectional, nationally representative, and district-level data with statistical control for district-level educational, economic, and demographic confounds indicated that minorities were overrepresented in special education. Minority children were overidentified as having intellectual disabilities and emotional disturbances. Sullivan and Bal\\'s (2013) cross-sectional analyses of an urban school district and statistical control for child-and school-level potential confounds indicated that Black children were more likely to be identified for special education services. Follow-up analyses indicated that Black children were overidentified for learning disabilities, although they were no more likely than White children to be identified as having cognitive impairments, speech or language impairments, other health impairments, or emotional disturbances and were underidentified as having intellectual disabilities. Samson and Lesaux\\'s (2009) analyses of a nationally representative, longitudinal data set with statistical control for individual-and family-level potential confounds indicated that language-minority children were overrepresented in special education by third grade, although they were underrepresented during both kindergarten and first grade. Sullivan and Bal (2013) also reported that Hispanic children were underrepresented in special education.\\nOther mechanisms have been theorized to result in racial-, ethnic-, and language-minority children being underrepresented in special education Morgan, Farkas, et al., 2012; Rosenberg, Zhang, & Robinson, 2008; Samson & Lesaux, 2009) , including socioeconomic, linguistic, and/or cultural obstacles that constrain access by minority families to special education services (Danesco, 1997; Coll et al., 1996; Harry, 1992; O\\'Hara, 2003; Pena & Fiestas, 2009 ). Some minority families may prefer to rely on the social support of extended families to assist their children and so may not agree to evaluation requests for special education (Coll et al., 1996) . Cultural factors may also result in some minority groups attributing their children\\'s perceived lower academic behavioral functioning to systemic prejudice or other nonbiological explanations (Danesco, 1997; Yeh, Forness, Ho, McCabe, & Hough, 2004; Yeh, Hough, McCabe, Lau, & Garland, 2004) . The stigma associated with disability identification may further reduce a racial-, ethnic-, or language-minority family\\'s willingness to have their child identified as disabled (Hervey-Jumper, Dougan, & Franco, 2008; O\\'Hara, 2003; Zuckerman et al., 2014) , particularly for conditions considered highly stigmatizing (e.g., intellectually disabled). Minority families may experience fewer interactions with pediatricians and other health professionals who often diagnose disorders (e.g., autism) during early childhood (Palfrey, Singer, Walker, & Butler, 1987) . This may occur as a result of less access to health care (Flores & Committee on Pediatric Research, 2010; Inkelas, Raghavan, Larson, Kuo, & Ortega, 2007) as well as language barriers (Flores & Tomany-Korman, 2008; Zuckerman et al., 2014) . Children attending disadvantaged schools may also be less likely to be identified as displaying atypical academic or behavioral functioning (Delpit, 1995) . For example, Hibel et al. (2010) theorized that minority children experience \"frog pond\" effects in regard to special education eligibility. Specifically, attending poorly resourced schools may result in referrals for special education only for those children displaying unusually low academic achievement or behavior relative to other, lowerperforming children attending these same schools.\\nConsistent with mechanisms posited to result in underrepresentation, Hibel et al.\\'s (2010) analyses of a nationally representative sample followed from kindergarten entry to the end of fifth grade indicated that, following extensive statistical control, Black, Hispanic, and Asian children were less likely to receive special education services than otherwise similar White children. Follow-up analyses indicated that minority children were less likely to be identified as having learning disabilities, speech or language impairments, and intellectual disabilities. Morgan, Farkas, et al.\\'s (2012) analyses of a population-based, longitudinal sample of preschool-age children found that those who were Black or from homes where a language other than English was spoken were less likely than otherwise similar White children or those from English-speaking homes to be identified as having developmental delays or disabilities. Shifrer et al.\\'s (2011) analyses of a nationally representative sample of high school students with statistical control for individual-, family-, and school-level potential confounds indicated that Blacks were less likely than otherwise similar Whites to be identified as having learning disabilities. Yeh, Forness, et al.\\'s (2004) analyses of a large sample of children receiving mental health services with statistical control for child-and family-level factors indicated that Hispanic children were less likely to receive special education services as a result of being underidentified as having emotional disorders.',\n       \"Our first set of results explores OLS estimates of the relationship between disaggregated measures of cognitive and socioemotional skills and labor market outcomes. The first set of outcomes is for log hourly labor earnings (wage for salaried workers and net profits for selfemployed), and these results are presented in table 3. The sample includes individuals between 15 and 64 years of age (both men and women).\\nOur main finding is that, controlling for other observable characteristics such as gender, age, mother's education, and regional indicators, reading proficiency is positive and statistically significantly related to labor earnings: an increase in one standard deviation in reading proficiency is correlated with a 15-percent increase in hourly labor earning from one's main job.\\nAs for socioemotional skills, only openness to experience seems to be significantly related to labor earnings: an increase in one standard deviation in openness to experience is correlated with an 8-percent increase in hourly labor earning from one's main job. These results remain when all skill dimensions are included in the same regression. It is important to note that the estimates in table 3 do not control for educational level, and so the coefficients capture the full association between different skills dimensions and labor earnings, irrespective of whether these skills were formed at school or at work.\\nSkills also have distinctive relationships with labor participation outcomes and occupational choices, including the likelihood of being a formal worker, of being a high-skilled worker, of being employed, of being active or studying, or of having pursued a tertiary education degree. Reading proficiency is again positively related with the probability of being a formal or high-skilled worker, but socioemotional skills seem to play no role in these outcomes\\uf0beexcept for more hostile individuals having a lower probability of holding a formal job (table 4) . However, some of these socioemotional characteristics seem to be relevant for labor or educational choice paths. For example, conscientiousness and decision-making are positively related with being employed and being active or in school (table 5) . A higher scale in openness to experience, emotional stability, decision-making, and hostile attribution bias seem to matter for pursuing a tertiary education. For comparison purposes, tables 4 and 5 also include regressions that control for the educational level of the individual, the only difference being in the role of reading proficiency, which becomes nonsignificant (except for being a high-skilled worker), suggesting that educational level is the signal to which the job market responds and that it serves as a guarantee of the reading proficiency the employer is buying when hiring an individual with a given educational level.\\nTables 6 and 7 present results for different gender, age, and education subgroups. 21 The main findings are that reading proficiency (without controlling for education) remains positive and statistically significant in relation to wages across gender and age, but only among the more educated individuals\\uf0bethat is, those with at least a complete upper secondary education (nine years of schooling). By contrast, it is only related with labor force or school participation among females, young people (less than 35 years old), and less educated workers (maximum incomplete upper secondary education). As for socioemotional skills, the role of openness to experience in wages seem relevant only among males, older, and more educated workers. The role of socioemotional skills in explaining labor and schooling decisions also has different effects across subgroups. Strikingly, among males socioemotional skills do not play any role in occupational decisions.\",\n       \"In the final model (Fig. 2b) , we examined mediation effects for relationships that occurred when any variable in the path diagram was significantly correlated with two others. Potential mediations were analyzed to determine whether systemic inflammation (measured through serum CRP) significantly mediated indirect effects on brain structure of (1) Metabolic_risk and (2) APOE.\\nSEM software usually evaluates mediation effects through a method consisting of multiplying the correlation coefficients. Two disadvantages of such a procedure are (1) the absence of confidence intervals, which prevents the user from performing a formal test for the mediation effects, and (2) the absence of a decomposition of the overall effects into direct and indirect effects. On the other hand, the counterfactual methodbased on resampling of the population by bootstrapping -allows for the construction and testing of confidence intervals (Preacher et al., 2007; Zhao et al., 2010) . To evaluate possible mediation effects, we took advantage of the Onyx SEM software to export the generated values for the latent variables of our model in order to run a detailed mediation analysis separately, using the R package 'Mediation' (Tingley et al., 2014) which implements a bootstrap method to generate confidence intervals and p-values for each mediation effect decomposed into direct and indirect effects (Table 5) .\\nThe mediation analysis was performed only for the relationships that were significant to the overall model fit (Fig. 2b) . Table 5 lists the mediations analyzed, a decomposition of each total effect into an average causal mediation effect (indirect effect), and an average direct effect (direct effect). Confidence intervals were obtained through a bootstrap resampling with 1000 simulations. The mediation analysis confirms a significant mediation of CRP between Metabolic_risk and Brain_structure (Indirect effect ¼ 0.077, p ¼ 0.01; Table 5 ). There was no significant direct effect of metabolic status on brain indices (Direct effect ¼ À0.015, The standardized partial correlation coefficient (partial β) and p-values for the partial correlation of CRP and IL-6 with brain measures were obtained controlling for the effect of age, sex, education, and APOE genotype. The FDR level of acceptable false positives was set to q < 5%. Bold type indicates a statistically significant result after FDR adjustment (over 20 comparisons). #Adjusted for ICV. *Indicates that the omnibus p-value (accounting for all covariates) was <0.05 (*) or <0.005(**). p ¼ 0.86, Table 5 ). The observed relationship between APOE genotype and cortical thickness would be underestimated if the mediation of CRP between APOE genotype and cortical thickness were not accounted for. This is because APOE genotype is negatively associated with CRP (β ¼ À0.17 AE 0.064, p ¼ 0.019, Table 4), while CRP is also negatively associated with cortical thickness (β ¼ 0.19 AE 0.094, p ¼ 0.043, Table 4 ). Together, there was a significant direct effect of 0.277 (p < 0.01) between APOE genotype and Brain_structure, and a significant indirect effect of À0.038 (p < 0.01) for the relationship between APOE genotype and Brain_structure mediated by CRP (Table 5) .\",\n       'In the midst of such a global crisis all possible opportunities need to be adequately explored and leveraged. Some countries have started taking several initiatives but often in a siloed manner. In Bangladesh, both the government and a few private organizations have started rolling out toll-free (or with minimal charge) hotline numbers for providing people with authenticated information and guidance on what to do if someone suspects they are infected by SARS-CoV-2 or actually shows symptoms of COVID-19. We also have seen the viral spread of videos with animated viruses or catchy dance steps promoting hand washing, aimed at a tech-savvy generation of connected millennials. Such measures are sporadic with likely minimal impact on the pace and consequences of this pandemic.\\nA thoughtful, concerted effort leveraging existing experience and robust enterprise-grade technologies can have a substantive impact on the immediate and distal consequences of COVID-19 as well as other future health care needs. Many countries have systems in place that could be leveraged in the current emergency. Building on existing infrastructure and systems will help speed digital interventions into practice and reduce costs. As we are seeing therapeutics and vaccines against this novel coronavirus being fast-tracked, so too must we identify and accelerate the use and adoption of digital strategies such as those described in this paper. Normative agencies like WHO can help rapidly convene the expertise needed to develop the content (eg, decision logic and workflows) and data models (eg, recommended variable types and names) that would help developers expedite locally appropriate solutions that are built on validated content but also interoperable—allowing deidentified data to be rapidly pooled as part of global efforts to understand emergent pandemic threats. Unlike the 1918 influenza pandemic, which claimed an estimated 50 million lives, we are confronting COVID-19 within the context of a digital, connected planet. Digital health solutions have been reviewed and vetted by global health agencies like WHO [7] and are available to be deployed in short order through public-private partnerships. This is only possible if we move quickly, like we have for other conventional mitigation strategies, to approve their use to prepare, detect, contain, and better understand this daunting pathogen.',\n       'The CasP approach has been developed over the last three decades by Jonathan Nitzan and Shimshon Bichler. The framework does not constitute a uniform school of thought, but rather a broadly conceived philosophy of research based on a quantitative-qualitative mode of analysis and a disaggregate method of accounting. The research philosophy of the CasP framework is centered on theorizing and investigating capital accumulation as the core power process of the capitalist political economy. This approach has given rise to a burgeoning array of research projects that take the accumulation of capital and the accumulation of capitalist power to be figuratively identical. Moreover, a new generation of political economists have drawn on aspects of the CasP framework to conduct their own research. This research has led to the emergence of novel contributions to a range of fields of investigation, from retail supply chain analysis (Baines, 2014), to trade and investment liberalization (Brennan, 2013), to animal rights and antiapartheid campaigns (Cochrane andMonaghan, 2012, 2014), public debt (DiMuzio, 2007; Hager 2014), the role of fossil fuels in capitalist social reproduction (DiMuzio, 2012), investment bank power (Hager, 2012), and the capitalization of cinema (McMahon, 2013). The disparate undertakings of these researchers are brought together in one political economy network. The emergence of this network is most clearly manifest in the creation of the journal  (Hager, 2013). The intellectual roots of the CasP approach are almost as numerous as the research projects that stem from it. At this point, it suffices to outline just four key influences: Karl Marx, Cornelius Castoriadis, Thorstein Veblen and Michal Kalecki. From Marx, Nitzan and Bichler borrow the all-important insight that capital accumulation is an inherently antagonistic process that is generative of a universalizing social structure of power. Where they part ways with Marx, however, is in their conceptualization of capital accumulation. While Marx\\'s broadly bottom-up perspective gives analytical primacy to labour\\'s relationship to capital, Nitzan and Bichler suggest that labour should be one of many considerations in a top-down, disaggregate analysis of the whole gamut of social relationships that may bear on the earnings capacity of business (Marx, 1867;Nitzan and Bichler, 2009;Cochrane, 2011). In making sense of the difference between Marxist political economy and the CasP framework, it is instructive to apprehend the influence of the philosopher Cornelius Castoriadis on the latter. In a powerful critique of Marx\\'s labour theory of value, Castoriadis claims that throughout his writings, Marx\\'s vacillates between different positions. On the one hand, Marx -as the avatar of a dialectical understanding of capitalist developmentilluminates the historicity of social categories and the centrality of conflict to social change, like no other figure in economic thought. On the other hand, Marx -as someone that was deeply impressed with breakthroughs by his contemporaries in physics and chemistry -sought to find in capitalist processes underlying basic units, along with abiding \\'laws of motion\\', that are amenable to scientific investigation (Castoriadis 1984;Nitzan and Bichler 2009). The basic, universal unit for Marx is abstract labour: \"[the] productive activity of human brains, nerves, and muscles... the expenditure of human labour in general... the labour-power which, on average, apart from any special development, exists in the organism of every individual\" (Marx 1867: 134). Nitzan and Bichler distance themselves from Marx\\'s attempt to find material units of inquiry. However, they embrace his emphasis on the transformative dynamics of conflict. In so doing, they concur with Castoriadis in arguing that value is not an objective-material substance (what Aristotle calls the physis). Rather, it is social and thus derives from the norms, laws and institutions of society (the nomos). Moving from this line of argument, Nitzan and Bichler contend that the researcher ought to be open to the multiplicity of power relations that may impact the valuation of commodities. From this perspective, the changing ratios of prices and incomes within capitalism do not reflect any intrinsic property of the goods and services that are traded, whether it is understood in terms of \\'utils\\', as postulated in neoclassical economics, or \\'abstract labour\\', as argued by Marx. 1 Instead, these changing ratios are quantitative manifestations of the overall patterns of conflict that re-shape the nomos (Castoriadis 1984;Nitzan and Bichler 2009). This shift by the CasP approach from the material to the social, and from the exploitation of labour to the totality of power, owes much to Veblen\\'s conception of capital. Whereas Marx begins his theory of capital with a materialist analysis of production, Veblen\\'s conceptualization begins with \\'the state of the industrial arts\\': the immaterial assets inherited from previous generations necessary to produce socially useful goods and services. The historically contingent, and context-specific, development of the technology that makes up \\'the state of the industrial arts\\' occurs through the integration of myriad streams of information and the synchronization of numerous industrial sub-processes. Veblen contrasts the cooperation involved in the collective advancement of technology with the pecuniary impulses of business. Business, Veblen argues, strategically inserts itself at the interstices of the multiple subprocesses of industry, so as to exact tribute from the community in the form of profit, in return for granting the community access to privately-controlled, but collectively-created, productive capacity. According to Veblen, the level of tribute that is demanded by business is a reflection of the bargaining power of owners vis-à-vis the rest of the community. This bargaining power will in turn be determined by such factors as the importance of the asset, the means by which it is controlled, and the ease with which it can be substituted (Veblen, 1904;Nitzan and Bichler, 2009;Cochrane, 2011). While Veblen alludes to the redistributional dynamics of relative price changes, Nitzan and Bichler rework Veblen\\'s insights in advancing a systematic power theory of value, based on new categories and new research methods. In constructing methodological tools for the power theory of value, the analysis of the neo-Marxian economist Michal Kalecki has been particularly instructive as he is perhaps the first scholar to have tentatively sketched a distributional measurement of corporate control. This measure comes in the form of \\'the degree of monopoly\\': the quantitative proxy for market power as registered in the profit ratio of sales. In advancing the concept of the degree of monopoly, Kalecki gestures towards the view that income redistribution is not merely the consequence of market power shifts, but rather its very definition. Notwithstanding its importance, Kalecki\\'s measure clearly only pertains to the narrow economic issues of monopoly and competition. Accordingly, Nitzan and Bichler devise other measures that quantify the patterns of power that inhere in the capitalist restructuring of social reproduction as a whole (Kalecki, 1943;Nitzan and Bichler, 2009).',\n       'Familial history of obesity (FHO) increases the risk of offspring being obese [1] [2] [3] [4] [5] [6] . Moreover, it is suggested that dietary habits might be also involved in the development of obesity. Indeed, the role of individual dietary components has been the focus of considerable research in the field of obesity [7] [8] [9] . However, since foods are consumed in combinations, several authors have proposed to analyze food consumption as dietary patterns [10] [11] [12] [13] . A factor analysis approach used to derive dietary patterns reduces the complexity of diets to a few important foods and has the ability to integrate complex and subtle interactive effects of many dietary exposures [13] [14] [15] . Moreover, by using this method it is possible to examine distinct dietary patterns reflecting different dietary habits which may be related to the development of obesity.\\nOnly few studies examined dietary patterns and obesity. However, Newby et al. [16] has examined prospectively whether eating patterns are related to anthropometric changes in men and women in the Baltimore Longitudinal Study of Aging. This study showed that a dietary pattern rich in reduced-fat dairy products and high-fiber foods is inversely associated with annual change in body mass index (BMI) in women and inversely associated with annual difference in waist circumference in both women and men. Knowing that certain dietary patterns may lead to a smaller weight gain [16] and that individuals with FHO are at high risk of obesity [1] [2] [3] [4] [5] [6] , it appears imperative to investigate the relationship between FHO and dietary patterns. The objectives of this cross-sectional study were 1) to derive dietary patterns using factor analysis in a population of men and women with and without FHO; 2) to compare mean factor scores for each dietary pattern between individuals with and without FHO; and 3) to examine the association between these patterns and anthropometric, lifestyle and sociodemographic variables.',\n       \"GRT Course Features. Projects continued to develop courses and/or curricula that will remain with the GRT-funded department after the GRT project has ended. A total of 316 new courses, other institutional offerings, or course requirements were developed by the 157 GRT projects during the 1998 reporting year alone. These institutional outcomes are in addition to the 1,061 that had already been reported during the 1997 reporting year for all years of project operation prior to the 1998 reporting year. As more trainees complete their Ph.D. programs, subsequent years of GRT trend data should provide additional useful information about subsequent employment in the workforce. Along with related analyses, such data should contribute to NSF's assessment of the GRT program investment and of related agency endeavors, in support of postdoctoral activities. Finally, in addition to this report on outcomes of the GRT Program, Abt Associates has prepared a report based on site visits to selected GRT projects. This report also is available through the NSF On-Line Publications website. viii 1 INTRODUCTION\",\n       'Job search methods. Students responded to the question \"What did you do to find a job?\" The response categories were (a) sent out resume. (b) went to campus job placement office, (c) looked through want ads, (d) asked friends/family members/professors, (e) attended recruiting fairs, (f) did volunteer work in field, (g) looked at job boards in unemployment office, (h) contacted \"head hunters,\" employment agency, professional recruiter, (i) placed own want ads, and (j) subscribed to trade journals. Number of job interviews. The survey question was phrased as \"We\\'re interested in the job search strategies used by college graduates to find employment. As a result of trying to obtain a new job upon the completion of your degree, how many jobs did you interview for?\" Number of job offers. The survey question was phrased as \"How many full-time job offers resulted from your job search efforts?\" Annual salary. Respondents\\' annual salary based on their current job was computed. This composite was constructed by multiplying the sum of the salary per pay period by the number of pay periods per year. Underemployment. This measure is a composite score of four indicators including degree not required for the job, working part-time, working multiple jobs, and not much career potential. Scores ranged from 0 to 5 with 5 as the highest estimate of underemployment. Job satisfaction. This measure is a composite of nine satisfaction factors: pay, employment benefit, job challenge, working condition, opportunities for promotion, job security, supervisor, coworkers, and educational benefits. The response format for each factor was (3) very satisfied, (2) satisfied, and (1) dissatisfied. Scores ranged from 1 to 27 with 27 as the greatest indication of satisfaction.',\n       \"Massification of doctoral education has been driven by the needs of a knowledge economy and national innovation policy and has been promoted systematically by the Ministry of Education and Culture (MEC) that provides the primary source of funding for the universities in Finland. Accordingly, between the 1990s and 2010 the number of doctoral degrees completed annually tripled. Currently, about 1600 doctoral degrees are awarded annually. The number of degrees completed yearly is highest in medicine, natural, and technical sciences. Although half of doctoral degrees are awarded to women, there are still some gendered disciplinary differences (Auriol, Misu & Freeman, 2013; KOTA-National Data Base, 2009; Puhakka & Rautapuro, 2013) . Doctoral education has become more mainstream and at the same time researcher mobility has become increasingly important in national doctoral education policy. One result is an increased number of international doctoral students. To promote this inflow, the MEC provides financial support to universities to attract international doctoral students earning their degrees in Finland. However, the proportion of foreigners in doctoral training is still relatively low (14.8%). Also, the outflow of Finnish doctoral students is slightly higher than the inflow of international doctoral students studying in Finland (Garam, 2013) .\\nThe need to provide a highly skilled workforce for labour markets and the need to improve the quality of doctoral education has led to increasing professionalization of doctoral education (Niemi et al, 2011; The Graduate School Working Group, 2012) . This resulted in the introduction of more structured forms of doctoral education, that is, the launching of a doctoral school system funded by the Academy of Finland (Finnish Ministry of Education, 1997). However, by 2010 only about 50% of the doctoral student population studied in these selected doctoral schools. In 2011, a national graduate school system reform was implemented that reversed this and as a result, most universities adopted a single graduate school model to support systematic doctoral education. Now all doctoral students belong to a doctoral school in their university and to one of the university's doctoral programs. There are no tuition fees, but funding for doctoral studies is not automatically provided by, for example, the universities, projects, or foundations for the doctoral students. As a result, some students receive little or no financial support. Despite taking a stance towards a more structured system, doctoral studies are still highly research intensive rather than course centred (Niemi et al, 2011) . To promote the attractiveness and predictability of researcher careers, a four stage researcher career model (first stage being completion of doctoral degree, followed by 2-5 year post-doctoral fellow that paves the way for becoming an independent researcher, and finally professorships and research directorships in the final stage) has been introduced (Academy of Finland, 2010). Also, a tenure track system that aims to promote the shift between stages three and four has been introduced. The employment rate of the doctoral degree holders is extremely high 97.6% (Treuthardt, & Nuutinen, 2012 ) and the majority (about 80%) work at the universities or research institutions in Finland (Sainio, 2010; The Graduate School Working Group, 2012) . This may explain why, despite the emphasis on learning transferable skills in doctoral education policy documents (Academy of Finland, 2010; OECD, 2012), efforts to ensure and support work/life relevance have still remained somewhat minor at universities (Niemi et al, 2011) .\\nThe Bologna process and adaptation to the European Qualifications Framework (EQF) to increase the potential to promote international mobility and to facilitate equal participation in European doctoral programs (Berlin Communiqué, 2003; Bucharest Communiqué, 2012; European Commission, 2014) has resulted in the enhancement of quality assurance in Finnish doctoral education (The Graduate School Working Group, 2012) and engagement in international benchmarking and global ranking systems. Quality assurance developments have included setting the target doctoral completion time at four years of full-time study; however, time to graduation has remained almost unchanged at six to seven years (Sainio, 2010) , Also, launching the Finnish Higher Education Evaluation Council that carries out audits of quality systems of the universities and assists universities in thematic and research evaluations, including doctoral education, is another development.\\n• increase in number of doctoral degree holders\\n• awarding universities for attracting international students completing the PhDs\\n• launching doctoral schools and programs\\n• harmonizing doctoral degrees according to European standards (adopting Bologna qualifications)\\n• introducing four stage researcher career model and tenure track system\\n• launching international doctoral programs\\n• adopting benchmarking, and international evaluation systems\",\n       'Without a unique identifier such as a verified U.S. Employer Identification Number (EIN), the name field typically has more distinguishing power than other fields such as address. The ability of name information to distinguish pairs can vary dramatically from one set of pairs to another.\\nFor instance, in one situation, properly parsed name information, when combined with other information, may allow good automatic decision rules; in other situations it may not.\\nAs an example of the first situation, consider the 1992 U.S. Census of Agriculture in which name parsing software was optimized to try to find surnames (or suitable surrogates) and first names.\\nBecause the overwhelming majority of farming operations have names of the form given in Table   8 ,\\n[ Table 8 General business lists can signify the second situation because of the way in which the name field can be represented. For instance, the same business entity may appear in the following forms given in Table 9 :\\n[ Table 9 about here.]\\nEven if name parsing software can properly represent subcomponents of the names, it may be difficult to use the subcomponents to distinguish matches. If the name information and clerical-review status were retained, then clerical review could be reduced during future updates.\\nEach business could be represented by a unique record that has pointers to significant name variations of matches and nonmatches along with match status. If a potential update record is initially designated as a possible link because of a name variation, then the associated name variations could be searched to decide whether a record with a name similar to the potential update record had previously been clerically reviewed. If it had, then the prior followup results could be used to determine whether the new record is a match.',\n       \"Structural market models, such as the supply and demand equations we estimate here, are characterized by jointly (simultaneously) determined prices and quantities. If these equations are estimated independently, without taking into account the information provided by other equations in the system, the regressions can yield biased results.\\nOne established econometric approach for estimating simultaneous equations is two-stage least squares (2SLS) IV [24] . To implement 2SLS IV estimation for the labor supply equation, we need one or more instruments that are highly correlated with biomedical scientists' wages or with changes in the quantity of jobs available, but are otherwise uncorrelated with unobserved factors affecting the number of students enrolling in graduate programs or completing PhDs in the biomedical sciences. In their analysis of the market for engineers, Ryoo and Rosen (2004) employ the third and fourth lags of defense R&D spending as instruments, also using their ratio with total U.S. GDP, which they argue reflect changes in demand that affect the supply of bachelor's-level engineers only through their prospective future earnings [20] . For our analysis, one might presume that using lagged values of NIH R&D (again relative to U.S. GDP) would be analogous. However, because we find empirically that NIH R&D funding is in fact a strong direct predictor of labor supply as measured by graduate enrollments (Fig 5) , it belongs in the supply equation as well as in the demand equation, and thus these measures cannot be used to resolve the system identification problem.\\nInstead, in the empirical analyses that follow, we instrument for wages and job growth using the third and fourth lags of our two measures of private biopharmaceutical industry R&D expenditures, relative to GDP, as described above. The relevance of industry R&D expenditures to total biomedical sciences employment is visually apparent in Figure 7 ; however, we also provide a quantitative test of the relevance of our instruments, reporting the partial F-statistic for the excluded instruments from our first stage regressions.\\nFor these lagged industry R&D expenditure variables to be valid instruments, they also must satisfy the IV exogeneity condition. That is, industry R&D expenditures can only be correlated with graduate student enrollments and PhD completions via the market price mechanism (wages) or changes in availability of jobs. At first blush, this seems reasonable: in FY2011, NSF's Survey of R&D Expenditures at Universities and Colleges found only 4.7% of academic life sciences research was funded by industry, compared to over 63% funded by Federal government sources. Similarly, FY2011 data from the NSF-NIH Survey of Graduate Students and Postdoctorates in Science and Engineering reveals that, for PhD-granting biosciences departments and programs, over 56% of full-time graduate students supported on research assistantships were funded by Federal agencies, but only 10% of research assistantships in these departments were funded by all external private sector sources combined, which includes industry funded R&D as well as nonprofit organizations (e.g., foundations).\\nIn addition, although exogeneity of proposed instruments can never be definitively proven, when the number of plausibly exogenous instruments exceeds the number of problematic, endogenous explanatory variables, analysts can employ Hansen's overidentification test to assess whether the evidence supports exogeneity. Whenever possible, we therefore also report these overid test results for our IV models.\\nFinally, in principle the microdata sources we use here would permit calculation of standard errors for each of the surveyspecific annual statistical estimates we derive and use in construction of our analytic dataset. However, when combining multiple microdata-based estimates, variation in precision due to differences in sample size across datasets, as well as within a given variable over time, can make statistical inference problematic, if one relies on the traditional least-squares assumption of constant variance. In the analysis that follows, we begin by treating the year, itself, as the unit of observation, and estimate survey-weighted averages and sums using microdata for each year. Because the individual surveys themselves are either population (census) surveys, or nationally representative samples, there is no reason to suspect these microdata would generate biased estimates of means or related linear combinations. Then, we present results for all models with their statistical significance tests based on corrected standard errors, robust to arbitrary heteroskedasticity.\",\n       'Mathematical training should follow certain principles (Hellmich 2007; Langhorst et al. 2013 ). In the following section, we will describe the training programme and its features along important principles for early numerical education. A typical structure of a Meerkat Maths chapter is demonstrated in Figure 1 .',\n       \"There are several limitations in the current phantom that could be addressed in future work. Notably, our method for selecting the PVA solution concentration and the number of FTCs relies on a subjective assessment. A better approach may be to measure the rheological characteristics of live human brain tissue using MR or US elastography or through direct intraoperative mechanical characterization and use these results to guide the choice of PVA concentration and number of FTCs. This would allow for the ability to select the PVA concentration and FTC as a function of the desired physical and imaging properties of the material. Experiments quantifying the rheological properties of human brain will have to account for the physiological state of the individual patient since different factors such as blood pressure, the administration of pharmaceuticals (e.g., mannitol), or other physiological conditions 27 can dramatically change the rheological properties of the human brain.\\nNevertheless, the concentration and number of freezethaw cycles chosen in this study resulted in a texture that was qualitatively comparable to live brain tissue when palpated by an experienced neurosurgeon. Moreover, the phantom created here had rheological characteristics that were similar to those found in the literature in terms of Young's modulus for live human brain and it deforms more realistically than previously proposed deformable brain phantoms in the literature.\\nThe phantom proposed in this study could also be improved by devising a method of simulating heterogeneous tissue, since the current version only allows for homogeneous simulated tissue with discrete punctuate insertions. Doing this would make it possible to simulate different brain tissues (e.g., white matter, cortical grey matter, deep grey matter). More sophisticated phantom casting techniques would also make it possible to simulate white matter tracts and blood vessels.\\nFinally, it was found that over time the CuSO 4 MR contrast agents tended to diffuse or leak from the landmark spheres into the surrounding tissue. This difficulty could be resolved either by sealing the landmark spheres to eliminate leaking, or find MR contrast agents that will not diffuse out of the spheres.\",\n       'In more complex datasets, the trained predictive models are often found to have high instability, a phenomena characterized by Breiman [68] , where many distinct models involving different feature subsets can achieve comparably good training or testing prediction accuracy. Ensemble learning was proposed to address the issue by aggregating over a large set of competing base learners. Base learners are predictive models trained separately or sequentially and are often weighted based on their prediction performance. The final prediction is thus decided through majority voting for classification and averaging for regression tasks.\\nBase learners are usually generated from training data by one or multiple learning algorithms, resulting in a homogeneous or a heterogeneous ensemble. The learning algorithms can be any classification or regression algorithm. In the most common ensemble learning algorithms, a homogeneous ensemble is comprised of diverse classification and regression trees (CART) [69, 70] . CART typically use internal nodes to represent features and use the best feature value cut-offs to spit samples into branches to reach leaf nodes representing the class labels (for classification) or target variable (for regression).\\nThere are different mechanisms that can effectively construct the ensemble of base learners. The most commonly-used ones are bagging and boosting. Bagging is short for bootstrap aggregating and uses bootstrapped samples of the training data to train independent decision trees [69] . A bootstrapped training set is obtained by randomly sampling the training data with replacement. Therefore, a training sample may have multiple copies or not be present in a bootstrapped training set. Each bootstrapped training set is used independently to derive one decision tree. Bagging then decides the final prediction/regression outcome by majority voting or averaging these decision trees. The random forests (RF) algorithm is the most well-known ensemble learning method that employs bagging.\\nBoosting, on the other hand, constructs an ensemble of base learners by deriving a new learner through improving the previous one in a sequential fashion [71, 72] . Boosting in fact refers to a class of such iterative ensemble techniques, among which gradient boosting machine (GBM) is a very popular and powerful boosting algorithm [73] . In GBM, at iteration i, a new decision tree approximation F i is derived by adjusting the decision tree approximation F i−1 using the gradient of the loss function ∇L(y, F i−1 ), where y is the expected outcome.\\nEnsemble learning has been reported to have stronger generalization abilities in comparison to other machine-learning algorithms that use single predictive models [70] . The search for a single optimal model might be imperfect especially for complex, noisy and incomplete training data, and thus, using multiple separately trained or sequentially evolved models may give a good approximation of the true nature of the data. Ensemble learning has seen increasing applications to a variety of machine learning problems and could be a powerful analysis tool for metabolic marker discovery given the complexity, high-dimensionality and incompleteness of metabolomic data.',\n       \"Finally, the reliability of several scales previously created from student, teacher, and school administrator data files was assessed. Many of these scales were created by the NELS:88 data collection contractor and are included in the public use data files. Other scales such as the teacher engagement, academic press, discipline climate, and student behavior scales, were created by MPR Associates for special analyses of the NELS:88 data. The inter-item reliability of these items was explored using the criteria of Cronbach's Alpha. In addition, the reliability and the dimensionality of these scales for different groups of students was also exploied. Comparisons with High School and Beyond. Many of the items in the NELS:88 questionnaire are similar (and in some cases identical) to the items used by Fetters et al. to evaluate the quality of student responses to the HS&B questionnaires. Therefore, in some instances the validity of the NELS:88 data was compared with the validity of the HS&B data. However, caution should be used in interpreting these comparisons. There are several differences in the context, population coverage, and pattern of nonresponse between the two datasets that preclude strict comparisons of NELS:88 and HS&B. For example, the 8th-grade population surveyed by NELS:88 is somewhat more heterogeneous than the 10th-grade population surveyed by HS&Bjust as the 10th-grade population is more heterogeneous than the 12th-grade population. Dropoutsthose persons lost between the 8th, 10th, and 12th gradesare disproportionately the least reliable reporters. Hence, the HS&B data should be more reliable because more of these less reliable students have dropped out by the 10th grade. Furthermore, the response rate for the NELS:88 base-year survey was much higher (93 percent) than for the base-year HS&B sophomore or senior cohorts (81 and 84 percent respectively). In addition, the last sections of the HS&B questionnaire had a nonresponse rate of more than 20 percent, while the nonresponse rates in the last portion of NELS:88 was 7.5 percent.24 Since the least reliable respondents tend to be less likely to participate and less likely to finish the questionnaire, the principal contributors of poor data quality are more likely to have been filtered out of HS&B.\",\n       \"National policymakers and the public at large have increasingly recognized that the prosperity of the United States depends on the successful functioning of the American education system. There is also growing awareness that school reform efforts cannot focus solely on the secondary and postsecondary years but must pay attention to the elementary and preschool years as well. Increased policy interest in the early grades and the early childhood period is reflected in an intensified recent national policy aimed at ensuring that children are capable of reading by the third grade, providing college student and adult volunteer tutors for children who are having difficulty learning to read, and preparing children to succeed in school with improved Head Start and early childhood development programs. Efforts to expand and improve early education will benefit from insights gained through analyses of data from the large scale, nationally representative ECLS-K data and the study's longitudinal design. The ECLS-K database contains information about the types of school programs in which children participated, the services they received, and repeated measures of the children's cognitive skills and knowledge. The ECLS-K database also contains measures of children's physical health and growth, social development, and emotional well-being, along with information on family background and the educational quality of their home environments. As a study of early achievement, the ECLS-K allows researchers to examine how children's progress is affected by such factors as placement in high or low ability groups, receipt of special services or remedial instruction, grade retention, and frequent changes in schools attended because of family moves. Data on these early school experiences are collected as they occur, with the exception of their experiences before kindergarten, which were collected retrospectively. This produces a more accurate 1-5 measurement of these antecedent factors and enables stronger causal inferences to be made about their relationship to later academic progress. The ECLS-K enables educational researchers and policy analysts to use a variety of perspectives on early childhood education, using techniques such as multilevel modeling to study how school and classroom factors affect the progress of individual children. The data collected enable analysts to examine how children's status at school entry and performance in school are determined by an interaction of child characteristics and school and family environments. Data collected during the kindergarten year serve as baseline measures to examine how schooling shapes later individual development and achievement. The longitudinal nature of the study enables researchers to study children's cognitive, social, and emotional growth and to relate trajectories of change to variations in children's experiences in kindergarten and the early grades. The spring-third grade data collection can be used to describe the diversity of the study children and the classrooms and schools they attend. It can also be used to study children's academic gains in the years following kindergarten and first grade. The ECLS-K sample includes substantial numbers of children from various minority groups. Thus, the ECLS-K data present many possibilities for studying cultural and ethnic differences in the educational preferences and literacy practices of families, the developmental patterns and learning styles of children, and the educational resources and opportunities that different groups are afforded in the United States.\",\n       'Continuing teachers were defined as public school teachers who continued teaching in any school (public or private) from one year to the next.',\n       \"As figure 2 shows, the shift toward a higher percentage of production taking place on large farms has been accompanied by an increase in the number of those farms, which more than doubled from 1992 to 2012. Distribution of farm numbers, production, and acres operated in 2014  2). Meanwhile, very-low-sales farms increased by over 60 percent during the period. Some of this increase may reflect greater efforts by NASS to count all farms, which has had the largest impact on the numbers of very-low-sales farms and point farms. 5 Previous work by Hoppe et al. (2010) analyzed small farms in detail. They found that small farms are largely residential, that many are point farms, and that they rely heavily on off-farm income. Small commercial farms have been able to persist, but often gradually go out of business because of the age of the operator and the farm's unprofitability. The decline in small commercial farms may be tied to both the increasing age of principal operators and their marginal profitability.\",\n       \"The correlation coefficient associated with residuals (ρ) from both adoption equations, ASC technologies and AG systems, was positive and statistically significant at the 1% level for all evaluated regressions, supporting the hypothesis that the error terms in the ASC and AG equations are correlated. Therefore, a bivariate probit-regression approach was appropriate for this analysis. This approach supports the hypothesis of the complementary relationship between these technologies and therefore incorporates this information into the parameter estimates associated with both adoption equations.\\nThe model-selection results from the three bivariate probit regressions evaluated in this study (e.g., no field geometry measure, SUMIRR, MEDIANIRR included in the ASC adoption equation) are reported in Table 5 . The AIC and BIC values were smallest for the bivariate probit regression that included LOGSUMIRR, suggesting that this field geometry measure is the most appropriate for this analysis. Results from the bivariate probit regression that included LOGSUMIRR as a measures of field shape irregularity are presented in Table 6 . Parameter estimates from individual probit regressions are also included in Table 6 for comparison purposes. Given that the error terms from both adoption equations are correlated, as suggested above, the joint maximum likelihood procedure used by the bivariate probit regression approach is a more efficient procedure for estimating the parameters associated with each variable included in the regression model (Wooldridge, 2010 The marginal effects for the joint probability of adopting both ASC and AG and marginal probabilities of adopting either ASC or AG are presented in Table 7 .\\nResults suggest that the farm business characteristics related to farm size (AVHA) and field geometry (LOGSUMIRR) positively influence the joint probability of adopting ASC and AG. Producers with an additional 100 hectares of cropland harvested were 2% more likely to adopt both ASC and AG, and a 1% increase in SUMIRR increases the likelihood of jointly adopting ASC and AG by 3%.\\nProducer characteristics influencing the probability of jointly adopting ASC technologies and AG systems include education level (BGDEDUCATION), age of producer (AGE), the use of farm dealers to gather information about PA technologies (FARMDEALER), and use other PA technologies (OTHER_TECH). Producers with a bachelors or graduate degree were more likely to adopt both ASC technologies and AG systems (Table 7) . Consistent with previous literature, older producers were less likely to jointly adopt ASC and AG systems. Finally, the use of farm dealers to obtain PA information increased the probability of jointly adopting ASC technologies and AG systems by 12%. Finally, those producers who have already adopted other PA technologies are about 28% more likely to adopt both ASC technologies and AG systems. The signs of all variables were consistent with the hypothesized sign values.\\nThe marginal effects associated with the likelihood of adopting only ASC or AG technologies were also evaluated. Similar to the results presented above older producers, who are more educated, use farm dealers to obtain PA information, and with larger harvested acreage are more likely to adopt ASC technologies and AG systems. Additionally, farms located in counties with more irregularly shaped farm fields are more likely to adopt ASC technologies. Specifically, a 1% increase in SUMIRR increase the likelihood of adopting ASC technologies by about 4%. Note. a Numbers in parenthesis are robust standard errors. *, **, and *** represent statistical significance at 10%, 5%, and 1% levels, respectively.\\nOverall, results from the bivariate regression presented above indicate that adopters of ASC and AG were likely to be younger, more educated, harvest more crop hectares, and use farm dealers as a PA information source than non-adopters. As hypothesized, farm operations located in counties that were more likely to have fields with higher levels of shape irregularity may be more likely to adopt ASC technologies. jas.ccsenet. A producer's decision to adopt ASC technologies and AG systems is influenced by farmer and farm business characteristics. These include crop hectares harvested, farmer age, educational attainment, and the use of farm dealers to obtain PA information. Producers who are older are less likely to adopt ASC or AG, which follows the hypothesis that these producers have shorter planning horizons than younger producers and are, therefore, less likely to make changes in their production systems. Additionally, consistent with previous literature (Martin et al., 2007; Banerjee et al., 2008; Velandia et al., 2013) , producers with larger farms are more likely to adopt ASC and AG due to their ability to spread the cost of the technology across more hectares.\\nIt is important to note that using farm dealers as a source of PA information has the second greatest impact, after adoption of other PA technologies, on the adoption of ASC technologies. This result may reflect the effectiveness of farm dealers in persuading producers to adopt PA technologies, but may also imply more informed producers are making better decisions for their farms. Consistent with the latter implication, education has the third highest effect on adoption probabilities, implying that a combination of higher levels of education (i.e., bachelors or graduate degree) with good information may help producers better understand the benefits of adopting ASC technologies. Additionally, more informed producers who are able to process complex information, may be more inclined to adopt these technologies in counties with more irregularly shaped fields. Although farm size and age influence ASC adoption decisions, their influence seems to be less important than the use of farm dealers as a source of PA information and field geometry.\\nThough results from this study suggest farms located in counties with more irregularly shape fields are more likely to adopt ASC technologies, the variable used in this analysis to measure field geometry should be used with caution. The lack of farm-level data on field geometry led to the creation of field geometry measures using the CDL data. This approach was not validated by matching fields obtained from the CDL data with actual field data from individual farms because these data were not available. Until the procedure used in this study to create the field geometry measures is validated, SUMIRR should be used with caution when evaluating field geometry at the farm-level. On the other hand, in the absence of field geometry information at the farm level, SUMIRR may be the best measure of field geometry when evaluating the likelihood of farms in a specific region to adopt and benefit from ASC technologies.\",\n       \"Not surprisingly, given the symbolic importance of puberty as a life course marker, issues of puberty and pubertal timing have long been a central focus of research on adolescence, including this decade (Ellis, 2004). Although viewed as an adolescent experience, it actually is part of an often prolonged life course process connecting childhood to adolescence and adolescence to young adulthood. As such, the declining age of puberty in the U.S. is significant, in that it could very well lead to a reconceptualization of what adolescence is and when in the life course the boundaries between childhood and adolescence are set (Herman-Giddens 2007). In this context, two influential but largely disconnected literatures related to pubertal timing, one concerning the antecedents of puberty and the other concerning the consequences, should be viewed together. Within the complex interplay of biology and environment that sets the start of puberty, family adversity-including instability in parents' relationships-has been identified as a fairly consistent accelerator of pubertal timing (Belsky, Steinberg, Houts, Friedman, DeHart, Cauffman, Roisman, Halpern-Felsher, Susman, & the NICHD Early Child Care Research Network, 2007). Turning from antecedents to consequences, the many developmental problems associated with early pubertal timing for girls (e.g., risky sex, substance use) are well-documented. Included in this phenomenon are a range of academic troubles that, given the highly cumulative nature of the American educational system, have potential to translate short-term behavioral disruptions into long-term life course disadvantages. For example, work by Cavanagh, Riegle-Crumb, & Crosnoe (2007) with the National Longitudinal Study of Adolescent Health (Add Health) echoes the educational work described above in that it demonstrated how the temporary disruptions of early puberty during middle school can negatively affect adolescents' high school starting points in ways that are difficult to reverse. The result is a lower end-of-school academic standing for early maturing girls years after the pubertal transition is complete. Connecting these literatures suggests how puberty during adolescence can link family disadvantages in childhood to socioeconomic disadvantages in adulthood in the U.S., echoing previously reported links between adolescence and adulthood in Sweden (Magnusson & Cairns, 1996). This possibility needs to be studied more explicitly. The links between pubertal timing and relationship experiences also indicate the great potential for developing better understandings of continuity and change by taking a crossstage view. For example, other analyses with the Add Health sample indicate that early maturing girls are more involved in romantic relationships and make earlier transitions to sex than later maturing girls (Cavanagh, 2004;Haynie, 2003). These romantic and sexual experiences in adolescence in turn anticipate the family formation processes that occur in young adulthood, with strong implications for well-being and status attainment. Adolescents involved in romantic relationships in high school, particularly ones involving sex, are more likely to form unions (cohabit or marry) in early adulthood; those who experienced nonromantic sexual relationships are more likely to cohabit in early adulthood, but are not more likely to marry (Raley, Crissey & Muller, 2007). Pubertal timing is not only associated with the formation of romantic and sexual relationships in adolescence. It may also combine with these experiences to affect other long-term outcomes for youth, as has been shown for trajectories of depression across adolescence into young adulthood (Natsuaki, Biehl, & Ge, 2009). Romantic relationships in adolescence offer the opportunity to develop skills useful for forming committed relationships later and provide practice managing multiple social roles that include boyfriend/girlfriend, but they also pose concurrent and long-term risks depending on the development of the adolescent in question and the contexts in which he or she comes of age (Furman & Shaffer 2003). Future research efforts should include studying these cross-stage trajectories and their contexts.\",\n       \"The general purpose of weighting survey data is to compensate for unequal probabilities of selection and to adjust for the effects of nonresponse. Weights are often calculated in two main steps. In the first step, unadjusted weights are calculated as the inverse of the probabilities of selection, taking into account all stages of the sample selection process. In the second step, these initial weights are    nonparticipants. This column also includes 26 parents who were found to be out of scope for reasons described in section 3.1.5. F2: Parent Component Data File User's Manual Asian students, public schools 0.8 7. Other students, public schools 0.5 8. White students, public schools, high socioeconomic status 0.3\",\n       'c Includes all schools that serve grades nine through twelve, but no grades prior to sixth, and only schools from states with charter schools that responded to the school questionnaire (201 charters schools and 1,943 traditional public schools). Sum of responses to proportion of parents participating in open house, schoolwide parent-teacher conferences, and subject area events, converted to a standard score with a mean of zero and standard deviation of one using the mean and standard deviation for all traditional public schools.\\ne Sum of responses to proportion of parents involved as volunteers in school, in instructional issues, in governance, and in budget decisions, converted to a standard score with a mean of zero and standard deviation of one using the mean and standard deviation for all traditional public schools. **Statistically different from traditional public schools at .05 level.\\nschool-as control variables. We hypothesize that these variables would be related to the level of parental involvement in a school regardless of the institutional arrangements under which the school operates. The remaining measures represent the organizational and institutional characteristics of charter schools (small size, autonomy, parent-school matches, and competition for students) that provide the mechanisms through which charter schools might elicit higher levels of parental involvement. Table 4 shows that, among primary schools, charter schools are almost twice as likely and, among secondary schools, are more than three times as likely as traditional public schools to be located in urban areas. Charter schools are also significantly less likely to be located in rural areas. Compared with traditional public schools, charter schools serve higher percentages of minority and low-income (as measured by free-lunch eligibility) students, although differences in the percentage of low-income students is significant only for secondary schools. 9 Among primary schools, charter schools have slightly lower percentages of limited English proficient (LEP) students than traditional public schools. There is no significant difference between sectors at the secondary level.',\n       'Quality assurance on the plane consisted of checking the photographic settings with an automatic exposure control, measuring sun angle with an indicator, plotting the flight lines, employing an intervalometer to provide 60/20 photographic overlap, coordinating scale/altitude/film/focal length to ensure a minimum mapping unit of 1 meter, and in flight pilot assessment of cloud cover over the target area (Walker, 1996).',\n       'The mechanisms through which expression of CSPB confers drought tolerance in the field continue to be explored. Early field trial evidence for these hybrids grown under water-limited conditions immediately prior to pollen shed show they increased leaf growth, chlorophyll content, and photosynthesis rates relative to non-DT controls. These hybrids also showed improvements in the number of plants with kernel-bearing ears and number of kernels per plant (Castiglioni et al., 2008). More recent research points to the role of reduced leaf growth during silking, which slows water taken up by the plant and thus increases water use efficiency. With these simultaneous reductions in stress, the plant devotes more energy to ear growth. These larger ears had increased kernel development, leading to higher corn yields under water-limited conditions than non-DT controls (Nemali et al., 2015).',\n       'The evaluation of the quality of a thematic map derived by an image classification should ideally be based on a set of criteria defined in advance of its production. As concern is typically focused on the accuracy of the classification, commonly its overall accuracy, the definition of a minimum level of accuracy required provides a simple criterion on which to base the evaluation of classification quality. Thus, classifications are often evaluated in relation to the magnitude of their estimated accuracy. A target accuracy value should be stated prior to undertaking the classification, not least because this reduces the potential for very subjective post-classification evaluations undertaken on a poorly justified ad hoc basis. Although a target accuracy is often not stated explicitly, one value that has been widely used as a target in thematic mapping via an image classification is to achieve an accuracy of \\uf0b385% correct allocation (e.g. McCormick, 1999; Scepan, 1999; Wulder et al., 2006) ; it is very rare to see any other target value specified in the literature. Sometimes this 85% target is qualified further to indicate that the component classes of the classification should be classified to comparable levels of accuracy. However, it is against this 85% target that the acceptability of thematic maps derived from remote sensing is commonly assessed. Indeed, the 85% target is often viewed explicitly by some as the standard of acceptability for thematic mapping from remotely sensed imagery (e.g. Wright and Morrice, 1997; Abeyta and Franklin, 1998; Brown et al., 2000; Treitz and Rogan, 2004) .\\nThe 85% target accuracy often seems to be used without question of its suitability and simply because there is some historical tradition associated with it. This target is sometimes stated without apparent need for justification or provision of supporting evidence from the literature, it is essentially seen by many as a universal standard for thematic mapping in remote sensing (e.g. Fisher and Langford, 1996; Weng, 2002; Rogan et al., 2003; Bektas and Goksel, 2004) . It is not surprising, therefore, that the 85% target has been used in studies spanning a vast range of applications including the mapping of broad land cover classes at a global scale from 1 km spatial resolution NOAA AVHRR imagery (Scepan, 1999) , mapping of very detailed classes such as those depicting variations in forest species cover at a very local or large cartographic scale such as ~1:5,000 from aerial photography (McCormick, 1999) and assessments of change detection with 30 m spatial resolution Landsat TM imagery . The studies reported in these three examples differ greatly in terms of the nature of the classes, the scale of the study and the characteristics of the remotely sensed data used, yet all adopted the same 85% accuracy target.\\nIn many cases the origin of this 85% target accuracy can be traced back to the influential work of Anderson et al. (1976) . Indeed this work is often cited explicitly in relation to the specification of the target accuracy in many projects (e.g. Fisher and Langford, 1996; Kaminsky et al., 1997; Rogers et al., 1997; Wright and Morrice, 1997; Brown et al., 2000; Franklin et al., 2001; Lewis and Brown, 2001; Carranza and Hale, 2002; Yang and Lo, 2002; Weng, 2002; Rogan et al., 2003; Shao et al., 2003; Kerr and Cihlar, 2004; Treitz and Rogan, 2004; Mundia and Aniya, 2005; Yang and Liu, 2005) . However, Anderson et al. (1976) do not discuss the matter in great detail or set out to propose a universally adoptable set of map evaluation criteria. For example, in the 28 pages of the article there is little discussion of the map accuracy criteria as the main focus was on the classification system. Indeed, within the article there are actually only two references to the magical 85% figure in the report (both p5), with the reader directed to an earlier publication by Anderson (1971) for further information. Anderson (1971) also only briefly discusses the map evaluation criteria. The main focus of both the Anderson (1971) and Anderson et al. (1976) articles was on the classification schemes that could be used with remotely sensed data and not on the evaluation of the accuracy of the derived classifications, although that was clearly an important issue. Both of the articles were explicitly tentative in their proposals, aware that the sensing technology was rapidly developing (the articles were written around the time of the launch of the first Earth resources satellite system, Landsat 1) and that it is unlikely that there is one ideal approach to promote.\\nFurthermore, both Anderson (1971) and Anderson et al. (1976) were explicit in relation to the nature of the thematic map under study and have a reason for the 85% figure, which is specified for a particular application scenario. That scenario was the mapping of broad land cover classes, such as those at Anderson level I (e.g. urban, agriculture, forest, water etc.) , at small cartographic scales in the range of 1:250,000 to 1:2,500,000. Moreover, the suggestion made was that \"The minimum level of interpretation accuracy in the identification of land use and land cover categories from remote sensor data should be at least 85%\" and that the \"accuracy interpretation for the several classes be about equal\" (Anderson et al., 1976; p5) .\\nThus, at the possible risk of misinterpreting the intended meaning, the focus was also not on overall classification accuracy but on what would be referred to today as a producer\"s accuracy.\\n8 This is not the emphasis used in some studies that quote the 85% target accuracy. Additionally, the basis of the 85% target was because this would be comparable to the accuracy of land cover maps derived from aerial photograph interpretation undertaken previously in work associated with the USDA\"s Census of Agriculture. That is, an aim was to emulate the accuracy that could be achieved for a specific task through the application of conventional approaches such as aerial photograph interpretation. Additionally, it must be recognised that the minimum mapping unit for mapping at the specified small cartographic scales is several hundred pixels in size. If, for example, it is assumed that the smallest unit to be depicted on a thematic map is 2.5 x 2.5 mm in size, the minimum area mapped at a scale of 1:500,000, which is appropriate for mapping at Anderson level I, is 150 ha (Lillesand and Kiefer, 2000) . Thus, in mapping from 80 m spatial resolution Landsat MSS imagery, the type of data considered by Anderson et al. (1976) , the smallest mapped area would comprise at least 234 pixels. Although the component pixels of the unit mapped might differ in terms of class of allocation the unit would be given a single label (e.g. dominant class). This is entirely sensible as the map is a generalization of reality but also highlights the inappropriateness of some pixel based evaluations of image classifications derived from remote sensing.\\nThe map evaluation criteria put forward by Anderson et al. (1976) were not proposed as being universally applicable. In the context of satellite remote sensing, the 85% target accuracy was, essentially, specified by Anderson et al. (1976) for mapping broad land cover classes (Anderson level I, 9 broad classes) from Landsat 1 sensor data (e.g. MSS with 80 m spatial resolution in 4 spectral wavebands). The criteria proposed were not, for example, suggested for detailed class mapping of local regions from imagery of the type available from contemporary satellite sensing systems. It is also questionable whether the 85% target is appropriate for other small scale mapping applications. For example, the 85% target was used in relation to the IGBP DISCover global land cover map (Scepan, 1999) yet this map contains 17 classes and was derived mainly from NOAA AVHRR data with a 1 km spatial resolution . Direct comparison between the IGBP DISCover mapping programme and that envisaged by Anderson et al. (1976) is difficult (e.g. the generation of the IGBP DISCover map used multi-temporal data and some ancillary information). However, it is evident that the Anderson et al. (1976) proposal was made in relation to mapping a small number of classes from, what may be considered in this context to be, fine spatial resolution multispectral data with a relatively large minimum mapping unit which is very different to the scenario used in the production of the IGBP DISCover map, the assessment of which was also based on pixel level evaluations (Scepan, 1999) . Although generalization is difficult, particularly because of inter-linkages between spatial and categorical scale (Ju et al., 2005) as well as a high degree of context dependency, classification accuracy commonly, but by no means always, declines with an increase in the number of classes (e.g. Foody and Embashi, 1995; Joria and Jorgenson, 1996) and/or a coarsening of the spatial resolution of the data (e.g. Irons et al., 1985) . An increase in the detail of the classes is, therefore, generally associated with a reduction in classification accuracy (e.g Stehman et al., 2003) . Note, for example, Vogelmann et al. (2001) report a 21% decrease in the accuracy for part of the US National Land Cover Data set when moving from the very general Anderson level I to the more detailed Anderson level II. It, therefore, seems reasonable to expect that achieving the 85% target would be more of a challenge for the IGBP DISCover map than the scenario presented by Anderson et al. (1976) , from which the target value stems. Indeed, in direct comparative studies of mapping at Anderson level I, Landsat MSS data have been used to derive more accurate classifications than NOAA AVHRR data, especially if the landscape mosaic is heterogeneous (Gervin et al., 1983) . In many contemporary mapping applications, the challenge encountered may also be more difficult than that presented by Anderson et al. (1976) , commonly a result of trying to map a large number of relatively detailed classes and often at a relatively local, large cartographic, scale. Consequently, in such applications the use of the 85% target suggested by Anderson et al. (1976) may be inappropriate as it may be unrealistically high for the application. Moreover, as mapping scenarios vary enormously in terms of key variables (e.g. scale and legend detail) and the difficulty of mapping is an interactive function of the classes (e.g. their number, detail, spatial arrangement etc.) and the remote sensor data used (e.g. spatial resolution, time of acquisition etc.), there probably is no single accuracy value that could be adopted universally as a target. Critically, the widely used target of 85% should not automatically be used as a criterion for the evaluation image classifications (Laba et al., 2002) .\\nIt may be that 85% is often a perfectly reasonable target to adopt but it should not simply be accepted for use without question as for many mapping applications it may be unrealistically high.\\nIt should be clear, therefore, that the main application scenario of Anderson et al. (1976) , from which the widely used 85% target accuracy appears to have arisen, is very different to many image classification analyses that have adopted the 85% target. Many studies seek to map detailed classes at a large cartographic scale (Wilkinson, 2005) . Such classes and scales were explicitly outside the scope of discussion of Anderson et al. (1976) who suggested that substantial amounts of ancillary information would be required for this type of mapping scenario. Yet much of the remote sensing community appears to have latched on to the 85% target accuracy as some general criterion to apply, irrespective of the specific nature of the analysis in-hand. Additionally, the community of map users seems to have followed suit and appear to have adopted the 85% target too. It is unclear why the 85% target has been used so widely, especially as it may not be realistic. If, for example, the aim is to map a small number of very spectrally separable classes then the target should perhaps be set at a higher value.\\nAlternatively, and perhaps more commonly, if there are many classes that are only subtly different it seems reasonable to ask if the target accuracy is too high and unachievable. To be of value, a target should really be specified for the particular application in-hand and be realistic.\\nInstead of seeking a single universally applicable target value, it would often seem to be more appropriate to set a target for the specific application in-hand; for general purpose maps, producer\"s can provide accuracy information to enable user\"s to determine the data set\"s suitability for their specific needs. The target value to adopt may be expected to vary as a function of variables such as the nature of the remotely sensed data set used (e.g. spatial and spectral resolution), the classes defined (e.g. number and detail of classes) and user needs (e.g. tolerance to error and impacts of variation in error severity). There are, therefore, no universally defined accuracy standards for thematic mapping from remote sensing (e.g. Loveland et al., 1999; Kerr and Cihlar, 2004) . However, since accuracy relates fundamentally to the fitness for purpose, it should be possible to define the level of accuracy required for the application inhand. This accuracy value represents the minimum required for the application, it may be less than the accuracy level wanted by users but is sufficient to meet their needs. The required degree of accuracy may also be relatively low. For example, in testing scientific hypotheses about tree species diversity and co-existence, Atkinson et al. (2007) required maps showing the spatial distribution of ash and sycamore trees in a mixed woodland. Although tree species may be considered to form very specific classes, more detailed than those at Anderson levels I and II, trees can sometimes be identified to species level with a high accuracy from remotely sensed data. However, a high accuracy may not actually be required. Indeed, for the seemingly complex application of mapping detailed classes, each representing an individual tree species, such that the degree of species aggregation in space can be determined required an image classification in which omission errors of 50% and commission errors of 5% for the species of interest could be tolerated (Atkinson et al., 2007) . In such circumstances, especially as there was a large number of other species in the woodland, the overall accuracy of an image classification that provided the necessary information could be very low, perhaps in the order of ~10%. Clearly one would normally want and should strive for a higher accuracy but a classification of apparently low accuracy can still yield the information required for the application in-hand.\\nOne issue on which the remote sensing community could, however, adopt a harsher approach is in deciding whether a thematic map produced by an image classification satisfies the target specified. Commonly, the basis of assessing the acceptability of a map is to calculate a measure of the map\"s accuracy and compare the derived value directly against the target value (e.g. Hayes and Sader, 2001 ). The map is typically judged to be sufficiently accurate if the calculated accuracy value equals or exceeds the target. However, the accuracy statement derived in most studies is just an estimate of the accuracy of the classification. In many instances it would be more appropriate to fit confidence limits to the estimate and consider these when evaluating the map and deciding if the target accuracy has been achieved.\\nAlthough the estimation of confidence limits is relatively simple and the literature encourages the community to use them (Thomas and Allcock, 1984; Morisette and Khorram, 1998; Mas, 2004) they are rarely defined and provided. In many applications, the accuracy statement for an image classification should, however, really take the form of the estimated value ± the half width of the confidence interval at some specified level of statistical confidence. Assuming that the analysis is based on a sufficiently large sample of data acquired by simple random sampling and that the data are normally distributed, the half width of the confidence interval may be derived from 1\\nwhere p is the proportion of correctly allocated cases, n the number of cases used to assess classification accuracy and the value of t is derived from the tdistribution at the desired level of confidence (for large sample sizes the value of t approaches that for the appropriate z-score).\\nThe fitting of confidence limits around the estimate of classification accuracy may have a marked impact on the evaluation of a classification. Sometimes the estimated accuracy of a classification may exceed the 85% target value but the confidence limits may suggest that it would be unwise to assume that this means the classification has achieved the target level desired. However, a classification with an estimated accuracy that barely exceeds the target value specified is often viewed as being of acceptable quality (e.g. Hayes and Sader, 2001 ). For example, and so as to not appear critical of others, Foody et al. (2004) accept a thematic map derived from a classification as being satisfactory as its estimated accuracy, 89.5%, exceeded the commonly stated target of 85%. Fitting, the albeit wide, confidence limits at the 98% level to the accuracy estimate, it may be stated that with 0.98 probability that the map\"s accuracy lies within the range 84.7 -94.2%. The lower limit of this confidence interval lies below the 85% target and so, at this level of assessment, the map might not be viewed as being sufficiently accurate. At the more widely used 95% level of confidence, the map just passes the threshold as its accuracy may be expressed as 89.5 ± 4.00%, with the lower limit on the confidence interval just over the target accuracy at 85.5%. Note, however, that with just 1 more misclassification in the testing set used to estimate accuracy the resulting classification would have had an accuracy of 89.0 ± 4.08% at the 95% level of confidence, failing to achieve the target as the lower confidence limit again lies below 85%. The casual comparison of the accuracy estimate directly against the target may, therefore, give an inappropriate basis for evaluating a classification. The confidence limits fitted around the estimated value provide important information that should influence the evaluation of the accuracy of the classification and its suitability for later application. The confidence limits are also useful in the comparison of classification accuracy statements. In such applications it is, however, also necessary to recognise the nature of the testing set used in the estimation of accuracy, particularly if the same testing set is used in the evaluation of different classifications (Foody, 2004) . Critically, however, the remote sensing community should be encouraged to fit confidence limits to classification accuracy statements and promote their use in evaluating the classification\"s fitness for its intended application.',\n       'Europe PMC Funders Author Manuscripts the original data set (Han et al., 2005) . Conversely under-sampling randomly removes a certain number of instances from the majority class to achieve a balanced data set. In general, random over-sampling may lead to overfitting while random under-sampling may result in insufficient training data.',\n       \"Weight\\nwhich groups the tasks into exactly g subspaces. k to make our model robust. According to experimental experience, the value of k can be determined in the range of [2, 3] .\\nThe parameters γ 1 and γ 2 are proposed to balance the importance of two regularization terms. Larger γ 1 lead to more attention on the low-rank constraint while larger γ 2 lays more emphasis on the sparse structure. These two parameters can be adjusted to accommodate different cases.\\nIn our empirical section, we didn't make too much effort on tuning these parameters. Instead, in fairness to other comparing methods, we just simply set each parameters to a reasonable value. Though these parameters brought about great challenges in solving our optimization problem, they make our model more flexible and adaptive to different settings and conditions.\",\n       'Missing lines are \"repaired,\" and other problems are resolved in each band before the scene is rectified to the UTM coordinate system. Once rectification has been conducted, errors will no longer coincide with horizontal lines of data and are virtually impossible to fix. Image line replacement is a simple procedure that allows the operator to fill-in missing lines with the line above, below, or with an average of the two.\\nIt also may be necessary to conduct destriping of the image if a linear pattern is prominent in the image. It is usually a more significant problem with Landsat MSS than with TM or SPOT, and affects dark objects such as water in particular. However, we have run several variations of a destriping procedure on scenes with an 8 to 9-line repetitive stripe and have concluded that the output image was not improved over the original. Although we do not conduct this procedure ourselves, it may be advisable in other situations, and, if necessary, must be attempted in this initial phase of preprocessing, before rectification.',\n       'children and all other children in the household who are related to the householder by birth, marriage, or adoption. Remedial education Instruction for a student lacking those reading, writing, or math skills necessary to perform collegelevel work at the level required by the attended institution.',\n       'The estimated mean loss in MMSE points per year for AD samples used in the protein analysis was ANM 1.5 (standard deviation [STD] 5 1.5), ARUK 2.9 (STD 5 1.4), and DCR 2.2 (STD 5 1.4). One hundred thirty-nine proteins were found to correlate with the rate of cognitive decline (P value ,.05), two of which passed multiple corrections (q value ,0.05), namely clusterin (R 2 5 0.08; b 5 2.50; q value 5 0.012) and nucleosome assembly protein 2 (R 2 5 0.06; b 5 0.74; q value 5 0.044). Both proteins were positively associated with the rate of decline, and therefore, the quantity of these proteins is higher in the plasma of patients with fast cognitive decline. Results for all proteins are summarized in Supplementary Table 1. A random forest regression model optimized for predicting the rate of cognitive decline achieved a training performance of R 2 5 0.20 (180 samples) and test performance of R 2 5 0.10 in the independent test set (59 samples).',\n       \"Introduction and Overview T he Bureau of Labor Statistics (BLS) of the U.S. Department of Labor has tracked expenditures of U.S. consumers for more than a century. This chapter provides background for the Consumer Expenditure Surveys (CE), an overview of BLS' recent efforts to improve the quality of the data collected in that survey, and the context within which this study was framed.\",\n       '\\n',\n       'Results from whole brain voxel-wise analyses using Hyperactive/Impulsive and Inattentive symptom counts. Age, gender, total gray matter volume (GMV), site, pubertal development, Performance IQ, Verbal IQ, and socio-economic status were controlled for in the analyses. An initial height threshold of p ≤ .005 was impleme ted at the voxel level, with a corrected family-wise error (FWE; p ≤ .05) subseque tly applied to ide tify significant clusters.',\n       nan,\n       'The Landsat 8 satellite is in the same near-polar, sun-synchronous, 705 km circular orbit and position as the recently decommissioned Landsat 5 satellite. Landsat 8 data are acquired in 185 km swaths and segmented into 185 km × 180 km scenes defined in the second World-wide Reference System (WRS-2) of path (groundtrack parallel) and row (latitude parallel) coordinates also used by the Landsat 4, 5, and 7 satellites (Arvidson et al., 2001) . Landsat 8 has a 16 day repeat cycle; each WRS-2 path/row is overpassed every 16 days and may be acquired a maximum of 22 or 23 times per year, as for Landsat 4, 5 and 7. Combined, the Landsat 8 and 7 sensors provide the capability to acquire any WRS-2 path/row every 8 days at the Equator and more frequent coverage at higher latitudes due to the poleward convergence of the Landsat orbits (Kovalskyy & Roy, 2013) .\\nThe amount of Landsat data in the U.S. Landsat archive has not been constant among Landsat sensors, from year to year, or geographically, because of differing Landsat data acquisition strategies, data reception capabilities, and system health issues Loveland & Dwyer, 2012; Markham, Storey, Williams, & Irons, 2004) . Landsat 7 was the first Landsat mission that adopted a systematic acquisition plan in 1999, and Landsat 7 data continue to be acquired systematically in an attempt to refresh annually the U.S. Landsat archive with sunlit, substantially cloud-free acquisitions that capture seasonal land surface dynamics (Arvidson, Goward, Gasch, & Williams, 2006) . Globally, outside of the conterminous United States, however, only a fraction of the potential Landsat 7 WRS-2 path/rows are acquired Ju & Roy, 2008) .\\nThe Landsat 8 data acquisition plan seeks to directly benefit global studies by acquiring the majority of the land WRS-2 paths/rows overpassed each day. The Landsat 8 satellite has improved high capacity onboard recording and satellite to ground transmission capabilities compared to previous Landsat systems. The data are transmitted via X-band to three primary ground receiving stations located at Gilmore Creek in Alaska, Svalbard in Norway, and at the USGS Earth Resource Observation and Science (EROS) Center in the U.S. International cooperator receiving stations, typically national space and mapping agencies, may receive real time Landsat 8 data transmissions within line-of sight of the satellite. Unlike previous Landsat missions, all of the Landsat 8 data available to the international cooperator receiving stations are stored and transmitted directly to the primary ground receiving stations and added to the U.S Landsat archive (Loveland & Dwyer, 2012) . Approximately 60% more Landsat 8 scenes are acquired per day compared to Landsat 7. This improved data acquisition provides near-global seasonal coverage and the possibility to generate global Landsat data sets with adjacent cloud-free path/rows acquired only months apart, particularly if combined with data from other contemporaneous Landsat and Landsat-like sensors (Kovalskyy & Roy, 2013) .',\n       \"Livestock producers take a variety of steps to prevent the emergence and spread of animal diseases among their herds and fl ocks. Practices include pathogen testing, vaccinations, provision of antibiotics, segregation of herds or fl ocks by age, sanitary protocols in housing units, and physical biosecurity measures. Antibiotics are used to treat sick animals, but they are also administered in subtherapeutic doses, usually in water or feed, to protect animals against disease and to promote growth. Subtherapeutic antibiotics (STAs) can promote growth, particularly in poultry and hogs, by improving nutrient absorption and by depressing the growth of organisms that compete for nutrients, thereby increasing feed effi ciency. 36 About 27 percent of broiler operations were also adjusting the nutrient content of litter in 2006 by using additives like phytase in feed or directly in litter. Such strategies were less common on dairy operations where 5 percent of farms, accounting for 11 percent of cows, were adjusting the nutrient content of manure through feed additives. 37 Feed effi ciency is positively correlated with the scale of production in the hog sector-larger operations generally use less feed per hog produced. Many drugs used to treat animals are the same as, or similar to, drugs used for human health care. Consequently, there is concern that the widespread use of antibiotics, especially STAs in animals, could promote development of drug-resistant bacteria that could pass from animals to humans, thus posing a danger to human health. In response to these concerns, the European Union (EU) has banned the use of antimicrobial drugs for growth promotion, and they are coming under growing scrutiny in the United States (U.S. Government Accounting Offi ce, 2008). Recent ARMS hog and broiler versions have included questions about antibiotic use as well as other health management technologies and practices on farms. The data obtained cannot be used to assess resistance and health hazards, but they can be used to identify the extent of STA use on livestock operations, the impacts of STAs on costs and productivity at different types of operations, and alternatives to STAs for disease prevention and growth promotion. In the ARMS 2004 hog version, producers were asked whether they provided antibiotics, the purpose for provision (growth promotion, disease prevention, or disease treatment), and the types of animals receiving the drugs (breeding animals, nursery pigs, or fi nishing hogs). Antibiotics were used most widely on nursery pigs in specialized wean-to-feeder operations (McBride et al., 2008). Eighty percent of the surveyed farms used antibiotics for disease treatment, and 85 percent provided STAs for either disease prevention or growth promotion. Among farms that fi nish hogs, STAs are widely provided for growth promotion, and larger operations are considerably more likely to provide them (fi g. 9). About 20 percent of small feeder-to-fi nish operations provided their animals with growth-promoting STAs in 2004, compared to 60 percent of the largest operations. Farrow-to-fi nish operations are generally more likely to provide STAs-nearly 40 percent of smaller operations and 75 percent of the largest. 38 38 Similar patterns hold for STAs provided for disease prevention: the smallest class of producers are less likely to use them than the largest, where 65-75 percent of producers use them. Economic Research Service/USDA STAs add to farm expenses, but can also add to productivity by increasing the amount of meat that can be produced from a given combination of breeding animals, feed, and time. This productivity impact likely varies over time and across operations, depending on factors such as animal genetics, feed formulations, production practices, housing features, and management skills. McBride et al. (2008a) investigated which hog farms provided STAs and the effects of provision on farm-level productivity. The provision of STAs seemed to reduce costs at the nursery stage: operations that did not use STAs at the nursery stage had costs that were 30 percent higher than those that did (in a model with controls for the size of the operation, its location, and a variety of production practices). This evidence suggests that STAs reduce mortality and improve feed effi ciency among nursery pigs. In contrast, McBride et al. found little impact of STAs on production costs at the fi nishing stage-farms that used STAs had costs of production that differed little from those that did not. Any productivity improvement from STAs was not large enough to offset the additional expenses, suggesting the viability of alternative practices or technologies to reduce disease or improve feed effi ciency at fi nishing stages. These results are consistent with studies of the EU ban on STAs, which also suggest that farm-level benefi ts vary across stages of production and are most pronounced in the nursery stage. A study of STAs in broiler grow-out operations provided evidence consistent with the fi ndings for hogs. Graham et al. (2007) evaluated the results of a large nonrandomized control trial run by one large integrator in which growthpromoting STAs were removed from some broiler houses, whose fl ocks' performance was then compared to fl ocks from houses on the same farm that continued to use STAs. STAs boosted feed effi ciency slightly, but not enough to offset the expense, so non-STA houses performed slightly better fi nancially. Producers and integrators may be able to substitute other practices and technologies for STAs in broiler production. In the 2006 ARMS broiler version, 42 percent of respondents stated that STAs were not provided to their fl ocks. In contrast to hog fi nishing operations, there was no relation between farm size and the use of STAs in broiler grow-out (virtually all broiler grow-out farms are contract operations, so it might be argued that all broiler production is industrialized). Farms not providing STAs instead used extensive testing and expanded sanitation controls (fi g. 10). Specifi cally, farms that did not provide STAs usually tested their birds (for avian infl uenza, salmonella, and other pathogens), and their feed (for salmonella), while farms that relied on STAs were much less likely to test. Farms that did not provide STAs were also much more likely to fully clean out and sanitize their houses after every fl ock, and typically were required to have a HACCP (Hazard Analysis and Critical Control Point) plan in place to guide food safety actions. \",\n       '• Almost all (99 percent) 2011-12 beginning postsecondary students had attempted some STEM credits during their enrollment through June 2017. Students who had attempted any STEM credits attempted an average of 27 STEM credits, earned an average of 22 STEM credits, and had a mean STEM GPA of 2.4 as of June 2017.',\n       'All teachers, including special education/service providers, received $25 for completing child-level instruments for sampled children in their classrooms. Teachers completing questionnaires for more than 10 children in their classes received remunerations of up to $55. Over 97 percent of teachers had fewer than 10 ECLS-K children. On assessment day, after collecting completed questionnaires, the test administrator (TA) scanned the questionnaires to ensure that there were no missing critical items. During the field period, the TAs followed up with the school coordinator by making an in-person visit to the school or prompting by telephone to review the status of the incomplete or missing questionnaires.',\n       'Relevant US West Coast and local data was compiled into an ArcGIS Online web map (see a link to the EFH Story Map in the \"Supplemental Resources\" section at the end of this chapter). The datasets that were referred to most often, aside from the base layers that oriented the user, were the biogenic habitat data layers and associated image links. Whenever possible and appropriate, MBNMS GIS staff converted regional US West Coast raster datasets to vector form and clipped them to the study area (as vector files use less service credits). The ArcGIS Online web map allowed all staff and stakeholders to explore the dataset and develop strategic input for meetings based on these datasets. The nontechnical staff of MBNMS also used the ArcGIS Online service while writing the introduction and discussion sections of the proposal. ArcGIS Online allowed them to quickly access biological and geological characteristics of each individual area and select ROV-captured images to include in the proposal to best illustrate the habitats of these areas. In addition, an Esri Map Book app was developed for individual areas using data-driven pages to showcase the biogenic, management, bathymetric, and geological data for each area. The Data Driven Pages toolbar was enabled, and the EFH boundary modification layer proposed for MBNMS was selected as the index layer. The map book was an asset because it provided a quick reference to larger-scale maps of each of the proposal areas, which focused on the locations of biogenic habitat from regional and local sources.',\n       \"Performance regarding sea level in the simulations is analysed by comparing them to altimetry over the available common period (1993-2004/2007/2009 ). Detailed comparison of mean sea level from altimetry and SSH from G70 can be found in Vidal-Vijande et al. (2011) and will not be repeated here. However, their main result in this regard was that the model was capable of correctly reproducing the seasonal cycle in both phase and amplitude as well as the interannual variability, but SSH from the model showed an exaggerated positive trend of 14.96±1.46 mm yr -1 when the trend for altimetry was 3.62±1.32 mm yr -1 . G85 also displays a correct seasonal cycle but the trend has increased even further to 20.27±1.37 mm yr -1 due to the effect of a reduced salinity relaxation term, which implies lower evaporation and therefore an increase in sea level. These positive trends of the ORCA models are not an isolated feature of the Mediterranean, but global due to an imbalanced freshwater budget. The freshwater budget is of vital importance because it contributes to the mean sea-level budget closure, impacting sealevel rise and density-driven circulation (Ferry et al. 2010) , but its contributors (precipitation, evaporation, river run-off and glacier melt) have great uncertainties, making it very difficult to achieve a balance. If the trends are removed, the power spectra of G85 and altimetry (not shown) are very similar in terms of peaks and energy. Essentially, both spectra have a marked annual cycle at 10-12 months and a less well-defined semi-annual peak, but if trends are not removed, higher energy is seen in G85 at lower frequencies.\\nIn GLORYS, the net water budget is artificially set to zero in each time step. It thus becomes perfectly balanced and does not affect the mean sea level (MSL) of the model. The consequence is that any changes in MLS are due to the assimilated data provided by altimetry (Ferry et al. 2010) . As a result, the comparison between GLORYS and altimetry data yields almost identical results, with no drift in the model's SSH and a trend very similar to altimetry\",\n       'The results presented below compare the CSM predictions for Hurricane Sandy in three ways: first, amongst themselves, to assess the effects of scale (30m or 10m DEM), and the contributions of waves (none, near-shore only, or deep-water and near-shore); second, against the stochastic SLOSH predictions, which apply to a generic storm; and third to \"reality\" as established in detail by the HSIA (ground-truth) for this storm. In all cases, the area and perimeter of the flooded area are taken as proxy for flood damage. The depth of floodwater and duration of flooding is not considered. The detailed shape of the flooded area, which certainly affects the spatial distribution damage, particularly in low-lying areas, is also not considered in any quantitative way, but only graphically. Differences in perimeter are quite visible. Figure 22 shows the CSM baseline \"out of the box\" scenario, without waves, for the entire NYC area (i.e. as a single study region), using a 30m (1 arc-second) DEM and a presurge tidal height of 1.46 feet. Only a very minor reduction in flooded area, from 36.92 mi 2 to 36.90 mi 2 (0.05%) resulted from using a 10m (1/3 arc-second) DEM, which took almost ten times longer to complete.  CSM scenarios involving waves could not be run for the entire NYC study area, apparently owing to array size limitations in HAZUS; rather, these were run for the five boroughs (each a county of New York state) as separate study areas. To obtain the most detailed results, all runs were made with the 10m DEM. Results are shown in Table 4, in comparison to SLOSH and HSIA for the individual boroughs, which were obtained by ArcGIS geoprocessing (as both SLOSH and HSIA considered NYC as whole). Curiously, the sum of the CSM no-waves scenarios for the individual borough study areas (33.61 mi 2 ) does not closely match the CSM no-waves scenario for NYC as a whole (36.90 mi 2 ). The reason for this large (~10%) discrepancy is unknown. ',\n       'Census characteristics were compared by the exposure variables of interest using an ANOVA. Poisson regression was used to estimate prevalence ratios (PR) and associated 95% confidence intervals (95% CI) for the association between rurality defined by RUCC and comparator geographic zones and prevalence of home dialysis utilization (per total population). Poisson models were adjusted for median age, race distribution, median per capita income, unemployment percentage, and percentage of population under the poverty level. A p-value of <0.05 was considered statistically significant, and SAS v9.3 was used for all analyses.',\n       'Abstract Quantitative information from magnetic resonance imaging (MRI) may substantiate clinical findings and provide additional insight into the mechanism of clinical interventions in therapeutic stroke trials. The PERFORM study is exploring the efficacy of terutroban versus aspirin for secondary prevention in patients with a history of ischemic stroke. We report on the design of an exploratory longitudinal MRI follow-up study that was performed in a subgroup of the PERFORM trial. An international multicentre longitudinal follow-up MRI study was designed for different MR systems employing safety and efficacy readouts: new T2 lesions, new DWI lesions, whole brain volume change, hippocampal volume change, changes in tissue microstructure as depicted by mean diffusivity and fractional anisotropy, vessel patency on MR angiography, and the presence of and development of new microbleeds. A total of 1,056 patients (men and women C55 years) were included. The data analysis included 3D reformation, image registration of different contrasts, tissue segmentation, and automated lesion detection. This large international multicentre study demonstrates how new MRI readouts can be used to provide key information on the evolution of cerebral tissue lesions and within the macrovasculature after atherothrombotic stroke in a large sample of patients.\\nKeywords MRI Á Lesion load Á Brain atrophy Á Microbleeds Á PERFORM Á Longitudinal brain disease progression',\n       'Some technologies are easier to protect with a patent and, once granted, a patent is easier to enforce because the technology is inherently easier to describe and delimit. This factor may affect the willingness of firms to use the patent system. The leading example of the importance of this is the chemicals sector, including pharmaceuticals, where patents protect a specific compound that can be described in a precise chemical formula. This leads to fewer (although not zero) disputes over the exact breadth of the patent and easier notice, in the property rights sense (Bessen and Meurer 2008) . The contrast with the information technology sector, where the precise breadth of patent is often unclear, is striking, and is one of the reasons that patents in that sector are generally not used primarily for appropriating the returns to innovation.\\nApplying for a patent requires direct and indirect financial expenditures. If a patent is granted, it protects an invention only in the jurisdiction in which it was granted. The published patent is visible, however, outside of jurisdictions where the patent is in force. Thus, the invention could be legally imitated and used in jurisdictions where the patent is not in force. Moreover, to keep the patent in force, maintenance fees have to be paid to each patent office that has validated the patent. However, secrecy is also costly because often it is vital that confidentiality agreements are used and the knowledge of the invention is guarded. Keeping innovations 22 The situation is slightly more complicated under a first-to-invent priority system such as prevailed in the United States until 2012. In this case, the firm holding an invention secret could, in principle, file for a patent if another firm filed for a patent on the same invention, provided it could be shown that the later-filing firm invented it first. An \"interference\" proceeding at the patent office might be the outcome and there is no guarantee that the first firm would win, given the difficulty of establishing priority.\\nsecret usually requires considerable effort and active knowledge management in the form of an internal secrecy policy, which may be costly to implement and maintain. For example, firms may rely on the splitting of R&D into different components across researchers and research labs such that individual pieces of R&D do not allow a complete understanding and functioning of a given technology.',\n       \"The theoretical basis for the estimated models, a profit function, is a variant of the Ricardian method. The Ricardian approach assumes that climate changes cause farmers to adapt to the most profitable alternative by switching enterprises, seed varieties, technology, etc., and that markets are functioning efficiently so that the land rents reflect land's best use and the associated profitability. With well-functioning markets, net profits should be equal to the rental value of land. In this study net profits are used as the Ricardian measure instead of land values. One advantage of net profits over land value is that land values may include a speculative component that is most likely not a function of climate. In addition, use of net profits does not require an assumption about the efficiency of the land market. A weakness of the Ricardian or profit function approach is that prices are held constant under the environmental change so profit losses are likely to be overestimated. 1 Mendelssohn,Nordhaus, and Shaw (1999) address this problem by examining the error in using the Ricardian measure as a measure of change in the sum of consumer and producer surplus. Using plausible values of elasticities of supply and demand for agricultural goods, they conclude that for the modest changes in supply due to changing climate the Ricardian welfare measures are reliable for modest supply shifts. Simulations using the Ricardian approach are also premised on the assumption that technology and policy are held constant.\\nA second limitation is that our methodology does not allow for movements of land in 1Price increases triggered by large reductions in supply would offset some of the losses in profit due to those reductions. and out of agriculture, i.e., the simulated impacts are predicated on the land base remaining constant after climate change. 2 To the extent that climate change induces the land base to change, the methodology used here does not capture the full impact of adaptation to climate change. Assuming the land base is constant almost certainly provides conservative estimates of loss due to negative climate change impacts,s The potential effects of carbon dioxide fertilization are also not included in our analysis. Including such effects is desirable, but the lack of a consistent data set on observed carbon dioxide levels precludes such an analysis.\\nIn the present study four regression models are estimated with observed profits per acre as the dependent variables and climate, intra-annual weather, edaphic and price variables along with time and regional binaries as independent variables. A listing of the variables is provided in the Appendix. Effects of changes in the climate distribution are determined by simulating total profits within the midwest before the climate change and subtracting the simulated total profits after the climate change.\",\n       \"Diversification of an agricultural operation's crop mix is considered an environmental and financial management strategy. Environmentally, crop diversification can stabilize the ecosystem via the introduction of biodiversity, allowing for more rapid response to physical and social changes. Economically, crop mix diversification can mitigate risk. Though there are environmental and economic benefits of crop diversification, little economic work has been conducted on crop diversification outside of the row crop industry. This study estimated how internal and external factors affect crop diversification among fruit and vegetable (FV) operations. External factors included access to markets and land; internal factors included farmer beliefs and access to information from extension and network sources. An OLS regression was conducted using data from 1532 farmers across 16 states in the United States. Endogeneity was addressed using an instrumental variable approach and a score endogeneity test indicated that endogeneity was not an issue. OLS results indicate that selling locally increases diversification, while reliance on other farmers for information decreases diversification. A conditional quantile analysis was conducted to reveal factors' effects across different degrees of diversification. Quantile results indicate that selling locally, season extension technologies, and use of organic practices positively influence crop diversification across all levels of diversification. Receiving information from farmers negatively influences diversification for specialized farms, but positively influences diversification for highly diversified operations.\",\n       nan,\n       'Estimation of the tropical ocean\\'s state is important for seasonal to interannual predictability. For example, ocean observing systems in the tropical Pacific are frequently evaluated by carrying out estimates of ocean state (\"reanalysis\") and comparing them to withheld observations. Errors in reanalysis products include both formal mapping errors arising from sparse or noisy observations and representation errors that arise from low resolution, missing physics, or errors in the model-data synthesis methodology. The evolution of the Tropical Pacific Observing System (TPOS) 2020 project recommends the use of data assimilation to combine observations and to assess the design of the TPOS. A necessary first step in this procedure is to have a measure of the errors and performance of the assimilation systems. Verdy et al. (2017) evaluated the performance of a 4-Dimensional Variable system that assimilates Pacific Ocean XBT transect data, as well as Argo and remotely-sensed sea surface height (SSH) data sets, as a necessary step to inform use of the output for dynamical analysis or for data impact studies. A comparison to independent observations from Tropical Atmosphere Ocean (TAO) moorings showed that for time scales shorter than 100 days the state estimate that included the Pacific XBT data improved estimates of TAO temperature relative to an optimally interpolated Argo product. The improvement was greater at time scales shorter than 20 days.',\n       'This article reports on the opportunities future teachers had to learn tertiary and school level mathematics and their performance on the tests of MCK. Because of differences across teacher education programs within countries, wholecountry comparisons are not the purpose of TEDS-M. Rather, TEDS-M results compare programs cross-nationally according to the intended grade level and specialization in mathematics of the teachers the countries expect to prepare-teachers who are being prepared to undertake similar roles once they are qualified. Among those who qualify to become primary teachers, most will become generalist teachers, which, depending on the country, may be no higher than Grade 4 or through Grade 6. In a few countries, generalist teachers are prepared to teach either primary or lower secondary grades up through Grade 10. Other future primary teachers qualify to become specialist teachers of mathematics. In contrast, most future teachers of lower secondary school are prepared as mathematics specialists. Some are qualified to teach only up to Grade 8, whereas others are qualified to teach to Grade 12 and beyond. Thus, the findings of future teachers who answered the primary surveys are presented for the four program groups:\\n1. Lower primary generalists (Grade 4 maximum) 2. Primary generalists (Grade 6 maximum) 3. Primary/lower secondary generalists (Grade 10 maximum) 4. Primary mathematics specialists 1. Tertiary-level domains are geometry, discrete structures and logic, continuity and functions, and probability and statistics. 2. School-level domains are numbers, measurement, geometry, algebra and functions, probability and statistics, calculus, and structure.\\nThe findings from future teachers who answered the secondary surveys are presented for two additional program groups:\\n5. Lower secondary (Grade 10 maximum) 6. Lower and upper secondary (Grade 11 and above) Table 1 shows the average proportion of areas studied by future primary teachers in the domains of tertiary-level mathematics and school-level mathematics. Opportunities to learn in the tertiary-level domain range widely within and between program groups. For example, in Group 1 the lowest average coverage was reported by future lower primary generalist teachers in Germany, and the highest in the Russian Federation. Among future primary specialist teachers, those from Germany again reported the lowest coverage, and mathematics specialists in Poland reported having the highest tertiary-level opportunities to learn. Overall, about half the program types reported mean coverage of 50% or more tertiary domains, and about half reported coverage of less than half; in contrast and also in Table 1 , future teachers in most primary program groups reported covering an average of at least 60% of the domains classified as belonging to school-level mathematics. A more detailed examination explored the percentage of teachers who reported covering individual areas in the domain (not shown in a table due to space restrictions). Among the tertiary-level domains, for instance, more than 80% of the future teachers in the TEDS-M primary samples reported studying number theory, and more than 70% reported studying probability. At least 60% of future primary teachers in most countries reported covering calculus. However, lower proportions studied calculus in Group 1 in Germany; in Group 2 in the Philippines, Singapore, and the United States; in Group 3 in Chile and Norway (ALU), and in Group 4 in Singapore and the United States. More than 70% of future primary teachers in some countries in each program group also reported studying linear algebra, for example, Poland and Switzerland in Group 1; Chinese Taipei, the Philippines, and Switzerland in Group 2; Botswana and Norway (ALU+) in Group 3; and Malaysia, Poland, and Thailand in Group 4.',\n       \"Introduction Child-focused poverty and well-being measurement has played and remains to play a marginal role in the overall poverty debate. It is widely acknowledged that a wide gap exists with respect to child-focused poverty definitions and measurements within the academic world as well as policy arena (e.g. Gordon et al. 2003a , Gordon et al. 2003b , Minujin et al., 2005 . However, due to greater recognition of the importance of developing and employing child-specific poverty measures, a range of approaches and methods have been developed in the last decade. The promotion of children' s rights and the ratification of the Convention on the Rights of the Child by almost all countries in the world in the early 1990' s have put children higher up the agenda of the poverty debate. A number of efforts have taken place since to draw a picture of children' s lives in the developed as well as developing world. These efforts have taken place erratically in various forms, ranging from global studies to small-scale reports. Nevertheless, this small array of experiences provides a valuable and crucial source of information for the development of future approaches to define and measure child poverty and well-being. This paper presents a review of the current state of literature on child poverty and well-being measurement 1 and aims to extract lessons learned to aid the future development of such approaches. We begin the paper by answering the question why the issue of child poverty deserves special attention. A number of reasons, wellrecognized within the existing literature, outline the importance of focusing on childspecific poverty measures apart from general poverty measures. Next, a broad overview of the existing literature is provided. This section discusses various aspects that characterize child poverty approaches. We discuss the field of child poverty measurement along their identification mechanisms, methods of aggregation, data requirements, advantages and disadvantages and their implementation to date. In terms of the identification methods employed, we find that child poverty approaches differ with respect to the degree of dimensionality and their unit of analysis. Moreover, the use of different aggregation methods results in different poverty measures that can roughly be categorized as child poverty count measures, child poverty index measures and holistic child poverty approaches. In the subsequent section, a number of approaches are discussed in detail along the lines of this categorization in order to highlight the approaches' specific characteristics. Finally, the paper is concluded with a summary and lessons learned.\",\n       \"97.5% and calculated as the GM multiplied by the n ϭ 180) and undisturbed (60 mg kg Ϫ1 , n ϭ 268) soils. Geometric square of geometric standard deviations (GSD) of the mean (GM) concentration of total P in the undisturbed soils decreased expected range of background concentration, has been in the order of Histosols (350 mg kg Ϫ1 ) Ͼ Mollisols (171 mg kg Ϫ1 ), used in several studies to assess changes in elemental Inceptisols (140 mg kg Ϫ1 ) Ͼ Ultisols (88 mg kg Ϫ1 ) Ͼ Alfisols (54 mg kg Ϫ1 ), Entisols (53 mg kg Ϫ1 ) Ͼ Spodosols (24 mg kg Ϫ1 ). Aquic compositions (Gough et al., 1994; Dudka, 1995;  Chen suborders tended to have greater P contents than the dry suborders, et al., 1999b) . Chen et al. (1999a) reported that soil e.g., Aquents (92 mg kg Ϫ1 ) Ͼ Psamments (47 mg kg Ϫ1 ) and Aquods UBC of total P equals 1374 mg kg Ϫ1 and could be used (27 mg kg Ϫ1 ) Ͼ Orthods (14 mg kg Ϫ1 ). Total P estimation based on as a reference level for assessing potential nonpoint digitized taxonomic soil maps suggested that native soil properties sources of P enrichment in Florida soils. However, no\\nwere primary factors in controlling total P in soils. The wide occurdetailed information is available on how this informarence of P bearing parent materials resulted in many soils having high tion can be applied to different land uses and soil P concentrations. Twenty-four P-elevated samples from the disturbed category.\\nsoils were identified using the UBC of P for the undisturbed soils at\\nAt the state level, Florida showed the most striking suborder level as reference criterion. Anthropogenic P inputs were pattern by having low soil background concentrations related to commercial PO 4 -fertilizer application and population growth as nonpoint sources.\\nof most elements in the U.S. A few soil samples with high P were associated with PO 4 deposits in Florida (Shacklette and Boerngen, 1984) . In southern Florida, however, P has been considered the element that most P hosphorus is a major nutrient essential for plant threatens the overall health of lakes and the critical growth. Fertilizer application of P and other nutrilevel of dissolved P allowable in drainage water entering ents has contributed more than any other measure to the Everglades is currently being debated (McPherson increasing crop productivity in agriculture. However, and Halley, 1996) . A large, rapidly growing population overabundance of P in the nation's waters and soils has in the coastal peninsula region and in central Florida become a major environmental concern (Muller et al., since the 1970s continues to impact the Florida environ -1995; Daniel et al., 1998) . Transportation of soluble and ment via elevated N and P levels in soils (McPherson soil-bound P influences soil fertility strategies and can and Halley, 1996) . Intensive cattle ranching and farming accelerate eutrophication in lakes and streams suris another nonpoint source of nutrients that can elevate rounding agricultural areas. Knowing the P status of soil P concentration because of the lack of economically soils is important for developing an effective P-manageviable alternatives for manure disposal (Harris et al, ment strategy (Chen et al., 1999a; Higgs et al., 2000 Higgs et al., ). 1994 Maluk et al., 1998; Sharpley et al., 1996) . Field Numerous studies on P assessment focused on plantstudies have shown that dissolved P concentrations in available soil P tests (Sharpley et al., 1996; Daniel et runoff were related to total P in surface soils (Sharpley al., 1998; Higgs et al., 2000) . Assessment criteria based et al., 1996) . Little is known about taxonomic and geoon total soil P are, however, not emphasized in the litgraphic distributions of P background concentrations in erature.\\nFlorida surface soils. inputs in soils by establishing normal total P ranges in at the time of sampling and ෂ40% (disturbed soils, n ϭ 180) different soils. In addition, understanding how P levels of the samples were described as having surface horizons that vary across the land can lead to information useful in had been disturbed either by plowing or clearing (p-subordideveloping site-specific P management strategies.\\nnate horizon designation). Sampling depth varies from the top 1 to 76 cm, with a mean of 16 cm. These soils were from 51 of 67 counties, and their map units covered ෂ80% of the total\",\n       'The output of VirStrain contains two files. The first is a report file in text format. This file contains all identified strains and their depth and site coverage, etc. The other file is an interactive HTML page to display the depth and uniqueness of sites. An example is shown in Supplementary Figure S5 .',\n       'Our analysis of more than 9,000 LOAD cases with age-at-onset information is the largest genetic study of LOAD AAO to date. Examining AAO associations at LOAD risk loci, we confirmed the association of APOE region variation with AAO and found additional strong associations with AAO among variants at three of the other nine established risk loci (CR1, BIN1, and PICALM). Burden analysis demonstrated that the cumulative variation explained by SNPs at nine LOAD risk loci was about a third as much as the percent variation in AAO from APOE, The smaller effect of APOE ε4 on differences in AAO here (3-4%) than in previous studies (7-9%) 29 may be due to differences in study design; for instance, all previous estimates were made in multiplex pedigrees, whereas most cases examined here were unrelated (2,302 of 9,162 [25.1%] of cases were from family datasets). However, in addition to confirming the predominance of the effect of APOE on AAO, we showed that the cumulative effects of risk loci associated with AAO may have an effect of similar scale on AAO. In our secondary analysis of genome-wide association, cumulative effects on genetic burden of SNPs associated with AAO but with little or no effect on LOAD risk accounted for more variation in AAO compared to the non-APOE risk variants (2.2% vs.',\n       'There were 25,050 NPSAS:08 sample members who were potentially eligible for membership in the B&B:08 cohort according to their NPSAS:08 interview, CADE, and/or enrollment list status. Between the NPSAS:08 data collection and the start of the B&B:08/09 data collection, sample members whose transcripts, NPSAS:08 student interview, or administrative data showed they were ineligible, as well as deceased sample members, were removed from the B&B:08 cohort. At the beginning of the B&B:08/09 data collection 18,500 individuals were included in the B&B:08/09 sample. Prior to the start of B&B:08/09 data collection, 1,320 individuals were found to be ineligible leaving 17,170 eligible individuals in the sample. At the end of the B&B:08/09 data collection 17,160 eligible sample members remained in the B&B:08 cohort (deceased cases were removed). Of the 17,160 eligible sample members 15,050 were considered B&B:08/09 student interview respondents, 16,070 were considered transcript respondents, and 14,010 were considered combined interview and transcript respondents. 7 A B&B:08/09 student interview respondent was defined as any sample member who was determined to be eligible for the study, was not deceased at the time of the B&B:08/09 data collection, and had a completed, partial, or abbreviated interview. A student transcript respondent was defined as any sample member who was determined to be eligible for the study, was not deceased at the time of the B&B:08/09 data collection, and had a transcript provided by the NPSAS:08 institution. A combined student interview and transcript respondent was both an interview and a transcript respondent.',\n       \"Three classifications of hog production chain network agents, identified by industry experts as critical players in the transmission of disease, are represented in the model. These are (a) producers, (b) feed mills, and (c) slaughter plants. Producer agents are assigned one of five industry roles based on the USDA's classification system for hog producers, these being ( The model's parameters and functions controlling pig movement and feed deliveries were further specified with the help of data provided by a Family Farm Company from the U.S. (per confidentiality, the company's name is not disclosed here). The database contains two-year records of each pig movement and each feed delivery involving producers in the Family system. The Family Farm Company consists of a network of 161 producer partners that raise pigs from birth to market. The pig movement records were used to derive realistic estimates of transfer frequencies and number of animals per transfer, as well as reinforcing USDA farm size and operational statistics (Table 2 ; Figure 2 ). The feed delivery records were used to estimate delivery frequencies ( Figure 3 ). Finally, a team of experts in veterinary medicine and in agent-based modeling has followed the development of the model and collaborated in parametrizing, calibrating and ground truthing it:\\n• \",\n       'rhe NELS program currently comprises three major studies: the National Longitudinal Study of the High School Class of 1972 (NLS-72): High School and Beyond (HS&B); and the National Education Longitudinal Study of 1988 (NELS:88). Taken together, these studies represent the educatirial experience of youth from three decades --the 197Gs, 1980s, and1990s. While NLS-72 sampled only high school seniors, two of these studies--HS&B, in ith sophomore cohort--and NELS:88, with its eighth grade starting point--have a particular contribution to make to the collection and refinement of national dropout statistics, as well as to analyses of the dynamics and consequences of dropping out. HS&B base year data collection was conducted in the spring of 1980. Students were selected using a two-stage probability sample with schools as the first-stage units and students within schools as the second-stage units. There were 1,015 public, private, and church-affiliated secondary schools in the sample and a total of 58,270 participating students. Unlike NLS-72, HS&B included cohorts of both tenth graders and twelfth graders. Since the base year data collection in 1980, three follow-ups of the HS&B cohorts have been completed, one in the spring of 1982, one in the spring of 1984, and the last in the spring of',\n       nan,\n       '• Other than 10 mL Sarstedt tube (n=36; 26%)\\n• Extra transfer to 0.5 ml Sarstedt tube needed for Elecsys measurement (n=7; 5%) • 1 tube sent instead of 2, thus extra f/t for Elecsys measurement (n=15; 11%) • CSF arrived thawed (n=11; 8%) • Other Fig. 1 . Schematic overview of preanalytical deviations during CSF sample collection for biomarker measurement. Please note that one sample could be subject to more than one protocol deviation. Percentages relate to the total sample size. Abbreviations: CSF, cerebrospinal fluid; f/t, freeze/thaw cycle.',\n       'For the evaluation we compare the accelerated implementations with the original implementations. Both runtime performance and accuracy are investigated.\\nTo evaluate performance we compare the runtime per iteration between both algorithms, t old and t new . The speedup factor is defined as F = t old /t new . The speedup will depend on the number of threads T that are used for parallelization. The parallelization efficiency is a measure expressing how much a program is accelerated compared to an ideal speedup equal to the number of threads, i.e., E = F/T.\\nTo evaluate accuracy we use a combination of measures, to make sure that the accelerated registration still returns similar \\nAll timings were measured on a second run of the program, where the pre-compiled GPU kernel is loaded from cache. CPU optimizations were evaluated using the Alzheimer classification application to compare original with optimized methods, see Section 4.4. While in our automatic testing environment (using CTest, part of the CMake package, www.cmake.org) we perform nightly evaluation on both 2D and 3D data, in this paper we only report 3D results. All timing experiments were run on a linux system, detailed in Table 1 . This systems contains an NVidia GTX 480 graphical card (market launch March 2010), while currently (August 2013) the GTX 780 generation is available. All registrations for the diagnostic classification of AD were run on a cluster of linux systems.',\n       'In this section, we present the details of the dataset, feature extraction technique, machine learning models and the experimental setup employed in our study. The primary task of the proposed technique is to use machine learning for predicting the maximum sustained wind speed or intensity of a hurricane (in knots or kilometers per hour) from infrared satellite images of the hurricane. Section 2.1 provides a description of the dataset used for training and evaluation of the machine learning model. In Sect. 2.2, we explain feature extraction methods. Analysis of feature importance is presented in Sect. 2.3. Different machine learning models analyzed in the study are described in Sect. 2.4. Post-processing and experimental setup used for performance evaluation are explained in Sects. 2.5 and 2.6, respectively.',\n       nan,\n       'Ferritin is the major intracellular iron storage protein in the body and has an important role in brain iron homeostasis [333] . Inherited ferritinopathies are associated with motor and cognitive dysfunction [333] , and ferritin levels are increased in AD brain tissue [58] . CSF levels of ferritin have been shown to be higher in APOE ε4 carriers than in non-carriers, but there was no difference in levels among subjects with AD or MCI and controls [15] . Increased CSF ferritin levels were associated with cognitive decline and predicted progression from MCI to AD, regardless of APOE genotype [15] . In a subsequent analysis, CSF ferritin was associated with cognitive decline in cognitively normal subjects, but the association was strongest in APOE ε4 carriers [14] . In the same cohort, high CSF ferritin was associated with accelerated depreciation of CSF Aβ42 in subjects with a high tau/Aβ42 ratio [13] . Plasma ferritin levels showed a modest correlation with CSF levels, but unlike CSF ferritin, there was no difference in plasma ferritin between APOE ε4 carriers and non-carriers [15] . In another study, plasma ferritin levels were elevated in cognitively normal subjects with Aβ pathology identified by PET when adjusted for covariates (age, sex, APOE ε4 status, and levels of C-reactive protein), although ferritin alone had a relatively minor effect compared with the base model (derived from logistic regression of the same covariates) [112] .\\nIn summary, the data are limited but a small number of studies suggest that both CSF and plasma ferritin may be useful as AD biomarkers (Table 1) . CSF ferritin may have a role as a prognostic biomarker, whereas plasma ferritin could be used for subject/patient selection (screening) to help identify preclinical AD (Table 2) ; however, further studies by independent groups are needed to validate the initial findings. Commercial assays are available.',\n       'Researchers and research assistants delivered the questionnaires to all participating schools. Principals will be asked to distribute to teachers and other administrators and to follow their plan to distribute questionnaires to parents. Dates were set to collect the questionnaires. We distributed one survey for the principal, 40 for teachers and 100 for parents. Each principal was given a deadline for the collection of the surveys (usually two -three weeks). In order to increase the return rate, several phone calls and visits were made to remind the principal or whoever was responsible for the surveys about the deadline. When a school did not achieve the desired return rate, we did extensive follow-up visits asking for the surveys to be returned. This proved effective and is demonstrated in the final return rates (see table 4.).\\nAfter the initial data analysis of the questionnaires from 18 schools, individual interviews were conducted with principals, teachers and parents. Several questions were developed from the themes of the questionnaires and used to guide the semi-structured interviews and focus groups. Interviews and focus groups were conducted in both schools that were surveyed and some that were not surveyed in order to develop a deeper understanding regarding some items on the questionnaire and to determine if the findings from the questionnaires were evident in other schools.\\n\\nWhen the data was saturated, findings were discussed and it was decided that there was no need to survey additional schools. Based on the data analysis, semi-structured interviews were conducted based on questions developed form the emergent survey themes. Questions were designed to probe further the themes. Upon completion of the interviews, a conceptual content analysis was conducted in the same manner as described above using emerging themes and information to better answer each research question.\\nThe interview data supports principals using a more democratic leadership style, using less micro-management, delegating tasks, giving others responsibilities, listening to their staff, negotiating utilize teamwork and providing opportunities for faculty and staff to be actively involved in the decision-making process. Only two principals said their style has not changed because they have always had a flexible attitude and tend to involve staff in decision-making and solving issues related to school. Several principal comments illustrate this shift; \"I use team work, listen to my staff and involve them in the decision making process\"; \"I work with them as a team\"; \"twenty percent of decisions are imposed and the rest are decisions coming from them. I have friendly relation with teachers and staff, I listen to their problems and help as much as I can.\" While all the surveyed principals admitted that EFNE led to a change and improvement in their leadership style, the majority of these principals indicated that EFNE also affected their relationship with faculty and staff that is directly linked to their leadership styles. Principal International Journal of Education ISSN 1948 -5476 2013 responses on questionnaires indicated that 66% believed that they give more power to their teachers and staff. This included providing teachers with a say in the decision making process, listening more to their teachers and staff, considering their teachers\\' special circumstances, develop more friendly relationships and spend more time consulting and discussing issues with faculty members. This is illustrated in a statement by a principal, \"Yes, the relationship between us (principals and teachers) changed. I started to discuss idea with them, listen to them and consider any circumstances they might have.\" Another principal indicated that he \"distributed responsibility, giving more power or permission to some of the staff/faculty to make decision, carry out tasks and more responsibility.\"\\nAll principals agreed that the reform caused a significant increase in communication with parents. Three principals wrote that there was some exchange of views and decisions with parents and some mentioned parents even help to solve particular problems. However, overall there is still a lack of parental involvement in most schools. Principals indicated that as a result of the reform, they now endure increased pressure from parents. One principal wrote that \"parents have become an attack on the leadership and principals take the responsibility of all mistakes\" School leaders have to deal with parents who become very demanding and tend to blame the principal for every mistake. Another principal\\'s comment summarizes this thought \"parents have become more aggressive with the leadership, blaming principals for all mistakes.\" Other respondents think that the reform had a positive impact on their relationship with parents. It created more opportunities for communication with parents and allowed for involving them in school activities.\\nHowever, interview data indicated that although the reform encourages parental involvement, not many parents are actually involved in the education process. They do not attend meetings in spite of all principals\\' efforts to attract parents, such as using SMS, phone calls and other forms of communication. Principals argued that parents either resist or refuse to be involved either because they are not interested in their kids\\' education or are too busy. Some principals said that mothers are more involved than fathers. One principal\\'s comment illustrates this point.\\nParents in this area are not very educated, they are not very involved in their kids\\' education, they are not enough aware of the value of education for their kids so they do not help them much, their academic involvement is missing.\\nThese principals realize the importance of parental involvement when one principal stated, \"When parents are involved in some school activities, this has a positive impact on their kids, they are proud of their parents and get more serious and more motivated about their studies, too, but this is very rare.\" However in the interviews, only one principal admitted that parents were very involved in the school and played a positive role in their kids\\' education \" 5.1.2 What are the challenges principals face as a result of the reform?\\nAnalysis of the principals\\' responses about the challenges that they and their schools face revealed one major challenge, namely, the continuous and sudden changes from the SEC and the unrealistic requirements imposed on schools and teachers from the SEC. One principal ISSN 1948 -5476 2013 wrote, \"there are lots of changes in roles and policies and they are sudden changes.\" There is \"continuous adaptation and change of policies made at the level of the SEC.\" This creates confusion and makes it difficult for schools to cope with and implement policies changes. One of the principals pointed out that the main challenge that his school faces is the \"confusion in the offices of the Supreme Education Council and the lack of clarity in planning.\" Furthermore, several principals indicated that their job is to obey orders. This point was succinctly described by one of the respondents who wrote that there is \"control from the Supreme Education Council for everything in schools, and my job is to obey the orders.\"\\nRegarding their schools, principals argued that they are hurried to get results from the reform however, they think that it takes time for the reform to develop and results will come later. Schools suffer from a lack of qualified teachers and principals face resistance to change by some parents, teachers and students. As mentioned earlier, several stated that an important challenge is communication with parents and getting them involved in their child\\'s education.\\nFinally, all principals who responded to the questionnaire were favorable to the move to bilingual (English and Arabic) as the language of instruction providing the following reasons for their support. First, the decision will enable students to understand content better because it is taught in their mother tongue. Second, Arabic is the language of the civilization and culture so students can connect with their identity and country. Third, the change provides students with more trust in themselves and ability to compete with other countries. Finally, it allows parents to follow up on their kids\\' achievement at school because not all parents speak or read English.\\nIn addition, the majority of participants who were interviewed, ten out of eighteen, were strongly in favor of the move back to Arabic as medium of instruction in content area subjects. They argue that teachers\\' level of English was not good enough to enable them to teach in English; students, also, did not have the required proficiency in English to enable them to achieve high in content courses. They support bilingualism because even if the textbooks are written in Arabic, the terminology is still in English. Some schools surveyed parents and students and results indicate that both parties favored bilingualism: instruction in Arabic, terminology in English.\\nIn spite of this positive attitude to the use of Arabic in content area courses, some respondents admitted that it was harmful to some students. On respondent stated:\\nIt is much better to go back to the bilingual approach, but we need to go slow . . . many students were hurt by the decision to teach in English. But now, exceptional students, are hurt in the move back to Arabic-they were great students in English and now they are suffering as they move back to Arabic.\\nFour respondents were more cautious and suggested they should wait to see the results of this practice to express their opinions. Two respondents thought that there should be schools that teach in Arabic and others that teach in English to accommodate the needs of students and to ISSN 1948 -5476 2013 www.macrothink.org/ije 120 offer options for the various types of students. Some parents prefer their children to be taught in English. Only two principals opposed the move because it happened too quickly.\\n\\nIn summary, the reform helped to increase teacher willingness to develop professionally, to change their teaching styles and adopt modern instructional strategies and techniques. This is due to the professional development that accompanied the reform and that helped teachers become aware of and able to incorporate new teaching techniques in classroom practices. It also helped some students benefit and improve academically.\\n\\nSecond, parents (14%) struggled with their own ability to help their students. Several parents stated \"I am trying my best to help my children,\" \"I started searching for magazines to find scientific information and knowledge to assist in the discussion with my child,\" and \"it is difficult in understanding what my son says he is studying in school.\" Parents indicated that since the introduction of the reform, the amount of time parents must spend helping their children complete the work has increased considerably. One parent wrote, I spend \"most of my time helping with homework, following up with my kids at home, finding tutors and learning resources.\\nFinally, 11% of the respondents stated that EFNE has influenced them as parents and their children through the amount of homework now required of their children and the lack of time the students have to complete all the work. Parents stated there were too many assignments, too much content and too much homework and this increased the burden of studying on the student. One parent stated, \"my daughter has trouble understanding the huge amount of information she is given. As these responses indicate, students have had much more homework since EFNE began and this has implications for not only the student but also the parents.\\nParents listed the difficulty of the curriculum (21%), teachers\\' concerns (19%) and pressure on parents, students and teachers (10%) as some of the disadvantages of EFNE. First, parents demonstrated a concern that the \"curriculum is higher than the level of students\" and \"the students need longer to learn the curriculum because it is difficult and it takes time for the student and the parents to get used to it.\" Second, parents thought that one disadvantage of the reform is that the high expectations and standards set for teachers required by EFNE created a large number of teachers who are unqualified. Recent statistics support the accuracy of this parental perception of the current teaching force. As previously mentioned above and worth repeating, more than 30% of school teachers in Qatar are not qualified to teach, 31% of teachers in Qatar have no formal qualifications to teach, 35% of whom are teaching in Independent schools. Also included in parental concerns about teachers is the \"instability of teachers\" or rather \"the high mobility of teachers (who change schools too frequently and even leave the country)\" seems to be one of the reasons parents are discontent with the reform. Finally, the lack of Qatari teachers was rightfully mentioned by some participants because there are only 25% of teachers in Independent schools are Qataris.\\nAnother disadvantage reported by parents was the added pressure on parents, teachers and children. This was only mentioned by 10% of the parents. One parent wrote, \"It (the reform) has a negative impact because it increases the pressure on the student and teacher and parents.\" These parents believed that the pressure for students was on studying and \"knowing so much information\" and the pressure for teachers was to \"be better at teaching.\" Parental pressure included following-up on students, \"keeping track that students do the increased work,\" and helping students deal with the added stress.\\nSimilar to the principals and teachers, parents are in full support of the decision to move to bilingual education with 91% commenting that this was a positive decision. Regarding the parents, 43% of the respondents expressed that Arabic preserved the mother tongue and 13% stated that Arabic is the language of the Quran and must be included in education. One parent wrote, \"moving back to bilingual is the right decision, learning in Arabic means belonging. It is also the language of the Quran.\" Four percent of the parents believed that the curriculum is now easier and the students will learn. One statement expresses this concern \"the curriculum has become easier on the child because it is the native language of the student. This means that the student will understand what he/she studied.\" Finally, there were a few parents who believed that Independent schools should not neglect English, that science and math should be in English and that in order to study outside Qatar, the students will need English.\\nFinally, 66% of the parents interviewed believe that their child is a \"better\" thinker supporting the 24% of the parents surveyed who stated that there was an increase in learning and thinking. The interviewed parents made statements that illustrate this belief. For example, one parents stated that her daughter \"adopted some ideas and opinions that she can discuss . . . she is more dependent on herself in many school activities & research.\" Another parent stated her daughter is \"better in thinking and depends on herself because the education system requires her to have her own opinion, support it with evidence and defends that opinion.\" Others argued that EFNE gets students to \"try to find answer not like the old system of memorizing\" and EFNE encourages students to \"discuss with students their age from other countries like Saudi Arabia & Kuwait because Education is developed a lot here in Qatar.\"',\n       'Results are given in summary fashion only in this paper. Future presentations will present numerical and graphical summaries as well as extensive tables. Estimation in general seems to work well, but there is one dominant issue that is being addressed in ongoing work. In short, it appears that it is very important for variance estimation to take into account the fact that some control totals are estimated from survey data. Estimated control totals have their own uncertainty, which needs to be propagated in analyses. Some literature on this topic is reviewed in the discussion section below. It will make more sense to present more extensive results once methods for properly accounting for uncertainty in estimation with calibrated weights is incorporated into analyses. Propagation of uncertainty in another scenario was considered by Lahiri and Larsen (2005). For the calibration methods (raking and linear regression) and the initial weight options (the four listed above) considered, very similar results were obtained. That is, estimates and estimated standard errors differed in a minor amount across the method-weight combinations. There are two differences to mention in comparing raking and linear regression. Raking does not produce negative weights, but it was possible for linear regression calibration to produce negative weights. Negative weights can be used in estimation, but in general they are not desirable. One cannot interpret calibration weights in the same way as one tends to interpret survey design weights or nonresponse adjusted weights; namely, as indicating the portion of the population that the observation associated with the weight represents. Calibration weights are supposed to be close to the initial weights but also satisfy the calibration constraints. The raking option, however, could not handle the full combination of options A-D; the R program ran into problems with the implied interaction among the and domain indicator variables. Negative weights and choices for weight restrictions are mentioned in the discussion. After calibration using data and targets from three years, estimates in a given survey year using the new weights accurately reproduce estimates from a single survey year using the original sampling weights for that year. Estimates of change (1 versus 2, 1 versus 3, or 2 versus 3) also are preserved. When population size and domain size are used as calibration targets, estimated standard errors for yearly totals and change between years are approximately the same as before. When calibration targets include the -variable total or both the overall and the domain -variable totals, standard errors are estimated to be much smaller (e.g., 60-80% of the value) than the original estimated standard errors. In general smaller standard errors is desirable. In this case, however, coverage of the known population values by confidence intervals based on the calibrated standard error estimates is lower than the nominal 95% level (e.g., 70-85% coverage). A reduction in coverage below the nominal level is not desirable. This drop in coverage is discussed in the next section.',\n       \"Another policy approach for improving NUE is to legally require farms to adopt and implement particular management practices. Such an approach would be a major change in the way most of agriculture is treated under current environmental laws. With few exceptions, agricultural operations are exempt from regulation under the Clean Water Act and Clean Air Act. A number of arguments have been used as justifi cation. First, agriculture is so diverse across the United States that the conventional regulatory approach of applying uniform standards is impractical (Nanda, 2006). Second, due to the nonpoint nature of agricultural pollution, individual polluters cannot be identifi ed except at great cost. Regulation can conceptually be placed on a continuum between performance standards and design standards (Ribaudo et al., 1999). Performance standards directly regulate emissions. Design standards dictate how producers manage their operations, including practices that should not be used and/or BMPs that should be adopted. Because of the nonpoint nature of agricultural pollution, design standards are the only practical approach for addressing nitrogen losses. One approach would be to require that farmers adopt specifi c BMPs to improve their nitrogen use effi ciency. Generally, a practice-based regulation is ineffi cient because it requires producers to adopt the same practice, whether it is appropriate for their particular farm or not. It may be more effective to defi ne BMPs locally so as to allow fl exibility and to account for agriculture's heterogeneous nature. For example, a nitrogen management plan is a fl exible practice that is based on a farmer's resources and cropping system. However, farmers may fail to implement the plan properly. The effectiveness of a regulation therefore requires effective inspection and enforcement by a resource management agency. Implementation costs would likely be high. Several States, such as Nebraska and Maryland, have required farmers in particularly vulnerable areas to adopt specifi c nutrient management practices to protect ground or surface water (Ribaudo, 2009). One of the few segments of the agricultural sector that has been subjected to regulatory environmental measures at the national level is animal feeding operations, refl ecting heightened concern over pollution from animal waste from the largest operations (USDA-EPA, 1999). Manure is estimated to be a source of about 17 percent of nitrogen entering U.S. waters (Smith et al., 1997). Clean Water Act regulations now require that animal feeding operations designated as Concentrated Animal Feeding Operations, or CAFOs, and needing a National Pollutant Discharge Elimination System (NPDES) permit (those CAFOs that discharge or propose to discharge to surface waters), develop and implement a nutrient management plan to cover fi elds that receive manure. Such a plan, which would meet NRCS standards, sets a limit on the amount of nutrients that can be applied per acre of land and specifi es erosion control measures to prevent the loss of sediment and nutrients. Also under the new regulations, CAFOs that are not required to have an NPDES permit but that wish to claim the storm water exemption (the provision in the Clean Water Act that exempts fi eld practices from requiring a discharge permit) for runoff from fi elds must develop and implement a nutrient management plan to demonstrate that due care is being taken to minimize polluted runoff from fi elds receiving manure. If a waterway becomes polluted with animal waste from fi eld runoff and a CAFO does not have an approved nutrient management plan, this would be a violation of the Clean Water Act. This approach sets a level of expected stewardship, namely the implementation of a nutrient management plan. Requiring not just CAFOs but all animal feeding operations to adopt nutrient management plans would be costly. ERS estimates that reductions in net returns in the livestock and poultry sector would be about $1.4 billion per year, and national economic welfare for producers and consumers would decline almost $2 billion per year (Ribaudo et al., 2003). The benefi t would be improved air and water quality. Targeting the regulatory approach only to those operations most susceptible to pollution problems would lower the overall costs. Chapter 5\",\n       'A k u t a n Is .\\nA k u t a n Is . A k u t a n Is .',\n       \"other structural MRI based approaches discriminated AD patients and healthy controls with 88% accuracy in both studies 15, 16 . Diffusion Tensor Imaging (DTI) is also a promising imaging technique whose development may provide much earlier evidence of the disease than the neuropsychological symptoms 17 . The Alzheimer's Disease Neuroimaging Initiative (ADNI) added DTI among several other imaging techniques in an effort to identify reliable biomarkers of AD 18 .\\nMachine learning approaches for classification between AD and controls based on fractional anisotropy (FA) as input features attained classification accuracies in the range of 75%-88% 16, [18] [19] [20] . FA decrease in AD patients revealed changes in the parahippocampal white matter 16, 19, 21 , uncinate fasciculus 16, 22, 23 , superior longitudinal fasciculus 16, [22] [23] [24] , cingulum 16, [22] [23] [24] , fornix 19, 22, 23 , genu and splenium of corpus callosum 24 . A recent classification based on DTI graph measures 25 has also achieved 80% accuracy for AD versus healthy controls. A multilevel classification techquine 26 combining FA values (voxel-level), fiber tracking (connection-level), and graph measures (network-level) achieved 90% accuracy between AD and controls. Multimodal MRI Analysis, combining DTI and fMRI achieved a comprehensive classification accuracy among AD, MCI patients and controls of 92% 27 .\\nMild cognitive impairment (MCI) refers to a cognitive decline in absence of dementia. It may indicate a transitional stage between healthy conditions and dementia 28 , including prodromal stages of AD or mild stages of other dementing disorders 29 . The criteria for diagnosis of MCI status include neuropsychological measures, such as Mini-Mental State Examination, Wechsler Adult Intelligence Scale-Revised, Wechsler Memory Scale-Revised, Dementia Rating Scale, Free and Cued Selective Reminding Test, and Auditory Verbal Learning Test 30 . MCI is associated with high risk for the development of AD, with conversion rates between 10% and 15% per year 31 . Therefore, prodromal AD is often categorized as amnestic MCI 29 .\",\n       'As a consequence of globalization, countries compete with one another to be able to provide better education and training to their citizens so that they do not fall behind in their cooperation and competition with other countries and they become more successful and possess the necessary technology (Ayas, 1995). Developed and developing countries along with the countries that are aware of this competition frequently review their education systems and make various arrangements accordingly. As a result, new teaching curricula as well as new methods and techniques, based on scientific studies, are being introduced. It is possible to argue that the science curricula of several countries are based on a dynamic process and have a dynamic structure, just like any other curricula. We can claim that the science curriculum in Turkey also has this kind of structure changing in accordance with the needs. Turkey gained European Union (EU) candidate status in 1999 and initiated the negotiation process in 2005. The harmonization process between Turkey and the EU has been continuing to be conducted through negotiation chapters, the 26th of which is on \"Education and Culture.\" This chapter consists of the expectations from Turkey regarding the steps that need to be taken to achieve common objectives with other countries. In this sense, Turkey has primarily declared that it shares the educational and cultural objectives and priorities of the EU. In addition, the country has taken significant steps toward the goals of the union regarding Strategy and Education and Training 2020; the EU 2020 work program and actively participated in EU-level work platforms (Ministry for EU Affairs, 2017). In this process, Turkey is expected to train individuals with higher level skills and qualifications and high employability and who are highly innovative and active, as it is also expected from other countries. From the moment it gained the status of a candidate country, Turkey has conducted more intensive studies. Although these studies under the heading of education and culture undoubtedly require the investigation of different phenomena and developments, this article has focussed on what was done in the field of science teaching on the basis of teaching curricula. The EU countries plan their education and teaching policies at the national level. It is reported in the EU cooperation, specifically the education and training framework (ET 2020) (URL-1), that the objective is to urge member/candidate countries to work in cooperation with and learn from each other, to adapt to the information society through qualified education and teaching, and to contribute to the development of national economies. Also, following the call by the researchers for international studies to be conducted on different countries\\' curriculum standards (Bümen, 2006;Edwards, 2010;Liang & Yuan, 2008), this article focused on the content related to cognitive learning outcomes listed in the science curricula in Turkey where the curricula were revised several times after the acquisition of candidate country status. Questioning the quality of the implemented teaching curricula in training, the qualification of individual mentioned above can provide significant gains first for the country and then for the EU in achieving their common goals. This study is also significant as Turkey is in the process of membership of the EU with mismatches of educational attainment that learners throughout the educational system experience in terms of EU Standards. Assessing and mapping curriculum standards may provide a tool for identifying deficiencies, especially for Turkey, according to EU standards. Moreover, it may assess and be attribute to the grounds of Turkey\\'s poor results in international tests such as TIMSS. The studies on standards of curricula measure the mapping between a state\\'s standards and objectives. Thus, these studies are important, and this will give the possibility of improvement by changing the standards. In this sense, examining the revisions made in science education in Turkey will not only provide information on the specific case of the country but also will contribute to the future experience of countries with similar features.',\n       'Proof. Obvious from the equation (12) . Q.E.D.',\n       \"Study data come from the Early Childhood Longitudinal Study (ECLS-B), a nationally representative study of over 10,000 children from the 2001 US birth cohort and their parents. The sample was drawn from a list of registered births provided by the National Center for Health Statistics (NCHS), using a clustered list frame sampling design with counties and county groups as the primary sampling units (PSUs) (Flanagan and West 2004) . This study examines biological mothers' survey responses at four time points: child approximately 9 months old (2001-2002, baseline) , 2 years old (2003) (2004) , 4 years old (2005-2006, preschool) , and 5 or 6 years old (2006-2007, kindergarten) . Trained interviewers visited the respondent (mother's) home at each wave with a $30 respondent fee for mothers and a book for the child.\\nSmoking and alcohol use measures at 3 months prior to conception and during the third trimester were retrospectively collected at baseline, with the advantage of circumventing potential antenatal underreporting due to respondent beliefs about socially undesirable behaviors. In population-based studies, retrospective reports of prenatal drinking have been more forthcoming and accurate than self-reported drinking during pregnancy (Alvik et al. 2006) . Similarly, retrospective reports of prenatal smoking have been shown to align with self-reports and cotinine measurements during pregnancy (Pickett et al. 2009 ). Sample n's are rounded to the nearest 50 in compliance with ECLS-B rules. Cases that missed all smoking/drinking measures (N = 100) or any exogenous variables (N = 1600) were excluded, resulting in an analytic sample of 8800 (84% of the original sample) adult biological mothers. The selected sample is significantly different from the excluded cases on a number of study variables; however, the average magnitude of the difference is small, i.e., 2% for smoking measures, 6% for drinking measures, and 6% for exogenous variables (see online supplement for details). In order to assess whether the exclusion of cases with missing data bias the study results, we conducted a sensitivity analysis and found that a six-class model using all cases is similar in structure as the presented model (results available upon request).\",\n       \"Despite an era of growth before No Child Left Behind, geography education, specifically Advanced Placement Human Geography was not immune to pedagogical challenges that plagued teachers of high school geography. Numerous authors highlight and address the lack of teacher training as a significant problem for A.P. Human Geography's future success (Lanegran & Zieglar, 2016;Murphy, 2000;Sharma, 2005;Trites & Lange, 2000). A 2014 report for the National Geography Society Education Foundation indicated the majority of pre-service social studies certified teachers had only three credit hours of college geography (National Geography Society, 2014). The current problem represents a persistent trend in high school geography's history. The problem underscores numerous attempts at increased training for high school geography teachers (Boehm, Natoli, & Peterson, 1994;Helburn, 1998;Koelsh, 2008;Meyer, 1946;Pattison, 1962b). As of 2015, A.P. Human Geography is the second fastest growing Advanced Placement course (College Board, 2015b). Such enrollment increases provide greater opportunities for professional development. One professional development opportunity is the collective re-examination of William Pattison's contributions to geography education. While trace elements of Pattison's pedagogy and planning stages exist, few researchers have investigated the pedagogical impact of his material in the modern geography classroom. This investigation focused on teachers of A.P. Human Geography and their incorporation and utilization of Pattison's educational and content guidelines. Such an investigation provides support and evidence needed for A.P. Human Geography growth among high schools nationally. Building on previous research, the researchers interviewed A.P. Human Geography teachers to gain the answers to the following question: To what extent do A.P. Human Geography teacher's pedagogy approaches align with William Pattison's \",\n       'The current clinical diagnostic criteria for AD were outlined more than 25 years ago by the National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer Disease and Related Disorders (NINCDS-ADRDA) Work Group. They depend largely on the exclusion of causes other than AD for dementia (McKhann et al. 1984) . These criteria state that a diagnosis of AD cannot be made until the patient has dementia, which is defined as \"cognitive symptoms severe enough to interfere with social or occupational activities.\" The DSM-IV and ICD-10 criteria, which are used for routine diagnosis, also require that a patient demonstrate dementia before a diagnosis of AD is possible (WHO 1992; American Psychiatric Association 2000) . If the new disease-modifying drugs prove to be effective and become clinically available, these present criteria will hinder patients in the early stages of the disease, certainly in the preclinical stage, from receiving effective therapy.\\nFor this reason, new criteria for different stages of AD have recently been suggested (Dubois et al. 2007; Albert et al. 2011; McKhann et al. 2011; Sperling et al. 2011 ). These criteria have been constructed to permit a diagnosis of AD in earlier stages of the disease, and are centered on the clinical identification of episodic memory impairment together with one or more abnormal biomarkers, including MRI, PET, and CSF markers. More detailed guidelines are needed to establish how the use of biomarkers can be implemented in the diagnostic procedure for early AD to be used in clinical practice. For example, details are needed regarding the scale that should be applied to measure memory impairment, which assays and cutoffs to use for CSF biomarkers, which brain region (whole brain, hippocampus, or entorhinal cortex) to use to evaluate brain atrophy by MRI, which amyloid ligand to use, and which brain region to use to evaluate brain Ab load by PET. Studies on these issues are just beginning to emerge (Frisoni et al. 2009 ). Biomarker assays also need to be standardized between laboratories and centers to allow for general implementation of cutoff points in the diagnostic algorithms. As a first step in this direction, a global quality control program for CSF biomarkers was recently launched. This program also covers practical details on lumbar puncture and CSF sample processing . Results from the first two rounds in this quality control program have recently been presented.',\n       'In the setting of clinical trial design, the anticipated sample size necessary for an outcome to detect a treatment effect is critically important and has been used as a standard of comparison for longitudinal outcomes. The standard estimate that has been used for this comparison is the sample size per arm required to detect a 25% reduction in atrophy/decline with 80% power and 5% significance [3] . This criterion reflects the sensitivity to decline or external responsiveness of a clinical outcome, which is the ability of a clinical outcome to change with time and disease progression. External responsiveness takes a pre-eminent role in evaluating clinical trial outcomes because the nature of AD, even in its very early stages, is degenerative, and the aspects of the disease which degrade over time offer the most promise in terms of outcomes that reflect the disease process.\\nAlthough CSF biomarkers and amyloid imaging seem to be the best at classifying or selecting individuals, volumetric MRI outcomes seem to be sensitive biomarker measures of disease progression in MCI and pre-MCI populations. Whole-brain volume and hippocampal volume have been shown to be more sensitive to longitudinal disease progression than cogni tive measures -ADAS-cog and mini-mental status exami nation (MMSE)in an MCI population [11] [12] [13] . Some of the MRI regions that show sensitivity to decline in an MCI population are also sensitive to decline in a pre-MCI population.\\nMany studies in MCI demonstrate that a smaller sample size is required with a biomarker such as volumetric MRI as the primary outcome than for studies with a primary clinical outcome [3] ; however, this does not imply that clinical outcomes are not important in these early stages. In fact, it may be harder to change an outcome such as volumetric MRI than to change a clinical outcome, particularly if a treatment has both a disease-modifying and a symptomatic effect. The ability of an outcome to change with a treatment effect is referred to as internal responsiveness or as sensitivity to treatment effects. If volumetric MRI has less internal responsiveness than a clinical outcome, factoring this into the sample size calculations would reduce the apparent advantage that volumetric MRI has over clinical outcomes. Also, relying exclusively on a biomarker outcome has the risk that a treatment effect on a biomarker may not translate into a treatment effect on a clinical outcome.\\nFor both of these reasons, it is advantageous to measure standard clinical outcomes in studies that have a biomarker as a primary outcome, even though the studies may not be powered to show significance on a clinical effect. Additionally, studies with both biomarker and clinical outcomes will facilitate future validation of biomarker outcomes and will provide data to support development or testing of future composite clinical outcomes combining items from standard instruments.',\n       'Lara is seeing herself saying \"which a lot of them are very.\" She sees herself incompletely detailed, but she sees herself in a third-person perspective from the front, upper torso wearing a white shirt and she has her hair up [today she\\'s wearing her hair down]. The words seem to be part of a larger sentence that preceded what she has written down and then would have gone on except for the interruption of the beeper. She is not sure who she is talking to, but the content is related to the beeper study.\\nSusan has been imagining herself bartering in Marrakesh, and she speaks to the seller in German: \"Was kostet das?\" Now she is innerly saying silly in a weird questioning tone that conveys her wonderment that she would speak in German to a person who probably doesn\\'t speak German.\\nthe most adequate procedure. Additionally, the six rigid body movement parameters were also included in the single subject GLM. Differential t-contrasts between (unanimous) internal vs. external events were calculated per subject and taken to group level analysis. On the second level, these differential t-contrast images were entered into a one-sample t-test. The whole-brain results were thresholded at p < 0.001, and to correct for multiple comparisons a significant effect was reported when the volume of the cluster was greater than the Monte Carlo simulation determined minimum cluster size above which the probability of type I error was <0.05 (Ward, 2000) . The resulting maps were overlaid onto a normalized T1 weighted MNI template (colin27), and the coordinates reported correspond to the MNI coordinate system.',\n       nan,\n       \"In this study I have introduced a method for assessing community college students' enrollment patterns and describing their variation. I have also created a typology of enrollment comprised of six clusters of enrollment types based on the information gleaned from enrollment intensity and continuity patterns. This section first discusses why there is so much variation in students' enrollment patterns. It then addresses the ways that the method employed in the study, along with the research findings, may prove useful for stakeholders such as college administrators, policymakers, and researchers. The section concludes with some directions for further research.\",\n       nan,\n       \"As I have noted, the world shares many common interests in the international reform agenda. New global realities require new global approaches and institutions, while still taking into account the unique characteristics of individual nations. With so many reforms to make, here are a few general lessons to help make them 'stick'.\\nWe must use the economic and political cycles to our advantage. Economic upswings are the best time to introduce labour-market reforms because the uncertainty that comes with the reforms is then balanced out by the fact that people will have more opportunities and rising incomes. As far as political cycles are concerned, the momentum that comes with a new government is often the best time to introduce substantial reforms; when governments cynically announce reforms in the year before an election they are either not serious or will not have the opportunity to properly implement them.\\nReforms will be successful only if they are first supported by solid evidence for the need to reform and some degree of community acceptance of that need. If these are lacking then opponents will question the evidence on which the reform proposals are based. And even if such questioning is false or simply expedient, it will not usually be interpreted that way by the media, who will argue the government has overlooked something crucial or that it does not have a solid response to criticism of its policy.\\nGovernments must communicate consistently about a proposed reform, because if there is an inconsistency in communication, the reform initiative will be hard to maintain. The wise policy maker proposes reforms in general terms so as to have the flexibility to adjust their instruments. If communication focuses on the goals of the reform and the instruments remain flexible then they will have greater chances of enduring success. It is also fundamental for the policy maker to monitor the process from day one because inevitably during the time it takes to introduce reform the context or some circumstances will change. Some opponents will become more powerful or less powerful. Some changes will emerge that were previously not there, perhaps relating to the international economy, changes in the labour market or whatever. In short, if we do not monitor the expected results of our reforms from the outset, the case for these reforms will weaken. In contrast, if we can prove through monitoring the results of such reforms that positive results have indeed been achieved, the case for reform can be strengthened.\\nReforms are more likely to endure if strong institutions are in place to make the case or even prosecute the need for a consensus for the reform. In Australia, for example, institutions such as the Productivity Commission and the Council of Australian Governments (COAG) have successfully acted as a mechanism to create the casework for reform. Australian institutions do not, however, usually include other important stakeholders such as the business community, whereas some countries have institutions that include business and unions and can therefore make solid forums in certain areas and establish the support and involvement of business from the initial stage of the reform process.\\nThe more you engage with your opponents beforehand, the more successful the implementation of the reform will be. This is difficult because engaging with opponents requires a clear distinction between building consensus and negotiating compensation for losses. The problem is that good negotiators-be they for business, unions, doctors or any other group-will start to make selfinterested claims in the process of consultation. In other words, they want to negotiate first and build consensus later. Alternatively, they want to use the fact that they are needed for consensus as a ticket to negotiate. Take doctors or teachers as examples. These professionals enjoy a high degree of public trust. Consequently, if as a group they believe a reform proposal will inhibit their ability to provide adequate health care or educational outcomes, the public will agree, and will blame the government. This means that if reformers do not include players such as teachers and doctors in the reform process, they will become powerful opponents with the potential to generate blocking votes or veto points. And while compensating the losers is often necessary, it must be remembered that reforms are first and foremost made to correct an injustice, so subsequent losses are not always socially unjustified.\\nThe final lesson for successfully 'making reforms stick' is the most obvious: set goals beyond election horizons, for governments to learn from each other to reduce the trial and error time. This is perhaps the wisest advice of all for authors of policy.\\nWith these lessons in mind, I would suggest that the challenge awaits.\",\n       nan,\n       'Learning sciences is an interdisciplinary field that studies teaching and learning. Learning scientists study learning in a variety of settings -not only the more formal learning of school classrooms, but also the more informal learning that takes place at home, on the job, and among peers. The goal of the learning sciences is to better understand the cognitive and social processes that result in the most effective learning, and to use this knowledge to redesign classrooms and other learning environments so that people learn more deeply and more effectively. The sciences of learning include cognitive science, educational psychology, computer science, anthropology, sociology, information sciences, neurosciences, education, design studies, instructional design, and other fields. In the late 1980s, researchers in these fields who were studying learning realised that they needed to develop new scientific approaches that went beyond what their own discipline could offer, and to collaborate with other disciplines. Learning sciences was born in 1991, when the first international conference was held, and the Journal of the Learning Sciences was first published. This new science is called the learning sciences because it is an interdisciplinary science; the collaboration among these disciplines has resulted in new ideas, new methodologies, and new ways of thinking about learning. The first comprehensive overview of the field was published in 2006: The Cambridge Handbook of the Learning Sciences (Sawyer, 2006b ).\\nLearning sciences researchers are working to design more effective learning environments -including school classrooms, and also informal settings such as science centres or after-school clubs, on-line distance learning, and computer-based tutoring software. These classroom environments combine new curricular materials, new collaborative activities, support for teachers, and innovative educational software. Learning sciences research suggests several alternative models of learning, particularly those that involve deep links between formal schooling and the many other learning institutions available to students -libraries, science centres and history museums, after school clubs, on-line activities that can be accessed from home, and even collaborations between students and working professionals. In this report, I draw on learning sciences findings to identify a set of principles that should guide the development of alternative models of learning.',\n       'Data employed in this paper are downloaded from Our World in Data [8] . Their data for COVID-19 related deaths are retrieved from the European Centre for Disease Prevention and Control. As explained by the COVID-19 Health System Response Monitor [9] , figures may vary among countries and may complicate cross-country analysis. This problem is most serious for the headline figures presenting the most recent day-to-day data. We use data published at the end of July, but only data up to the first week of July. In Section 4 we will discuss the implications of such uncertainties for the conclusions of the analysis.\\nThe official figures from China suffer from 40% discontinuous increase in death numbers on April 17. The official explanation is that home deaths before that date were included. In our analysis, we account for this by adjusting the numbers before April 17 by a factor 1.4, thus removing the discontinuity.',\n       nan,\n       \"This study is being conducted in Columbia, SC. According to the 2000 Census [28] , Columbia is the largest city in SC, with a total population of 116,278, within a standard metropolitan statistical area (SMSA) of 536,691. Of persons naming one race, 46% of residents were African American (non-Hispanic) and 49% Caucasian (non-Hispanic). Among all residents, 3% report Hispanic ethnicity. Almost one-quarter (22%) of the city's residents are below poverty (compared to 14% for the state, which ranks 12th poorest in the nation), and 18% have less than a high school education. Among female-headed households with children under age 5, 57% live below poverty. Of 61 census tracts in the city, 18 tracts have at least 25% of all residents living below poverty (mean 39.1%, range = 25% to 62%). STARS participants were recruited from these 18 census tracts, which have a combined population of 44,317 and a median household income of $20,719.\",\n       'WGS was performed on blood-derived genomic DNA samples obtained from 818 ADNI participants. Samples were sequenced on the IIIumina HiSeq2000 using paired-end read chemistry and read lengths of 100bp (www.illumina.com). The resulting IIIumina qseq files were converted into fastq files, a text-based format for storing both sequence reads and their corresponding quality information in Phred format. Short-read sequences were mapped to the NCB I reference human genome (build 37) using BWA, allowing for up to two mismatches in each read. During the alignment, we use only bases with Phred Quality> 15 in each read to include soft clipping of low quality bases, retain only uniquely mapped pair-end reads, and remove potential PCR duplicates. After completing initial alignment, the alignment is further refined by locally realigning any suspicious reads. The reported base calling quality scores obtained from the sequencer are re-calibrated to account for covariates of base errors such as sequencing technology and machine cycle. Finally, the realigned reads are written to a BAM file for further analysis (see Figure 1 ). Variant Discovery: The analysis-ready BAM files are analyzed to identify all variants with statistical evidence for an alternate allele present among samples using the HaplotypeCaller module of GA TK for multi-sample variant callings. The first type of multi sample variant caller is to reduce the analysis-ready reads (BAM) file to a manageable size by keeping only essential information for variant calling that allows greater performance and scalability for multi-sample variant callers (\"REDUCE\"). The second type of multi-sample variant caller is to first call variants individually on each sample to produce a comprehensive record of genotype likelihoods and annotations for each site in the genome and then perform a joint genotyping analysis of the variant files produced for all samples in a cohort (\"JOINT\"). The HaplotypeCaller module of GATK calls SNVs and indels simultaneously via local de-novo assembly of haplotypes in an active region. The quality of the variant calls was assessed by comparing sequencing-derived SNVs with those obtained from the IIIumina Omni 2.5M genotyping array in order to estimate the concordance rate. Among 818 subjects, two subjects had concordance rates less than 99% and had been removed from our analysis. We used a same pre-calling procedure and two different multi-sample variant calling methods to identity SNVs and indels from 818 ADNI WGS data. First we compared the numbers of SNVs and indels across two multi-sample variant callers. Figure 2 and Table 1 JOINT increased the proportion of called variants, i.e., identified 43% and 14% more SNVs and indels compared to REDUCE. 98.1% (351,648 SNVs) and 9l.0% (48,101 indels) of the REDUCE SNV and indel calls, respectively, are also present in the JOINT set. The concordance ratios of the common SNVs and indels from two caller methods are 99.60% and 99.06%, respectively. The observed transition-to transversion ratios for the SNV sets on chromosome 22 for JOINT and REDUCE are 2.39 and 2.36, respectively. In order to assess the quality of the variant calls, we compared sequencing-derived SNVs with those obtained from the Illumina Omni 2.5M genotyping array and overall genotype consistency rates are 99.7% for the JOINT SNV set and 99.5% for the REDUCE SNV set.',\n       'The existing research on graduate school enrollment was conducted using older data, or examined enrollment behavior over a narrow timespan. Although this study is built on models developed by Dumais (2002), Perna (2004) and others, it differs in the number and representation of variables included in the analysis based on a recent review of the literature. Utilizing a more recent dataset also distinguishes this dissertation from other existing research. Additionally, of the handful of studies exploring the cyclicality of graduate enrollment, only Goh (2009) utilized IPEDS to examine graduate enrollment cyclicality using a 30-year time period. 7 This dissertation analyzes more recent data and incorporates county, state and national level unemployment rates as proxies for labor market conditions. The introduction of the Higher Education Reconciliation Act of 2005 (HERA) provides an opportunity to examine the credit 7 Goh examined IPEDS data from 1976 -2005 constraint effect pre-and post-2006, when the law took effect and lifted the ceiling on graduate school debt.',\n       'As we have shown, SETs moved to a different school at a significantly higher rate than GETs (10.2% vs. 7.4%, in 2000 GETs (10.2% vs. 7.4%, in -2001 . In this comparison, the migration of SETs appears to have been excessive. Without more information, however, it is not possible to determine whether the migration rate of GETs was excessive. Nonetheless, a mover is just as costly to a school as a leaver is even though school migration does not represent a loss to the national teaching force (e.g., Ingersoll, 2001) .\\nUnfortunately, we do not have specific information to report on the reasons for SET migration. From this research, we have shown that more than half the amount of school migration of both SETs and GETs occurred between schools within the same school district. In prior national research based on all public school teachers, the main reason for within-district migration was involuntary on the part of teachers (i.e., 51 % was due to school staffing actions; Boe et aI., 1999) . Thus, education administrators are responsible for a considerable amount of migration.\\nBased on TFS data for 2000 -2001 , Luekens et al. (2004 computed the percentage of all public school teachers who rated various reasons for moving as very important or extremely important. By far, the rwo most important reasons (reported by about 40% of movers) were opportunity for a better teaching assignment (subject area or grade level) and dissatisfaction with administrative support at the previous school.\\nEven though the major factors driving school migration (staffing actions, unfavorable teaching assignments, and inadequate administrative support) are subject to administrative interventions, migration held steady at a fairly high level during the 1990s. Either interventions intended to reduce migration have not been successful or policy makers have not initiated such interventions. Consequently, it is not realistic to expect that the management of public education will be im-proved sufficiently in the foreseeable future to reduce teacher migration substantially.',\n       'However, true experiments in education are difficult to conduct and maintain under the best of circumstances. Many educational researchers have therefore relied on observational survey data to make cautious inferences about policy effects on achievement gains. While these studies have many well known inherent flaws, most educational researchers and policy makers have been determined to not let the \"perfect be the enemy of the good\" and have conducted well thought out and executed policy studies with the longitudinal studies data systems provided by NCES (Heyns & Hilton, 1982, pp. 89-102).',\n       \"We compared the number of neurons in the four study groups by one-way ANOVA followed by Tukey's post-test using GraphPad Prism version 4.00 for Windows (GraphPad Software, San Diego, CA, USA). Since analysis of data distribution for neuronal volumes revealed signiWcant departures from normal distribution with signiWcant right skewness, we used parametric and non-parametric (Kruskal-Wallis) ANOVA to test the null hypothesis. With signiWcant main eVects, ANOVA was followed by post hoc tests (Kolmogorov-Smirnov) to analyze diVerences between particular groups. The Kolmogorov-Smirnov test was chosen because it is particularly sensitive to changes in the shapes of distribution. The results are represented as arbitrary categories of small (0-9,000 m 3 ), medium (9,001-18,000 m 3 ), large (18,001-27,000 m 3 ), and very large (27,001-36,000 m 3 ) neurons to simplify the description of changes in the shape of distributions. This range of neuronal sizes from small to very large neurons included more than 97% of all neurons and was divided into four equal groups to represent small, medium, large, and very large size neurons. Neurons with cell body volumes greater than 36,000 m 3 were omitted from the Wgures representing the Kolmogorov-Smirnov test because of negligible percentages observed (Figs. 6,  7) . Statistical analyses were also performed for the whole range of raw (not-clustered) data. These analyses were performed using Statistica 6.0, StatSoft, Inc. Tulsa, OK, USA.\",\n       'Participants at the SURA coastal resilience workshop in October 2014, agreed with the urgency of adopting far-reaching interdisciplinary approaches to modeling future risks and resilience of socio-eco-techno-logical systems, as articulated by the IGBP and the Stockholm Resilience Centre. The complex interdependence among human communities, coastal ecosystems, climate and ocean physics is accepted as axiomatic by the vast majority of the scientific community. However, many universities are not up to the task of true interdisciplinary research. Part of the problem relates to the accreditation system and its discipline-specific standards. This impedes interdisciplinary work at many traditional universities. Multi-discipline papers with many authors are not really valued and young untenured faculty who engage in too much interdisciplinary work may be denied tenure. The discipline-based distribution of faculty on campuses is also a discouraging factor: social scientists and natural scientists may be based on opposite sides of large campuses or even on different campuses of multi campus state universities. The world is likely to be very different in 2050, as will the missions of universities that remain relevant.',\n       \"Subjects were recruited from five multicenter memory-clinic based studies:\\nDESCRIPA (Visser, et al., 2008) , German Dementia Competence Network (DCN) (Kornhuber, et al., 2009) , EDAR (www.edarstudy.eu), the European Alzheimer's Disease Consortium (EADC)-PET study (Morbelli, et al., 2012) and American\\nAlzheimer's Disease Neuroimaging Initiative (ADNI-1) study (Mueller, et al., 2005) ;\\nand nine centers of the EADC and/or European Medical Information Framework (EMIF)-AD: Amsterdam (van der Flier, et al., 2014) , Antwerp (Somers C., In press), Barcelona (Alcolea, et al., 2014) , Brescia (Frisoni, et al., 2009) , Coimbra (Baldeiras, et al., 2008) , Gothenburg (Wallin, et al., 2016) , Kuopio (Seppala, et al., 2011) , Liège (Bastin, et al., 2010) and Lisbon (Maroco, et al., 2011) . For subjects who participated in more than one study, we used data from the study with the longest follow-up.\\nInclusion criteria consisted of baseline diagnosis of MCI according to the criteria of Petersen (Petersen, 2004) , and at least one of the following biomarkers available at baseline: amyloid-beta (Aβ) 1-42 and tau (total tau and/or phosphorylated tau) in CSF, hippocampal volume on magnetic resonance imaging (MRI) or cerebral glucose metabolism on [18F]FDG-PET of the brain. Moreover, baseline data had to be available on at least one of the selected risk factors, as well as information on M A N U S C R I P T\",\n       'The following databases were used in this analysis:\\n-JTWC http://www.usno.navy.mil/NOOC/nmfc-ph/ RSS/jtwc/best_tracks/index.html (Chu et al., 2002) -IBTrACS (v03r06: http://www.ncdc.noaa.gov/ibtracs/ 50index.php?name=ibtracs-data) -SPEArTC (data set updated 28 January 2014: http: //apdrc.soest.hawaii.edu/projects/speartc) (Diamond et al., 2012 ).\\n-Lourensz TC Database -digitised from Lourensz (1977 Lourensz ( , 1981 -Callaghan and Power Landfalling TC databases -digitised from Callaghan and Power (2011) .\\nThe Supplement related to this article is available online at doi:10.5194/nhess-16-1431-2016-supplement.',\n       'We are thankful for a postdoc that helped us with crafting the idea and executing the research but decided to remain anonymous due to the potential negative consequences arising from a study highlighting the \"broken system\" s/he is part of.',\n       'The complexity and multidisciplinary nature of the subject of migration in conjunction with the proliferation of articles on specialized topics have made the study of this phenomenon extremely difficult (Greenwood, 1993). It is reasonable to assume states prefer to retain graduates from their own institutions for employment within the state. Land-grant Universities, for example, receive large sums of money from taxes. Therefore, when students move to another state and are not replaced by someone moving to the state, the tax revenue raised from that employee does not benefit the cost of education incurred by the degree granting state (Ballweg & Li, 1992). It is interesting to note the extent to which states deviate in the numbers of individuals who cross state lines either to enter or to leave. (Tornatzky, et. Al., 1998). Understanding general population migration will help us understand who is likely to leave among science and technology professionals and what those patterns look like. Fulton, Fuguitt, and Gibson (1997) examined the demographic and socioeconomic characteristics of migration streams between metropolitan and non-metropolitan areas between 1975 and 1993. This study utilized data obtained by the U.S. Census Current Population Surveys (CPS). The CPS contains a one-year migration question which along with information regarding respondents current residence allows respondents to be classified into four migration groups: metro-to-non-metro movers, non-metro-to-metro movers, metro stayers and non-metro stayers. Demographic variables examined in this study included sex, race, age, education, poverty status (poor, near poor, not poor) and occupation (lower blue collar, upper blue, lower white collar, and upper white). In terms of specific demographic characteristics, Fulton, et al. (1997) found age to be inversely related to migration such that younger individuals tend to move more. The general relationship between age and propensity to migrate has been known for some time (Greenwood, 1993). The authors assert that increased movement among younger individuals may have been necessitated by a search for a particular educational program, employment or a better job. Similarly, the authors note higher levels of migration among males, respondents with 4 or more years of college, those above the poverty level and those whose occupational status was classified as white collar. In terms of general migration patterns between rural and urban areas, they found three major (and unanticipated) shifts. Until the 1970s, non-metropolitan areas typically observed net population losses. However, the first observed shift occurred during the 1970s when nonmetropolitan areas witnessed an increase in net in-migration and retention particularly of young and better-educated residents. The second trend occurred in the 1980s where the trends of the 1970s were reversed and non-metropolitan areas witnessed net migration losses particularly of young, better-educated residents and those working in white-collar occupations. In the 1990s, migration patterns have been similar to those in the 1970s such that non-metropolitan areas again showed net gains among higher status groups. Shelley and Koven (1993) examined state level factors thought to impact interstate migration in the United States during the 1970s. They argue that migration has political, economic as well as social relevance. Politically speaking, migration affects the electoral power of states and the size of a states\\' delegation to congress. Since economic environments are said to influence ones decision to move, migration is also related to the economic health of a given area. Similarly, migration is obviously related to the amenities present in a given area or the \"quality of life\" that is available to area residents. This study examined the relationship between net migration and a wide array of variables that are subsumed under five composite dimensions; fiscal policies (taxes and spending), social hostility (crime and wealth), labor relations (right to work laws, unemployment, union membership, wages, and work stoppages), ecological context (size of state population, population density, temperature, school enrollments, public assistance, and maturity of interest group structures), and infant mortality. Multiple regression was used to estimate how well the independent variables predicted state migration patterns. Results based on the full model were ambiguous due to problems with multi-colinearity, and an exploratory factor analysis was conducted. As described above, four dominant factors and one minor factor consisting of only a single factor (infant mortality) emerged. Principle components multiple regression was then performed (F=18.44, p< .0001, R 2 =.68). The hypothesized model proved to be highly salient in predicting rates of net state migration in the 1970s. Specifically, results of this study indicate that ecological context (quality-of-life) variables exert the greatest predictive power, and that composites of fiscal policy and labor relations (e.g., factors that are more controllable by public decisionmakers) were also significant. Infant mortality was significant and Social Hostility did not prove to be statistically significant. It is my opinion that while these patterns were true of the 1970s they may be different today. Our nations population movement patterns have become much more complex than they were in the past. The study of migration and retention may provide answers to some basic questions that are frequently cited in the literature. \"Why do some areas grow while others decline? Why do people move? Do different types of people move for different reasons? And, are we in danger of a population explosion that test the capacity of our resources?\" Research by Long (1988) shows that more than one half of households in the U.S. moved for job-related reasons in the late 1980s. Understanding population composition, distribution, and change is essential for making decisions in both the public and private sectors (Plane & Rogerson, 1994). In the private sector, the success of an organization is often directly linked with the ability to identify and target specific demographic groups or segments of the local population down to the smallest geographic units for both market and employment selection purposes. In the public sector, for example, school enrollment levels depend critically upon the size and age composition of the population at the neighborhood or community level. Until recently, a thorough knowledge of how local populations are likely to change would have dramatically affected an organizations planning process for personnel recruiting and selection. However, today, demographic knowledge of the local population is not enough to sustain quality work-force needs. Organizations, both public and private, now need a more comprehensive understanding of how to attract quality personnel from widely disbursed geographic locations. Therefore, an understanding of demographic trends that are associated with the composition, distribution, and change of a particular population (i.e. recent science and engineering graduates) is of more immediate concern to many decisionmakers, practicing planners, and business people.',\n       \"The FACES 2006 cohort followed a nationally representative sample of 2,020 three-year-old and 1,295 four-year-old children enrolled in 125 Head Start centers across the country between their enrollment in Head Start (fall 2006) and the end of their kindergarten year (spring 2008 or 2009). To achieve the goal of a nationally representative sample, FACES 2006 used a probability-proportional-to-size design in the first three stages (program, center, and classroom) followed by a fourth stage (children) that used equal probability sampling. In all, 60 programs were selected, two centers per program, and up to three classrooms per center, for a total of 415 classrooms. Approximately 10 children were selected per class, with an oversampling of 3-year-olds to account for the additional year of follow-up. FACES 2006 also used stratification at each stage of selection to ensure sample representativeness (for more information, see West et al., 2010). For the purposes of this study, we restricted our sample to children who (a) had experienced Head Start for 2 years, at the age of 3 and 4 (n = 1,203), which allowed us to examine these longitudinal processes within the context of the Head Start program; (b) did not switch language of assessment; (c) had a center-level identification number for clustering purposes; and (d) had a longitudinal sampling weight. The last two exclusions were required for our modeling procedures. These restrictions resulted in a sample of 1,020 children (51% female) enrolled in 118 Head Start centers (see Table 1 for sample demographics). It is not surprising, given our exclusion criteria, that the 15% of 3-year-old children who were excluded from our final sample were more likely to be Latino and from a language minority household. These families were also more likely to have lower levels of education and to be unemployed (but not of lower income); however, these latter differences were largely due to the overlap between indicators of socioeconomic status and children's race/ethnicity and language minority status. Thus, our sample was not representative of Latino dual-language learners in Head Start. Children were, on average, 40.83 months old at beginning of the Head Start program, and parents averaged 28.5 years. The majority of parent respondents were mothers (87%) and identified themselves as being of Black race (41%) or Hispanic ethnicity (27%), with a smaller number of children coming from families whose race was White (22%) or some other racial group (10%). It is important to note that in FACES 2006 only the parent respondent reported on the family. The majority (66%) of children came from single-parent families, 1 in 3 children had mothers with less than a high school education (32%), and slightly less than half had mothers who were unemployed (44%). Analyses of variance and chi-square tests indicated that family background variables were stable over time; thus, we considered these variables to be time invariant and used values from children's 3-year-old year as covariates.\",\n       \"As part of the National Estuarine Research Reserve System's System Wide Monitoring Program (SWMP) bio-monitoring protocol, in 2014 ANERR staff began long-term monitoring of the freshwater and brackish emergent vegetation in the marshes of the lower Apalachicola River and the salt marshes of Little St. George Island. Monitoring locations are intended to represent natural estuarine communities that have not been significantly altered by natural causes or human activity (Moore 2009). Three transects at each location are monitored annually at the peak of biomass following Moore (2009). Two wells for monitoring pore water were installed adjacent to each transect. Elevation and sediment accretion has also been studied in the Apalachicola region. In 1996 two sediment elevation tables (SETs) were installed by the Florida Geological Survey in a distributary of the Apalachicola River that drains into East Bay. Data from these SETs showed that the marshes did have high rates of accretion (as much as 0.5-0.75 in./14-19 mm per year). Nevertheless, overall elevation changes were negative due to compaction and subsidence in the river delta (Hendrickson 1997, Edmiston 2008. Additionally, 20 SETs were installed in 2011-12 to monitor erosion and accretion rates in the lower-river marshes of the Apalachicola floodplain and in the salt marshes of the barrier islands. These monitoring efforts are part of the National Oceanic and Atmospheric Administration's Sentinel Site Program designed to track ecosystem integrity and socioeconomic health indicators for specific management initiatives. The data will be provided to researchers for modeling biological feedback to sea-level rise. These models will allow stakeholders and decision makers to understand how sea-level rise will affect freshwater and saltwater marsh habitats in the Apalachicola area.\",\n       'The National Occupational Competency Testing Institute (NOCTI) is a nonprofit organization, headquartered at Ferris State University in Big Rapids, Michigan. According to Whitener (1981 andin personal communication, 1994), NOCTI provides high-quality occupational competency examinations on a national level. Each examination has two parts: a written test that measures technical knowledge required in the occupation and a performance test that measures skills typical of the occupation. The services at NOCTI include test development, revision, updating, and scoring, plus job and task analyses that lead to test development. NOCTI occupational competency examinations are the most widely used tests for trade and industrial teachers in the United States. NOCTI administers 56 occupationally specific exams, ranging from air conditioning, heating, and repair to welding. Others include, for example, the most frequently administered exams: auto mechanics, carpentry, cosmetology, industrial electrician, machine trades, electronics technology, printing, and quantity food preparation. In addition to establishing occupational competency and meeting state requirements for teacher certification, colleges and universities grant credits for successfully completing NOCTI teacher tests. According to the most recent survey data (1989), 92 colleges and universities in 43 states grant from 9 to 45 semester hours of credit for persons who successfully complete a NOCTI teacher test(s). Thirteen states use the national mean as their cut-off score for \"passing\" the NOCTI test; 11 use the national mean minus one standard deviation, 4 grant \"passing\" to anyone who scores above the 40th percentile on the national norms, and other variations are used by 11 states. Norms are updated each time tests are scored. National data are not available on the number who \"pass\" the exam nor on the number of retakes. One large state (Michigan) reports a 17 percent retake group. Eighteen states require NOCTI tests for either initial certification, to recertify within the first year of teaching, or for preservice teachers who lack work experience. In addition, two states use their own tests and two more are considering requiring NOCTI exams as a condition for continued employment. In most cases, a candidate pays the fee (an average of $238) to take the exams. A total of 14,576 persons have taken the test since 1975, an average of 767 per year. As stated in interviews and correspondence with the director and assessment specialist of NOCTI (Whitener, personal communication, December 1993 and January 1994; Rupe, personal 55 5 4 communication, January 1994), no relationships between teachers\\' occupational test scores and subsequent performance of teachers or their students have been ascertained. There are no predictability studies. It is curious that so little research has been done on scores on NOCTI exams and any relationship they may have to quality measures of vocational education students and programs. NOCTI does collect information on each examinee about the number of years of work experience, grade or education level completed, and the number of years of teaching experience (if any) prior to taking the exam. No correlations on these variables with teacher scores, student achievement, or teacher performance have been done at NOCTI nor apparently at its area testing service centers. In the early 1980s, Whitener (1981) and Stewart (1984) completed dissertations using NOCTI test scores. Whitener found differential results using scores of a sample of 1,556 persons who had taken the examination from 1974 to 1980 in four specialized occupational areas: auto mechanics, carpentry, machine trades, and quantity food preparation. His general conclusion, \"It is apparent that occupational competency is related in different degrees to occupational experience, teaching experience, educational level, and their aggregate. There is evidence that these relationships vary from occupation to occupation\" (p. 159). Some occupations had no relationship. Based on his data, Whitener suggests that these variables not be used for teacher selection and certification. Stewart (1984) correlated occupational competency with such variables as job satisfaction, job satisfactoriness, demographic characteristics, and self-ratings by teachers. His population consisted of 155 Georgia secondary and postsecondary trade and industrial teachers who had passed NOCTI examinations as a condition of their employment during a two-year period, 1981-82. The teachers\\' average age was 39, and their average occupational experience included 11.5 years. The Minnesota Satisfaction Questionnaire and Minnesota Satisfactoriness Scale instruments were used to gather data from and about the teachers. Stewart found no reliable relationships among job satisfaction, perceived job satisfactoriness, written and performance occupational competency tests, age, teaching experience, teaching level, and occupational experience in the population. He concluded that the relationship between occupational competency and job satisfactoriness was not sufficiently strong to reliably predict job satisfactoriness with measures of NOCTI occupational competency nor occupational experience. Wilson (1984) reported on three state studies conducted in Pennsylvania in the 1960s and 1970s. The authors investigated the relationship between years of occupational experience and scores on Pennsylvania\\'s state-developed occupational competency examinations. One study investigated degreed teachers (with less than five years of occupational experience) and nondegreed teachers (mean of more than 14 years of occupational experience). All three studies concluded that years of occupational experience did not correlate with higher occupational test scores. In fact, in some occupational areas, there appeared to be a negative correlation between years of occupational experience and occupational competency examination scores. Occupational Experience vs. Education Tests/Credits/Degrees Doerfort (1989) compared two groups of beginning T&I and agricultural education teachers, one prepared through an industry route and one prepared through traditional teacher education programs, with their results on the National Teacher Examination Core Battery Tests. Those teachers prepared through teacher education scored higher on the communication skills, general knowledge, and professional knowledge components of the test. The amount of professional education received accounted for the largest portion of variance in each of the dependent variables.',\n       \"The 2019 Atlantic hurricane season ended up slightly above normal for most tropical cyclone (TC) parameters, with a total of 18 named storms, six hurricanes, and three major hurricanes occurring. By far, the most significant and devastating hurricane of the 2019 season was Hurricane Dorian. Dorian will be most remembered for the devastation that it caused in the northwest Bahamas, especially on the Abaco Islands and on Grand Bahama Island. It was also the longest-lived (14 days as a named storm and 10 days as a hurricane) and most intense (1-minute maximum sustained winds of 160 kt (82 ms −1 ) hurricane of the 2019 season (Avila et al. 2020). Dorian also generated the most Accumulated Cyclone Energy (ACE) of any Atlantic hurricane, accounting for ~40% of basinwide ACE accrued in 2019. This sidebar summarizes the meteorological history of Dorian along with the notable records that the hurricane achieved during its track across the Atlantic. Historical landfall records from 1851present are taken from the National Hurricane Center/Atlantic Oceanographic and Meteorological Laboratory archive located at: http://www.aoml.noaa.gov/hrd/hurdat/All_U.S._Hurricanes. html, and Dorian's observed values are taken from Avila et al (2020). Dorian became a tropical depression (TD) on 24 August in the central tropical Atlantic and was upgraded to a tropical storm (TS) shortly thereafter (Fig. SB4.1). Despite moving through an environment of relatively low wind shear and a warm sea surface (~28°-29°C), considerable mid-level dry air inhibited Dorian's intensification early in its lifetime. Dorian passed through the Windward Islands on 27 August as a TS. Dorian's center reformed farther north after interacting with Saint Lucia, and its center also reformed downshear (i.e., to the east) due to moderate westerly shear. This northeastward shift in track from where the models were originally forecasting the storm allowed it to avoid the elevated terrain of Hispaniola and Puerto Rico, which would have likely weakened the storm. It then turned northwestward and intensified as it moved into a more moisture-rich environment. Dorian became a hurricane as it tracked over Saint Croix on 28 August and reached major hurricane intensity on 30 August as it approached the Bahamas. SIDEBAR 4.1: Hurricane Dorian: A devastating hurricane for the northwest Bahamas-P. J. KLOTZBACH AND R. E. TRUCHELUT\",\n       'Logistic Regression examines the non-linear relationship between a binary outcome and categorical or continuous predictor variables. The logistic model outputs a probability of an event between 0 and 1 as the log of the odds ratio (3) (5) where β is the parameter coefficient and x is the value of the independent variable. SAS® PROC LOGISTIC was used to build the model and stepwise elimination with α = 0.05 was used to eliminate redundancy and keep the strongest predictors in the model. 10-fold cross validation was used for model evaluation. Table 1 and Table 2 illustrate the significant effects remaining in the Logistic Regression models, for models with and without network characteristic metrics included. Age, BMI, hypertension, and cholesterol all have increased odds of diabetes outcome, while education has decreased odds. This is consistent with outcomes of previous research and known risk factors for diabetes. In the model with graph metrics included, only closeness centrality remains as significant, in addition to the same demographic variables from the previous model. If this were a real network in the dataset (not simulated), this would indicate that people with shorter total paths to other people in the network would have increased risk of diabetes. Table 3 shows the model performance results for models with and without the social network graph characteristics. The models were evaluated based on the sensitivity, specificity, and ROC index for the validation data set. The logistic model performs best for models with and without graph metrics included, and the SVM model with polynomial kernel is comparable but the ROC index is affected by lower sensitivity. Figure 8 and Figure 9 provide a visual comparison of the area under the ROC curves for models with and without graph metrics, respectively. ',\n       'coronaviruses typically cause respiratory and enteric infections. 9 Clinical features and risk factors are highly variable, making the clinical severity range from asymptomatic to fatal. 10 Initially, the coronavirus belongs to a family of viruses that may cause various symptoms such as pneumonia, fever, breathing difficulty, and lung infection. 11 The SARS-CoV-2 infection mainly presents flu-like symptoms such as fever, cough and asthenia, similar to other coronaviruses. 12 Susceptibility seems to be associated with age, biological sex, and other health conditions. 13 Although severe lung injury has been described at all ages, in some high-risk individuals, such as the elderly or those affected by multimorbidities, the virus is more likely to cause severe interstitial pneumonia, acute respiratory distress syndrome (ARDS) and subsequent multiorgan failure, which are responsible for severe acute respiratory failure and high death rates. Typically, affected individuals display a variable extent of dyspnoea and radiological signs. 14 The SARS-CoV-2 is a novel RNA virus, with a typical crown-like appearance under an electron microscope due to the presence of spike glycoprotein on its envelope. 15 The SARS-CoV-2 virus belongs the same family as severe acute respiratory syndrome coronavirus 1 (SARS-CoV-1) and Middle East respiratory syndrome coronavirus (MERS-CoV). 16 There are four genera of CoVs: α-coronavirus (αCoV), β-coronavirus (βCoV) probably present in bats and rodents; while δ-coronavirus (δCoV) and γcoronavirus (γCoV) probably represent avian species. 10, 16, 17 These viruses are common in animals worldwide, but very few cases have been known to affect humans. The sources of SARS-CoV2 may be combined natural and zoonotic origin. Two conditions that can reasonably clarify the origin of SARS-CoV2 are: (i) natural selection in a lower animal host before zoonotic transmission; and (ii) natural selection in human beings succeeding zoonotic transmission. 10, 17 In response to the outbreak, the Chinese Centre for Disease Control and Prevention (China CDC) dispatched a rapid response team to accompany health authorities of Wuhan city to conduct epidemiological and etiological investigations. The WHO confirmed that the outbreak of the coronavirus epidemic was associated with the Huanan South China Seafood Marketplace, but no specific animal association was identified. 18 Scientists immediately started to research the source of the new coronavirus, and its genome sequence of COVID-19. 19 This virus blow-out quickly throughout China within a month. After infecting and causing the death of thousands This article is protected by copyright. All rights reserved. of persons in China, the virus has spread, reaching Italy and other European countries   and the USA 20-22 with the number of confirmed new cases currently increasing every day.',\n       'G(N, E) is the formal expression of a graph, where N represents a node, storing a geographic entity, and E represents an edge, storing a relationship between two geographic entities. Here, an entity may be a process, a sequence, or a state. As mentioned above, a process consists of one or more basic evolution sequences, and the basic sequences are linked together through instantaneous states to construct the geographic process. These instantaneous states, responding to linking the basic sequences, are defined as linked nodes. Thus, in a process-oriented graph model, there are four types of node, i.e., a process node, a sequence node, a state node, and a linked node. According to process semantics, there are three kinds of relationships among processes, i.e., a spatiotemporal relationship, an including one, and an evolution one. As a spatiotemporal relationship could be calculated in real time through geographic entities, a node stores it implicitly, e.g., a spatiotemporal distance, direction, and topology. Thus, there are two types of edge in our proposed model; one edge stores an including relationship, and the other stores an evolution relationship. To store four types of node and two types of edge, this paper designs a process-oriented two-tier graph model named PoTGM which includes a process graph and a sequence graph, as shown in Figure 5 . The process graph is the first-layer graph in which the nodes consist of process nodes, sequence nodes, and linked nodes, and the relationships consist of the including ones, between the process nodes and sequence nodes and the evolution ones and between the sequence nodes and the linked nodes. The sequence graph is a second-layer graph which consists of sequence nodes and state nodes and consists of the including relationships between the sequence nodes and state nodes and the evolution relationships between the state nodes. In the PoTGM, each sequence node in a process graph is represented and stored by a sequence graph.',\n       'The first randomized, controlled, metabolic study to test the hypothesis that whole-grain consumption improved insulin sensitivity was conducted in 11 overweight, hyperinsulinemic adults. Insulin sensitivity, determined by the euglycemic-hyperinsulinemic clamp test, improved after 6 weeks on a high whole-grain diet compared with a refined-grain diet. 23 The authors also observed a 10% decrease in fasting insulin concentrations following 6 weeks on the whole-grain intervention period. Since then, 3 additional randomized dietary intervention studies 24Y26 have been published, testing the A shortfall of cross-sectional studies is that they reflect snapshots in time and thus do not illuminate the event sequence between exposures and outcomes. hypothesis that whole-grain consumption improves insulin sensitivity. However, none of these studies support a strong association between whole-grain consumption and increased insulin sensitivity (Table 3) . One intervention study, 24 which was designed to compare the effects of whole wheat vs whole oat cereal consumption, showed no effect of either whole-grain cereal on insulin sensitivity over the course of 12 weeks. Insulin sensitivity in this study was determined by an intravenous glucose tolerance test, a more direct measure of insulin sensitivity. This study included middle-aged older adults with elevated blood pressure, and although overweight, participants were considered healthy. It is possible that modification of diet with whole grains in healthy adults alters insulin sensitivity only over much longer periods. In a randomized crossover study, overweight adults were given either whole-grain or refined-grain products to include in their habitual daily diet for two 6-week periods. 26 No changes in insulin sensitivity were observed in this healthy population, in contrast to the findings of the similarly designed study in hyperinsulinemic adults noted above. 23 The incorporation of whole grains in diets of those with more pronounced metabolic abnormalities, as observed in obese or hyperinsulinemic individuals, may exert a stronger effect than in those who are healthy. A randomized crossover trial in postmenopausal women 25 examined the effects of a high-fiber rye bread compared with a low-fiber white wheat bread on measures of glucose and insulin metabolism, as assessed by a FSIVGTT. In this study, no significant changes in insulin sensitivity or glucose were observed over the 8-week intervention; however, the acute insulin response increased significantly with the rye-fiber breadVperhaps indicative of improved insulin secretion by the pancreatic $ cells. More recently, findings from a large intervention study designed to examine the effect of supplemental whole-grain foods on several CVD risk factors in 316 overweight adults found no significant improvements in insulin sensitivity. 28 It is possible that adding whole-grain foods while failing to compensate by omitting other foods may have affected the findings of this study, as discussed elsewhere. 29 The reasons for the different findings among these interventions and between the observational and intervention studies are complex and varied. Components of whole-grain foods vary between the intervention foods and may have a differential impact on glucose and insulin metabolism. Thus, the interpretation of these intervention studies may be complicated by the variations in the dietary interventions themselves. For instance, the form of the food and the botanical structure have been found to modify postprandial glucose and insulin response in healthy adults. 30 Probably more Abbreviations: AA, African American; BMI, body mass index; DAM, dietary assessment method; DGA, 2005 Dietary Guidelines for Americans; DR, diet record; F, female; FFQ, food frequency questionnaire; FSIVGTT, frequently sampled intravenous glucose tolerance test; H, Hispanic; HOMA-IR, homeostatic model assessment of insulin resistance; IS, insulin sensitivity; M, male; NHW, non-Hispanic white; PUFA, polyunsaturated fatty acid; Q1, lowest quintile category of whole-grain intake; Q5, highest quintile category of whole-grain intake; SFA, saturated fatty acid; T1, lowest tertile category of whole-grain intake; T2, middle tertile category of whole-grain intake; T3, highest tertile category of whole-grain intake; W, white.\\nimportant are the varied study populations (eg, obese, nondiabetic subjects, hyperinsulinemic subjects, age range) and the relatively short-term nature of most of these interventions.',\n       '≈ 0.000002336, hence, at 95% power, we used a threshold of 2E − 6. If we consider less power, 90%, we get a gene-wide threshold of 0.10 21, 407 ≈ 5E − 6. For each Pascal result (and for both brain segregation and integration measures) we constructed a table that contains the most ten significant SNPs. These tables contain six columns; the gene symbol, the genes start and end locations, respectively, the gene ID, the number of SNPs in that gene and the p-value corresponds to the gene score calculated with Pascal. Tables for Louvain modularity, transitivity, global effect and charactaristic path length are Table 1 Table 2 Table 4  and Table 3, respectively  Table 5 shows all genes with p-value less than 0.005 for all global network metrics. The genes in this table are known genes that play a key role in AD brain function 1 .',\n       'A first series of studies investigated the relationships between lifestyle and neuroimaging markers controlling for cognitive performance (see above for justification), in samples of cognitively normal Aβ positive subjects classified by means of CSF Aβ 1-42 levels. In a sample from the prospective Alzheimer\\'s Disease Neuroimaging Initiative (ADNI) study, Ewers et al. (2013) found that higher education was associated with lower FDG-PET glucose metabolism in Aβ positive, but not in Aβ negative subjects. In the same vein, ArenazaUrquijo et al. (2013b), using a comprehensive proxy of cognitive reserve, reported that relatively to Aβ negative, Aβ positive subjects with higher scores on this variable presented greater atrophy in the hippocampus and cortical thinning in the supramarginal gyrus. In agreement with previous evidence in AD, authors claimed that in asymptomatic individuals with abnormal levels of Aβ (which may reflect the preclinical stage of AD) higher exposure to positive lifestyle variables compensates FDG-PET hypometabolism or gray matter atrophy/cortical thinning to maintain cognitive performance (see Figure 2) . Using a different approach, Rentz et al. (2010) were able to show that a cognitive reserve factor (education and reading test) could moderate the relationships between Aβ deposition and cognitive performances in a sample including cognitively normal and mild AD patients (see also \"Preclinical Alzheimer\\'s Disease: Lifestyle Effects on AD Neuroimaging Biomarkers\\'\\' Section). In the subsample of cognitively normal older adults, they showed that Aβ deposition in the precuneus was associated with reduced memory performance and this relationship was weaker in subjects with higher cognitive reserve proxies, suggesting that cognitive reserve may be protective against Aβ-related cognitive impairment.\\nThe studies by Chételat et al. (2010) and Johnson et al. (2014) provide complementary information. Rather than studying the relationships between lifestyle variables and brain measures, these studies highlighted the differences between cognitively normal older adults with high (Aβ-positive) vs. low (Aβ-negative) Aβ deposition. Chételat et al. (2010) showed that asymptomatic Aβ-positive had larger temporal (including hippocampal) gray matter volume than Aβ-negative individuals, which was associated with better episodic memory performance. In the same vein, Johnson et al. (2014) reported increased metabolism in the superior temporal lobe, and increased gray matter in the lateral parietal lobe, in Aβ-positive compared to Aβ-negative subjects. In accordance with the brain reserve hypothesis, we could suggest that the deleterious effects of Aβ deposition on cognition might be delayed in individuals with higher gray matter volumes or higher glucose metabolism. Although these mechanisms may be somehow passive, neural compensation related to more detailed memory encoding in older adults with Aβ deposition has also been reported (Elman et al., 2014) . Thus, cognitively normal older adults with Aβ deposition may recruit extra neural resources, including increases in brain activation in the parietal and occipital cortex that may allow them to maintain normal cognition.\\nAltogether, some of these studies showed greater tolerance to neurodegeneration (atrophy and hypometabolism) in asymptomatic (Aβ-positive) older adults with higher exposure to positive lifestyle factors, which is in line with evidence in AD patients. Other studies, point to brain reserve mechanisms that may help delaying the effects of Aβ deposition on cognition. Studies that directly assess the link between AD neuropathological processes, potential reserve mechanisms and lifestyle are not available yet.\\nLifestyle Effects on AD Neuroimaging Biomarkers in Cognitively Normal Subjects and APOE ε4 Carriers Liang et al. (2010) provided novel associations between exercise engagement and biomarkers levels (measured with PET-PIB and CSF levels of Aβ 1-42, tau and phospho-tau). They observed that subjects with greater physical activity had lower Aβ deposition. In fact, when they compared exercise scores between individuals with \\'\\'at risk\\'\\' biomarkers levels and individuals with \\'\\'normal\\'\\' levels those with more altered biomarkers were more sedentary. A later study by Landau et al. (2012) further supported the idea that lifestyle factors may have a direct effect on brain Aβ pathology. They showed an inverse association between Aβ deposition (measured using PIB) and lifelong cognitive activities in cognitively normal subjects. In line with the report in physical activities, healthy older participants in the lowest cognitive activity tertile had Aβ levels similar to patients with AD. This finding was confirmed in a subsequent study from the same laboratory using path analyses and an increased sample size (Wirth et al., 2014a) . Further, a series of recent investigations from different laboratories suggest that effects of cognitive and physical activities on Aβ deposition might be exacerbated in those individuals with an APOE ε4 allele (Head et al., 2012; Brown et al., 2013; Wirth et al., 2014b ; see Figure 3 ). Finally, a recent study in late middle-age cognitively normal individuals showed that age-related alteration in Aβ deposition, glucose metabolism of the precuneus and hippocampal volume was attenuated in subjects with higher involvement in physical activity.\\nSimilar direct effects on hippocampal atrophy were shown in a longitudinal study by Valenzuela et al. (2008) where complex mental activity across the lifespan was related to reduced hippocampal atrophy in cognitively normal older people. In line with this observation, occupational complexity, specifically supervisory experience at work, has also been related to a lower rate of hippocampal atrophy in cognitively normal older adults (Suo et al., 2012) , and higher educational attainment was reported to be related to decreased hippocampal perfusion (Piras et al., 2011) . Beyond the hippocampus, an effect of education, occupation and leisure activities on temporal and parietal gray matter volumes and cortical thinning has also been reported (Foubert-Samier et al., 2012; Liu et al., 2012; ArenazaUrquijo et al., 2013a) . However, two studies including a large sample of cognitively normal individuals from ADNI found inconsistent results. A cross-sectional study did not replicate these positive effects of lifestyle factors on AD neuropathological processes (Vemuri et al., 2012) . Thus, no significant correlation was found between lifetime or current intellectual activity and Aβ PET, glucose metabolism or hippocampal volumes (however, all these biomarkers were associated with cognitive performance). A subsequent longitudinal study using ADNI subjects (Lo et al., 2013) , investigated the effects of cognitive reserve (i.e., education, IQ and occupation) and brain reserve (i.e., intracranial size) proxies on AD pathological progression, including CSF Aβ 1-42 , FDG-PET and hippocampal volumes. Only CSF Aβ 1-42 was found to be influenced by cognitive reserve proxies in cognitively normal elders.\\nIn summary, emerging evidence suggest that lifestyle factors such as cognitive and physical activity may also have direct effects on Aβ deposition and rates of Aβ accumulation. These direct effects might be restricted to cognitively normal older adults, and may be exacerbated in APOE ε4 carriers. Finally, direct effects of lifestyle in key regions such as the hippocampus and the temporal lobe have also found support in several cross-sectional studies.',\n       'In order to conclude that the caregiving and play factors were comparable over time even if items changed, models assessing factorial invariance were compared (see Widaman, Ferrer, & Conger, 2010 , for a full description of factorial invariance). Results showed evidence for weak invariance for both caregiving and play, indicating that these constructs were meaningfully similar across time even though the items change in a developmentally appropriate manner with infant age. Factor scores determined from the weak invariance models were used in further analyses as composite caregiving and play scores. Unweighted means, standard deviations, and correlations between caregiving and play at each time point are presented in Table 2 , in addition to other study variables. Because we have support for factorial invariance, longitudinal analyses assessing changes in caregiving and play factor scores can be appropriately conducted.',\n       \"In this paper, we reviewed five applications of machine learning techniques on radiologic images: image segmentation, computer-aided detection and diagnosis, functional brain studies and neurological disease diagnosis, image classification and retrieval, ands image registration. While machine learning techniques are active in computer-aided systems to assist radiologists in daily diagnosis and studies, the use of machine learning techniques in radiology is still evolving. There are many strategies that this field could investigate in the future -Previous contributions have shown that machine learning-based systems showed accurate results comparable to those of radiologists themselves. However, the system accuracy of these techniques must still be improved, that is, systems must be more accurate than those of radiologists. Otherwise, the widespread application of machine learning techniques will be limited. A possible approach to achieve this superior performance is to design better machine learning models or to gather more representative data that can be continuously used to improve the algorithms.\\n-Although the core advancement of deep learning is its ability to learn useful features directly from data, its accuracy and performance are highly limited by the size of data. Traditional machine learning methods still play a role in the case of small amounts of labeled data. Understanding how to choose and use features from images effectively is still a significant direction for these traditional methods.\\n-Another issue deals with the translation of these techniques to clinical practice. While many machine learning algorithms have already shown good results, it still needs to pass clinical trials required by the government. Additionally, many people still believe in human decisions, as clinicians always consciously tend to decide with all the relevant information in mind. These decisions make it difficult to justify the use of algorithms for clinical decision making in all possible cases, but through rigorous research contributions, we can justify the use of machine learning algorithms in the cases when patients outcomes can be improved.\\nThe application and development of machine learning techniques to radiological images is a hot topic currently and a large number of algorithms are being developed to ensure higher accuracy and lower computational complexity. We expect that machine learning techniques will become essential components in clinical tools and will be widely used to assess patients' health in the future.\",\n       nan,\n       '; 0.52% variance explained) showed evidence of altering the expression of the KTN1 gene in both brain and blood tissue. Variants influencing putamen volume clustered near developmental genes that regulate apoptosis, axon guidance and vesicle transport. Identification of these genetic variants provides insight into the causes of variability in human brain development, and may help to determine mechanisms of neuropsychiatric dysfunction.\\nAt the individual level, genetic variations exert lasting influences on brain structures and functions associated with behaviour and predisposition to disease. Within the context of the Enhancing Neuro Imaging Genetics through Meta-Analysis (ENIGMA) consortium, we conducted a collaborative large-scale genetic analysis of magnetic resonance imaging (MRI) scans to identify genetic variants that influence brain structure. Here, we focus on volumetric measures derived from a measure of head size (intracranial volume, ICV) and seven subcortical brain structures corrected for the ICV (nucleus accumbens, caudate, putamen, pallidum, amygdala, hippocampus and thalamus). To ensure data homogeneity within the ENIGMA consortium, we designed and implemented standardized protocols for image analysis, quality assessment, genetic imputation (to 1000 Genomes references, version 3) and association (Extended Data Fig. 1 and Methods).\\nAfter establishing that the volumes extracted using our protocols were substantially heritable in a large sample of twins (P , 1 3 10\\n), which encodes the protein kinectin, a receptor that allows vesicle binding to kinesin and is involved in organelle transport 11 . Second, we identified an intronic locus within DCC (rs62097986; 18q21.2; n 5 28,036; P 5 1.01 3 10',\n       'Respiratory specimens collected from confirmed cases per the Ministry of Health national case definition were used in this study. Samples were selected from main clusters since the start of local transmission for a period of two months, from March 23 to May 5, 2020.',\n       'In an imaging feature space of dimension , the dimension of parameter space of CHIMERA-affine is to the order of , while for CHIMERA-duo and CHIMERA-trans to the order of . In the low sample size settings that are typically observed in medical imaging studies, this large dimension yields ill posed problems. This issue is commonly mitigated by regularizing/penalizing the parameters of the transformations [23] , [24] . We have adopted this approach, which improves also the generalization and the robustness of our model. In order to derive an analytical solution, we have chosen to penalize the Frobenius norm of and the norm of , where is an identity matrix. This regularization, is equivalent to posing Gaussian priors for the parameters. Beside the explicit regularization term , our model can also be considered as being \"implicitly\" regularized. Instead of focusing on the points at the border between the different groups, like support vector machine [25] and relevance vector machine [26] , our model always consider the entire point distributions. We aim, in that way, to reduce the sensitivity of clustering produced with respect to the individual subject variability.\\nThe next section describes the MAP estimation strategy that was adopted for optimizing our model.',\n       'Using a near census of all US graduates, we found that nearly two thirds of all doctoraltrained public health graduates had secured employment at or around the time of graduation. Of those, more than half secured employment in academic settings, while the frequency of nonacademic employment differed by public health discipline. Furthermore, we found that a number of demographic characteristics, doctoral training attributes, and institutional factors were associated with the likelihood of securing employment in either academic or nonacademic settings. Notably, we found a downward trend in secured employment (irrespective of employment type) over the study period from 2003 to 2015. Concurrently, we observed an increase in the number of respondents who were seeking employment during the same time period. Some have suggested that there is an \"overproduction\" of doctoral-trained individuals in many sciences and, thus, a shortage of available positions. 29 Notably, during this time period, we observed an increase in the number of graduates trained in general public health or who received degrees from for-profit institutions and institutions with lower research intensity (data not shown)-all groups that were less likely to secure any employment at graduation. However, our findings could also be attributed to the fact that the SED is administered at or around the time of graduation, which varies by doctoral recipient and institution. Thus, depending on the type of employment, the employee search and hiring cycle may not align with graduation dates. For trends observed with academic employment, an alternate explanation for the decline over time could be the documented increase in onerous administrative processes associated with faculty recruitment, which delays the time to successfully secure an academic position, especially in large institutions. 33    While academic employment accounted for the highest proportion of secured employment across all disciplines examined, we found differences among the types of nonacademic employment. Second to academic employment, degree recipients in biostatistics most often found jobs in for-profit industry. We suspect that these positions are in organizations that rely heavily on quantitative analyses, such as pharmaceutical companies or medical device manufacturers. Graduates of environmental health, epidemiology, and general public health programs most often secured nonacademic employment in governmental settings. These settings could include state and local health agencies, the Environmental Protection Agency, Centers for Disease Control and Prevention, and other units of the US Department of Health and Human Services. By contrast, the mostoften-secured nonacademic employment for graduates of health services administration was nonprofit industry, which may include hospitals, health delivery systems, and foundations that fund health services research or other public health causes. More research is needed that catalogs the exact type of nonacademic positions in which graduates from public health disciplines find placement. Importantly, we found that Black and Asian graduates were less likely than were their White counterparts to secure any employment at the time of the survey, even after we controlled for differences by discipline and type of degree-awarding institution. Further analysis suggested that Blacks and Asians were both less likely to secure academic and nonacademic positions alike. This is similar to findings of other studies that note historical difficulties in hiring minority faculty members. 34 Aguirre states that long-running attempts to improve diversity among academic faculty have been ineffective because of structural barriers and a lack of understanding of the social forces faced by underrepresented groups who wish to enter academia. 34 Previous research suggests that these disparities can be attributed to the fact that minority graduates more frequently emanate from for-profit institutions with lower levels of research productivity and are thus less competitive for academic positions. 35 In our analysis, we found that minority status was independently associated with no secured job irrespective of the institutional factors previously examined. Given that our data suggest that doctoral students who receive full tuition remission were more likely to secure academic employment, more research is needed to examine whether minorities receiving full tuition remission have better job prospects overall, including in academic settings.',\n       'Very little is known about the role of person-level qualities, or personality, in the teacher labor market. This study explores the role of perfectionism in teacher occupational commitment and retention. One hundred eighteen graduates of a competitive teacher preparation program with widely varying levels of total years commitment to the job completed a measure of three dimensions of perfectionism-standards (holding oneself to high standards), order (valuing neatness, tidiness, and being disciplined), and discrepancy (perceiving a gap between ambitions and abilities)-and gave information about their personal backgrounds and work histories. Results suggest that none of the dimensions of perfectionism predict teacher commitment in the sample as a whole, but that the order dimension significantly predicts long-term commitment to struggling urban versus affluent suburban schools. These results imply that long-term urban teachers may be adept at overlooking difficult and sometimes chaotic circumstances to sustain themselves in the occupation.',\n       \"The demographic, clinical and biomarker findings in the 207 patients in whom [ 18 F]flutemetamol PET was performed due to diagnostic uncertainty are presented in Table 1 . Most patients received an initial diagnosis of MCI (131, 63%), followed by AD (41, 20%), dementia NOS (20, 10%), non-AD (10, 5%) and SCD (5, 2%). Figure 1 shows the structural imagingbased ratings for atrophy and white matter changes in the different patient groups before amyloid PET. MTA scores of 0 or 1, indicating no or minimal atrophy, respectively, were predominant across the MCI, AD, dementia NOS and SCD groups. In contrast, MTA scores indicating mild atrophy were noted in half of the patients with non-AD disorders. In terms of GCA and white matter lesions, most patients showed mild changes. [ Table 1 , Fig. 1d ). A metabolic pattern suggestive but not typical of AD was the most common finding in those with a diagnosis of MCI or AD prior to [ 18 F]flutemetamol PET (16 patients, 36%, and 9 patients, 56%, respectively); in the remaining groups, patterns not typical of AD or suggestive of a non-AD disorder were predominant (non-AD, 4 patients, 100%; dementia NOS, 10 patients, 91%), with only minor patchy changes seen in patients with SCD. Similar findings were obtained using the diagnoses obtained after [ CSF was sampled in 152 patients according to the Swedish clinical practice guidelines for memory assessment at specialist memory clinics. Overall, 103 patients (68%) showed abnormal CSF biomarkers: 24% showed only abnormal Aβ 1-42 , 21% showed abnormal Aβ in combination with elevated ttau or p-tau, and 32% showed negative CSF; a further 23% showed only abnormal tau. The main reasons for performing amyloid PET were a clinical suspicion of AD in combination with either a negative or an unclear (i.e. isolated positive or borderline Aβ or tau) CSF profile (117 patients, 57%) or AD Alzheimer's disease, GCA global cortical atrophy, MCI mild cognitive impairment, MMSE Mini-Mental State Examination, MTA medial temporal atrophy, NOS not otherwise specified, SCD subjective cognitive decline, SUVR standardized uptake value ratio a MMSE: dementia NOS < AD (p < 0.01); MCI, AD, non-AD, dementia NOS < SCI (p < 0.01) In patients with an initial diagnosis of AD, the diagnosis was dismissed in seven amyloid-negative patients. Finally, all patients with an initial diagnosis of dementia NOS group, and almost all patients with a non-AD disorder were amyloid-negative. Overall, [ 18 F]flutemetamol PET led to a significant change in diagnosis (92 patients, 44%; p < 0.05). Among the patients with MCI, dementia NOS, AD and a non-AD disorder, the highest percentage change in diagnosis was observed in those with MCI (67 patients, 51%) as well as in those with dementia NOS (11 Table 2 Agreement between CSF positivity and [ \",\n       'We estimated the population attributable fraction of MVC deaths associated with prolonged response times in rural/ wilderness and urban/suburban counties (eTable 3 in the Supplement). The median county response time among rural/ wilderness counties was 10 minutes (IQR, 8-12 minutes) compared with 7 minutes (IQR, 6-9 minutes) among urban/ suburban counties. In rural/wilderness counties, the proportion of crash fatalities associated with EMS response times of 10 minutes or longer was 9.9% (95% CI, 4.8%-14.1%), representing approximately 333 of 3363 passenger vehicle deaths. In urban or suburban counties, the proportion of crash fatalities associated with EMS response times of 7 minutes or longer was 14.1% (95% CI, 11.5%-16.4%), representing approximately 1796 of 12 735 passenger vehicle deaths. Taken together, an estimated 2129 passenger vehicle deaths per year (13.2% of all crash fatalities within the 2268 counties evaluated) might have been prevented if county response times were shorter than the specified benchmarks in rural/wilderness (10 minutes) and urban/ suburban (7 minutes) areas. Blue circles represent the mean county MVC mortality rate associated with each incremental increase in county response time. Lines represent the best-fit polynomial to mean MVC mortality rates (solid line) and 95% CIs (dashed lines) associated with increasing county response times. ',\n       'Educators in the Source Country. It was learned in the course of this work that knowledge representation structures are specific to curricula-or, more concretely, to curriculum traditions. For example, a knowledge representation structure that uses the same object-in Genie 2\\'s terminology, the \"objective\" object-to refer to a knowledge component as to a unit of instructional content is less well-suited 10 to modeling Russian teachers than a structure where these notions have separate entities. This is because Russian mathematics course plans include a variety of lesson types, including lessons introducing new material, lessons meant to discuss connections between two or more previously covered topics, lessons meant to review some combination of previous topics in context of one another, and many other lesson types besides. This results in a complex, many-to-many relationship between topics (or knowledge components) and lessons. By contrast, American teachers usually organize instruction by knowledge components-with each unit introducing and then practicing a specific topic-and therefore their practices could be modeled more faithfully with a structure combining the two notions.\\nThis point has several implications. First, it illustrates how the pedagogical model can be interconnected with the student model; Wenger (1987) argues that both models are necessary, and the present example shows that they must be designed in each other\\'s context. Second, it suggests that modeling teachers from different traditions may be a method for uncovering additional classes of adaptive mechanisms. And third, it determines constraints on the generalizability of the Genie 2 platform to other nations\\' curricula. More specifically, modeling other nations\\' curricula through this kind of approach is not simply a matter of modifying the instructional content and the pedagogical rules: rather, the adaptive mechanisms and knowledge representation structures need to be customized for each curriculum tradition. This having been said, the degree to which they must be changed varies depending on how different the curriculum tradition is; for example, implementing a Singaporean curriculum-where, like in Russia, teachers take a primarily didactic approach-would be substantially easier than attempting to implement a Japanese curriculum. This is because in the Japanese approach, students work in small groups to find problem solutions, which are subsequently discussed with the teacher and the whole class (Stigler & Hiebert 2009) . Capturing this group dynamic would require not only different knowledge representation structures, but also a fundamentally different student interface.',\n       \"Recent findings suggest an important role of neuroinflammation in neurodegenerative diseases [18] . Thus, selective mutations in microglia/myeloid-specific genes, including the gene encoding TREM2, have been associated with AD [16] . Experimental AD studies have suggested that TREM2 is instrumental in neuroinflammation [24, 35] and drives DAM microglia [27, 30] . Gal3, a carbohydrate-binding protein, is one of the most upregulated genes associated with DAM [30, 34, 38] . Holtman et al. (2015) anticipated gal3 as a gene strongly related to microglial DAM phenotype [22] . To study the role of gal3 in AD pathogenesis, we have analyzed human brains from AD patients and 5xFAD mice lacking gal3. The main conclusions from our study are (1) gal3 protein is increased tenfold in human AD brains and is mostly restricted to plaque-associated microglia in humans and 5xFAD mice, (2) certain SNPs linked to the LGALS3 gene are associated with AD, (3) gal3 deficiency reduces Aβ plaque burden and the overall proinflammatory response and plaque size and improves cognitive performance in 5xFAD/ Gal3KO mice, (4) gal3 is an endogenous TREM2 ligand, (5) gal3 is released in response to fAβ and (6) gal3 interacts with Aβ, affecting amyloid plaque morphology. Hence, gal3 emerges as a central upstream regulator of AD-associated pathology.\\nGiven the relationship between innate immune systemrelated genes and AD incidence [18, 19] , deciphering how Aβ aggregates trigger neuroinflammation is of critical importance. For this purpose, we first analyzed the role of gal3 in the Aβ-induced inflammatory response in microglial cells. To this end, we took advantage of both chemical inhibition and gene deletion. Gal3 inhibition robustly reduced fAβ-induced iNOS expression in BV2 cells, an + cells expressing gal3 in 5xFAD (% of Αβ area). e, f Gal3 and TREM2 in plaque-associated microglia in the brain of 5xFAD mice reveals colocalization of gal3 and TREM2. g Gal3 and TREM2 colocalization in 5xFAD mouse brain using STORM microscopy. Statistical significance was calculated by Student's t test. *p < 0.05. Data are shown as mean ± SEM. All images were taken in 5xFAD mice at 18 months ◂ effect that was extended to proinflammatory cytokines, including TNFα, IL6, IL8 and IL12, which we confirmed in primary microglia cultures from WT and Gal3KO mice. These data suggest that gal3 is a critical alarmin that amplifies the Aβ-induced proinflammatory response. Since inefficient clearance of Aβ may play a determinant role in AD pathogenesis [67] , we analyzed the effect of gal3 in two major mechanisms involved in Aβ clearance: Aβ phagocytosis and IDE-1 levels, a key metalloprotease involved in Aβ degradation by microglia [48] . Gal3 was able to alter Aβ phagocytosis in vitro showing a complex role of gal3 in Aβ phagocytosis depending on Aβ species (monomers or fibrils) or if pretreated or not (see suppl. Fig. 2e -f, online resource 2). Hence, further experiments are needed to clarify the exact role of gal3 in the whole dynamic process of Aβ phagocytosis and clearance. We found gal3 deficiency to increase IDE-1 levels in vitro and in vivo, suggesting a detrimental role of gal3 in Aβ clearance. Supporting this, CSF levels of Aβ42 were significantly higher in 5xFAD/Gal3KO as compared with 5xFAD. Overall, our study demonstrates an instrumental role of gal3 in driving the fAβ-induced proinflammatory response and in contributing to a deleterious effect in Aβ clearance.\\nRecent transcriptomic analysis of microglia at the singlecell level has identified a common disease-associated microglia phenotype, which has been suggested to be driven by TREM2 and apoE [27, 30, 38] . Interestingly, Krasemann et al. (2017) identified gal3 as one of the most upregulated genes in plaque-associated microglia, supporting our findings in human AD brains and 5xFAD mice [30] . Recently, Yang et al. overexpressed TREM2 in 5xFAD mice (5xFAD/ TREM2) and found gal3 as one of the main genes affected by TREM2 overexpression [34] , suggesting that gal3 plays an important role in microglial function under disease conditions. We generated 5xFAD/Gal3KO mice to answer whether gal3 plays a role in microglia-associated AD pathogenesis and to test if gal3 signaling is associated with TREM2. To answer this question, we performed an inflammatory gene array, demonstrating an age-dependent inflammatory response, involving complement components, chemokines, interleukin receptors, toll-like receptors and DAM genes in 5xFAD mice. This inflammatory response was highly attenuated in 5xFAD/Gal3KO mice, thus confirming gal3 as a master regulator of AD-associated brain immune responses. Pathway analysis in mice at 6 and 18 months of age identified TLR and DAP12, a TREM2 signaling adaptor protein, as one of the most significant pathways associated with gal3 in the 5xFAD mice. This gene expression data suggest an important role of gal3 in the AD neuroinflammatory response, perhaps by stimulating NFκb-related regulators whereby TLR4, and/or other TLRs [9, 25] or glycoproteins, such as TREM2 [14, 65] , are involved. Despite the clear connection between gal3-, TLR-and TREM2-related pathways, the regulation of these pathways is not known, and further experimentation is needed.\\nConsequently, we investigated whether gal3 interacts with TREM2, a key receptor that has been suggested as a driver of the DAM phenotype. Our confocal microscopy study demonstrated a remarkable cellular colocalization of TREM2 and gal3 in microglial cells around Aβ plaques and a near 100% correspondence between both microglial markers, an indication that gal3 specifically labels DAM. [11, 12] . The ability of gal3 to stimulate TREM2 was finally confirmed by a TREM2-DAP12 reporter cell line. Overall, our data suggest that the switch from homeostatic microglia to DAM is accompanied by significant upregulation of both TREM2 and gal3. Gal3 may thus bind to and activate different microglial receptors including TREM2 (present study), TLR4 [9] , IGFR [33] and MerTK [43] . In fact, because gal3 is relatively promiscuous in its interactions with glycoproteins, it may be behind the chronic and detrimental activation of microglia in AD. Regardless of this possibility, what our study demonstrates is that the pleiotropic activity of gal3 drives important amyloid-associated immune responses. Additionally, our study has uncovered an unexpected role of gal3 as a powerful Aβ binding agent. We have previously demonstrated the ability of LPS-induced reactive microglia to release gal3 [9, 64] . More recently, we have demonstrated a significant increase of gal3 levels in CSF from mice exposed to traumatic brain injury [64] , an indication that gal3 is released by reactive microglia. In this study, we provide evidence that gal3 is actively released by microglia in response to a fAβ challenge. In APP mice, gal3 is present in the extracellular space and associated with amyloid fibrils (unpublished). Taken together, the possibility that gal3 directly interacts with Aβ to affect aggregation becomes plausible. To test this possibility, we incubated Aβ monomers with or without gal3 for 1 h at 37 °C and injected 2 µl of each condition into either the left or right hippocampi of WT mice. A ThT assay demonstrated fAβ as the predominant form of Aβ present in both injections. Aβ deposition was analyzed 2 months after injections. While no Aβ deposition was found in animals injected with Aβ monomers alone, co-injection of Aβ monomers and gal3 resulted in evident insoluble Aβ aggregates, suggesting that gal3 is directly involved in Aβ plaque formation. To our knowledge, this is the first report showing insoluble Aβ aggregates long after Aβ brain injections in WT mice. Thus, Meyer-Luehmann et al. [40] injected brain homogenates from aged APP mice in the hippocampus of WT mice and found no evidence of Aβ aggregates 4 months after injection. In contrast, injections of the same brain extracts in AD transgenic mice led to robust deposition of Aβ [15, 28, 40, 44] . Further, repetitive Aβ injections into the hippocampus of WT mice demonstrated that Aβ deposits are drastically reduced from day 1 to day 7 after injections [15] .\\nRecently, Heneka et al. have demonstrated that ASC specks released from reactive microglia physically interact with Aβ, acting as an Aβ cross-seeding agent [57] . Eliminating microglia has been shown to prevent plaque formation in APP transgenic mice [54] , suggesting that factors released from microglia may seed amyloid plaques. Our study reinforces the view that reactive microglia play a critical role in Aβ plaque dynamics and associated immune responses by releasing Aβ cross-seeding agents (i.e., ASC specks) and gal3. Aβ plaque formation is believed to precede the appearance of clinical symptoms, so elucidating the earlier molecular mechanism involved in Aβ plaque formation appears critical for the establishment of early promising therapeutic strategies aiming at halting the development of AD early. Gal3-inhibition appears to be a promising Aβ therapeutic target. The ability of gal3 to drive proinflammatory fAβ-associated immune responses and hinder Aβ clearance makes this protein a strategic upstream regulator of AD pathology. Indeed, the pathogenic role of gal3 was confirmed in adult 5xFAD mice lacking gal3 as those mice had a significantly lower Aβ load and ameliorated cognitive/ spatial memory deficits, which was observed in the Morris water maze test.\\nOur results suggest that LGALS3 gene variants affect the risk of developing AD, as indicated by the 5 SNPs in the LGALS3 gene that we found to be related to increased AD frequency. However, according to the GWAS catalogue (https ://www.ebi.ac.uk/gwas/), none of the variants reported in suppl. Table 3 (online resource 10) or those in linkage disequilibrium with them (suppl. Fig. 3b, online resource 3) , have been previously associated with AD. This could be due to the fact that the GWAS approach requires large samples to detect modest effects, partly due to the multiple testing corrections applied. However, the impact of these LGALS3 SNPs appears to be similar to other SNPs included in the GWAS, supporting a role for this gene in the development of AD. Our meta-analysis only comprised those SNPs belonging to a linkage disequilibrium block located at the 5′-end of the LGALS3 gene, therefore, we do not know if other regions have genetic variants that could lead to stronger effects. At present it is difficult to speculate in what way the SNPs of gal3 is altering the function or expression of the protein.\\nFurther replication studies covering the entire genetic region of this locus will be necessary to confirm our results.\",\n       \"Effectiveness is a multi-dimensional concept that relates to the issue of quality and refers to educational tools and processes that result in the achievement of educational goals. To measure the effectiveness of a school, a systematic approach is needed that involves many key factors such as managerial function, individuals' performances, ethics, level of trust, culture and climate of schools, parental involvement, teachers' performances and their job satisfaction (Ostroff& Schmitt, 1993 , as cite by Uline, Miller & Tschannen-Moran, 1998) . Arar and Nasra (2018) summarized the definition of effective school in the educational literature in four facets: (a) the output goal approach, which claims that an effective school is a school whose achievements are above those that can be expected under defined prediction conditions; (b) the goal approach, according to which a school is effective if it achieves the goals it sets for itself within a defined period of time; (c) the resource approach, according to which a school is considered effective if it can mobilize the necessary resources to fulfill its tasks; (d) the internal processes approach, by which a school is considered effective if its functioning is smooth and its organizational climate is healthy; (e) the stakeholders' satisfaction approach, according to which an effective school is defined as a school that meets the expectations and needs of the stakeholders (parents, students, and the community); and (f) the combined approach, by which an effective school is an educational institution that works systematically and continuously for self-improvement in order to achieve its goals by maximizing its physical and human resources while maintaining the well-being of teachers and students. Hallinger and Heck (2011) determine five characteristics of an effective school: developing high expectations of students and teachers, order, discipline, emphasis on student-centered activities, and monitoring students and teachers work and strong leadership of the school principal. The literature on effective schools has also developed in the direction of diagnosing the characteristics of successful schools around the world, including the characterization of management practices and their effect on school results. The mindfulness in this study is a characterization of management addressed directly and indirectly by mediation of the school climate and OBC.\",\n       'We have observed that there is a significant positive correlation between measured SUVRs and WM uptake in amyloid-negative patients, and that this correlation is reduced by using reference regions including WM. This could be an explanation for the good performance of reference regions including WM. By using MC simulation, we demonstrated that this correlation is largely produced by PVE, and that it can be removed by using PVC. These results shall be of special interest for situations where primarily healthy populations are used, such as the calculation of SUVR positivity thresholds. We have proposed a correction that can be applied directly to previously calculated SUVR values in such cases.\\nCSF together with the GM specific atlas and BrainVIset input (iteration 0) activity and attenuation maps. Figure S2 . Comparison of SUVR values measured on our lab (x-axis) and data calculated by the ADNI PET Core at Berkeley (y-axis) for our final subject sample. Average differences between Berkeley calculations and ours were found to be ± 4.6%. These small differences are mainly due to different processing pipelines, such as different segmentation methods (CAT vs. Freesurfer), atlas (Hammersmith vs Desikan) or quantification space (patient vs. MNI). Figure S3 . Distribution of WM values across the studied ADNI2 sub-sample. Only amyloid-negative patients are presented. The bars represent the number of patients on each bin of the histogram, while the black solid line represent the Gaussian distribution of the histogram. Figure S4 . Visual comparison of the correlations of cortex SUVR (x-axis) with WM-SUVRCGM, using both the whole WM (green) and the Eroded WM (purple) as WM regions. Figure S5 . Relation between WM-SUVRCGM (x-axis) and GM SUVR using the whole cerebellum as a reference region (SUVRWC). The Figure represents the actual cohort used for this work (amyloid-negative patients, blue points), and patients excluded for being positive according to the ADNI SUVRWC =1.11 threshold (red and orange dots). The correlation coefficient between WM-SUVRCGM and SUVRWC when including amyloid-positive patients was r=0.55. Figure 6 . Example images for the different levels of processing for some of the simulated images. Each row shows the original image (left), smoothed image (center-left), atlas used for the PVC (center). RBV-corrected image (center-right) and iY-corrected image (right), for each of the cases. Figure 7 . Comparison of ground truth and measured SUVRCGM values for RBV (blue lines). Table S1 . Measured PF for each of the scanners present in the ADNI database (measured by the ADNI) and smoothing applied to each of the scanners to obtain an isotropic 8-mm resolution (as proposed by the ADNI). Table 2 . Quantification results for all the analyzed patients, including the ADNI label for the patient (Patient), the quantified Cortex average (Cortex AVG), cerebellum grey matter average (CGM AVG), whole cerebellum average (WC AVG), white matter average (WM AVG), SUVRCWM, SUVRWC and the PET scanner.',\n       \"The dynamic biomarker model, in the AD pathological cascade first proposed by Jack in 2010 [134] , has been an area of intense interest. However, this inverse relationship between fibrillar amyloid plaque burden (on PIB imaging) and corresponding decrease in CSF Aβ42 and elevated tau, has led to the simplistic interpretation that the AD pathological cascade is purely driven by the amyloid cascade (Figure 1 ). This is partly due to extrapolation from crosssectional studies, where in fact, longitudinal studies are required to determine the temporal order of the appearance of various pathogenic processes involved in this complex disease. Storandt et al [135] has recently demonstrated in a community cohort that CSF Aβ42 and tau were minimally correlated, suggesting that they represent independent processes. Additionally, they accounted for only 60% of variance on PIB imaging, suggesting that a third process may be related to brain atrophy or plaque formation [136] .\\nIn addition, understanding longitudinal biomarker change allows its potential inclusion in clinical trials, with recent studies advocating the use of neuroimaging biomarkers [137, 138] , CSF biomarkers [139] and/or combination biomarkers [137, 140] to boost the power of clinical trials and decrease sample size in MCI trials. An integrated analyses approach using patient (age) severity-and disease-related (severe baseline cognitive, global or behavioural status) factors in established AD has been shown, with the potential of symptomatic AD therapy, to decrease likelihood of faster decline [141] .\\nFurther work on biomarkers is important because of their multiple potential roles. Biomarkers have the potential to be used as a prognostic tool for the prediction of AD conversion in MCI subjects and rapid AD progression, with translation into clinical practice by using a most practical algorithm, and as a diagnostic tool in prodromal/ preclinical stages of AD. Biomarkers may also lead to a deeper understanding of the complex pathogenesis of AD diseaseincluding stage-specific and stage-independent processes. There is also currently an unfulfilled potential in biomarker-enriched clinical trials and the use of biomarkers in preclinical AD, especially in the advent of newer therapeutic targets. Finally there is also potential to extrapolate biomarker findings 'backwards' into the earliest stages of disease so that we may be able to identify those at risk and consider instituting interventions. This would enable earliest therapeutic intervention for at-risk subjects most amenable to disease-modifying treatments, and exclude those for whom the possible risks from investigational treatment would be more difficult to justify. At the very least, it would identify those who might benefit most from intensive monitoring and management of clinical factors, e.g. blood pressure, diabetes and lipids, and also non-invasive interventions, e.g. cognitive training. This vital work can only been done through multi-center studies and standardized evaluation techniques using various systems biology and statistical modeling approaches. syndrome, mild cognitive impairment, and progression to dementia. The Italian Longitudinal Study on Aging. Neurobiol Aging 2011;32(11):1932-41.\",\n       'Each subgroup k ∈ {1 . . . K} has the same set of p covariates, but subgroupspecific sample size n k . Total sample size is n = K k=1 n k . For subgroup k, X k is the n k × p feature matrix and y k the corresponding n k × 1 vector of observed responses. Subgroup-specific regression coefficients are β k ∈ R p .\\nWhere convenient we collect all regression coefficients together in a p×K matrix B = [β 1 . . . β K ] and accordingly we use β j,k to denote the coefficient for covariate j in subgroup k.',\n       'BEST COPY AVAILABLE The unemployment rate for individuals with children in the home (1.4 percent) was lower than the unemployment rate for those without children living in the home (1.9 percent) (table 2). Like marital status, having children produced significant differences in the unemployment rates of men and women. Although women with children had unemployment rates exceeding those for women without children (2.4 percent compared with 1.2 percent), men with children had lower unemployment rates than their childless counterparts (1.2 percent compared with 2.1 percent). Standardization on the non-demographic variables did not reduce significantly the strength of this interaction effect. In order to determine if the impact of family status on unemployment was different for men and women in the general population, unemployment rates by sex and family status were calculated from Bureau of Labor Statistics data for March 1996. In the general population, the unemployment rates for both married women and men were below those for unmarried individuals. However, the difference in unemployment rates was more dramatic for men than for women. The unemployment rate for unmarried men was 8.7 percent, compared to 3.6 percent for married men; the corresponding rates for women were 5.6 percent and 3.3 percent. As was true in the doctoral science and engineering population, the impact of children on unemployment within the general population was different for the two sexes. Men with children had relatively low unemployment rates compared with men without children (4.0 percent versus 5.5 percent); while for women, the unemployment rate for those with children was higher (4.5 percent compared to 3.8 percent).\"',\n       'Cosine similarity scores were entered individually into logistic regression models with category membership (AD vs. NC or MCI-c vs. MCI-n) as the dependent variable. Age and sex were considered as potential covariates but were removed if they failed to improve the overall fit of the model. Scores on the MMSE and Functional Activities Questionnaire (FAQ) and interactions of these scores with cosine similarity scores were considered as covariates only for the MCI-c versus MCI-n logistic regression models. MMSE and FAQ scores were not included in the AD versus NC logistic regression model due to concern of circularity, because these diagnostic classifications were assigned when subjects originally entered the study, and these scores might have influenced the classification itself. Thus, the maximal possible logistic equations were represented by Equation (1), where cosim represents the appropriate cosine similarity scores and the terms in parentheses were considered only for the MCI-c/MCI-n contrast.',\n       \"This section describes hours, wages and benefits of dairy farm managers, distinguishing between those with middle and top-level managerial responsibilities. It should be noted that respondents were not given any guidance on distinctions between middle and top managers; they defined each based on their own business situation. As dairy farms grow over time, employers recognize a commensurate need for capable middle managers. Management positions in dairy businesses, especially large ones, require long work weeks. Figure 10 shows that 68% of middle managers worked 50 hours or more per week. By contrast, 79% of top level managers worked 50 or more hours per week (Figure 11). When the average weekly hours of top managers, middle managers and milkers, and general laborers were compared, we found that the top managers work the most weekly hours on average at 56.2, followed by middle managers at 52.6 and general laborers at 44.5 ( Figure 12). New York's dairy farm managers earn a wide range of salaries, as shown in Figures 13 and 14. As expected, top level managers earn substantially higher salaries than middle managers. For example, 38% of top level managers receive salaries of $50,000 or more compared with 8% of mid-level managers who receive $50,000 or more. The average value of benefits, both top and mid-level dairy managers received is shown in Figure 15. Over 65% of dairy managers on average received benefits totaling $5,000 or more and 10% received benefits on average totaling $15,000 or more.    \",\n       'In Fig. 4 , the mean FPDS values and classification accuracies (based on 0.5 FPDS threshold) computed from image subsets taken across different ranges of time to conversion within the the pNC and pMCI groups are shown. The time to conversion is defined as the number of years from the image scan date to the earliest future timepoint at which the subject associated with the image was given a clinical diagnosis of DAT. The pMCI group exhibited relatively high mean FPDS values (0.64 -0.71) among the 0-3 years to conversion range. But, in the later time to conversion ranges, especially beyond the 4 years to conversion range, a considerable decrease (0.26 -0.46) in the FPDS means was observed.\\nTherefore, for the pMCI group, good classification accuracies (0.7 -0.78) were only observed in the 0-3 years to conversion range, past which the pMCI images were frequently misclassified as DAT−, reducing the overall accuracy to 0.68. The pNC group showed low FPDS mean values (0.17 -0.52) across all the time to conversion ranges, leading to incorrect labeling of more than 72% of the pNC images as DAT− (0.28 overall classification accuracy).',\n       'Based on the three-tier test related studies (Caleon, & Subramaniam, 2010; Tan et al., 2002; Yang, & Li, 2017 ), incorrect response rates for the first and second tier that exceeded 18.3% were defined as significant misconceptions. At the same time, mean confidence scores higher than 3.3 (out of 5) that were associated with significant misconceptions were defined as strong misconceptions (Caleon, & Subramaniam, 2010; Tan et al., 2002; Yang, & Li, 2017) . Data revealed Hong Kong sixth graders had strong misconceptions for 17 out of 40 items, the Hong Kong fifth graders had strong misconceptions for 18 out of 40 items, and Taiwan fifth grade students had strong misconceptions for 25 out of 40 items. It seems reasonable to believe that students from different subgroups all have serious misconceptions when responding number sense related questions. Especially, TW5 students had over a half of questions belonging to strong misconceptions which is obviously higher than the HK5 and HK6 students who had less than a half of questions belonging to strong misconceptions.',\n       \"1. Automated quality checks for MRI sequence parameters are advisable (for each scan, comparison of the acquisition parameters with the intended protocol for each scanner). Also, images should be monitored for motion and other artefacts such as magnetic susceptibility artefacts, aliasing artefacts, or Gibbs artefacts. A minimum standard for quality control will have to be defined to exclude problematic data (for example, based on the severity of artefacts or from automatic measures of signal-to-noise ratio, background noise, etc.). Indeed, even if the computational method seems to run properly, artefacts can dramatically alter the data. 2. MRI gradient nonlinearities have to be corrected at least with corrections proposed in options by the manufacturer as they can affect dramatically measures of brain atrophy. 3. Subject's positioning in the scanner is another key aspect and strict instructions for MR technicians may be particularly useful to reproduce confidently the position of the head, from one subject to another, and from the first time point to the following time points, in longitudinal studies. 4. Considering the hardware, the same coil has to be used during an MR protocol both in cross-sectional and longitudinal studies. When upgrades or repairs are needed during the study period, a strategy should be in place to scan some participants twice, before and after the upgrade, to estimate the potential bias for a posteriori analyses. Otherwise, the imaging protocol and hardware should be kept as constant as possible over the whole time of the study.\\nRecommendations to evaluate given the targeted trade-off between sensitivity and variability 5. Within-center reproducibility is much higher than between-center reproducibility. The need for a multicenter setup should then be carefully evaluated while taking into account specific aspects such as number of participants, subject transportation, data centralization, quality control, and cost. For multicenter studies, the use of only one vendor or only some models can be an option. 6. In multicenter studies, protocol harmonization is a mandatory step to obtain similar sequences with acquisition parameters as close as possible. There is no way to get a perfect harmonization of MRI protocols between different vendors and models. Therefore, it is a trade-off between a high reproducibility of markers (few centers, same vendors) and a sufficient statistical power through the recruitment of a large number of subjects, generally in several centers with different scanner vendors and models. Specifying acceptable ranges for different acquisition parameters may help obtain satisfactory harmonization for large-scale clinical studies. 7. Spatial resolution is a key parameter in assessing correctly the variation of any MRI marker such as brain volumetry, longitudinal atrophy, volume of WMH, or quantification of lacunes and PVS. Increasing spatial resolution decreases signal-tonoise ratio and increases acquisition duration. Sensitivity for quantifying small objects such as lacunes and PVS will be compromised if using voxels as big as the object itself or where there is a slice gap. Selection of optimal spatial resolution must therefore be targeted to the primary question. 8. When analyzing multicenter MRI data, the use of statistical models to take into account the induced variability is recommended (adjusting on center or using random effects models for example). 9. While some quantitative MRI markers of SVD are regularly assessed (brain atrophy, WMH volume), others are difficult to quantify given their small size in relation to conventional anatomical MRI (lacunes, PVS) or their strong dependence on MRI parameters (CMB). High-field MRI and detailed studies of these particular markers may be performed in studies nested within larger studies assessing more common markers in a clinical setting. For CMB characterization, SWI or quantitative susceptibility mapping are promising techniques. Since the total magnetic susceptibility of a CMB is an intrinsic physical property independent of imaging characteristics, mapping CMB by using quantitative susceptibility mapping may be an alternative approach more consistent over a wide spectrum of imaging parameters. 79 10. It is important for studies to acknowledge that variation is inevitable and they should detail the steps that have been made to minimize this variation.\",\n       \"Early childhood has become a priority within policy in many countries (Garvis et al. 2018). There is a wide notion that high quality early childhood education (or learning environments) will provide many benefits for children and families both in short as well as in long term. However, according to Taguma et al. (2012) these positive benefits are related to the 'quality' of early childhood education. The challenge in this lies in the fact, that the definition of quality differs across countries or across different interest groups. There are to be found research on the quality of early childhood environments from different perspectives, but lesser focus has been paid towards family's influence, thus parents are child's first educators. Parents provide their child, alongside with other learning environments, a broad mathematical and early literacy input. What type of an input this is, is of importance, since early years mathematical knowledge is strongly correlated with later mathematical and reading skills (Watts et al. 2014). Similarly, according to Hannover Research (2016) early academic skills related to literacy and math are the most significant predictors of future academic achievement. Also children's early non-academic skills, such as social competence and self-regulation, also contribute to school success. However, not all kinds of support have an impact on the child's skills. Zippert and Rittle-Johnson (2018) found barely any links between parent support and children's broad mathematical skills. Further, according to a recent longitudinal study of 554 three-yearold children, conducted by Lehr et al. (2019), show that book exposure and the quality of verbal interaction regarding mathematics both predicted mathematical outcomes in secondary school and those effects were mediated through early language and arithmetic skills. Reading outcomes in secondary school were not directly predicted by early home learning environments but indirectly via early language and literacy skills. Path models revealed that the different dimensions of the early home learning environments were differentially associated with preschoolers' early competencies. All effects remained significant when including the concurrent home learning environments during secondary school which predicted reading outcomes directly. Therefore, the quality of early learning environments seems to have an impact on later outcomes, which in turn have an impact on student future prospects. The impact of early academic skills on students educational outcome can in turn vary dependent on gender, socioeconomic status and English proficiency (Hannover Research 2016). With this study we answer one research question: Do family-related background variables parental educational level, parental attitudes towards mathematics and science, parental perception of child's early skills in TIMSS 2015 data have a different effect in different areas of Finland on students educational outcome? In order to answer the question we fit a linear regression model to the data and the results are then displayed in the form of a contour map of Finland, visualising the effect of the family-related background variables on student achievement geographically. Something that has not been done previously in educational research.\",\n       \"Innovative on-the-fly solutions to problems that hampered earlier data recovery efforts let CES surmount a series of technical, operational, and administrative challenges that each threatened to halt the recovery in mid-process. While CES led the recovery effort, success on this scale required help from many partners. The project got off the ground because of the strong support of C. Harvey Monk, Jr., then Associate Director for Economic Programs at the Census Bureau. We also appreciate the support of the Census Bureau's Computer Services Division. They granted us additional time to access the Unisys machine and graciously allowed us to use their office facilities. been proven for the ASM and SIRD. See Text Box 4-2. The 2009 recovery effort built on work that had been going on at a much lower intensity for The recovered files have the potential to bring new data series to CES, extend existing data series at CES by a decade or more, and fill gaps in existing data. This potential has already The data recovery project has already borne fruit. Annual Survey of Manufactures (ASM) data dating as far back as 1954 have been recovered, and as many as 15 years of additional historical data from the Survey of Industrial Research and Development (SIRD) have been recovered for large firms. 1 The oldest microdata available to researchers at the Center for Economic Studies (CES) and in the Research Data Centers (RDCs) as of December 2009 were manufacturing data from the 1963 Census of Manufactures (CM). Extending the manufacturing microdata series provides opportunities to study the evolution and behavior of plants and industries over longer time periods and more business cycles.\",\n       'A key characteristic of statistics is to develop accurate predictive models (Dawid, 1984) . Indeed, as pointed out by Bernardo and Smith (2000) , all other things being equal, a given model is to be preferred over other competing models if it provides better predictions of what actually occurred. Thus, a critical component in the development of accurate predictive models is to decide on rules for gauging predictive accuracy-often termed scoring rules. Scoring rules provide a measure of the accuracy of probabilistic forecasts, and a forecast can be said to be \"well-calibrated\" if the assigned probabilities of the outcome match the actual proportion of times that the outcome occurred. The development of accurate predictive models has, arguably, been overlooked in education where the goal has been instead an orientation toward finding well-fitting models, particularly in the context of SEM (e.g., Kaplan, 2009) .\\nA number of scoring rules are discussed in the literature (see, e.g., Gneiting & Raftery, 2007; Jose, Nau, & Winkler, 2008; Merkle & Steyvers, 2013a; Winkler, 1996) ; however, for this article, we will primarily evaluate predictive performance using the 90% predictive coverage criterion (Hoeting et al., 1999) and the log of the percentage predictive coverage for continuous outcomes referred to as the log score. Predictive coverage is used productively in frequentist and Bayesian settings and is assessed using the proportion of predicted observations that fall in the corresponding 90% prediction interval. For this article, the predictive coverage criterion is implemented via the R routine \"predict\" in the program \"stats\" (R Core Team, 2017) . For the prediction of a dichotomous outcome, it is common to use the Brier (1950) score defined as:\\nwhere over each forecast instant t, f t is the probabilistic forecast and o t is the observed event (1 if the forecasted event took place, 0 otherwise). Both the log score and the Brier score are so-called proper scoring rules insofar as the score is maximized (or minimized in the case of the Brier score) when the reported forecast probability is the same as true probability. In both cases, the log score is a local and strictly proper scoring rule that assesses the quality of the prediction by providing a numerical score based on the accuracy of the match between the predictive distribution and the actual obtained values. The log score is strictly proper in the sense that it is unique (see, e.g., Gneiting & Raftery, 2007; Merkle & Steyvers, 2013b , for more detail).',\n       'A statistical study was performed to evaluate the discriminative power of each feature, that is to say, if it offered a good differentiation between the three populations considered in this work: AD, EMCI and CN. The final purpose of this analysis was to check the feasibility of these features as biomarkers of AD.\\nTo compare the distributions of the texture parameters for the three classes, data visualization techniques and statistical tests were employed. Box-and-whiskers plots and Scatter plots were used to visualize and compare the data distribution of each class. In the case of statistical tests, each feature was evaluated with the p-value provided by the one-way Analysis of Variance (ANOVA) F-test to find statistically significant differences between the three populations, considering significant values of p < 0.05. However, it is important to mention that finding features with p < 0.05 using this test implies that there exists a significant difference between at least two of the three populations, but it is not possible to know which ones are significantly different only with this test. Therefore, additional evaluation of the difference between individual groups was needed. To this end, the Mann-Whitney-Wilcoxon (MWW) test, also called Mann-Whitney U test or Wilcoxon rank sum test, was used to compare the populations in pairs. This non-parametric test analog to the independent samples t-test does not require the normality assumption of the t-test, and it is recommended when the sample sizes are relatively small [44] .\\nIn statistics, when the number of statistical tests performed increases, the contrast test became more permissive, rejecting the void hypothesis more easily and increasing in this way the number of false positives by increasing the probability of randomly obtaining a significant result [44] . This problem is usually referred to as the multiple comparisons problem. To counter this effect, we decided to apply two multiple comparisons correction methods before determining which features were statistically significant. The first method applied was the Bonferroni correction, which controls the family-wise error rate. This method compensates the type I error (incorrectly rejecting the null hypothesis) and attempt to limit the probability of even one false discovery, so it is relatively strong (conservative) and, in some cases, it may lead to a very high rate of false negatives, thus increasing the type II error (accepting the null hypothesis when the alternative is true). The second method used was the Benjamini and Hochberg (BH) procedure, which controls the false discovery rate. This method attempts to control the expected proportion of false discoveries, that is, the proportion of discoveries (significant results) that are actually false positives, thus being less sensitive than the Bonferroni correction.',\n       \"Several limitations in this study should be acknowledged. First, a huge body of ASL studies in AD was excluded because of the chosen voxel-wise approach and this approach was based on summarized coordinates and their effect sizes rather than on raw imaging data or statistical brain maps, which might limit its accuracy [93] . In addition, the patients in the included studies were the clinical samples, who compared with community-based normal control volunteers. In this context, although they were matched or adjusted regarding age, sex ratio and education, some other critical factors, such as socioeconomic status, vascular risk burden, cognitive reserve, racial/ethnic make-up, and genetic vulnerability were not addressed in most of the original studies, which might lead to some heterogeneity in the conclusions and remains to be further addressed. Further analysis of multicenter raw imaging data in large homogeneous samples, like ASL-MRI scans from the Alzheimer's disease Neuroimaging Initiative (ADNI) subjects [94] , would confirm the present findings. Second, our meta-analysis that synthesized the findings from the cross-sectional studies could not determine whether decreases of rCBF in the identified brain areas are the cause or consequence of AD [14] . Longitudinal studies could provide further insights. Third, ASL acquisition parameters, pre-and post-processing steps, such as scanner field-strength, inversion time, labeling duration, post label time delay, volume coverage, readout approaches, partial volume correction, and GM correction, may bias the results that warrant careful consideration by investigators. Further investigations in high field-strength MRI scanners with optimized and standardized imaging acquisition and analytical methodology are recommended [15] . \",\n       \"that describe regional atrophy rates and changes in atrophy rates across the disease spectrum from normal aging to early AD. [5] [6] [7] We examined longitudinal rates of regional neocortical atrophy over 1 year in 137 healthy controls and 335 individuals at different stages of clinical impairment. Annual atrophy rates were of interest because they have demonstrated high sensitivity to subtle brain changes in AD and may discriminate between diagnostic groups better than baseline brain measures. 1 Furthermore, we used cross-sectional data to estimate the change in atrophy rates with increasing levels of disease. Change in atrophy rates may be particularly useful for evaluating the efficacy of disease-modifying therapeutics. 5, 8 We hypothesized that in normal aging, mild atrophy would be observed in prefrontal and parietal regions with relative sparing of posterior neocortex. Conversely, we predicted that in patients with mild MCI, atrophy rates would be greatest in medial temporal lobe regions and would spread to other cortical regions along a posterior-to-anterior gradient with increasing levels of clinical impairment. The longitudinal and cross-sectional MRI data reported in this study provide a qualitative and quantitative depiction of regional brain changes that accompany normal aging and progression from prodromal to early AD. METHODS Alzheimer's Disease Neuroimaging Initiative. Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). The ADNI was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies and nonprofit organizations, as a $60 million, 5-year public-private partnership. ADNI's goal is to test whether serial MRI, PET, other biologic markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\\nThe principal investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California, San Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations. Subjects have been recruited from more than 50 sites across the United States and Canada (www.adni-info.org).\\nStandard protocol approvals, registrations, and patient consents. This study was approved by an ethical standards committee on human experimentation at each institution. Writ-ten informed consent was obtained from all patients or authorized representatives participating in the study.\\nParticipants. ADNI eligibility criteria are described at http:// www.adni-info.org/index.php?optionϭcom_content&taskϭ view&idϭ9&Itemidϭ43). Briefly, subjects are aged 55-90 years and have a study partner able to provide an independent evaluation of functioning. Control subjects have a Mini-Mental State Examination (MMSE) score between 24 and 30 (inclusive) and a global Clinical Dementia Rating (CDR) of 0. Subjects with MCI have MMSE scores between 24 and 30, a subjective memory symptom, objective memory loss measured by education-adjusted scores on Wechsler Memory Scale Logical Memory II, a global CDR of 0.5, preserved activities of daily living, and an absence of dementia. Subjects with mild AD have MMSE scores between 20 and 26, have a global CDR of 0.5 or 1.0, and meet National Institute of Neurological and Communicative Disorders and Stroke-Alzheimer's Disease and Related Disorders Association criteria for probable AD. 9 In this study, MRI data were included on all ADNI subjects for whom baseline and 12-month MRI scans were available and had passed local quality inspection by December 2008. To estimate the level of clinical impairment, the CDR Sum of Boxes score (CDR-SB) was calculated at baseline for all participants. This score has been described as a useful and reliable measure for detecting subtle clinical change, ideal for use in longitudinal assessments of dementia. 10 The CDR-SB score was used to divide the study sample into groups reflecting degree of impairment. From highest to lowest, our initial sample included 151 individuals with a CDR-SB ϭ 0 (normal controls), 105 with a CDR-SB 0.5-1.0, 126 with a CDR-SB 1.5-2.5, and 104 with a CDR-SB Ͼ2.5 (mild AD). Because we wished to compare atrophy rates in normal aging with those across different patient groups, individuals in the normal group who progressed to a CDR-SB of 0.5 or greater at follow-up (n ϭ 14) were removed from the analysis. Final group demographics are presented in table 1. Information on CDR-SB progression of individual participants over the course of 1 year is included in table e-1 on the Neurology ® Web site at www. neurology.org. Groups did not differ in age [F(3,471) ϭ 1.9, p Ͼ 0.05] or sex distribution [ 2 (3) ϭ 0.28, p Ͼ 0.05]. The groups did differ in level of education [F(3, 471 ϭ 2.8, p Ͻ 0.05]. Those with a CDR-SB ϭ 0 obtained a higher level of education relative to those with a CDR-SB Ͼ2.5.\",\n       nan, 'https://orcid.org/0000-0001-9332-7982',\n       'From a multinational perspective, the CCVI enabled a basic CCVA for our focal migratory birds, and allowed relevant information to be incorporated from Canada, US and Central America into the CCVA ( Table 2) . The CCVI considered i) exposure of the species to climate change within the breeding range, ii) indirect climate exposure resulting from human responses to climate change, iii) sensitivity to climate exposure and adaptive capacity, iv) an exposure index for the overwintering grounds, v) modeled distributional changes (or changes in climate envelope) expected under specific climate change scenarios, and vi) documented responses (peer review) to climate change [38] . A concise assessment is provided here, and details of the literature review to complete the CCVI assessment and the full CCVI table is provided in Section A and Table A in S1 File. Some of the readily assessable data required to populate the CCVI table was restricted to specific jurisdictions (Table 2 ). For example, to estimate moisture severity we used the Hamon AET:PET moisture metric [22] , which measures the moisture deficit between the 2050 time horizon and the 1961-1990 baseline (ensemble GCM and medium A1B climate scenario) for the continental US [23] . AET:PET values were taken from the packaged climate summaries available on the CCVI site, and produced by the Climate Wizard development team [25, 39] . For the Canadian portion of the basin, however, we had to extrapolate values to estimate approximate moisture deficit values. Approximately 53% of the basin is in Canada, and the amount of extrapolation required differs among species. Approximately 25% of the hooded warbler breeding range required extrapolated moisture change values, while close to 50% of meadow lark and wood thrush required extrapolated values (and encompassed a much greater area than the hooded warbler range). Patterns of moisture change were fairly homogenous on the US side, so we assumed that they would be equally homogenous on the Canadian side. The extrapolation was not precise and is subject to unquantifiable error, but provides an approximate estimate of how moisture conditions will change in the area.\\nPredictive temperature data required for the suggested GCMs and scenarios for CCVI were available in GIS for only for the US portion of the basin, although the issue here may be related to technical problems with web-based tools to generate data for a specific geographic area rather than management decisions to restrict extent of data. To resolve this issue, we used predictive CC data that included both the US and Canadian portions of the basin, and that was based on a mid-century (2050s), ensemble GCM and a medium emission scenario (RCP 4.5) and is roughly equivalent to the CCVI suggested A1B scenario. An additional consideration was GIS conversion of Celsius to Fahrenheit, as the North American predictive temperature data we used was mapped in Celsius while the CCVI requires estimates in Fahrenheit. We had no issues estimating the severity of the exposure to climate change in the southern US and Central American overwintering grounds as all the necessary data was available at the CCVI site [25] .\\nThe basic modeling frameworks, including species distribution (SDM) and vegetation models varied among both jurisdictions and species (Table 1) . Geographically, SDMs were generally available for the entire breeding range, while more specific vegetation response models were largely restricted to the US portion of the basin. For eastern meadowlark both the Canadian-based CC-QBD and US-based CC-TABA modeling frameworks were used to evaluate changing climate envelopes and habitat. For the 2041-2070 time horizon the CC-QBD SDM models predicted a net gain in habitat area resulting in a northward expansion of 307 km from 1990 to 2070 (Fig 3) , and 465 km by 2071-2100 horizon [32] . However, the grassland habitats that eastern meadowlarks inhabit are unlikely to move northward as quickly, as the northern region lies on the boreal shield, where grasslands are rare. In contrast, the CC-TABA models predicted a decrease in abundance in the western part of the eastern meadowlark range by 2100 and do not predict any movement in the core of the range or northern expansion [27, 28] . Model predictions encompassed the eastern US, but did not include any predictions north of the border due to a lack of climate-vegetation modeling in Canada. The overall CCVI score for meadowlark was highly vulnerable (Table 2) . Wood thrushes are mature forest species found in mixed wood and deciduous forests, often preferring previously disturbed sites [41] . They nest in interior edges [42] , selecting deciduous trees such as American beech (Fagus grandifolia), American elm, and red maple (Acer rubrum) for nesting [43] . For wood thrush the CC-QBD model predicts a range expansion with an increase in habitat area by 25.5%, a northward range expansion of 28 km/decade for an overall range extension of 304 km by 2041-2070 (Fig 4) with an additional 119 km by 2071-2100 [32] . However, the deciduous trees that wood thrushes select for nesting are unlikely to move northward as quickly. The northern range limit lies on the boreal shield, where nesting tree species such as American elm, American beech and red maple are rare. In contrast, the US-specific CC-TABA CC-vegetation models predicted a decrease in abundance in the western and southern parts of the wood thrush range [28] . The top predictors in this model were red maple distribution, annual precipitation, American beech distribution, American elm distribution, and the mean difference between July and January temperatures [27] . Given the jurisdictional boundary of the modeling, it is difficult to extrapolate the changes at the northern range limit into Canada; however, current research suggests that American beech may decline throughout its range across eastern US, while American elm may increase within the northern extent of its range across the northern US [31] . The overall CCVI score was highly vulnerable (Table 2) for wood thrush.\\nHooded warblers breed from southern Ontario east to Rhode Island, south through northern Florida, and across the Gulf Coast through northeastern Texas [44] (Fig 5) . These birds breed in mature forest that has gaps or openings where early successional vegetation grows. Within the Great Lakes, hooded warblers are found only in the basins of Lake Michigan, Lake Erie, and Lake Ontario. Hooded warblers occupy deciduous forest stands dominated by maple, American beech, and oak (Quercus spp.) [44] . For hooded warbler all three modeling frameworks predicted northern expansion in range in relation to different climate scenarios.\\nThe Canadian CC-QBD models predicted a range expansion with an increase in habitat area by 28.6%, a northward range expansion of 34 km/decade for an overall range expansion of 378 km [32] by 2041-2070, and a further 156 km expansion by 2071-2100 (Fig 5) . The US CC-TABA model predicts a decrease in abundance in the southern part of the hooded warbler range and an increase in the northern US part of their range through the eastern seaboard as well as northern Minnesota, northern Wisconsin, and Michigan, including the upper peninsula [27] .\\nFor hooded warbler only, the coupled species distribution-metapopulation dynamics model (CC-SDM/MPD) offered additional insights related to uncertainty based on choice of GCM and demographic parameter uncertainty. This modeling effort was applied to the entire Great Lakes Basin, so extrapolation of results across jurisdictional boundaries was not necessary. The researchers found that interpretations of vulnerability were influenced in part by the particular GCM selected for modeling climate change, but that vulnerability was also strongly affected by habitat loss. These results point to concerns with using alternative GCMs or emission scenarios to estimate temperature and to the inadequacy of using only climate based predictions and not accounting for habitat change. The overall CCVI score was less vulnerable ( Table 2 ) for hooded warbler.',\n       \"In our TRIAD models, we control for a number of baseline demographic measures. Information regarding child gender, race, age at preschool entry, whether qualified for free or reduced price lunch, whether limited English proficient, and whether designated for special education was obtained from the study districts and schools.\\nMother's reported their highest level of education on a parent survey administered during the preschool year. Figure 1 . For the estimates presented in Figure 1 , we simply standardized each respective measure of math and reading achievement and teacher-rated anti-social behavior, and we then regressed each respective measure on the kindergarten measures of math, reading, and anti-social behavior. In order to make the correlations between math and anti-social behavior comparable to the correlations between math and reading, we reverse-scaled each anti-social behavior measure. Standard errors were adjusted for school-level clustering, and the regressions were weighted using the panel-weight for data spanning from kindergarten through fifth grade (see Table S2 note). Coefficients and standard errors from each of these regressions can be found in Table S2 . Causal inference in studies of skill development 7 site, and blocking group. All of these regression results were estimated only within the control group (n= 396), and full information maximum likelihood was used to account for missing data.\",\n       '• As reported elsewhere, 26 embedded in research imaging studies should be the anticipation of such findings and a protocol in place for managing them.\\n• In clinical medicine, findings are followed up routinely, although the rising cost-benefit ratio as in genomic testing has become a source of concern.',\n       \"Throughout the previous section, limitations of the current work are identified, and areas of future study are suggested. In this section, I will outline the three areas in which further analysis would help to better our understanding of gender funding disproportionality among STEM graduate students. Furthermore, I will address some of the data limitations of the current study, and the need for intersectional data and approaches for understanding inequality in labor mechanisms for STEM graduate students. Although the current work is certainly suggestive of the importance of fields for understanding differences in gender funding disproportionality, more research is needed in this area. One approach to this may be to conceive of the structure of the data as departments nested within fields, instead of departments nested within institutions, due to the small but significant differences between institutions. Alternatively, a three level, multilevel analysis may also be attempted in order to test whether department, institution and field levels all significantly vary across P-GFD and R-GFD. Another approach would be to develop within field analyses, potentially compared over time. While such studies would be limited in terms of comparing one field to another, they may help researchers to best tease out the important factors for predicting gender funding disproportionality. The next area, the process of commodification of research, likely requires qualitative inquiries. One of the key assumptions underlying the current work is that industry influence restructures the workplace (Slaughter & Leslie, 1997;Slaughter & Rhoads, 2004), a consequence of which is the commodification of academic research (Irzik, 2013;Jacob, 2009;Radder, 2010) thereby reinforcing gender norms (Acker, 1992(Acker, , 1992). However, further analyses aimed at understanding the processes and mechanisms by which industry influence results in the commodification of academic research may help researchers to find a better operationalization of the concept of industry influence. For example, while the share of R&D expenditures originating from industry speak to a dimension of industry influence, it may not necessarily reflect more or less degree of commodification of academic research. It's possible that commodification does not occur on a spectrum, but instead is conceived of as a dichotomy. Therefore, further understanding of this process could help quantitative researchers to select variables that better reflect change in commodification of academic research. Another weakness of the main independent variable of interest relates to the third area of future research. Although the main independent variable of interest, the share of 121 R&D expenditures originating from industry certainly speaks to a dimension of industry influence, its low correlation with the department funding ratio suggests that this variable may not adequately address industry's influence on graduate students specifically. As with the commodification of research, further analysis of the ways that industry influence within a department affect graduate students would also greatly inform research int his area. One example of an approach that would be informative would be considering an interaction between type and source of funding. For example, does productive funding from federal sources affect gender funding disproportionality differently than productive funding from private/industry sources? The current work represents preliminary steps into investigating gender funding disproportionately among graduate students in STEM fields. However, it is severely limited by the structure of the publicly available NSF datasets. Specifically, the primary survey used in this analysis, the Survey of Graduate Students and Postdoctorates in Science and Engineering, disaggregates data by either gender or race, making an intersectional approach to this study impossible. Although this purpose for this structure of data may relate to privacy concerns, it nonetheless affects how analyses must be carried out. Furthermore, the inability to consider the intersecting oppressions stemming from identities, particularly race alongside gender, in the current analysis is a major limitation. Thus further work which takes intersectional approaches, and future efforts to collect data, should be greatly informed by critical quantitative works on data collection/surveys (H. E. Metcalf, 2014), as well as intersectionality theoretical frameworks (Crenshaw, 1990).\",\n       'A system is an object or a collection of objects. Objects are treated as having no internal structure.',\n       \"Note that the above reconstruction of x i , i.e., the i th ROI's construction, is independent of the reconstructions Figure 2 . Framework of the proposed brain functional network construction. Given brain functional signals X, we can compute a Pearson's correlation (PC) matrix P, which will be used to define both the connectivity strength weight C for the l 1 -norm and the group partition for the l 2;1 -norm in the proposed model. The brain network W will be constructed with optimization. [Color figure can be viewed at wileyonlinelibrary.com] r Weighted Sparse Group Model for MCI Classification r r 2373 r of others. To further make the connectivity strengthweighted penalty consistent across all links which have similar functional connectivity strength, we propose a group constraint on the similar links (within a subnetwork) for allowing them to share the same penalty during the whole-brain network construction. In this way, we can model the whole-brain network jointly, instead of separately modeling each ROI. Of note, we use connectivity strength to group the ROIs into subnetworks although existing other grouping ways, such as using diffusion tensor image-based tractography to group the ROIs.\\nTo identify the group structure in the brain network, we partition all links, i.e., pairwise connections among ROIs, into K nonoverlapping groups based on the PC coefficients. Specifically, assuming that the numerical range of the absolute value of the PC coefficient jP ij j is P min ; P max ½ with P min ! 0 and P max 1, we partition P min ; P max ½ into K uniform and nonoverlapping partitions with the same interval D 5 P max 2P min ð Þ =K. Then, the k th group can be defined as G k 5 i; j ð ÞjjP ij j 2 P min 1 k21 ð ÞD; P min 1kD ½ È É . Figure 3 shows an exemplar grouping results with K 5 5 from a randomly selected subject, for illustration purpose.\\nTo integrate constraints on functional connectivity strength, group structure, as well as sparsity in a unified framework, we propose a novel weighted sparse group regularization as formulated below:\\nwhere\\nis l q -norm (with q 5 2 in this work). d k is a predefined weight for the k th group, i.e.,\\njP ij j and jG k j represents the number of links in the k th group (G k ). r is the same parameter in Eq. (2), which is set as the mean of all subjects' standard variances of absolute PC coefficients. After obtaining groups, with E 1 < E 2 < . . . < E K , we can penalize the group with higher E k by smaller d k and vice versa. Eq. (4) can also be expressed in a matrix form as follows: \\nis the F-norm of matrix, denotes the elementwise multiplication. Unless specifically noted, we denote jjÁjj 1 def 5 P N i;j51 jÁ ij j in this article. To avoid a trivial solution of W5I, we further enforce the constraint W ii 50, equivalent to remove signals of the i th ROI from X when representing itself.\\nIn Eq. (5), the first regularizer (which can be regarded as l 1 -norm penalty) controls the overall sparsity of the reconstruction model, and the second regularizer (l q;1 -norm penalty) contributes the sparsity at the group level. k 1 and k 2 are the two parameters used to balance the tradeoff between the (first) l 1 -norm regularization and the (second) group regularization in the objective function. It is noteworthy that our proposed model can be treated as a generalized form of sparse brain construction models. Specifically, if C ji 51 and k 2 50 in Eq. (4), our model reduces to the SR model. If k 2 50, it will degrade to the WSR model. Moreover, if C ji 51, the proposed method shares the same formulation with the SGR (Simon et al., 2013) . In the experimental section, we also include these three special cases for comparison. To our best knowledge, (1) using the connectivity strength-based weights derived \",\n       'Most, about four out of five, tenth graders said their parents made at least some attempt to know where they went at night. A few, 6%, said their parents made no attempt to know. About three-fifths of the tenth graders thought their parents made at least some attempt to know where they were after school. About three-fifths of the tenth graders thought their parents made at least some attempt to know what they did with their free time.\\nParents appaared to be most concerned about the night activities of their tenth graders.',\n       \"tive Records Research and Applications (CARRA) at the Census Bureau has developed a probabilistic record linkage system in which a protected identification key (PIK) is created for each entity and the PIK is used to link records from different sources behind a secure firewall. Records are matched against a reference file that contains each person's PIK, which is associated with the SSN, name and variants of the name used, date of birth, sex, and current and previous addresses. The linkages provided by CARRA are used in numerous research projects. 3 Jones (2016), for example, used linked data from the CPS and from W-2 records collected by the Internal Revenue Service to study wages of tipped workers in the restaurant industry. Linkage also allows for the study of entities that are related but not necessarily the same. In a medical study, it may be desired to link electronic medical records of patients with information about their health care providers or with records of other patients of those providers. Baldwin et al. (2015), for example, linked the records of women who had delivered an infant to the records of the infant using the surname, address, and dates of birth and delivery for the purpose of evaluating effects of therapeutic interventions during pregnancy. Hospitals selected to participate in the National Hospital Care Survey are asked to submit electronic health records for all patient discharges and all emergency department and outpatient department visits. NCHS plans to link these records with other data sources, such as the National Death Index and Medicare and Medicaid data, to measure mortality after discharge and other health outcomes (see DeFrances et al., 2012). Such outcomes would be difficult to study without linking records. Levant et al. (2016) illustrated the types of new analyses possible by linking records from a hospital's emergency department to its inpatient treatment records and its outpatient department to show the outcomes of people with traumatic brain injury. Research conducted for the National Household Food Acquisition and Purchase Survey of the U.S. Department of Agricultire (FoodAPS; see Ver Ploeg et al., 2015) 4 links survey responses from a probability sample of approximately 5,000 households with administrative data on SNAP participation and purchases, as well as information about the food items and prices that are accessible to the surveyed households. The linked information from SNAP is used to determine SNAP eligibility in the 30 days prior to the survey, resolve data discrepancies, and provide information on usage of the electronic benefit transfer card (U.S. Department of Agiculture, 2016). The U.S. Bureau of Justice Statistics (BJS) is linking records of admissions and releases from state correctional facilities with other administrative record data to better understand why prisoners recidivate. CARRA gives BJS access to numerous data sources that can be used to identify activities and changes in status that can affect both criminal activity and return to prison (Carson, 2015). For example, Social Security data will indicate whether the former inmate has a job, while data from the decennial census or the ACS will indicate whether the former prisoner is married. These data indicate events that can be turning points leading to or away from prison. All of these examples illustrate the potential benefits of record linkage for more efficient use of information. At the same time, it is not a panacea. Linkage rates vary across studies and for subpopulations within studies. Wagner and Layne (2014) found correct matches for more than 90 percent of the records in the 2010 census and more than 70 percent of the records in two commercial files, but match rates for other sources can be much lower. For example, Bucholtz (2015) found links between American Housing Sur-vey records and tax assessment information for more than 70 percent of single-family detached homes but for only 13 percent of condominiums in multifamily buildings. Rates of missed links and false links depend in part on the linkage method used, but they depend even more on the quality of the linkage variables. Better statistical methods and algorithms can reduce linkage errors, but their utility is limited if the data sources have little identifying information about the records. Harron et al. (2014) wrote that linkage errors can lead to biased conclusions, particularly when the linked and unlinked populations differ. Statistical methods have been proposed that account for linkage bias (see, e.g., Lahiri and Larsen, 2005;Hof and Zwinderman, 2012;Judson et al., 2013), but these, like nonresponse adjustments, are not guaranteed to remove the bias in key variables of interest.\",\n       'For the analyses included in Tables 1 and 2 , one-way ANOVAs were used for quantitative normally distributed variables. For non-normally distributed quantitative variables, Kruskal-Wallis and rank-based two-way methods were used. Chi-square tests were applied for qualitative variables. Due to the heteroscedasticity of the cognitive tests in different cognitive groups, a robust test (percentile bootstrap one-step M-estimator) was used to compare the different groups [66] . For further analyses, distributions of the variables and residuals were tested and power transformations applied as needed. Based on data from previous studies [12, 20] , we selected 200 ng/ml Hgb in CSF as the cut-off to exclude cases in which interpretation of α-syn might be confounded by rBC contamination of CSF samples. Association between biomarkers in covariate-adjusted models was tested using linear regression models. Partial correlation (r pc ) was obtained in these adjusted models. Distribution of residuals and absence of multicollinearity was tested in these models (variance inflation factor <4).\\nA logistic regression analysis was applied for classification of CN against AD and MCI subjects for each of the studied CSF biomarkers. Importantly, in biomarker research, classifiers, especially when multiple markers are included, while fitting well in the studied population, might work poorly in an independent cohort. This is largely due to the fact that the classifier(s) may also be fitting the noise that is characteristic to the studied samples, a phenomenon referred to as \"overfitting\". To avoid this potential problem, the subjects were randomly divided into discovery (70 %) vs. validation (30 %) sets. To train a classifier and crossvalidate the cutoffs in the discovery set, the subjects in the discovery set were further randomly split 10 times to form training (70 %) vs. test (30 %) sets. The cutoffs of the model were selected in the discovery set using accuracy and kappa index as performance metrics [22, 63] . The obtained logistic regression model was then applied to the validation set and sensitivity, specificity and the area under the curve (AUC) in the receiver operating characteristic (rOC) curve were obtained [45] .\\nThe mismatch between the expected α-syn levels with respect to p-tau 181 levels (α-syn-p-tau 181 -Mis) was calculated as the standardized residual of the linear regression model that predicted α-syn based on p-tau 181 , adjusted for Hgb in the whole ADNI cohort.\\nA Cox hazards model, with age, gender and education as covariates, was used to study the conversion of MCI to AD for the different CSF biomarkers studied here. Standardized values (mean = 0, standard deviation = 1) were used for the biomarker values in order to compare the effect size of the association. We analyzed different quantitative outcome measures longitudinally using mixed-effects models [23, 43] to assess their association with the CSF biomarkers studied. Two random effects were included: an intercept and follow-up time measured in weeks. Age, gender and clinical diagnosis at baseline were included in the model as fixed effects. In addition, in the first model the t-tau/Aβ 1-42 ratio, the α-syn/ Aβ 1-42 ratio and the combination of the t-tau/Aβ 1-42 ratio with α-syn or α-syn-p-tau 181 -Mis were included as fixed effects in different models. An interaction between time and clinical diagnosis, and time and CSF biomarker ratio was also included. Statistical tests were two-sided and significance was set at p < 0.05. In the case of multiple comparisons, Benjamini-Hochberg correction for multiple comparisons was applied. Analyses were performed using r v. 2.15.3 [44] . Notably, besides the analysis of the demographic and biomarker variables in Table 2 , the only analyses performed in the PPMI cohort involved the association between t-tau and α-syn (summarized in Fig. 1h ), and the association of α-syn with age or gender. All other analyses described were performed in the ADNI cohort. Fig. 1 Display of Hgb and α-syn levels with both axes square-rooted (a). Display of α-syn levels on the y-axis and Aβ 1-42 (b), t-tau (c) and p-tau 181 (d) on the x-axis. Display of age against adjusted α-syn levels, after regressing out t-tau (e). Display of t-tau and p-tau 181 on the x-and y-axis, respectively; α-syn levels are represented using a color scale with violet indicating lower α-syn levels and red indicating higher α-syn levels (f). Histogram showing the distribution of α-syn levels stratified by AD-based cut-offs for Aβ 1-42 and t-tau. Vertical black lines represent the median α-syn values of the whole sample (center line) and the low (left) and high (right) α-syn groups (g). t-tau and α-syn CSF levels in CN (red) and PD (blue) PPMI subjects and their regression lines (h). Box plots showing the distribution of α-syn levels based on clinical diagnosis and CSF based groups (i)',\n       'These sample members had never completed a NELS:88 questionnaire in any round prior to 1994 2. Poor responders These are sample members who did not complete either a second follow-up questionnaire or a questionnaire in their first eligible round.',\n       'that follow report, only a very small number of children answered too few items for scores to be calculated. For each of the three content domains, the performance of the two-stage procedures, reliabilities, score statistics, and analysis of differential item functioning (DIF) will be presented. First, an expanded explanation and interpretation of DIF is in order. Very Low: Child doesn\\'t try or attempt many items, even with encouragement. 1.7% 1.6% 1.0% 1.2% Low: Child frequently says \"I don\\'t know\" without even trying, consistent encouragement needed. 9.9% 10.4% 7.5% 8.1% Average: Child works on most items, says \"I don\\'t know\" or refuses to answer items after s/he has begun doing some work or after making some attempt to figure the item out.   5-4',\n       nan,\n       \"Estimated coefficients for control variables. Notes: Shown are estimates (standard errors) for the control variables included in OLS regressions of a child's test score or teacher assessment on his/her body mass index ( \",\n       \"A second prominent theory is that SEC influences educational outcomes indirectly through a variety of school effects that are associated with both SEC and educational outcomes. That is, SEC serves as a proxy measure for a number of intercorrelated school factors that are associated with achievement or attainment. For example, low SEC schools tend to have lower levels of per pupil funding, be less able to attract and retain quality teachers, and be less able to staff college prep courses (Betts, Rueben, & Danenberg, 2000) . Thus, those factors may mediate the effect of SEC on achievement or attainment.\\nSchool effects can be classified into four types including compositional effects, resources, structures, and practices (Gamoran, 1996; Morgan & Sorensen, 1999; Rumberger & Palardy, 2005a) . The first three are considered school inputs because they are typically ''given'' to public schools, whereas school site personnel typically have far greater control over the practices they utilize. Yet, while classifying school effects as either under the control or not under the control of school personnel is helpful for assessing policy implications and accountability, many factors do not strictly conform to this dichotomy (Willms, 2010) . That is, factors may be partially under the control of teachers and administrators, but also influenced by student inputs and district, state, and federal policies. An example of this is disciplinary practices. Whereas school personnel typically have flexibility in determining their disciplinary practices, they must also respond to student inputs and are restricted by various external policies (Coleman, 1966; Thrupp, 1999) .\",\n       'For analytical purposes, we distinguish between four subcategories of educational marketization, although they may often partly coincide in reality: school choice, competition, privatization and commercialization (Linick, 2014; Molnar, 2006) . In many countries, competition and school choice are enacted in an organizational field with both public and private (increasingly commercial) players (Ball & Youdell, 2008) . However, school choice, competition and commercialization are also present in countries where education is almost completely provided by public players, for example, Norway and Finland (cf. Berge & Hyggen, 2011; Bjordal, 2016; Blossing et al., 2014; Kosunen & Seppänen, 2015) . In the following sections, we briefly discuss the subcategories. Table 3 summarizes and exemplifies indicators of the different aspects of marketization of education.',\n       'In Table 4 , we show the segmentation performance by all the comparison methods for the left and right hippocampi (HC). Each value in the table shows the mean Dice ratio (and standard deviation) across 30 leave-one-out cross-validation experiments.\\nAs we can see from these results, our proposed method (MCfull) achieves the best performance among all the methods, followed by the degraded version (MCdeg) which outperforms the rest of competing methods (according to a paired t-test at 5% significance level). Specifically, our proposed method (MCfull) outperforms both the reconstruction-based (LWV, NLWV and SPBL) and 5 Computational times of MATLAB/mex scripts on 4 Intel Core i7 CPUs at 2.5 GHz the classification-based (LogReg) approaches by ∼1.5% and 1.1%, respectively. Regarding the degraded version of our method (MCdeg), we can see that it also outperforms both SPBL and LogReg by ∼1% and ∼0.6%, respectively, thus confirming the superiority of our combined, matrix-completion based approach, compared to the separate reconstruction-based and classification-based approaches. By comparing the results of the two versions of our method, we can see that the sequential confidence-guided framework provides a further improvement of ∼0.5% with respect to MCdeg. Another interesting observation is that NLWV outperforms LWV by >1%, thus confirming the advantage of including neighboring atlas patches in label fusion as already noted by Rousseau et al. (2011) . This has to be taken into account when interpreting the results of STEPS, which, like LWV, does not include the neighboring atlas patches in the dictionary. Thus, the ∼0.3% performance improvement of STEPS over LWV is due to both the superior statistical estimation technique and the MRFbased regularization. Regarding the comparison of SPBL and LogReg, we observe that the classification-based approach outperforms the reconstruction-based approach by an average of ∼0.3%. Each column in Fig. 8 shows two consecutive slices with the typical segmentation results by each comparison method.\\nThe arrows point to the areas with the most significant differences among the methods. In general, the proposed methods, MCdeg and MCfull, show the highest true positives (green). Particularly, reconstruction-based methods tend to have more false negatives (blue). Comparing the results by STEPS and LWV, we can see that STEPS manages to reduce the false negatives in the area pointed by the purple arrow, probably due to the MRF regularization. LogReg obtains worse results than the proposed methods, MCdeg and MCfull, in the areas pointed by the black and purple arrows, respectively.',\n       'We performed structural equation modeling using IBM SPSS AMOS 21 to determine the impact of the latent variables (ROIs for brain responses, personality, behavior, and genetic variations) on AUDIT score. We used three models: one with AUDIT at early adolescence score as outcome variable (basic model), one with AUDIT score two years later (prediction model), and one model involved the difference in the AUDIT scores between early and late adolescence to uncover relations that might specifically determine the increase of alcohol consumption. In addition, we tested mediational models, examining the indirect influence of genetic variations via personality, behavior, and brain responses to reward on AUDIT score for all three models. We tested the influence of the applied genetic variations on the other latent variables separately as well as simultaneously as we had no a priori hypotheses about the possible pathways in our models. This approach was chosen because one could assume that genetic variations might not directly influence drinking behavior but through altering behavioral, neural, and / or personality-related traits.\\nInclusion criteria for model fit were goodness of fit indices ≥ 0.9 and a root mean square error of approximation (RMSEA) not significantly exceeding 0.05. When modeling was impossible because single variables did not explain sufficient variance in AUDIT score, these variables were removed from the model. This had to be done in one step for rs26907, striatum, nucleus accumbens, and thalamus. A new factor analysis without the omitted variables was conducted to ensure validity of the latent variables. We report the final factor analysis in the results section.\\nUsually the effects of single genetic variations are quite small and large sample sizes are needed to detect them. Therefore, we initially conducted our analysis with the entire sample to be able to detect small effects. Nevertheless, in neuroimaging in mental disorders the problem of inflated predictions has been raised (Whelan and Garavan, 2014) . Although our sample seems to be large enough to exclude inflated predictions, the question has come up whether an overestimation of effects had occurred. Therefore, we randomly chose half of our sample and repeated the analyses. We chose this validation approach as the best way to avoid multiple testing. In order to test for center effects of our multi-center study, we applied the \"dropping one site\" approach for fMRI multicenter studies (Friedman et al., 2008) . Using this approach, we repeated the analyses three times, each time leaving out one randomly chosen examination site. To test for sex effects, we conducted the models separately for male and female adolescents.\\nUsing the latent variables ROIs, personality, behavior, and candidate genes as predicting factors, three models could be established on alcohol drinking behavior as measured by AUDIT score at early adolescence, late adolescence and on the increase (difference of scores at these two time points) (see Figures 2-4 ). For the model explaining alcohol drinking behavior at early adolescence (R 2 = 0.13), reward-related personality traits were the most important factor with a standardized regression weight of 0.35 (Figure 2 ). In predicting alcohol drinking behavior two years later (R 2 = 0.14), rewardrelated personality traits and the candidate genetic variations contributed almost equally with standardized regression weights of 0.26 and 0.27, respectively (Figure 3 ). Within the model predicting the increase in AUDIT score (R 2 = 0.11), the contribution of the factor containing rs7713917 and rs1800497 was most important with a standardized regression weight of 0.33 ( Figure  4 ). Overall, there was a very good model fit with squared multiple correlations of 0.13, 0.14, and 0.11, goodness of fit indices were 0.944 and the RMSEA did not significantly exceed 0.05 (p > 0.05). The mediational models (examining the indirect influence of genetic variations via personality, behavior, and brain responses) failed to reach the inclusion criteria for model fit.\\nThe models conducted separately for male and female adolescents showed comparable effects (see supplemental Table S1 ).',\n       'Objective: Measures of health-related quality of life (HRQL), including the Health Utilities Index Mark 3 (HUI3) are predictive of mortality. HUI3 includes eight attributes, vision, hearing, speech, ambulation, dexterity, cognition, emotion, and pain and discomfort, with five or six levels per attribute that vary from no to severe disability. This study examined associations between individual HUI3 attributes and mortality.\\nStudy Design and Setting: Baseline data and 12 years of follow-up data from a closed longitudinal cohort study, the 1994/95 Canadian National Population Health Survey, consisting of 12,375 women and men aged 18 and older. A priori hypotheses were that ambulation, cognition, emotion, and pain would predict mortality. Cox proportional hazards regression models were applied controlling for standard determinants of health and risk factors.\\nResults: Single-attribute utility scores for ambulation (hazard ratio [HR] 5 0.10; 0.04e0.22), hearing (HR 5 0.18; 0.06e0.57), and pain (HR 5 0.53; 0.29e0.96) were statistically significantly associated with an increased risk of mortality; ambulation and hearing were predictive for the 60þ cohort.\\nConclusion: Few studies have identified hearing or pain as risk factors for mortality. This study is innovative because it identifies specific components of HRQL that predict mortality. Further research is needed to understand better the mechanisms through which deficits in hearing and pain affect mortality risks. Ó',\n       'Our study shows microstructural alterations in the Crus I of the cerebellum as well as in the pars orbitalis of the prefrontal cortex in patients with MWoA. These findings were restricted to MWoA and absent in MWA confirming partially our hypothesis. We applied a biparametric approach at high-field MRI in order to study the microstructural integrity (T1) and the myelin content (MTR) of the cerebellum and of connected areas of the frontal lobe. The MTR was reduced in the right Crus I of the cerebellum of MWoA vs. MWA patients as well as healthy subjects. Crus I is a posterolateral cerebellar lobe involved in language as well as spatial transformation and working memory; for review, see [36] . It is also implicated in emotion, with responses to unpleasant images [37] , fear, and anger [38] as well as in pain processing to noxious heat [37] .\\nCrus I is functionally interconnected with the prefrontal cortex (PFC), especially with pars triangularis, orbitalis, opercularis, and the superior frontal gyrus [32] . Therefore, we subsequently focused our analysis on those three areas and observed bilateral abnormalities in the pars orbitalis of MWoA compared to MWA patients. These abnormalities were characterized by longer T1 and lower MTR. The same characteristics were observed when comparing MWoA vs. HC, though the difference did not reach significance due to the gender unbalance between groups.\\nThe pars orbitalis of the PFC is one of the regions showing the highest peaks of functional connectivity with Crus I [32] . It is located in the inferior frontal gyrus and functionally appears to be involved in value representation and stimulus evaluation [39, 40] as well as, to a minor extent, to processing of affective social signals [41] [42] [43] [44] . Interestingly, migraine patients (with and without aura) were shown to exhibit stronger activations in this area as a response to pain-related vs. nonpain-related negative affective adjectives [45] , suggesting an enhanced affective involvement towards pain cues.\\nLower MTR and longer T1 point at a lower content of macromolecules (myelin and/or cellular proteins) or at a micro-edema effect in these regions [35] . The presence of these phenomena may be the consequence of repeated micro-inflammatory processes due to recurrent CSD during migraine attacks [46] . Even though CSD has been essentially described in migraine aura [16, 47] , there is in fact evidence suggesting hyperexcitability of the central nervous system [48] [49] [50] and the presence of CSD-like phenomena in MWoA [17, 18] . In addition, many studies have underlined the commonalities that exist between both migraine groups and epilepsy (for review, see [51] ) and the fact that CSD suppression seems pivotal in migraine prophylaxis [52] .\\nCortical demyelination has been previously linked to increased CSD velocity and cortical excitability, most probably due to decreased myelin-dependent stabilization and buffering of extracellular ion content [53] . Moreover, a decreased expression of ion channels in the cellular membrane, as it has been shown in migraine like basilar-type [54] and familiar hemiplegic migraine type 1-3 [55] [56] [57] , might also lead to the same phenomena. Therefore, lower macromolecular content in Crus I and pars orbitalis of MWoA might be at the origin of silent cortical spreading depression waves in migraineurs without aura, as previously suggested by Vincent et al. [5] .\\nOn the other hand, we did not find any significant correlation between MTR/T1 abnormalities in Crus I/pars orbitalis of MWoA patients and migraine frequency and duration. This may be due to the limited number of subjects studied but could also suggest that the observed microstructural alterations are a condition promoting migraine attacks.\\nNo alterations of the frontocerebellar circuitry were observed in MWA patients, in contrast with behavioral studies showing that both MWA and MWoA seem to have cerebellar dysfunction. Several hypotheses can be put forward to explain these findings: (1) CSD-related circulatory changes may predominate in MWA, leading to microlesions in the posterior fossa [58] , though we did not observe any difference in the lesion load in the present study. (2) The possibility that the group of MWA patients studied here were not presenting cerebellar signs: one limitation of the present study is the lack of stabilometric or other cerebellar function measures in MWA and MWoA patients, and further studies should address this question. (3) The presence of alterations in indirect circuits comprising pars orbitalis and Crus I (e.g., the nonmotor basal ganglia loop [59, 60] or the anterior cingulate gating loop [39] ).\\nIn summary, our work provides evidence of microstructural alterations in the cerebellum-prefrontal circuit in MWoA patients, which could promote increased excitability of the prefrontal cortex and cerebellum, leading to \"silent CSD\" [5] . Future electrophysiological and functional studies should help to determine the implication of the reported structural abnormalities in migraineurs without aura.',\n       'To analyze the structure of the Medicare Part B clinical laboratory market, we used data from CMS, the American Hospital Association (AHA), and the United States Department of Agriculture (USDA). Our primary data were the Medicare claims and enrollment files for a 5 percent national random sample of Medicare FFS beneficiaries in calendar year (CY) 2006. These data contain basic demographic and geographic information about beneficiaries and Medicare-paid services they received in CY 2006. End Stage Renal Disease (ESRD), Disabled, Dual-Eligible (Medicare/Medicaid), and Aged entitlement status are derived from information in the Medicare Denominator file. Indicators for beneficiaries who are institutionalized or receiving home health services were created using the Nursing Home Minimum Data Set and the home health claims files, respectively. We restricted our analysis to Healthcare Common Procedure Coding System (HCPCS) test codes included in the 2006 Medicare Part B CLFS. We differentiate between laboratory test codes (HCPCS codes) and laboratory tests, because often a single laboratory test code is used to bill for multiple laboratory tests. In other situations, one laboratory test is billed using multiple test codes. For these tests, when billing Medicare, the laboratory will often \"stack\" the laboratory test codes that comprise the laboratory test (Carlson, 2010) .\\nKnowledge of automated chemistry test panels is important to understanding the Medicare clinical laboratory payment system. There are 22 automated chemistry test codes (e.g., HCPCS 82310, Assay of Calcium) which are combined into automated chemistry test panels (CMS, 2011) . The combination of any two or more of these test codes is referred to as an automated test panel (ATP). Payment for each of these test codes depends on the number Kandilov, A.M.G., Pope, G.C., Kautter, J., Healy, D.',\n       'Placing traps along primary roads was the most influential variable we examined that affected capture success of island foxes. Although the parameter estimate for ROAD was not statistically significant, the incidence-rate ratio indicated the directionality of the effect was unlikely to be from random chance alone, and therefore suggested biological significance. Plausible explanations include: foxes used areas near roads more than surrounding areas (e.g., higher density of foxes near primary roads), traps were more easily detected near roads, or both of these. Our placement of traps near roads was specifically designed to attract foxes from roads (i.e., trap placed upwind from road with door facing road); therefore, we cannot discern whether capture rates were higher because more foxes were using the roads, or because traps were more easily detected there. Foxes on SCI were occasionally observed foraging, traveling, and scent-marking along roads (Gould 2010) ; thus, we suspect foxes encountered traps more frequently near roads than at random locations where foxes may not have traveled as often.\\nRoads may represent desirable habitat features for island foxes, because roads create edges that many generalist mammalian predators prefer (Heske et al. 1999 , Dijak and Thompson 2000 , Svobodová et al. 2011 . Swift foxes (Vulpes velox) used roadsides for denning (Harrison and WhitakerHoagland 2003) and San Joaquin kit foxes (V. macrotis mutica) possibly used roads to obtain food (e.g., road-kill; Cypher et al. 2009 ). Red foxes (V. vulpes; Macdonald 1985) and Iberian wolves (Canis lupus signatus; Macdonald 1985 , Barja et al. 2005 placed their scats on roadways so that they may be visible and easily detected by conspecifics. Similarly, gray wolves (C. lupus) in Alberta, Canada used roads for travel lanes (Whittington et al. 2005) . Snow et al. (2012) found that foxes on SCI used areas <100 m and >100 m from roads in similar proportions, but the scale of that study was too large to infer frequency of use <10 m from roads, as was the scale in this study. Coinciding with the nightly setting of our traps, foxes moved across roads more at night, which also coincided with periods of higher fox-activity and lower volumes of traffic (Snow et al. 2012) . Determining how island foxes use roads appears to be an important line of future research on SCI. Variation in population density of foxes on SCI, with regard to roads, is not well-understood. Population monitoring during 2009 estimated higher densities near primary and secondary roads, but not during 2007 , 2008 reports; N. C. Gregory, Institute for Wildlife Studies, unpublished report). We found 67% capture success for all traps set 100 m from primary and secondary roads, 46% success for all traps set >100 m from roads, and 30% success at all traps set >500 m from roads, suggesting that considerable numbers of foxes likely inhabit most areas. Regardless, it is clear that roads play substantial roles in the population dynamics of foxes on SCI. Road-kills are the leading cause of mortality, effectively reducing the survival rate of foxes living near roads (Snow et al. 2012) . Our findings support recent evidence that roads are a focal feature on the island landscape affecting the population of foxes on SCI.\\nAlthough TPI indicated relative importance for influencing captures, canyons and arroyos showed little evidence of increasing capture success. No other variables tested influenced the success of capturing foxes, yet our capture rates were quite high for a carnivore species. Foxes that were not previously exposed to traps (<2 years old) did not appear to bias our results, because similar proportions of foxes <2 years old were also captured in a later study when trapping efforts had occurred for 5 previous years (N. C. Gregory, Institute for Wildlife Studies, unpublished report). The drought conditions on SCI during this study may have increased our capture success, because available food resources were limited. Additionally, our placement of dry cat food inside traps may have enticed foxes for supplemental feeding. We did not attempt to capture foxes without food inside traps because of concerns for the welfare of captured foxes, and therefore we do not know whether food influenced captures. The attractants we tested did not appear to affect the age and sex classes of island foxes that we captured, suggesting that the attractiveness of food was modified by each attractant in a similar manner. Similarly, gray foxes on the mainland showed no preference to specific attractants (Steelman et al. 2000) , although this was not the case for all wildlife (Andelt and Woolley 1996) .\\nAnother reason explaining our high capture success could be related to island foxes evolving in isolation, and therefore exhibiting ecologically naïve behavior. In the absence of predators, island foxes developed reduced avoidance behaviors (Swarts et al. 2009 ) similar to other species (e.g., Griffin et al. 2000) . Island foxes reportedly lack fear of humans (Laughrin 1977, Moore and Collins 1995) , are naïve of novel predators (Roemer et al. 2001 , Swarts et al. 2009 ), and show low avoidance toward approaching vehicles (Gould 2010) . We expect this phenotypic trait may extend to decreased wariness of traps, and may have resulted in higher capture success when compared with non-naïve species.\\nThe timing of our trapping events was not evenly spaced throughout the study. The majority of traps placed along roads were operated approximately 1 month after the majority of traps placed at random locations throughout the study area, thus, potentially confounding our data. We also did not trap enough during both wet and dry seasons, or during each biological season to examine for seasonal effects. We expect that more evenly spaced trapping events among biological seasons could better identify whether capture success varies throughout the year. ',\n       'Multiple subclinical and clinical conditions seem to underlie both the physical and cognitive age-related declines. For example, depression has been related to hippocampal atrophy and subsequent mild cognitive impairment (MCi) (38) as well Table 1 Currently adopted definitions of the different dementia/ad stages',\n       'Not all name/address/phone records contained complete information, and some records included duplicate information.',\n       'Osceola County is located in central Florida ( fig. 1 ) and encompasses 1,350 square miles (Purdum, 1994) . The northcentral and western parts of the county, near Kissimmee, are highly urbanized and adjacent to the Walt Disney World theme park complex ( fig. 1 ). The population of Osceola County showed very little growth until the late 1970s when the theme park opened; the population has grown rapidly since the 1980s (fig. 2 ). The population of Osceola County increased more than tenfold from 26,000 in 1970 26,000 in , to 50,000 in 1980 26,000 in (Dietrich, 1978 , to 174,000 in 2000, and 270,700 in 2010 (U.S. Census Bureau, 2012). Most of this population and urban growth has occurred in the north-central and western parts of the county. The county population is projected to reach 450,000 by the year 2030 (Smith and Rayer, 2012) .\\nThe north-central part of the county, including the city of St. Cloud, Lake Tohopekaliga, and East Lake Tohopekaliga ( fig. 1) , is a mix of urban, rural, and agricultural land use, whereas the southern and eastern parts of the county, including Holopaw and Yeehaw Junction, are almost entirely rural with large areas of crop and cattle lands. According to the 2012 Census of Agriculture, Osceola County accounted for more pasture land than any other county in Florida, with more than 216,000 acres (U.S.',\n       'Fasted plasma metabolomics biomarkers were examined seeking evidence of systemic pharmacological effects of T3D-959 to confirm the expected pharmacology of a PPAR delta agonist. It would be expected that systemic and central pharmacology would be similar, although no direct measure of central pharmacology such as CSF metabolomics were made. Over 800 chemically defined metabolites were examined for each dose group and a ratio before and after drug treatment (EOT/BL) was calculated with an associated p value. In general, the 30 and 90 mg T3D-959 dose groups had the largest impact on the metabolomics profile, each with 120 metabolite changes with p < 0.05, while the 3 and 10 mg groups had smaller effects (40 and 61 metabolite changes with p < 0.05, respectively). Metabolites were split into 60 families. Several of these families showed consistent and significant changes with T3D-959 treatment. All three branched chain amino acids (BCAA), Leu, Ile, and Val were significantly decreased (p < 0.05) in the 90 mg T3D-959 group. BCAAs are positively correlated with insulin resistance and diabetes [24, 25] . Supporting this observation, several key products of BCAA catabolism in the form of acyl carnitines are similarly decreased in treatment in the 90 mg group ( Table 5 ). Some of these metabolites, such as isovaleryl and \\nisobutyryl carnitine are part of a principal component shown to be positively associated with insulin resistance [24, 25] . A limited number of ceramides (8) and N-acyl sphingosines (2) were included in the exploratory metabolomic analysis. T3D-959 decreased the levels of several ceramides, including N-palmitoyl and N-stearoyl sphingosine at the higher doses, as shown in Table 6 . Ceramides are postulated to be mediators of insulin resistance and metabolic disease. Recent reports suggest strong association of specific ceramide species (e.g., C16:0, or Npalmitoyl-sphingosine) with metabolic diseases [26] .\\nThe overall profile from the metabolomic data suggests that higher doses of T3D-959 increased fatty acid oxidation. Thirty-three (33) acyl carnitine species were measured in this metabolomic analysis. The two highest doses of T3D-959 increased a wide array of these fatty acid-derived acylcarnitine species, ranging from the end-product C2 (acetyl) Numbers are ratios of the dose group averages end of treatment (EOT) to baseline (BL). Green highlighted numbers indicate a statistically significant (p < 0.05) decrease in metabolite. The light green highlighted number indicates the p value is between 0.05 and 0.1. Statistical comparisons between doses and visits were conducted using Two-Way Repeated Measure ANOVA. carnitine through the even-chain medium (C4 to C12) and long chain (C14-C22) species (Table 7) . This profile is consistent with increased flux of fatty acids into the beta-oxidation pathway [27] .\\nConsistent with a profile of decreased insulin resistance, the ratio of glycine levels (EOT/BL) was increased to 1.26 (p < 0.05) in the highest dose group. Glycine is negatively correlated to insulin resistance Table 6 T3D-959 elicited changes in ceramides Numbers are ratios of the dose group averages end of treatment (EOT) to baseline (BL). Green highlighted numbers indicate a statistically significant (p < 0.05) decrease in metabolite. Statistical comparisons between doses and visits were conducted using Two-Way Repeated Measure ANOVA. Table 7 T3D-959 increases levels of acyl carnitines\\nThe red highlighted entries in Table 7 indicate a statistically significant (p < 0.05) increase in the ratio (EOT/BL) for a metabolite, while pink indicates a p value between 0.05 and 0.1. and diabetes [24] . Another interesting observation from the data was a significant increase in six of eleven measured plasmalogens (data not shown). Decreases in plasmalogens have been postulated to correlate to AD [28] .',\n       'The PSA wild type cDNA was cloned into the pcDNA3.1 vector using the BamHI and XbaI restriction sites. The DNA sequence representing the PSA pro domain (APLILSR) was subjected to site-directed mutagenesis to convert it to a sequence derived from the bacterial proaerolysin gene (KVRRAR) recognized and cleaved by the ubiquitous furin family of peptide convertases. With use of the Gene Editor (Promega, Madison, WI) product and directions we subcloned the wild-type PSA cDNA into the pGEM-11Zf(+) vector (Promega, Madison, WI). Mutagenic oligonucleotides for the PSA pro domain facilitated new strand synthesis. The incorporation of the mutation into the PSA prodomain was confirmed by sequencing, and the gene was subcloned back into the pcDNA3.1 vector. Both the wild type (WT) and mutant (FR, furin-recognized) PSA variant constructs were used to transfect the DU145 human prostate cancer cell line which was then subjected to selective pressure in 500µg/mL of zeocin (Invitrogen, Carlsbad, CA). Clonal outgrowths were tested for their production of PSA before subsequent characterization.',\n       'To develop a better understanding of the difference in worker injury risk between the two production systems, the greatest contributing factors of each production system were determined. This consisted of a separate Monte Carlo simulation for each of the operations (establishment, management, and harvest) in each production system. Worker injury risk was calculated for each operation and summed over the ten-year life cycle. The simulation for each individual operation of each production system was run for 500,000 iterations. This resulted in establishment, management, and harvest worker injury risk distributions that were overlaid with one another to visualize the differences in worker injury risk between operations of a production system.\\nThe worker injury risks for establishment, management, and harvest in each production system were determined over a ten-year life cycle and are summarized in table 5. The summary provides an overview of the mean, median, and mean life cycle injuries per 100,000 workers for each of the probability distributions of worker injury risk for each operation. The mean and median for each operation show little difference, indicating that these distributions are approximately normal. The mean values also show the mean life cycle injuries that occur in each operation for each production system to provide a comparison value between operations and production systems. The mean life cycle injuries per 100,000 workers show that harvest has the greatest mean life cycle injury rate in both production systems, making it the greatest contributing factor to worker injury risk in each production system. Furthermore, harvest is also a greater contributing factor in corn when compared to biofuel switchgrass, making corn harvest the greatest contributing factor in this risk assessment. Figures 3 and 4 show worker injury risk graphically for each operation in each production system over a ten-year life cycle. Figure 3 shows the worker injury risk distributions for corn production, and figure 4 shows the worker injury risk distributions for biofuel switchgrass. ',\n       \"Approximately 99.3 million cattle (Bos taurus), 7.6 million sheep (Ovis aries) and goats (Capra hircus), 8.4 billion poultry (Gallus gallus), and 59.9 million swine (Sus scrofa) are produced annually in the United States, with a total value exceeding $98.8 billion (USDA Census of Agriculture, 1997) . Manure generated from these industries is estimated to exceed 834,000 Mg of dry matter per day. With nitrogen (N) concentrations ranging from approximately 15 -55 kg N Mg 21 dry manure (Griffin et al., 2003) , this means approximately 12,500 -45,900 Mg of manure-derived N is produced each day. Concurrently, the land base available for manure application continues to decrease due to urban sprawl and other factors. Management practices must be developed that optimize recycling of manure-derived N to crops, while minimizing adverse environmental consequences of manure application to cropland. Development of these practices is hindered by inabilities to quantifiably predict the impacts of soil properties, manure composition, and climate on manure N availability.\\nA team of United States Department of Agriculture, Agricultural Research Service (USDA-ARS) scientists has been organized to address these issues in a nationally coordinated research project. The study represents increased emphasis by USDA-ARS for national research collaborations to solve problems of national/global importance, and is, therefore, conducted within the USDA-ARS Manure and Byproduct Utilization National Program (http://www.ars.usda.gov/research/programs/htm). The objective was to develop, validate, and employ predictive relationships that quantify the impacts of key soil, environmental, and soil X environmental factors on manure N mineralization (Nmin). Accurate predictions of manure N transformation dynamics would promote efficient, environmentally sustainable use of this resource in crop production. The research team's overall approach is to characterize selected agricultural soils and manures from each participant's location; conduct laboratory incubation studies across a range of soils, manures, and controlled climatic parameters; compile all laboratory data to develop generalized predictions of manure Nmin; and to validate Nmin predictions under field conditions across manures, soils, and environments.\\nIn conducting complementary studies across multiple laboratories, explicit planning is paramount to a project's success. Not only does information available in the literature need to be incorporated into the study design, but detailed procedures and examples should be provided to each participant because individual interpretations of common words or phrases can vary. The details developed by scientists conducting this project are provided to 1) document the procedures used and 2) offer others a protocol for conducting research on nutrient transformation processes involving collaboration across locations or complementary research between field and laboratory environments. Recognizing different objectives will require protocol alteration, these procedures are offered as initial guidelines amenable to appropriate modification for other studies.\",\n       'The remote sensing data used in our study include Landsat imagery, NAIP aerial imagery, C-CAP land cover maps, and a Lidar DEM. Landsat5 TM data provided nearly continuous coverage of the earth surface from 1984 to 2011 at a spatial resolution of 30 m. Landsat7 ETM+ SLC-on data (1999)(2000)(2001)(2002)(2003), with similar spectral distribution and same spatial resolution, is an efficient way to enhance the imagery availability. The images from 1984 to 2011 for a Landsat scene centered over the VCR (Path: 014, Row: 034) were acquired from the United States Geological Survey (USGS) Earth Explorer and were used for the construction of monthly NDVI time-series (MNTS). NOAA Coastal Change Analysis Program (C-CAP) maps cover intertidal areas, wetlands, and adjacent uplands. These maps include 25 land use and vegetation classes with the spatial resolution of 30 m and update every five years starting in 1992. In total, 5 C-CAP land cover maps were obtained from NOAA office for coastal management, for the years 1992, 1996, 2001, 2006, and 2010. In the C-CAP classification system, the whole salt marsh area of the VCR was categorized into a unique class labeled Estuarine Emergent Wetland. Thus, the maximum extent of this class from each period was used to delimit the salt marsh region of our study (Fig. 1b). The National Agriculture Imagery Program (NAIP) acquires aerial imagery at a resolution of 1 m for the United States during the agricultural growing season. A total of 6 NAIP county mosaic images were collected from United States Department of Agriculture (USDA) Geospatial Data Gateway for the years 2004, and 2011). These images were used as the reference data for accuracy assessment of classification maps during 2002-2011 by labeling the corresponding salt marsh vegetation community with random points (Fig. 1c), generated with the assistance of ArcGIS software (Foody 2002;Theobald et al. 2007). A LiDAR DEM of the VCR was downloaded from Virginia Coast Reserve Long Term Ecological Research (VITA 2011). The LiDAR DEM with a cell resolution of 3.048 m, was created from LiDAR points (~ 1 m spacing) acquired in March 25-30, 2010. The horizontal and vertical datum are NAD83 and NAVD88, and the vertical accuracy was validated at less than 0.15 m. A water mask file was also attached to control for tidal regime during data collection (Fig. 1d). In our study, a coordinate transformation and a spatial resampling were first applied to the LiDAR DEM to match the datum (WGS84 UTM 18N) and spatial resolution (30 m) of the Landsat images. Then, areas without tidal inundation were used to ascertain the elevation for each salt marsh vegetation community.',\n       '1. Systematic review: While economic and decision models of AD have advanced in recent years, there remains a need for a comprehensive model that covers the full course of the disease, particularly the earliest stages of decline.\\n2. Interpretation: The model described here was developed to reflect progression from normal cognition to advanced AD across multiple pathophysiological and clinical facets of the disease. This permits modeling of treatments ranging from very early disease modification to symptomatic treatments in advanced disease.\\n3. Future directions: Research in the progression and economic impacts is a large and active field and the model will need to be tested and updated to incorporate new findings from that research. In particular, the model presented here relies heavily on extrapolation of natural history data.',\n       \"This paper uses data from the National Science Foundation's surveys on business research and development (R&D) expenditures that have been linked with data from the Census Bureau's Longitudinal Business Database to produce consistent NAICS-based R&D time-series data based on the main product produced by the firm for 1976 to 2008. The results show that R&D spending has shifted away from domestic manufacturing industries in recent years. This is due in part to a shift in U.S. payrolls away from manufacturing establishments for R&D-performing firms. These findings support the notion of an increasingly fragmented production system for R&D-intensive manufacturing firms, whereby U.S. firms control output and provide intellectual property inputs in the form of R&D, but production takes place outside of the firms' U.S. establishments.\\nKeywords: Business R&D, industry classification, factoryless goods producers, U.S. manufacturing firms, establishments 1 The authors wish to thank David Byrne, Ian Mead, Guci Ledia, and John Jankowski for valuable comments. We also wish to thank Cheryl Grim for sharing the codes that were used to match firms to establishments. Corresponding author: Christian.awuku-budu@bea.gov.\\n2\\nChristian Awuku-Budu and Carol A. Robbins Analysis and Special Studies Branch, Regional Product Division, BEA August 26, 2014\\nThe National Science Foundation (NSF)'s annual surveys on business research and development (R&D) expenditures are the primary federal sources for industry R&D expenditures in the United States. The business R&D data sets provide an important window into the growth dynamics of industries that fund and perform R&D. In addition, the information in the NSF-tabulated data sets is used extensively by government agencies and private enterprises to evaluate the impacts of R&D expenditures on economic activity. 2 Despite the relevance and usefulness of these annual R&D expenditure data sets, these data as collected have been affected by changes in how firms have been classified into industries over the years.\\nThis paper uses data from the NSF's surveys on business R&D expenditures that have been linked with data from the Census Bureau's Longitudinal Business Database to produce data series with a consistent set of industry classification standards based on the main product produced by the firm for 1976 to 2008. The adjustment to industry classifications is done on both a broad and more targeted basis to analyze how much focus needs to be placed on adjustments to firms initially classified into the management of companies, scientific R&D services, and wholesale trade industries.\\nWe then evaluate our estimates by comparing them to estimates of R&D expenditures obtained using the line of business information reported in the 2008 Business Research and Development and Innovation Survey (BRDIS) data. Our estimates are comparable to the BRDIS tabulation for computer and electronic product manufacturing. Relative to BRDIS for 2008, we somewhat underestimate R&D expenditures for chemical manufacturing and overestimate R&D expenditures for other manufacturing industries. Our main contributions to the literature are a consistent methodology and NAICS-based business-performed R&D data for 1976 to 2008.\\nOur results support earlier work done at BEA during the development of its R&D satellite accounts to address changes in how firms have been classified into industries in the NSF data. This paper differs from this previous work by using micro data and the same classification technique over the entire period rather than adjusting more aggregate data with special tabulations from the NSF and the U.S. Census Bureau.\\n2 For example, the Bureau of Economic Analysis (BEA) incorporated R&D expenditures as investment in the National Income and Products Accounts (NIPAs) in July of 2013, which increased the measured level of GDP back to 1929. The NSF R&D expenditures are the main source of data for the change in BEA's measures of industry R&D investment. 3 These adjustments to the classification methodology rose from efforts to meet the needs of data users as the structure of U.S. economic activity shifted and the Federal Statistical System switched from the Standard Industrial Classification (SIC) to the North American Industrial Classification (NAICS).\\nThe results of our paper show that R&D spending has more recently shifted away from domestic manufacturing industries. This is due in part to a shift in U.S. payrolls away from manufacturing establishments for R&D-performing firms, and also a shift upward in the proportion of firm-level payrolls for R&D-performing firms that are engaged in trade and other services activities. These findings support the notion of an increasingly fragmented production system for R&D-intensive manufacturing firms, whereby U.S. firms control output and provide intellectual property inputs in the form of R&D, but production takes place outside of the firms' U.S. establishments.\",\n       'In calculating the net returns to a bachelor\\'s degree, we take into account the present value of the stream of earnings that accrue to graduates earning bachelor\\'s degrees 11 minus the direct costs to students earning the degree (e.g., tuition, books, room and board), the \"opportunity costs\" of earning the degree represented by the wages the student could have earned if he or she had not attended college, and the federal and state income taxes students paid. We measure earnings and opportunity costs at one point in time and over the work life of the graduate by calculating what economists call the net present value (NPV) of the 10 Appendix Table 2 lists some representative schools in each of these categories. The full list of schools in any category or categories we use is available upon request.\\n11 Here, as elsewhere, we use the IPEDS graduation rates at four, five, and six years to create a weighted average of how long it takes students to graduate from each campus represented in our sample.',\n       nan,\n       'Due to close historical, cultural, and linguistic ties, the Nordic region (Denmark, Finland, Iceland, Norway and Sweden) has traditionally been an important arena for cooperation within education and research. In addition to Norway, Finland, Denmark, and Iceland are also small nation states. In recent years there has been a growing concern about the need for extended Nordic collaboration on education, research and innovation, in order to reach the critical mass necessary to fulfil national goals of educational and research excellence in relation to EU policy and the increasing global trade in higher education. Nordic cooperation in higher education seems to be rapidly changing its agenda and partly also its rationale for cooperation. Some of the main developments taking place suggest that the traditional academic and cultural motives are being supplemented by economic and more market-based motives. The new policy vista has triggered the establishment of a Nordic Innovation Centre. NordForsk, also operating under the Nordic Council of Ministers for Education and Research was established in January 2005. NordForsk sponsors Nordic Centres of Excellence, research programmes, networks, researcher training schools, particularly where Nordic collaboration are assumed to produce added value, of which current examples are Molecular medicine, Food Nutrition, Health and Welfare. Central players of NordForsk are the national research councils. NordForsk also cooperates with the Nordic Innovation Centre as both organisations work for the positioning of the Nordic research and innovation area. In higher education, several initiatives have also been taken. As is the case for the Nordic countries, the Baltic States are also nations of relatively small populations, and are therefore believed to benefit from collaboration on education and research. The Baltic countries as well as North West Russia have had access to Nordic collaboration and funding through the Nordplus Neighbour programme. The Nordic Council of Ministers has also suggested launching \"Nordic joint degrees\" in areas where the Nordic region has specific and high-level expertise (Stensaker and Danø 2006). For more than twenty years, the Nordic Council has supported research and teaching within the academic field of Nordic language and literature. Whether Nordic collaboration should be based solely on the Nordic/native languages or also on English has been highly disputed among various actors. Undoubtedly there is, in the Norwegian context, competition between different discourses on how to internationalize education and research, with respect to the languages used rationale for cooperation, but also with regard to region, for instance to what extent it should be directed towards aid and solidarity with more underdeveloped regions (Frølich and Stensaker 2005).',\n       'Elaine K. Lam',\n       'In addition, four statistical policy directives provide important guidelines for ERS and the other Principal Statistical Agencies. Statistical Policy Directive #1 in particular has implications for the structure as well as the functions of ERS. This directive, which is a codification of the National Academy of Sciences\\' Principles and Practices for a Federal Statistical Agency , affirms the 5 fundamental responsibilities of federal statistical agencies and recognized statistical units in the design, collection, processing, editing, compilation, analysis, release and dissemination of statistical information. It emphasizes the need for cabinet departments to facilitate the ability of statistical agencies to meet their responsibilities. Directive #1 is explicit in recognizing the need for public trust of federal statistics and the autonomy necessary for a statistical agency to be trusted. It states: \"Federal statistical agencies and recognized statistical units must be able to conduct statistical activities autonomously when determining what information to collect and process, the physical security and information systems security employed to protect confidential data, which methods to apply in their estimation procedures and data analysis, when and how to store and disseminate their statistical products, and which staff to select to join their agencies. In order to maintain credibility with data providers and users as well as the public, Federal statistical agencies and recognized statistical units must seek to avoid even the appearance that agency design, collection, processing, editing, compilation, storage, analysis, release, and dissemination processes may be manipulated.\" Having USDA\\'s Research, Education and Economics mission area provide the organizational home for ERS helps ensure the objectivity of agency research and forestall inappropriate influence on research findings that might run counter to program agency preferences.',\n       'Four months after the photo stories were created, the group reconvened to participate as co-researchers in analysis. Each participant\\'s photo story, consisting of three to five photos, was viewed via PowerPoint, and the participant was invited to talk about his or her story in terms of what messages it portrayed about healthy relationships, as well as any master narratives it countered.\\nOther group members were then invited, with the photographer\\'s permission, to share what meanings they connected with the photo story.\\nBell\\'s (2010) model, with its theoretical underpinnings in CRT, engages people in creative storytelling for social justice within four types of narratives. The themes that emerged from the Wind River UNITY Photovoice for Healthy Relationships project naturally fit into this schema due to the group process and the CRT focus on master and counter narratives, as follows: 2) Concealed narratives: The Photovoice participants expressed that they rely on storytelling from their elders and multicultural educational opportunities to learn about AI and Tribal history that has been hidden by stock narratives. Knowledge of these stories strengthens their cultural identity and self-esteem. They expressed that the more they learn, the more they are determined to survive.\\nOne participant expressed, People think we are incapable. That we are incapable of taking care of ourselves and each other. They think on the rez we are surrounded by negative stuff. They don\\'t even know how the government played a role in the alcoholism of our people. But we are keeping a strong spirit. A lot of us young people live a drug-and alcohol-free life because we know how those things have hurt our people over the years. It has been another way to try to do away with us. But we are still here! We keep a strong spirit and we take care of each other. The powwow helps us come together and celebrate.\\nWe are still here. We keep a strong spirit and we tell each other, \"You can do it.\"\\n(UNITY Photovoice participant, personal communication, December 30, 2010) And with regard to teen parenting, another expressed,\\nWe do have teen pregnancy among our youth. But this isn\\'t about that. To me, this shows something else about healthy relationships in our culture. This shows responsibility, priorities, family. Our key is to honor the children and protect them.\\nEven at a young age, the decision of having a child is sacred and a child is a sacred gift. Her child will be in her heart always. This shows the importance of supporting her as she is a mother to this sacred child. They are our responsibility too. We all have to support her and be her family. Even if she is so young--she needs our respect even more for the decision she has made to be the most important person, a mother.\\nWe have respect for our moms. 3) Stories of resistance: Through the Photovoice project, the co-researchers expressed that their heroes and sheroes support them in building and maintaining healthy relationships. Historical chiefs, sub-chiefs, current Tribal leaders, youth council advisors, ancestors, parents, elders, siblings, and peers are all a part of one connected family upon whom they rely for strength and wisdom for engaging in healthy relationships. Drumming, singing, and dancing at powwows; listening to the stories of their elders and Tribal leaders; learning their native language; and sharing this knowledge with others are all ways to perpetuate stories of resistance. For example, one participant stated:\\nThere\\'s a wealth of knowledge from our elders and chiefs. We carry their spirits within and we honor their struggles and their pride. A way to do this is through teaching our children--they are our first priority. We can all carry the legacy by sharing their wisdom with our children. Their pride, their wisdom--hold it in your heart--we are descendents of Chief Washakie-and his leadership is always within us. It\\'s not about bragging rights to say we\\'re Indians--it is about honor and pride and carrying their And that circle at the powwow is also very important and has meaning about our youth. The ones who are in the circle--drumming, singing, dancing--they are in a safe place with a community around them. They are sacred. We want all of our children to be in that circle. The farther you are from that circle, the more you are subject to those stereotypes of drugs and alcohol and other risks. We want to bring our children and our youth into that circle. So UNITY and powwows are a way to bring them back into the circle. We are setting an example and when people see that, and they feel it, they also want to be in that circle, be a part of the family. Lots of us young people are trying to be fluent speakers of our language. Whenever you speak your own language it brings pride. I want to be able to say I\\'m a fluent speaker. In boarding school days we died if we spoke our language. We are still here.\\nWe are still here! And there\\'s the importance of knowing who we are--we learn our language, follow dances, learn songs, follow prayers. When you pray in your own language it makes our whole world brighter. If we could all be free to pray in our own languages, it could be so powerful and peaceful. Fremont County, including parents, grandparents, Tribal leaders, college counselors, teachers, youth, and community mental health providers attended this event, which entailed an art exhibition featuring the Photovoice project. A community dialogue followed the art reception, during which audience members discussed their reactions to the display and, in turn, discussed community strategies for next steps in addressing HIV, STIs, unintended pregnancy, and related youth risks.\\nStrategies developed by the community and the Photovoice co-researchers involved identifying additional community events and group meetings at which to present the project, as well as clarifying a key take-away message of the project. The overall message they identified is that it is vitally important to integrate AI values and historical and contemporary cultural wisdom in a culturally congruent manner when providing sexuality education to young AI populations. Indeed, the Photovoice project and its analysis highlight that AI youth draw heavily on their cultural heritage for wisdom and support in building and sustaining healthy relationships. Having strong adult AI role models who promote this wisdom, in turn, has empowered the Wind River UNITY Photovoice co-researchers to become positive role models for their younger peers. • Provide opportunities for nonverbal creative expression: Photovoice fits well within AI cultures, as storytelling is a powerful key to cultural survival. The intergenerational stories that are passed on \"are not fairy tales or entertaining stories for children-they are lived values that form the basis for Indigenous governance and regeneration. Experiential knowledge and living histories…comprise part of the core teachings that Indigenous families transmit to future generations\" (Corntassel, Chaw-win-is, & T\\'lakwadzi, 2009, p. 137) . The visual stories that are told through Photovoice are also a strong fit for facilitating communication among AI youth, caregivers, and elders, as they allow stories to be told without verbally speaking the message. For disenfranchised populations whose voices and language have historically been oppressed and denied, Photovoice can send a powerful visual message.\\n• Integrate cultural heritage into the fabric of the work: Reactions to the Wind River UNITY Photovoice stories tend to be powerful and emotional. An unintended emergent theme of the Wind River UNITY project is that of hope for healing from historical trauma and grief. Community members shared that the display brings them hope and a sense of pride that their ancestors, cultural values, and wisdom about healthy relationships (e.g., respect, peace and harmony) live on in their young people. This finding emphasizes how reducing health disparities may be possible through culturally responsive empowerment and a focus on positive social determinants of health (e.g., cultural heritage) in health promotion and prevention activities. A key pathway to healing, and, potentially, improved health and educational outcomes is the message that the wisdom and strength of AI cultures lives on in the young people. The co-researchers believe this is an important message to maintain within themselves for empowerment and to share with others with whom they have relationships, as well as with community members, educators, and policy makers, through all layers of the socioecological model.\\n• Continue to learn. Future research is needed to determine the efficacy of prevention and education efforts that are based on Photovoice projects, especially in the realms of reducing rates of HIV, STIs, and unintended pregnancy, as well as disparities in health and educational outcomes of marginalized populations. Likewise, more research is needed to learn about the effectiveness of Photovoice projects in bringing attention to social determinants of health, countering master narratives, increasing cultural competency in education and prevention of HIV/STIs and unintended pregnancy, and, in turn, reducing health disparities. Finally, research is needed to examine the benefits of Photovoice projects with additional populations and additional social determinants of health. In the late summer of 2011, the Wind River UNITY Photovoice for Healthy Relationships project will be presented to community leaders of the Wind River Indian Reservation and Fremont County. An art reception will be held in which the photovoice artists will each present their stories, and, in turn, a community dialogue will be facilitated to develop community strategies for next steps in addressing HIV, STIs, unintended pregnancy, and related youth risks.\\nGrand entry on a Friday night. It only means one thing.\\nPow-wow season is here.\\nThe smell of frybread and smoked buckskin in the air.\\nDancers from different tribes come together to celebrate a time of welcoming.',\n       'Row 1: basin-wide and U.S. landfalling TCs for (a) the MPI model, (b) the CCSM4 model, (c) the IPSL model, (d) the MIROC model. Row 2: basin-wide and U.S. landfalling hurricanes for (e) the MPI model, (f) the CCSM4 model, (g) the IPSL model, and (h) the MIROC model. Row 3: basin-wide and U.S. landfalling major hurricanes for (i) the MPI model, (j) the CCSM4 model, (k) the IPSL model, and (l) the MIROC model. Basin-wide time series are shown by dark lines; landfalling time series are shown by light lines. . . . . . . . . . . . . .',\n       'Discrete samples for carbonate system measurements were collected at selected stations along the cruise track using 12 L Niskin bottles mounted on the CTD rosette. Total alkalinity was measured using potentiometric gran titration [Brewer et al., 1986] , calibrated against certified reference material (batches 100 and 105) supplied by Doctor Andrew Dickson, Scripps Institution of Oceanography [Dickson et al., 2007] . The precision of the alkalinity measurements was 1.5 μmol kg',\n       'Vaval, L., Bowers ',\n       \"Eighth grade students came from backgrounds that were evenly divided in terms of Socio-Economic status: Quartile 1 -  Table 2). To reduce the number of variables, principal component factor analyses and examination of simple and multiple correlations were performed for evidence of redundancy and nonsignificant relationships with the dependent variables. Variables were standardized when scaling .and standard deviations were not similar before being averaged or summed to create three factor scales derived for the three classifications of 14 school violence independent variables presented in Table 3. The independent variables were coded so that the higher end of the scale indicated more, better, or higher values.   The 14 independent variables were factored to seven and then to three school violence factor scales presented in Table 3. Regression analyses using three factor scales plus students' background characteristics: sex; race/ethnicity; school type; city type; family income; socio-economic quartile (class); and percent of students receiving free or reduced lunch 46 were performed on a single variable representing the average of the two dependent variables (reading and mathematics performance scores).\",\n       \"Type 2 diabetes mellitus (T2DM) and Alzheimer's disease (AD) are both more prevalent with ageing, but it has generally been assumed that this is coincidental, not a reflection of co-morbidity. However, evidence suggests that patients with T2DM are at an increased risk of getting AD and that hyperinsulinaemia and insulin resistance -hallmarks of T2DM [1-3] -can lead to memory impairment. Animal models of T2DM have reduced insulin transport to the brain, reduced insulin uptake and reduced neuronal insulin [4] [5] [6] , consistent with reported reduced insulin levels, insulin receptor expression and insulin resistance in brains of AD patients [7] [8] [9] . Recently, Takeda et al. [10] reported studies in which they crossed two wellestablished mouse models of T2DM (viz. ob/ob and NSY mice) into an APP23 transgenic mouse background. They found that in both APP + -ob/ob and APP + -NSY mice, diabetes exacerbated cognitive dysfunction, which supports impairment in insulin signalling as a mechanistic link underlying these seemingly disparate disorders. From a clinical perspective, a link would suggest that currently available 'antidiabetic' drugs might be beneficial in treating AD patients. The present review summarizes the two disorders, the postulated common biochemical links, and the clinical trials of 'antidiabetic' drugs in AD.\",\n       nan,\n       'Several study limitations warrant mention. The Head Start sample followed here consisted of low-income families, and hence it is unknown whether the results would also generalize to a nationally-representative sample that included greater heterogeneity in family socio-economic status. Although the teacher ratings showed sufficient inter-rater reliability to support predictive validity, the level of agreement was somewhat low for ratings of socialemotional competence, suggesting that additional measure refinement may be needed. Low levels of interrater agreement in social and behavioral domains are not new problems in psychometrics, as the varying expectations of raters and differences in task demands across contexts are difficult to resolve. Further precision around operationalizing item content and response options should be a continued area of investigation in measurement, in particular for young children. The present study assessed the general predictive validity of these brief rating scales, but did not validate their use as diagnostic screeners. In the advent of prevention-oriented screening practices in school (e.g., School-Wide Positive Behavior Support, Response to Intervention and Instruction) it may be tempting to consider ways in which brief screenings can be used to identify children for possible support or intervention. Although the early identification of students who may benefit from support on non-cognitive skill domains is a worthy goal, the current study did not test for sensitivity and specificity in the identification of individual children. In other words, it remains unclear whether these scales could be used to identify individual children who need additional services in either domain of approaches to learning or social-emotional adjustment.',\n       '[10].',\n       \"Understanding and measuring school quality Following practice established in the literature, the contribution of school quality to student outcomes can be measured as the variation in student outcomes that remains after controlling for the observable characteristics of students and the school (Fuchs and Woessmann, 2004). In order to estimate school quality, so called 'value-added' models of student outcomes are used. In the context of PISA and TIMSS data, the value-added model of student outcomes is defined as: Here, the score of student i in school k in setting 12 j at time t is modelled as a function of student characteristics , observable school characteristics Γ, and indicators for specific policy settings (such as differences across schooling systems) . After controlling for these observable characteristics, what remains are idiosyncratic student level variations (driven by omitted variables that systematically influence student outcomes, such as innate ability or attitude; as well as randomness-such as falling sick on the test day) and school quality or value-added . In plain language terms, value-added models (also known as multi-level models) are used to isolate the effects of differences in school practice on student performance, while controlling for observable student and school characteristics. Comparisons of performance between schools (in PISA) and across classrooms in schools (in TIMSS) can then be made on a 'like-for-like' basis to provide an estimate of the effect of school quality. Chart 3.1 below provides a simple illustration of how school level value-added is estimated. In these charts, a variety of students with a variety of outcomes attend each school. After controlling for the starting ability of students, and assuming that each school has a similar profile of students, the vertical difference between the red and green lines can be considered as the additional value added by School A to the outcomes of its students, relative to School B. It is a measure of the additional score an average student receives simply by attending School A (due to its higher quality) instead of School B. This approach measures the variation in outcomes resulting from the different practices and management of schools (in PISA) and across classrooms (in TIMSS).\",\n       'CAREER PATHS AND INTERNATIONAL MOBILITY',\n       'Since the LSA began collecting data in 2013, men have averaged a higher number of full professor positions per department each year. Data from 2015-2016 suggests that the difference in number of female full professors per department remains fairly stable at around 2.4, but male full professors during that same time show a slight, negative trend. Figure 11 shows a breakdown of the average types of position per department by gender from 2013-2017. Overall, women make up the majority of departmental employees for each year, while simultaneously averaging a lower number of tenure track positions compared to men. ',\n       \"In the 1992-93 academic year, the National Center for Education Statistics (NCES) embarked on its first long-term longitudinal study of bachelor's degree recipients, the Baccalaureate and Beyond Longitudinal Study (B&B). Students who had completed or expected to complete a bachelor's degree between July 1992 and June 1993 were selected from a cross-section of students in all levels and sectors of postsecondary education in the 50 states, the District of Columbia, and Puerto Rico. These graduates were interviewed in 1993, then located and surveyed again in 1994and 1997. In 2003 years after they had completed a bachelor's degree, the final follow-up of this cohort took place. This report provides a descriptive summary of the activities of this cohort of college graduates at the time of their interviews in 2003. Where similar information is available for the graduates when they were last interviewed in 1997, it is provided for context. However, this report does not test for changes over time within this cohort of bachelor's degree recipients. The 2003 survey, like the previous follow-ups, asked bachelor's degree recipients about many aspects of their lives, including their education, employment, family, civic participation, and finances, as well as their attitudes and opinions about many of their experiences. The wealth of information available in the 2003 survey-not to mention the combined information of four separate waves of data-is vast, and no single report can begin to do justice to the complexity of the data. Instead, this report provides an introduction to the 2003 data, offering a snapshot of activities in five areas by the 1992-93 cohort 10 years after they had graduated: subsequent education, employment, opinions about education, family status, and civic participation. Following a description of the data and methods, the body of the report provides a general overview of the graduates' responses to selected items in these five areas, addressing the following questions: • How much education beyond a bachelor's degree had 1992-93 graduates completed by 2003? This report also contains a table compendium as a reference to the wider range of information collected in this study. The compendium is organized into sections that correspond to the major sections of the overview: Education, Employment, Opinions About Education, Family Status, and Civic Participation. In each section, selected statistically significant findings are presented at the beginning, followed by the tables for that section. The tables provide further detail about the questions discussed in the overview, as well as additional information about other outcomes in each topic area. The variables used in this report are defined in appendix A. More information about the B&B:93/03 data is available in appendix B, and the standard errors for all the estimates presented in this report are available at http://nces.ed.gov/das/library/reports.asp.\",\n       \"To demonstrate the utility of the probabilistic mapping, we applied our probabilistic localization to a subset of ADNI participants. We limited the ADNI image volumes to datasets that contained good quality reconstructions and accurate spherical registration. We examined the cortical thickness in the perirhinal cortex (defined as area 35) and entorhinal cortex (defined as area 28) in the selected ADNI dataset of normal controls, (NC, n = 215, mean age= 75.9 years± 5.5), mild cognitive impairment (MCI, n = 358, mean age = 75.0 years ± 7.1) and Alzheimer's disease (AD, n = 167, mean age= 75.5 years ± 7.7). The cortical thickness was larger for the control group in both predicted locations of the perirhinal and entorhinal cortices. The perirhinal cortex (black bars, Fig. 6 ) was slightly smaller than the entorhinal cortical in thickness (gray bars, Fig. 6 ) and with each diagnostic increment of disease (NC > MCI>AD). Thus, the cortical thickness was smaller in MCI and AD compared to normal controls (Fig. 6) . Error bars stand for standard error of the mean for each group. Perirhinal thickness in normal controls was approximately 3.15 mm and decreased with MCI diagnosis to 2.8 mm and to 2.5 mm in AD in the left hemispheres. The same pattern was observed in the right hemisphere where controls showed a cortical thickness of 3.15 mm, MCI patients showed 2.8 mm and AD patients showed 2.5 mm. The differences were highly statistically different among each diagnostic group (pb 1.0 −9 and t b 1.0\",\n       \"On the basis of the rural-urban continuum codes, 26 key informants were selected from communities representing varying degrees of rurality. The 5 national experts came from large metropolitan areas, but all had experience working with rural communities. Experts had worked for 14 to 31 years in end-of-life care. All held a master's degree, 4 also held a doctorate, and 2 were registered nurses. Three experts worked as palliative care health policy analysts, 1 worked as a palliative care improvement initiatives director, and 1 worked as a nursing home administrator.\\nThe other 16 key informants came from a variety of communities in 7 states. They worked in rural communities and held positions supervising or training NAs. Six were directors of their facility (eg, administrators, executive directors), 4 were primary supervisors (eg, director of nursing, NA coordinator), 4 worked in teaching and training NAs (eg, clinical education coordinator, staff education, and resource coordinator), and 2 held combined positions and acted as NA supervisors and education coordinators. Respondents worked in nursing homes, home health agencies, and hospice care. Four of those interviewed were employed by hospital-based facilities. The number of years respondents had worked in their current positions ranged from 1 to 23. The median number of years each had worked with NAs including prior experience was 16, but ranged from 5 to 38.\\nThe highest level of education was a master's degree (n = 2), bachelor's degree (n = 6), associate's degree (n = 4), and some college but no degree (n = 3). The highest level of educational attainment reported by the other respondent was an NA certification. Fourteen persons had received specialized on-the-job training in end-of-life care, and 8 had attended conferences and workshops on endof-life care.\\nKey informants described a variety of facility and NA characteristics. The number of persons served at the facilities ranged from 43 to 300 (employed NAs = 5-65). Two agencies were described as having mostly African American residents, 1 had about a quarter of African American and three-fourths White residents, 2 had about a quarter of Hispanic American and three-fourths White residents, while the rest had mostly White residents. Key informants reported that most NAs held high school diplomas and the majority either held or were earning certification status. The majority noted that some NAs had prior experience at other facilities. Three reported high turnover rates among NAs, 4 described rates as low compared with previously higher rates, and 9 characterized turnover rates as consistently low.\",\n       'Even with all the limitations outlined above, our study provides essential information on the geographic distribution of the HCSD sites in all the 48 contiguous states and their association with chlamydia, gonorrhea, and syphilis rates. The association between STI morbidity and HCSD sites disappeared when we adjusted for population-this suggests that the existing FQHC infrastructure may not be sufficient to provide all STD safety net services needed in some jurisdictions. The results from this study can provide useful information for local public health officials considering alternative safety net STI service providers within their jurisdictions. Given that the scale at which we conducted this study might have resulted in the loss of important physical accessibility information, future studies may improve upon these estimates by examining the association between STI morbidity and the availability of HCSD sites at smaller subcounty levels. Determining the magnitude of the unmet need for safety net STI services and the ability of the existing HCSD sites to meet that need is an important area for further work.',\n       'A course that focuses on the techniques and methods of creative self-expression in visual or plastic media, such as painting or sculpture.',\n       'Abstract: As cyclonic wind storms (hurricanes and typhoons) increase in frequency and intensity with climate change, it is important to understand their effects on the populations and communities of tropical trees they impact. Using tree demographic data from four large, tropical forest dynamics plots that differ in cyclonic storm frequency, we compare tree population and community dynamics. Additionally, we assess the effect of cyclonic storms on three functional traits, specific leaf area, wood density, and tree height of the dynamic tree assemblages. Mortality, growth and recruitment rates and the intrinsic rates of population growth of species differed across the plots, and were most dynamic, especially for stems 1-2 cm in diameter, at the plot which had an intermediate level of cyclonic storm frequency. Functional assemblages of species had the greatest degree of temporal variation in relation to disturbance, as measured by the change in functional divergence for the two plots with more intermediate cyclonic storm recurrence. Therefore, cyclonic storms affecting these plots generally have a greater effect on forest composition and dynamism than comparable cyclonic storms do on the plot which experiences cyclonic storms more frequently. Thus, we provide some evidence that community-wide demographic resistance to cyclonic storms is generally lower at an intermediate frequency of storms. While cyclonic storm strength and timing are important determinants of the within forest variation in tree dynamics and functional trait assemblages, we also show that cyclonic storm timing and frequency shapes tropical forest dynamics and functional composition across forests. We conclude that, over a given time interval, sites with intermediate levels of damaging cyclonic wind disturbance express a greater potential for life-history variation in the forest community, when compared to sites with less or more frequent disturbance.',\n       'In disease, the power law describing changes as a function of time may',\n       'Abstract. The nitrogen stable isotope ratio of biological tissue has been proposed as an indicator of anthropogenic N inputs to aquatic ecosystems, but overlap in the isotopic signatures of various N sources and transformations make definitive attribution of processes difficult. We collected primary consumer invertebrates from streams in agricultural settings in Wisconsin, USA, to evaluate the relative influence of animal manure, inorganic fertilizer, and denitrification on biotic d 15 N. Variance in biotic d 15 N was explained by inorganic fertilizer inputs and the percentage of wetland land cover in the watershed, but not by animal manure inputs. These results suggest that denitrification of inorganic fertilizer is the primary driver of d 15 N variability among the study sites. Comparison with previously collected stream water NO 3 -N concentrations at the same sites supports the role of denitrification; for a given N application rate, streams with high biotic d 15 N had low NO 3 -N concentrations. The lack of a manure signal in biotic d\\n15 N may be due its high ammonia content, which can be dispersed outside the range of its application by volatilization. Based on our findings and on agricultural census data for the entire United States, inorganic fertilizer is more likely than manure to drive variability in biotic d\\n15 N and to cause excessive nitrogen concentrations in streams.',\n       nan,\n       'Although we have data for the population of students and schools taking specific tests in each year, we expect some random fluctuation in the population over time. Accordingly, we did not treat pass rate information as fixed population data, but instead we used statistical tests to determine whether the differences we observed exceeded what we would expect to see with random fluctuation. We used t-tests at the 95 percent confidence level. A 95 percent confidence level for t-tests implies that we would have less than a 5 percent chance of observing the differences that occurred by chance.',\n       'To date, studies on instructor adoption of new teaching strategies have focused on faculty rather than graduate students. Simply sharing the \"evidence\" behind EBT does not seem to be enough to incite adoption of EBT among science faculty; for example, interviews with physics faculty revealed a mistrust of physics education research and education researchers (Henderson and Dancy, 2008) . Similarly, biology faculty prioritize their personal experiences of success over education research as rationale for sustained adoption of case study teaching (Andrews and Lemons, 2015) . This indicates that faculty likely need more structure and support to successfully adopt EBTinforming instructors that specific strategies \"work\" is likely insufficient.\\nFurther, the propensity toward adoption of EBT is likely highly context specific. A study of science faculty at one research institution revealed that faculty across scientific disciplines have high awareness of specific EBT strategies, but levels of interest and rates of adoption of EBT strategies vary greatly among faculty in different departments (Lund and Stains, 2015) . Such differences were thought to be caused by differences in departments, learning environments, personal experiences, and attitudes toward teaching. Given the different contextual influences faculty and graduate students are exposed to, it would be negligent to assume that graduate students approach EBT with the same attitudes, beliefs, and goals as faculty. It is therefore vitally important to understand not only how faculty perceive EBT, but how graduate students perceive it as well, if we are to best facilitate adoption of EBT in the newest generation of biology faculty.',\n       nan, nan,\n       'The donepezil data repository comprises primarily US Pfizer and US Eisai clinical trial data from phase 2, phase 3 and phase 4 trials. In total, the database contains 27 integrated AD studies (23 in patients with mild/moderate AD, one in patients with moderate to severe AD and three in patients with severe AD), including 18 double-blind, placebo-controlled trials, four open-label extension studies, two clinical experience trials, and three others. Data for more than 6000 unique patients, treated with donepezil for up to 5 years, are contained in the repository [2] [3] [4] [5] [6] [7] . Table 1 displays summary information for all 18 randomised, controlled trials, which comprised the data base for the data mining project.',\n       'The low accuracy of areas measured across multiple spatial extents for wetlands, barren, and rangeland is probably due to their low classification accuracy in the NLCD. These classes tend to have low overall classification accuracies in quality assurance testing conducted by the USGS (2003) and relative to our reference data (Table 1-3). The spectral signatures for wetland vegetation may be similar to forest (for forested wetlands) and rangeland (for shrub/scrub wetlands). This contributes to a high rate of misclassification for the wetland classes derived from the digitally processed NLCD. Our results and prior accuracy assessments suggest that it may not be advisable to use the NLCD to estimate wetland area. Although used as an ancillary source for labeling of clusters in the development of the NLCD, the National Wetlands Inventory (NWI, http://www.nwi.fws.gov) may provide better information as a sole source of data on wetlands because it is less likely to reflect the spectral confusion of the NLCD. The remaining poorly depicted classes, barren and rangeland, make up only 1.1% of the total study area. The rarity of these two classes likely contributes to the poor classification results. The confusion identified by the site-specific accuracy assessment between the developed, forest, and agriculture classes is likely due to differences in classification methodologies of the reference and NLCD data and not necessarily due to misclassification. For instance, LULC data derived from satellite imagery often identifies individual trees within an urban or agricultural setting as forest. However, photo-interpretation of those same sites will include individual trees within the overall developed or agricultural land use class and thus, add to misclassification within the error matrix as well as the overestimation indicated in Figure 1-2 (Y.Q. Wang, personal communication, 2002) Finally, my study only considers NLCD area accuracy within Massachusetts and Rhode Island. To fully understand the accuracy of NLCD-derived metrics within other geographic settings, similar multiple-extent assessment methods should be conducted in other regions. Published site-specific accuracy assessments show similar overall and individual LULC accuracies across a range of geographic locations; thus, I may expect a multiple-extent accuracy assessment to find similar results in these other regions as well (Yang et al. 2001, United States Geological Survey 2003. It is also reasonable to expect that the suite of poorly classified and correctly classified classes could change. For example, the rangeland class of the NLCD is exceedingly rare in Massachusetts and Rhode Island; however, in many western states rangeland would be a much more common class and it is possible that the NLCD would accurately depict it. The fact that rangeland was poorly depicted in this study probably reflects its rate of occurrence in the New England landscape.',\n       'The study was conducted within the San Pedro Riparian National Conservation Area ( NCA ) of Cochise County, Arizona (Fig. 1) . The San Pedro River is 310 km long and is the only permanently undammed river in the southwestern United States. The NCA is 3-8 km wide, 69 km long, and nearly 23,000 ha in size; it varies in elevation from 1295 m at the international boundary to 1113 m at the northernmost boundary. Within the NCA, the San Pedro River is perennial for 35 km and ephemeral elsewhere. Cattle removal from the study area began in late 1987 after the vegetative growing season and avian breeding season. On 1 January 1988, a 15-year grazing moratorium was initiated, which eliminated the season-long, cow-calf regime of 6,500-13,000 head that had been characteristic for the study area in previous years.\\nA total of 355 species of birds has been recorded within the NCA (Krueper 2000) . Of the 108 breeding species, 63 are migrants and 45 are permanent residents. Between 5 and 10 million migratory songbirds use the San Pedro River annually for migration and breeding ( Rojo et al. 1998) .\\nVegetation in the NCA was divided into 21 communities (Brown et al. 1979) . We combined these categories to obtain three broad communities: riparian, mesquite grassland, and Chihuahuan desert-scrub. In 1987 the riparian community covered approximately 1000 ha of the NCA and was dominated by Fremont cottonwood, Gooding willow, seepwillow, forbs, and perennial grasses such as side-oats grama ( Bouteloua curtipendula ) and tobosa ( Hilaria mutica ). The mesquite grassland community covered approximately 5000 ha and was dominated by mesquite, whitethorn acacia ( Acacia neovernicosa ), and sacaton grass ( Sporobolus wrightii ). The Chihuahuan desert-scrub community covered 12,000 ha and was dominated by whitethorn acacia, creosote ( Larrea divaricata ), inkweed ( Suaeda torreyana ), zinnia ( Zinnia acerosa ), dogweed ( Dyssodea acerosa ), and native and non-native grasses.',\n       \"FAQ is as a self-administered functional assessment which provides information on the patient's physical, psychological, social, and role functions. It can be used useful to monitor the patient over time with 0 score corresponding to no impairment and 30 to severely impaired. In our study, we considered all 11 measures reported in the ADNI database: FAQFINAN, FAQ-FORM, FAQSHOP, FAQGAME, FAQBEVG, FAQMEAL, FAQEVENT, FAQTV, FAQREM, FAQTRAVL, and FAQ total.\\nFinally, in our work, a total of 131 measures were used. Table 1 shows the entire list of these measures, a short description of what they represent together with the reference tests.\",\n       \"at step (e) of our processing pipeline (c.f. figure 1 ) using TractQuerier's tract math tool [24] . \\nwhere W i and W j contain binary values (1 inside the fascicle and 0 otherwise) and v is a voxel index.\\nAs is, the Dice coefficient greatly penalizes for spurious streamlines that would be far from the core of the fascicle. Given that WM fascicles have more tracts in the middle than in their periphery, we propose a weighted Dice coefficient which accounts for the number of streamlines per voxel. In that perspective, each voxel in W i and W j contains a value between 0 and 1 expressing the fraction of tracts passing through that position. Our weighted Dice metric sums the voxels that overlap in W i\\nand W j and divides by the total sum of voxels:\\nwhere v ′ stands for the voxels that are within the intersection of the W i and W j fascicles. Our weighted Dice gives more importance to areas with dense fibers.\\nWe also quantify the test-retest reproducibility of the tract profiles. We assume that the tract profile of a given pathway should be closer to one from the same subject than one from any other subject. This procedure is illustrated in figure 2 . Here, FA tract profiles of the corticospinal tract are shown in (a), and in (b), tract profiles of fascicle #2 described later as the connection between associative cortex and putamen. The first two profiles (green and blue) were extracted from two acquisitions of the same subject while the last one (in red) was extracted from another subject. In (a), we have the situation where the tract profiles of the first subject are more similar to each other than to the other subject chosen randomly. This is in line with intuition as the brain structure of an individual is more similar to itself than to that of another person. Conversely, in (b), we see that the tract profile of the first acquisition of the first subject is more similar to the tract profile of the other subject than to its second acquisition. In this case, the tractography pipeline induced a distortion to the fascicle that makes it unreliable for a population study. In this example, we would 6\\n\",\n       \"Maximal heart rate was defined using the peak heart rate recorded from a maximal treadmill exercise test using a modified Balke protocol (9) . Oxygen consumption was measured via indirect calorimetry using a Medgraphics Gas Exchange System (Medical Graphic Corp, St. Paul, MN). Oxygen consumption and heart rate were averaged in 30 second intervals, and the highest values were termed VO 2 peak and maximal heart rate, respectively, and verified with a respiratory exchange ratio of ≥1.1. Resting heart rate was defined using the nighttime heart rate data collected with the Actiheart. After defining each individual's HRR, minute-level PA intensity was classified using the following thresholds: sedentary/sleep (<20% HRR), light (20-39% HRR), moderate (40-59% HRR), and vigorous (≥60% HRR) (22, 23) . Although participants taking medications for arrhythmias were excluded from these analyses, to reduce the potential for error introduced by transient arrhythmias, all minutes associated with zero activity counts were considered to be sedentary/ sleep time regardless of heart rate ( Figure 1 ). For the remaining minutes, given that the participant was moving (eg activity counts > 0), the amount of time spent at each level of intensity was calculated for each participant as follows: where i denotes a subject, J i is the number of days of Actiheart data for subject i, e t ij ( ) is the percent effort at minute t for subject i on day j, s t ij ( ) is the activity state at minute t for subject i on day j, and I is a 0/1 indicator function. Figure 2A -D shows the unadjusted associations (β, p-value) between the number of minutes per day spent in each level of relative intensity as a function of age. Based on the appearance of these plots, we explored continuous and log-transformed versions of crosssectional linear regression analyses between the time spent at each level of intensity and age, adjusted for sex, body mass index (BMI), race, functional status, and comorbidities. Given that these results did not substantially differ, we present the untransformed results to facilitate interpretation (Tables 1 and 2 and Supplementary Table 1) .\",\n       nan,\n       \"Here, we introduce the following two real independent datasets that are used to validate our MGNN framework.\\nADHD-200 dataset. Our experiments were conducted on the public ADHD-200 dataset with data collected from 973 participants in eight different sites [1] . We adopted rs-fMRI data which were registered to Automated Anatomical Labeling (AAL) atlas with 116 regions of interest (ROIs). We used Pearson's correlation coefficients between different ROIs construct functional brain connectivity for each participant, i.e., a 116 × 116 adjacency matrix whose rows and columns correspond to ROIs. We had four labels depending on the progression of the ADHD: 1) Typically Developing Children (TDC), 2) ADHD-Combined (ADHD-C), 3) ADHD-Hyperactive/Impulsive (ADHD-H/I) and 4) ADHD-Inattentive (ADHD-I). We removed ADHD-H/I group since it had only 11 samples; taking all the available samples (without any artifact), we ended up with total of N = 756 samples for our experiment. The demographic of ADHD-200 dataset is given in Table 1 . ADNI dataset. ADNI dataset is a public dataset for longitudinal AD study. From the initiative, we obtained Diffusion Tensor Images (DTI), and individual images were processed by tractography pipeline to extract structural brain networks using Destrieux atlas with 148 ROIs. Each brain network is representated as an adjacencey matrix whose elements denote number of neuron fiber tracts connecting two different ROIs. The dataset includes 5 classes: 1) AD, 2) Cognitively Normal (CN), 3) Early Mild Cognitive Impairment (EMCI), 4) Late Mild Cognitive Impairment (LMCI) and 5) Significant Memory Concern (SMC), and the demographics of the ADNI dataset can be found in Table 2 . In our experiment, we merged CN and EMCI groups as Preclinical AD group and combined LMCI and AD groups as Prodromal AD group to ensure sufficient sample size and compare their differences. \",\n       'Solves a word problem by finding the missing term in a proportion.',\n       nan,\n       'In this article, RMS serves as a feature reduction way, although other methods of generating features from each sliding window will produce much more features for each subject. However, in this article, RMS is mainly selected based on the two considerations. First, RMS is able to characterize the overall activity level of dynamic FC and dynamic FCT. Second, RMS is invariant to the chronological order of sliding windows as temporal information collapses based on the definition of RMS. Using features from each sliding window could initiate the problem of phase mismatching across different subjects due to inclusion of temporal information. In this case, utilization of temporal features becomes another difficult problem. One possible solution is to further extract time-invariant highlevel features based on sliding-window features, such as distribution-and frequency-spectrum-based or statustransformation-probability-based features. Besides RMS, other statistics can also be used to extract features from the time series, such as entropy, Hurst index, kurtosis, etc. Since different features characterize the same signal from different viewpoints, combining them appropriately is likely to further improve the outcomes.',\n       \"Note that p(Y |Z) is entirely determined by the observable relationship between Y and Z; consequently p(Y |θ,Z) is completely determined once p(θ|Y,Z) is specified.\\nProof. Observe that\\nFor the denominator in (12), we note\\nClearly, equations (12) and (13) Proof. We first observe that, as in the proof of Theorem 4.1,\\nwhich again depends only on p(θ|U,Z) and p(U |Z). Then,\\nIf Y ⊥ ⊥ U |θ,Z then the first term under the integral in (14) reduces to p(Y |θ,Z), and there is no constraint. However, if Y ⊥ ⊥ U |θ,Z, then (14) determines p(Y |θ,Z). 2\\nTaken together, these results are pessimistic about the use of standard PV methodology to explore predictive inference using θ and other covariates: in order to ensure unbiased estimation of s(X, Y,Z) by s(X, Y,Z, Z), the variable to be predicted, Y , must be in the institutional conditioning model, and the secondary analyst's model p SA (Y |θ,Z) must be the one implied by the survey institution's conditioning model. The bias when these conditions are violated can be substantial, as we will show below in Section 6, and may lead to incorrect scientific or policy conclusions.\\nAs survey institutions typically release only the plausible values and not the details of the model associated with them, it is unlikely that the secondary analyst can specify p SA (Y |θ,Z)\\ncorrectly. Thus, for predictive inference using θ, the secondary analyst is better off building a model from scratch, not making use of institutional PVs. We turn to this process in the next section.\",\n       'We assessed how well each automated method performed on segmenting hippocampi in the full dataset (N=229) by first quantifying the percentage of hippocampal output failures (where no segmentation was output by the method at all). This was calculated as the number of output failures divided by the total sample size (N=229).',\n       'In April 1981, survey forms were mailed to a total of 1,788 faculty members in chemistry, computer sciences, geosciences, mathematics, physics, and statistics/biostatistics. The evaluators were selected from the faculty lists furnished by the study coordinators at the 228 universities covered in the assessment. These evaluators constituted approximately 13 percent of the total faculty population-13,661 faculty members-in the mathematical and physical science programs being evaluated (see Table 2.3). The survey sample was chosen on the basis of the number of faculty in a particular program and the number of doctorates awarded in the previous five years (FY1976-80)-with the stipulation that at least one evaluator was selected from every program covered in the assessment. In selecting the sample each faculty rank was represented in proportion to the total number of individuals holding that rank, and preference was given to those faculty members whom the study coordinators had nominated to serve as evaluators. As shown in Table 2.3, 1,461 individuals, 82 percent of the survey sample in the mathematical and physical sciences, had been recommended by study coordinators. 10 Each evaluator was asked to consider a stratified random sample of 50 research-doctorate programs in his or her discipline-with programs stratified by the number of faculty members associated with each program. Every program was included on 150 survey forms. The 50 programs to be evaluated appeared on each survey form in random sequence, preceded by an alphabetized list of all programs in that discipline that were being included in the study. No evaluator was asked to consider a program at his or her own institution. Ninety percent of the survey sample group were provided the names of faculty members in each of the 50 programs to be evaluated, along with data on the total number of doctorates awarded in the last five years. 11 The inclusion of this information represents a significant departure from the procedures used in earlier reputational assessments. For purposes of comparison with previous studies, 10 percent (randomly selected in each discipline) were not furnished any information other than the names of the programs. The survey items were adapted from the form used in the Roose-Andersen study. Prior to mailing, the instrument was pretested using a small sample of faculty members in chemistry and psychology. As a result, two significant improvements were made in the original survey design. A question was added on the extent to which the evaluator was familiar with the work of the faculty in each program. Responses to this question, reported as measure 11, provide some insight into the relationship between faculty recognition and the reputational standing of a program. 12 Also added was a question on the evaluator\\'s field of specialization-thereby making it possible to compare program evaluations in different specialty areas within a particular discipline. A total of 1,155 faculty members in the mathematical and physical sciences-65 percent of those asked to participate-completed and returned survey forms (see Table 2.3). Two factors probably have contributed to this response rate being approximately 14 percentage points below the rates reported in the Cartter and Roose-Andersen studies. First, because of the considerable expense of printing individualized survey forms (each 25-30 pages), second copies were not sent to sample members not responding to the first mailing 13 -as was done in the Cartter and Roose-Andersen efforts. Second, it is quite apparent that within the academic community there has been a growing dissatisfaction in recent years with educational assessments based on reputational measures. Indeed, this dissatisfaction was an important factor in the Conference Board\\'s decision to undertake a multidimensional assessment, and some faculty members included in the sample made known to the committee their strong objections to the reputational survey. As can be seen in Table 2.3, there is some variation in the response rates in the six mathematical and physical science disciplines. Of particular interest is the relatively high rate of response from chemists and the low rate from physicists-a result consistent with the findings in the Cartter and Roose-Andersen surveys. 14 It is not surprising to find that the evaluators nominated by study coordinators responded more often than did those who had been selected at random. No appreciable differences were found among the response rates of assistant, associate, and full professors or between the rates of those evaluators who were furnished the abbreviated survey form (without lists of program faculty) and those who were given the longer version. Each program was considered by an average of approximately 90 survey respondents from other programs in the same discipline. The evaluators were asked to judge programs in terms of scholarly quality of program faculty, effectiveness of the program in educating research scholars/scientists, and change in program quality in the last five years. 15 The mean ratings of a program on these three survey items constitute measures 08, 09, and 10. Evaluators were also asked to indicate the extent to which they were familiar with the work of the program faculty. The average of responses to this item constitutes measure 11. In making judgments about the quality of faculty, evaluators were instructed to consider the scholarly competence and achievements of the individuals. The ratings were furnished on the following scale: In assessing the effectiveness of a program, evaluators were asked to consider the accessibility of faculty, the curricula, the instructional and research facilities, the quality of the graduate students, the performance of graduates, and other factors that contribute to a program\\'s effectiveness. This measure was rated accordingly:  14 To compare the response rates obtained in the earlier surveys, see Roose and Andersen, Table 28, p. 29. 15 A copy of the survey instrument and accompanying instructions are included in Appendix C. Evaluators were instructed to assess change in program quality on the basis of whether there was an improvement in the last five years in both the scholarly quality of the faculty and the effectiveness in educating research scholars/scientists. The following alternatives were provided: 2 Better than five years ago 1 Little or no change in last five years 0 Poorer than five years ago X Don\\'t know well enough to evaluate Evaluators were asked to indicate their familiarity with the work of the program faculty according to the following scale: In the computation of mean ratings on measures 08, 09, and 10, the \"don\\'t know\" responses were ignored. An average program rating based on fewer than 15 responses (excluding the \"don\\'t know\" responses) is not reported. Measures 08, 09, and 10 are subject to many of the same criticisms that have been directed at previous reputational surveys. Although care has been taken to improve the sampling design and to provide evaluators with some essential information about each program, the survey results merely reflect a consensus of faculty opinions. As discussed in Chapter I, these opinions may well be based on out-of-date information or be influenced by a variety of factors unrelated to the quality of the program. In Chapter IX a number of factors that may possibly affect the survey results are examined. In addition to these limitations, it should be pointed out that the evaluators, on the average, were unfamiliar with almost one-third of the programs they were asked to consider. 16 As might be expected, the smaller and less prestigious programs were not as well known, and for this reason one might have less confidence in the average ratings of these programs. For all four survey measures, standard errors of the mean ratings are reported; they tend to be larger for the lesser known programs. The frequency of response to each of the survey items is discussed in Chapter IX. Two additional comments should be made regarding the survey activity. First, it should be emphasized that the ratings derived from the survey reflect a program\\'s standing relative to other programs in the same discipline and provide no basis for making cross-disciplinary comparisons. For example, the fact that a much larger number of chemistry programs received \"distinguished\" ratings on measure 08 than did computer science programs indicates nothing about the relative quality of faculty in these two disciplines. It may depend, in part, on the total numbers of programs evaluated in these disciplines; in the survey instructions it was suggested to evaluators that no more than 10 percent of the programs listed be designated as \"distinguished.\" Nor is it advisable to compare the rating of a program in one discipline with that of a program in another discipline because the ratings are based on the opinions of different groups of evaluators who were asked to judge entirely different sets of programs. Second, early in the committee\\'s deliberations a decision was made to supplement the ratings obtained from faculty members with ratings from evaluators who hold research-oriented positions in institutions outside the academic sector. These institutions include industrial research laboratories, government research laboratories, and a variety of other research establishments. Over the past 10 years increasing numbers of doctoral recipients have taken positions outside the academic setting. The extensive involvement of these graduates in nonacademic employment is reflected in the percentages reported in Table 2.2: An average of 40 percent of the recent graduates in the mathematical and physical science disciplines who had definite employment plans indicated that they planned to take positions in nonacademic settings. Data from another NRC survey suggest that the actual fraction of scientists employed outside academia may be significantly higher. The committee recognized that the inclusion of nonacademic evaluators would furnish information valuable for assessing nontraditional dimensions of doctoral education and would provide an important new measure not assessed in earlier studies. Results from a survey of this group would provide an interesting comparison with the results obtained from the survey of faculty members. A concentrated effort was made to obtain supplemental funding for adding nonacademic evaluators in selected disciplines to the survey sample, but this effort was unsuccessful. The committee nevertheless remains convinced of the importance of including evaluators from nonacademic research institutions. These institutions are likely to employ increasing fractions of graduates in many disciplines, and it is urged that this group not be overlooked in future assessments of graduate programs.',\n       'This analysis aims to shed light on how support is distributed amongst farms and its relation to various income components. The degree of concentration of support is presented and inequalities of its distribution is analysed, particularly in relation to gross agricultural output, gross receipts, income and assets. It also analyses differences between the average level of support and income of various farm groups (by farm size, farm type and types of regions) and the average of all farms, and in the share of support in farm receipts between different types of farms. Finally, overall inequality in the distribution of support and other income components is examined using Gini coefficients. Definitions of variables used in this report can be found in Box 2.1. When used alone, the term -support‖ refers to -total support‖ and -payments‖ to -total payments‖.',\n       'Ruthi Barkai, Tommy Dreyfus, Dina Tirosh and Pessia Tsamir Kibbutzim Teacher College Tel-Aviv Tel Aviv University Proof is an essential entity of mathematics. The processes of examining the validity of conjectures are at the core of any student\\'s mathematical development. Thus, a major task of mathematics teachers is to communicate to their students the spirit of mathematics as a science of conjectures and proofs. This task implies that teachers lead and encourage their students to rationally examine conjectures, to assess their validity and to prove or refute them. It is therefore vital for teachers to be familiar with both formulating conjectures and reacting to arguments that purport to prove or refute mathematical conjectures. A number of researchers have studied teachers\\' conceptions of proofs, and almost all of them described teachers\\' knowledge of valid universal statements (e. g., Dreyfus, 2000;Knuth, 2002aKnuth, , 2002b). The major aim of our project is to explore practicing secondary school teachers\\' conceptions of proofs. The study focuses on valid and invalid, universal and existential statements and addresses Subject Matter Knowledge and Pedagogical Content Knowledge issues. This paper focuses on the teachers\\' reactions to adequate and inadequate justifications to valid and invalid universal and existential statements. The statements are taken from elementary Number Theory, and the given justifications include numerical examples, algebraic arguments and non-formal generalizations. Twenty-two secondary school teachers answered written questionnaires, and then participated in a one semester research-based-intervention that dealt with proofs and refutations of different types of mathematical statements. The findings show that all teachers responded correctly to those justifications, which include numerical examples -they accepted them as proofs when adequate and rejected them when inadequate. However, with regard to algebraic arguments and non-formal generalizations, approximately half of the teachers\\' responses were unsatisfactorythey accepted invalid justifications or rejected valid justifications. In the presentation, these and other results will be described.  , situation theory (Devlin, 1991) and the theory of information flow (Barwise & Seligman, 1997). My discussion is based on the detailed case study of a pair of 8th grade students (out of 9) working through a number of problems asking them to produce and interpret a table, equations and graphs. The student work session was videotaped. In a subsequent analysis, the interaction was transcribed and each (topical) utterance was then modelled within situation calculus. Each of the representations was considered as a self-contained discursive situation, which was explored by the students. Particular attention was given to reference relationships and constraining relations between situations. Constraining relations turned out to be of one of four types: a) Simple constraints between states of affairs in a situation, b) Constraining relations as elements of modelling causal (and temporal) relations within the statement of events, c) Constraining relations between different situations and d) Constraining relations between different objects of different situations. I hypothesize that mathematical conceptual understanding frequently operates with constraints of the last type (d). Such constraints afford the statement of a given situation to infer the existence or properties of other objects in other situations. Stated this way, it seems not surprising that students frequently have difficulties \"reversing\" interpretations. E.g. when generating an equation from some situation students may find it difficult to interpret the equation as model for the situation. I will argue that a reversal requires students to break down the original total situation in relevant features (Martin & Schwartz, 2005), and to attune to what elements may carry mathematical significance. I will discuss this using concrete examples from my material and draw implications for mathematics teaching.  .897] to compare advanced math and STEM careers. Results indicate that high school girls\\' choice of mathematics maybe driven by career choice rather than success or failure in high school calculus. Further analysis finds that none of these girls are interested in majoring in computer science or physics. One of the 86 intended to major in mathematics and one in teaching mathematics. When asked to list strengths they will bring to the workplace, they describe themselves as hardworking, determined, organized leaders who are good with people. Only one girl described herself as \"smart.\" While girls are taking more advanced mathematics in high school, it appears that these choices have little impact on high achieving young women\\'s perceptions of self. Perhaps it is still true that young women are not recognized for their intellectual contributions, especially in mathematics (Chipman, 1996).',\n       \"This study found a relationship between parents' education level and the likelihood that students would undertake a more rigorous high school curriculum and, consequently, enroll, perform well, and persist in 4-year postsecondary institutions. Overall, first-generation status was shown to have a negative association with students' academic preparation and persistence.\",\n       \"Throughout this discussion the term report is used to refer to an individual publication, such as an article or dissertation, and the term study to refer to the data-gathering effort, such as an experiment or other type of intervention, on which a report was based. (As explained in greater detail in the results section, a report could involve more than one study and a study could result in more than one report.) We limited our analysis to studies that examined DI in the EngelmannBecker tradition and omitted studies that used only elements of the approach. We omitted studies that combined interventions, presenting data for students exposed to both DI and other programs, and also omitted studies that reported only the impact of more or less exposure to the program (i.e., with no data regarding results with no exposure). Within studies we omitted comparisons that were aggregates of others. The coders noted any issues regarding the quality of the research procedures and reporting. Studies that were regarded as having moderate or serious quality issues were omitted. Finally, we omitted outliers, any effects with an absolute value greater than 3.0. (The impact of the exclusions related to quality and outliers were examined in the sensitivity analysis explained below. Appendix B, in the online version of the journal, includes a list of all reports and studies examined, both included and excluded, as well as their associated effect sizes.)\\nBeyond these limitations, we purposely took an inclusive approach to developing the sample. Articles published in peer-reviewed journals, dissertations, masters' theses, and technical reports and other nonpublished material (so-called gray literature) were included in our initial review. Although most of the reports examined student academic achievement, we also found and included reports that considered ability (IQ) measures; student affective outcomes, such as attitudes, self-esteem or behavior; teachers' perceptions of effectiveness; and teacher or parent views of the programs. We had no limits on date of publication, beginning our analysis with the first published reports on DI in the mid-1960s. We also had no limits on the location or site of a study and included all research designs for which we could compute a valid effect size. Only reports published in English were included.\",\n       \"The Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS) is designed to provide timely and detailed estimates of income, poverty and health insurance coverage, and to measure change in those estimates at the national level. Conducted by the Bureau of the Census for the Bureau of Labor Statistics, the CPS ASEC is the offi cial source of the national poverty estimates calculated in accordance with the Offi ce of Management and Budget's Statistical Policy Directive 14. (Though the Census Bureau also reports income and poverty estimates based on the American Community Survey, part of the 2010 Decennial Census Program, it recommends people use ASEC/CPS for national estimates because it provides more complete and thorough estimates of income and poverty.) The sample is scientifi cally selected to represent the civilian noninstitutional population living in the U.S. The unit of observation is the household. About 70,000 households are interviewed each year. Analysis sample: Because CPS collects data for a larger sample relative to CE, we use CPS to calculate estimates of well-being measures based on household money income for all U.S. households (tables 1 and 2). We also use it to benchmark the CE data, including the estimates of household \",\n       \"As    NetMap, a landscape analysis computational platform that contains a geospatial data structure for use in a GIS to support natural resource management planning (TerrainWorks, 2016). NetMap was used to compute topographic and watershed attributes over a 10-meter DEM traced channel network to create 100-meter stream segments with a calculated steelhead IP score. A final IP score for a stream reach is created by taking the geometric mean of the product of the un-weighted index scores of three stream attributes: mean annual flow, calibrated valley-width index, and channel gradient (Burnett et al., 2007). This approach works on the assumption that each of these stream attributes are approximately equal in importance and the one with the lowest index score has the greatest influence on the final IP score for that stream reach (Burnett et al., 2007). Juvenile steelhead are able to ascend steep stream gradients and have been found rearing in reaches with a 16% gradient (Bryant, Zymonas, & Wright, 2004). However, they generally rear in the lower gradient areas of those high gradient stream reaches (Engle, 2002). As shown in Figure 4, the gradient index score remains constant for reaches above 7%, and is 'zero' for reaches with a gradient above 10%. Juvenile steelhead tend to use stream reaches with hill slopes that constrict the channel and limit interaction with the floodplain (Hicks, 1990;Reeves, Bisson, & Dambacher, 1998). This preference for confined channels is reflected in the calibrated valley-width index (Figure 4, p.21). Juveniles can rear in a wide range of flows, from large mainstem rivers up to small headwater tributaries (Meehan & Bjornn, 1991). With the exception of those stream reaches with very little to no mean annual flow, the mean annual flow index is fairly high across a very large range of flows (Figure 4, p.21). However, there has been considerable deliberation on the actual productivity of large mainstem rivers, over 50 meters wide, as much of the channel area is typically not used by juveniles in the absence of instream cover (PSSTRT, 2013b).\",\n       nan,\n       'It has long been known that the foreign-born play an important role in US science and engineering. The basis for much of this understanding has The Role of Foreign Graduate Students and Postdoctoral Scholars 157 been the role the foreign-born play as faculty or when working in industry. The results of the present study suggest that the foreign-born play an important role in doing research, much of which is of a basic nature, while they are graduate students and postdocs. The fi nding is not surprising, but prior to this study no one has set about to investigate the degree to which the foreign-born contribute in this way. The contributions of the foreign-born graduate students and postdoctoral scholars to US science, of course, do not end when their training is completed. Many choose to stay in the United States. Finn, for example, fi nds that approximately 70 percent of PhD recipients on temporary visas in science and engineering were in the United States two years after receiving their PhD degree; the fi ve-year stay rate was only slightly lower (Finn 2005, table 3). The rate is highest for Chinese, who have a fi ve-year stay rate of 90 percent, followed by Indians, with a fi ve-year stay rate of 86 percent. (Finn 2005, table 7.) No one has made comparable estimates for postdocs, but the assumption is that a number who come to train stay on after their training is completed. The ethnicity of faculty authors in this study is suggestive of this; approximately one in fi ve had neither English nor European names. The group making up the highest percent of nonnative faculty was of Chinese ethnicity. This is not to say that scientists and engineers contribute to US science only when they stay. Many who return end up co-authoring papers with colleagues in the United States. We see some examples of this in our data. The work of Adams et al. (2005) fi nds that the international co-authorship patterns of faculty at US universities are infl uenced by the number of foreign students trained in their department who return to their home country. Moreover, co-authorship is not the only way by which scientists in one country benefi t from the work and expertise of others. Published science is a public good; regardless of whether they stay or leave, these researchers will continue to contribute to the creation of knowledge. That foreign-born graduate students and postdoctoral fellows play an important role appears indisputable from this research. But it does not follow that their places would be left unfi lled if they were not to come. Considerable debate has focused on the degree to which foreign-born students displace US students. The question is difficult to answer but there is reasonable agreement regarding several facts. First, natives, especially native males, when choosing a career are responsive to alternative opportunities. In the last twenty or so years many of these opportunities-for example, law and business-have proved relatively more attractive, requiring shorter training times and offering higher salaries. Second, if the incentive structure were to change, the number of US citizens entering S&E would arguably change as well. By way of example, Richard Freeman (2005) fi nds the size of the applicant pool for NSF Graduate Research Fellowships to be responsive to the relative value of the stipend and concludes \"that the supply of highly skilled applicants is sufficiently responsive to the value of awards that increases in the value of stipends could attract some potentially outstanding science and engineering students who would otherwise choose other careers.\" Third, and by way of contrast, foreign-born have had fewer alternatives available that offer the option of support while in school and employment at a favorable relative wage. Fourth, the alternatives open to the foreign-born are changing. Programs outside the United States are becoming more and more competitive. Since the late 1980s the number of S&E PhD degrees awarded in Europe has surpassed the number in the United States. In the late 1990s, the number of degrees awarded in Asian countries surpassed the number awarded in the United States. In China alone the number accelerated from virtually zero in 1985 to approximately 13,500 by 2004 (National Science Board 2008, appendix tables 2-42 and 2-43). At the same time, programs in the United States are at risk of becoming less attractive to foreign-born students and postdoctoral scholars. This is not only because funds for graduate and postdoctoral support are diminishing as agencies such as NIH experience real decrease in funding levels, but also because of problems faced by foreign nationals in the United States since 9/ 11. A case in point is the special vetting required for foreign nationals to work on research supported by federal agencies and considered \"sensitive but unclassifi ed.\" 46 Nor does it follow that the demand for graduate students and postdocs to work at universities will necessarily persist at its current level. The technology of discovery is changing. By way of example, in 1990 the best-equipped lab could sequence 1,000 base pairs a day. By January 2000 the twenty labs involved in mapping the human genome were collectively sequencing 1,000 base pairs a second, 24/ 7. The cost per fi nished base pair fell from $10.00 in 1990 to under $.05 in 2003 (Collins, Morgan, and Patrinos 2003) and was roughly $.01 in 2007 (http:/ / biodesign.asu.edu/ news/ nih-funds-next-gener ation-of-dna-sequencing-projects-at-asu). As the technology of discovery changes, the need for skilled lab workers-many of whom are graduate students and postdocs-may decline. Moreover, as equipment becomes increasingly sophisticated and more expensive, research procedures may increasingly be outsourced to nonuniversity facilities. Mail-in crystallography, where crystals are sent to large nonuniversity labs for analysis, is but one example. There is also the question of whether the Federal government will continue to provide resources for graduate research assistants and postdocs at the level it has in the past. The heavy reliance on graduate students and postdoctoral scholars in the performance of university research has contributed to the US eminence as a training center for both native and foreign-born students. It provides not only hands-on learning but also fi nancial support for graduate study and postdoctoral work, something that many other countries cannot provide. Factors that reduce either the demand for or supply of graduate students and postdocs have the potential of threatening the United States\\'s eminence as a training center and producer of research.',\n       'for students in schools with an enrollment of at least 750 students (52.3 percent). The overall response rate for the student-level teacher questionnaire was 55.1 percent. The response rates by subgroup follow a similar pattern as those for the teacher-level questionnaire.  1 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the spring first-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this Tables 5-13 through 5-15 present response rates that reflect response across the fall and spring first-grade collections combined. These rates are referred to as longitudinal response rates.',\n       'Purpose -Postdocs make up a significant portion of the biomedical workforce. However, data about the postdoctoral position are generally scarce, and no systematic study of the landscape of individual postdoc salaries in the USA has previously been carried out. The purpose of this study was to assess actual salaries for postdocs using data gathered from US public institutions; determine how these salaries may vary with postdoc title, institutional funding and geographic region; and reflect on which institutional and federal policy measures may have the greatest impact on salaries nationally.',\n       \"Sequential extension of the oligonucleotide length from dinucleotide will provide clues to elucidate the molecular adaptation mechanisms, but we next analyzed the occurrence of 20-mers in SARS-CoV-2 genomes, as previously conducted for ebolavirus and influenza virus (Wada et al., 2016 (Wada et al., , 2017 . For these highly mutable viruses, the time-series analysis of long oligonucleotides such as 20-mers is unambiguously important for designing the PCR primers and therapeutic oligonucleotides that can be used for a sufficiently long period (Wada et al., 2017) . Since the polyA-tails were removed in advance, none of the 20-mers was present more than once in each genome. Therefore, it is important to mention that the occurrence level (%) of a certain 20-mer in each month's viral population corresponded to the occurrence level (%) of the strains with the 20-mer sequence in the viral population.\\nWhen we happened to search for 20-mers that were absent in all 16 strains isolated in December 2019, then emerged and increased in occurrence, we unexpectedly found a group of rapidly increasing 20-mers. This remarkable peculiarity is shown in Fig. 3a; here, we focused on all 39,990 types of 20-mers that were present in April 2020 but absent in December 2019 and analyzed their occurrence level (%) in the April population. The first histogram (reddish brown) displays the number of types of 20-mers present within each 3% width of the occurrence level in the April population.\\nIn the area with a very low occurrence level, the vertical axis value is near 40,000, which is almost the same to all 39,990 types emerged after December 2019; i.e., almost all 20-mers that newly emerged exhibit a very low occurrence level in the April population. However, when the vertical axis is displayed as a log (blue) scale, peculiar data (marked with a long arrow) become evident at the position where the horizontal value is more than 80% in the April population and the vertical log value is 1.903 (= 80).\\nTo be more precise, sixty types of 20-mers have an occurrence level of 82%, and twenty types have an occurrence level of 81%. When analyzing time-series changes in the occurrence level of the eighty 20-mers, a very similar pattern of monotonic increase was observed for all eighty 20-mers (Fig. 3b) . \",\n       'In 1982, there was considerable confidence that more sophisticated crop identification and land-cover analysis was imminent. The new Thematic Mapper instrument had just been put into orbit on Landsat 4 (Townshend et al., 1983;Salomonson, 1984;Williams et al., 1984). This advanced sensor incorporated significant improvements with seven spectral wavelengths extending across the visible, near infrared, short-wave infrared, and thermal infrared; 8-bit radiometric precision; and, 30-m spatial resolution. The early results were indeed highly promising and it looked like a whole new generation of information possibilities was looming (Salomonson, 1984;Williams et al., 1984;Markham and Barker, 1985). However, the affairs of U.S. Government and industry intervened, and, in 1984, Landsat was shifted to a commercialization effort under the guidance of the National Oceanic and Atmospheric Administration (NOAA) in response to the Landsat Commercialization Act of 1984. Landsat privatization also coincided with a sharp increase in scientific attention to the global Earth system and the role of land conditions in this system (Duvigneaud et al., 1979;Matthews, 1983;National Aeronautics and Space Administration, 1983;Eddy, 1986;National Aeronautics and Space Administration, 1986;National Research Council, 1986b;Willmott and Klink, 1986). It quickly became evident that satellite remotely sensed observations could provide a critical improvement in assessing land conditions and dynamics (Scientific Committee on Problems of the Environment, 1984) and Landsat was the primary candidate observatory. Unfortunately, the impact of the Landsat commercialization effort was immediate and negative on Landsat-based scientific activity. The high costs and limited availability of Landsat TM observations caused a sharp scientific shift away from Landsat observations with regard to addressing these global-scale land issues. Research scientist\\'s interests shifted toward the newly refined NOAA Advanced Very High Resolution Radiometer (AVHRR) instrument, flown on the polar-orbiting meteorological satellites. The AVHRR sensor, designed primarily for weather observations, had been refined in 1981 to include visible and near infrared observations along with three thermal infrared sen-sors, collecting daily global images at 1-km and 4-km spatial resolutions (Allison and Schnapf, 1983). The resultant observations were available, at cost of reproduction, to any interested user. It quickly became evident that these \"low\" spatial resolution observations could be used to study regional vegetation patterns, as well as to provide valuable measurements of the seasonal dynamics of land vegetation (Townshend and Tucker, 1981;Schneider and McGinnis, 1982;Goward et al., 1985). This shift to AVHRR research was made possible initially by the understanding of SWS, as a general descriptor of vegetation foliage conditions, derived from Landsat MSS observations between 1972 and 1984. This shift to the AVHRR sensor also reinforced the emphasis placed on the simple, twoband, visible-near infrared spectral vegetation index as the primary terrestrial satellite remote sensing contribution to Earth Systems Science (Tucker and Sellers, 1986;Fung et al., 1987;Box et a]., 1989;Heimann and Keeling, 1989;Sellers et al., 1994;Townshend et al., 1995).',\n       \"The operational definitions of cognition and cognitive load are as follows: Cognition can be understood to represent mental processes that allow for information processing, access, and storage and includes domains such as verbal and nonverbal memory, visuospatial ability, and executive function. While some domains (eg, executive function) are functionally and neuroanatomically well delineated, other cognitive domains such as working memory and processing speed are required for multiple cognitive abilities and subserved by a widely distributed neural network. 19, 106 Cognitive load refers to the cognitive effort required for a set of cognitive tasks. Using the model of resource capacity developed by Kahneman, 68 difficulty in doing multiple tasks is related to the limited pool of available cognitive resources, and simultaneous performance of tasks is feasible only when resource capacity limits are not exceeded.\\nCognitive psychology studies over the past 40 years have consistently demonstrated that hearing loss is associated with poorer performance on auditory and nonauditory cognitive measures of working memory as first described by Rabbitt. 97 These studies have generally tested the working memory of adults under conditions of normal versus artificially degraded hearing or in matched pairs of adults with and without hearing loss. 25,51,55,82,83,87,88,94,96,97,110Y112,127 Under degraded auditory conditions and/or hearing loss, adults do poorer on tests of working memory where the confounding effect of the participants' inability to understand the verbally presented tasks is eliminated by ''shadowing'' (that is, participants are able to repeat back what is said to them indicating reception of the spoken message). These results suggest that hearing loss imposes top-down, auditory perceptual processing requirements that result in a smaller pool of resources being available for other cognitive tasks, as consistent with Kahneman's resource capacity model. 127 Based on this model, the load on cognitive resources induced by hearing loss would lead to decrements in overall resource capacity, and these effects would be most pronounced under cognitive loads that overwhelm available cognitive resources (Figure 10 ). Of note, the effects of hearing loss on resource capacity in this model are independent of the progressive decrements in processing resources associated with aging. 106, 107 Consistent with this hypothesis, recent population-based cross-sectional studies have demonstrated that hearing loss is independently associated with both verbal and nonverbal measures of cognitive ability. 76, 77 \",\n       'Both conditions used the same textbook and accompanying materials. The materials were adopted by the district for eighthgrade science. Teachers also used the high-stakes test adopted by the state for guidance in selecting the most important content to emphasize.\\nControl Condition. Materials in the traditional instruction condition consisted of teacher lecture, class notes, laboratory-like class activities, and supplementary textbook materials. These materials consisted of worksheets that accompanied each chapter with fill-in-the-blank, matching, vocabulary, and short-answer items. Teacher-led presentations were accompanied with questioning, note-taking (with the assistance of an overhead projector), audio-and video recordings, and class activities.\\nExperimental Condition. We developed experimental materials for this investigation, including curriculum enhancements that taught the \"Scientific Investigation\" units of instruction, covering charts and graphs, measurement, independent and dependent variables, and qualitative and quantitative research methods. For example, one set of materials focused on creating charts and graphs from different types of data. Other materials presented research scenarios and required students to identify independent and dependent variables. Another set of materials required students to engage in different types of measurement from pictorial representations.\\nWe developed three levels of materials for each area such that differentiation of activities was possible within inclusive classes. For example, for the \"Quantitative/Qualitative\" activity within the Scientific Investigation unit, Level 1 materials required students to read a statement on a series of cards and identify whether it was a quantitative or a qualitative statement. Level 2 materials required students to generate three quantitative and three qualitative observations from each of a series of illustrations, with prompting when needed. Level 3 materials required students to generate quantitative and qualitative observations from illustrations, without prompting. For the \"Experimental Design\" activity, students were required to match independent with dependent variables (Level 1) and then produce relevant independent variables, dependent variables, and hypotheses for each given scenario, with prompts when needed (Level 2) and without prompts (Level 3). Names of all activities, with key concepts and goals, are provided in Table 1 .\\nEach level was represented by a different color folder, so that all students could work on the same content at their particular level of instruction. Yellow represented Level 1, which required identification of science concepts from an array of alternatives and contained supports and prompts to assist students. Blue represented Level 2, which required production responses of the information and contained some prompts. Red represented Level 3, which required production responses but did not include prompts. We designed all activities to be used as many times as necessary for mastery of the content. Each activity had explicit, easy-to-follow directions, and students worked in groups of two or three to complete the activities.',\n       '[45] This paper investigates the processes controlling the sea surface cooling induced by Tropical Cyclones (TCs). To that end, we use an ocean general circulation model forced from reconstructed wind perturbations associated with more than 3000 observed TCs over the 1978-2007 period. Reanalysis products usually used to force ocean models Figure 15. Same as Figures 13a-13f but for the wake warming phase DT WW (difference between T WW temperature averaged from day + 3 to day + 63 and T CW in the time integral of each ML heat budget term). strongly underestimate the amplitude of TCs wind forcing and the resulting TC-induced cooling. We developed an original methodology that allows realistic TC wind forcing based on an idealized vortex [Willoughby et al., 2006] to be included, constrained by observed TC characteristics (location, amplitude) and applied at each ocean model time step. [46] The statistics of the simulated ocean surface temperature response to TC compare reasonably well to satellite estimates. Average surface temperature anomaly is $1°C and extends typically over 5 radii of maximum wind (RMW). The modeled cold anomaly amplitude also agrees well with observations at individual locations (0.71 correlation), although the model tends to overestimate cold wakes associated with the strongest and slowest TCs. Overall, the good agreement between the model and observations allows us to estimate the contribution of various oceanic processes to TC cooling for a very diverse sample of observed cyclones over 1978-2007, providing a more general insight than case studies. [47] The amplitude of the TC-induced cooling depends on the strength of the TC forcing. Following Vincent et al. (submitted manuscript, 2011), we use the wind power index (WPi) as an integrated measure of the cyclone\\'s wind forcing. WPi is a proxy of kinetic energy transferred to the upper ocean by the cyclone, and integrates important parameters for the cold wake amplitude (cyclone size, intensity and translation speed). TC-induced cooling within 200 km of the TC eye increases linearly with WPi: vertical mixing at the base of the mixed layer explains from $30% of the cooling for weak WPi up to $80% for large WPi (above 2.75). Surface heat fluxes however overcome the mixing contribution for lowest WPis (for WPi < 2, surface fluxes contribute to $50%-70% of the cold wake). Away from the cyclone\\'s eye, latent heat fluxes contribute increasingly to the cooling: surface fluxes explain 50 to 80% of the weak cooling further than $250 km away from the track. [48] Lateral advection plays a modest role compared to mixing and surface fluxes. For the strongest and/or slowest cyclones, it can however explain up to 30% of the cooling to the right of the TC track. While mixing dominates the coldwake asymmetry for weak and intermediate WPis, our results suggest that the anti-symmetric pattern of along-track currents is the main contributor to the cooling asymmetry for the most intense cyclones (WPi > 4.5). This asymmetry is primarily related to the forward advection of cold wake water by geostrophic currents on the right side of the TC. While heat fluxes control to a large extent the damping of the CW in the months following the TC passage, our analysis also reveals that advective processes play a nonnegligible role, contributing to as much as 70% close to the TC track for the strongest TC wind forcing. [49] The pre-storm ocean state also modulates the amplitude of the TC-induced cooling. The Cooling Inhibition index (CI) is a measure of the ocean \"resistance\" to cooling by the TC (measured as the amount of potential energy required to cool the ocean surface by 2°C (Vincent et al., submitted manuscript, 2011)). Using this measure, Vincent et al. (submitted manuscript, 2011) showed that ocean background state can modulate the cooling amplitude by up to an order of magnitude for a given cyclone wind power input. We show here that this modulation is related to the increasing efficiency of mixing to cool the ocean surface when CI decreases. In the case of strong CI, the surface current kinetic energy is dissipated to produce vertical mixing but in this case, little cold water is entrained into the ML. In contrast, weak CI is usually associated to a shallow ML and/or steep temperature stratification below the ML before the TC passage, allowing mixing to efficiently incorporate cold water into the ML.',\n       'Traditionally, graph theoretical analysis has been applied to measure brain network topological features. In this study, three global network indices based on graph theory are investigated, including characteristic path length (CPL) [6] , network diameter (ND) [9] , and modularity (Mod) [7] .\\nBriefly, CPL can be understood as indicating a network with \"easily\" transferred information. It is the average shortest path length between all pairs of nodes in the graph, and is calculated as CPL = 1 N(N−1) i∈V,j∈V,i j d i,j , where d i , j is the shortest path length between nodes i and j. Note that infinitely long paths (i.e., paths between disconnected nodes) are not included in computations. ND is the greatest distance between any pair of nodes, and is defined as ND = max i∈V max j∈V d i,j . It enables understanding of the size of a network. A graph with a large ND and small CPL would therefore be considered an efficient network. Mod describes the extent to which a network has modules that differ from others, each of which is independent and functionally specialized [7] . Computationally, it is\\n, where i and j are individual modules in the set of all modules M, and c is the proportion of existing connections between two modules.\\nIn practice, we filtered the weighted network before computing these graph-based indices by only selecting the edges whose corresponding p-values passed through a statistical threshold (Bonferroni corrected p < 0.05) and then adopted the Brain Connectivity Toolbox (https://sites.google.com/site/ bctnet/) [7] for their implementation (step 6 (right) of Figure 7 ).',\n       ').',\n       \"Manual segmentation contours were converted to voxelbased representations, where each voxel was included if the contour enclosed at least half of it. For 3T data, a double slick thickness was used to account for alternate slice segmentation. A template database of high quality 3T scans and bilateral manual segmentations was created by a neurologist (GW) and radiographer (EW) reviewing the scans sequentially until 400 scans without artifacts (e.g., motion) and with reliable manual segmentations had been selected. This is significantly larger than previous template databases used in Alzheimer's disease, but was done in order to encompass a wide range of hippocampal pathologies.\\nSegmentation was performed using the STEPS (Similarity and Truth Estimation for Propagated Segmentations) algorithm (Cardoso et al., 2013) . This is a multi-atlas-based segmentation propagation method based on STAPLE (simultaneous truth and performance level estimation) (Warfield et al., 2004) but adapted to use a local ranking strategy (according to image similarity) for template selection, thus enabling reliable segmentation of hippocampi with variable morphologies, as may occur in HS.\\nFirst, group wise templates of the anatomic scans and hippocampal segmentations were created using a previously described method (Rohlfing et al., 2004) . The first iteration selects one arbitrary individual image as a reference and registers each of the remaining images to the reference using an affine transformation. Using these transformations, an average image is computed. In the second iteration, all individuals including the initial reference are registered to the average image by nonrigid transformations. A new average image is generated using the new transformations and used as the reference for the following registration iteration. The procedure is repeated until convergence.\\nNext, the subject scan to be segmented was nonlinearly registered to the group template using Fast Free Form Deformation, and the hippocampal segmentation templates were each propagated to the subject scan to give an approximate location for the hippocampus. A region-of-interest was extracted from the subject scan encompassing this region.\\nEach scan in the template database was coarsely registered to the region-of-interest, and the most representative 75 subjects were selected on the basis of the highest normalized cross-correlation between the subject scan and the registered template. A finer more accurate registration of these 75 subjects to the region-of-interest was performed, and the same transformations were applied to the manual hippocampal segmentations for these scans.\\nThe 15 most similar subjects at each spatial location were selected according to the locally normalized crosscorrelation over a Gaussian kernel with standard deviation of 2 voxels and fused using a probabilistic framework that iteratively estimates the most likely true segmentation and performance parameters (Fig. 1) . Spatial smoothness was enforced using a Markov Random Field (beta = 0.5).\\nThe entire process, from retrieving the images from the scanner database to the final hippocampal segmentations and volumes of each side, took approximately 30 min per subject using a Linux workstation with a quad-core Intel Xeon processor (Dell, Round Rock, TX, U.S.A.) running at 3Á20 GHz with less than a minute of operator time.\",\n       nan,\n       \"Adaptive management provides a blueprint for incorporating new data and to re-assess adaptation strategies. Probabilitybased predictions, such as those discussed in this article, are likelihood estimates, where the outcome depends on the input data and assumptions. As the data and information improves or is updated, the results correspondingly improve. Thus, models and outputs need to be reevaluated and data monitored for continuous improvement. Such an adaptive management approach presumes necessary technical, financial, and other important resources to support local planners and the community's capacity to adapt. We do not expect local jurisdictions to build their own shoreline model and project future erosion hazards zones. The construction of the probability-based shoreline model presented here involved significant technical and scientific resources built off years of data collection and research at the University of Hawai`i with input from coastal managers. In Maui, there has been a long-time collaboration among local planners, the State, University of Hawai`i researchers, and the Sea Grant College Program, and successful adaptation may hinge on such a network (Berke and Lyles 2013; Keys et al. 2013 ).\",\n       'DLB is the second most common neurodegenerative form of dementia after AD, but it is a rare disease with its prevalence in persons of 65 years and older having been reported to be about 7 in 1000 (McKeith et al., 2004) . In DLB, attention, visuo-spatial cognitive functioning and domains associated with subcortical areas are affected most prominently, while memory is not affected as severely. Further symptoms include predominantly visual hallucinations, fluctuating cognition, psychiatric symptoms, as well as parkinsonism and additional motor symptoms which need to be present within a year from the onset of cognitive symptoms, thus defining the border to dementia resulting from PD (Geser et al., 2005; McKeith et al., 2004) . For this review we included eleven papers including 223 healthy controls and 181 patients with DLB.\\nThe default mode network has been assessed in several studies on resting-state functional connectivity in DLB. Default mode network connectivity was reported to be unchanged in DLB with fluid cognition in one study (Franciotti et al., 2013) . In contrast there is evidence that parts of the default mode network are affected by changed connectivity (Galvin et al., 2011a; Kenny et al., 2012; Lowther et al., 2014; Peraza et al., 2015a) with overlap in reported regions towards the cerebellum and visual processing areas. Findings of patients with DLB displaying altered connectivity in visual areas and networks have also been reported in the literature (Peraza et al., 2014; Sourty et al., 2016) , suggesting that changes in this domain might be characteristic for the disease.\\nAdditional networks that have received attention are the right and left fronto-parietal networks. These have also been reported to display changes in connectivity in patients with DLB compared to healthy individuals (Peraza et al., 2015a) . One investigation found that the left fronto-parietal networks does not only display reduced connectivity, but that this also correlates with cognitive fluctuations (Peraza et al., 2014) .\\nFurther connectivity changes that have been reported in DLB affect subcortico-cortical connectivity (Kenny et al., 2013) , the sensorimotor network (Peraza et al., 2016) , the salience and executive control networks (Lowther et al., 2014) as well as the fusiform gyrus and pons (Borroni et al., 2015) , but regarding these findings the literature is lacking in number.\\nFinally, there is one report employing a whole-brain graph theory based approach. Here, alterations of connectivity are characterised by increased global efficiency, increased nodal clustering and decreased characteristic path length. Looking at local alterations, parietal and posterior temporal areas have been found to have decreased nodal degree in patients of DLB compared to healthy subjects (Peraza et al., 2015b) .\\nA large part of the body of literature available on DLB also includes a comparison against AD, but no consistent differences seem to emerge from such analyses (Franciotti et al., 2013; Galvin et al., 2011b; Kenny et al., 2012 Kenny et al., , 2013 Lowther et al., 2014; Peraza et al., 2016; Peraza et al., 2015b) . When comparing DLB against PD with dementiaboth diseases involve Lewy bodiesthe localisation of regional homogeneity alterations differs between the diseases (Borroni et al., 2015) , but seedbased analysis suggests no difference between both diseases (Peraza et al., 2015b) . This is concerning for potential diagnostic use of restingstate data, but comparisons on large datasets might yield more reliable results.\\nIn summary, the literature on resting-state functional connectivity in DLB is rather limited and inconclusive at this point. Connectivity alterations seem to be widespread, but so far there is no consensus regarding their localisation and extent. In particular, differentiation from AD or PD seems to pose severe problems as of now. Consequently, more research is necessary to characterise DLB in terms of functional network connectivity.',\n       'All analyses were run with R (version 3.3.3; The R Foundation). Nonparametric Mann-Whitney and χ 2 tests were used to determine group differences between the studies (HABS vs ADNI) on demographics and biomarkers. Mann-Whitney U tests determined unadjusted sex differences between tau regions and global Aβ. A series of hierarchical linear regressions were conducted to examine the influence of sex on the association between tau and Aβ, after adjusting for age and delay between tau and Aβ scans (model 1). There were some missing data for APOE genotype (n = 34 for HABS; n = 7 for ADNI), and as such, we ran separate analyses including main associations of APOE (model 2). The following analyses were run in the HABS and ADNI cohorts separately: 1. TauROI~Aβ+Sex+Age 2. TauROI~Aβ+Sex+APOE+Age 3. TauROI~Aβ×Sex+Age 4.TauROI~APOE×Sex+Age For the tau ROIs, we examined the EC, IT, the meta-ROI for tau, and 2 extratemporal regions (precuneus and superior parietal lobe). Models 3 and 4 are fully factorial.\\nEach model was compared against a prior model to determine goodness of fit using log likelihood ratio. We did not include sex × Aβ × APOE interactions as a stand-alone analysis in the current study owing to low statistical power; however, we included it as an exploratory meta-analysis estimate. We conducted post hoc analyses examining the influence of outliers using robust linear regression (using M estimation with Huber with the rlm package) on findings of interest. On models of interest, we probed the association of differing levels of Aβ burden on the percentage sex differences on tau retention. As extratemporal regions were used to test for specificity in the temporal regions, we refer to these as post hoc. We ran models of interest with non-PVC tau data for temporal tau regions, including an additional covariate of whole-brain gray matter volume, and included these in eTable 1 in the Supplement.\\nFor models of interest, we conducted exploratory linear mixed models of interactions of sex and regional tau on cognitive decline after adjusting for age and education, including random intercept and slopes (eTable 2 in the Supplement). To measure cognition, we used the Preclinical Alzheimer Cognitive Composite, 35 which has been applied across these cohorts in previous publications. 35, 36 The baseline cognitive time point was considered within 18 months of the tau scan; for HABS this resulted in up to 5 follow-up time points and for ADNI, up to 3 follow-up time points.\\nA final meta-analysis estimate was calculated for sex, sex × Aβ, sex × APOE, and sex × Aβ × APOE on EC tau in clinically normal older adults from both HABS and ADNI using the Metafor package, version 2.0 (R Project for Statistical Computing). In brief, all standardized β weights, along with their SEs, for each of the aforementioned estimates were run in the rma function to fit a meta-analytic fixed-effect model with a predefined weighted estimation (inverse-variance weights).',\n       'This was a single-centre moderately-sized cohort of patients diagnosed with COVID-19. We only examined patients who were hospitalised and could not examine patients in isolation facilities outside of our hospital. Additionally, owing to the nature of active J o u r n a l P r e -p r o o f case-finding in the dormitories, there was preselection of those without symptoms or with minimal disease from the migrant worker cohort. Clinical progress and outcomes were only measured within the hospital admission, and we were unable to longitudinally examine patients after discharge for medium to long-term sequelae of the disease, which has been described even amongst those with mild disease initially (Yelin et al. 2020) . In our hospital, at the time of data capture, deaths from COVID-19 had been attributable to COVID-19 if the patient has PCR-confirmed disease and died from overwhelming lung infection, sepsis, or acute respiratory distress syndrome.',\n       'Heavy precipitation days Heavy precipitation days > 10 mm (days)',\n       'American Indian and Alaska Native boys: early childhood risk and resilience amidst context and culture. . Sarche M, Tafoya G, Croy CD, Hill K. Infant Ment Health J. 2017;38(1):115-127. American Indian and Alaska Native (AIAN) adolescent and adult men experience a range of health disparities relative to their non-AIAN counterparts and AIAN women. The current article reviews sources of strength and challenge within AIAN communities for AIAN children in general, including cultural beliefs and practices that support development, and contextual challenges related to socioeconomic and health disparities and historical trauma affecting the AIAN population as a whole. The research literature on early development is reviewed, highlighting what this literature reveals about early gender differences.\\nAssociations between problem behaviors and early vocabulary skills among Hispanic dual-language learners in pre-K. Hagan-Burke S, Soares D, Resendez N, et al. Topics Early Child Spec Educ. 2016;36(2):91-102. This study examined the relations between problem behaviors and early learning outcomes among 138 children in dual-language pre-K programs who were identified at the beginning of the school year to be at risk for difficulties in early language and literacy development. Children\\'s expressive and receptive vocabulary, listening comprehension, and conceptual thinking skills were assessed at the beginning of pre-K and again at the end of the school year. Their problem behaviors (externalizing, bullying, hyperactivity, and internalizing) were assessed midyear via teacher ratings. With the exception of internalizing problem behaviors, bivariate correlations indicated virtually no associations between children\\'s entry-level academic skills and midyear ratings of problem behaviors. However, multilevel models controlling for student-and teacher-level variables revealed that midyear ratings of problem behaviors were statistically significant predictors of poor outcomes on several vocabulary-related measures administered at the end of pre-K. Objective: Background television (TV) exposure is harmful to young children, yet few studies have focused on predictors of exposure. This study\\'s objectives were to elucidate demographic, environmental, and behavioral correlates of background TV exposure in low-income Mexican-American preschoolers and to explore caregiver beliefs about the impact of such exposure. Methods: A convenience sample of low-income Mexican-American female primary caregivers of preschoolers (3-5 years old, n = 309), recruited in safety-net clinics, were surveyed by phone. Caregivers reported the frequency of their child\\'s exposure to background TV and responded to questions on the home media environment, TV use, and whether they had thought about background TV exposure and its impact on their child. Results: Background TV exposure was common; 43 % reported that their child was often, very often, or always exposed to background TV. More hours of TV viewing by the caregiver and greater frequency of TV viewing during meals were associated with an increased frequency of exposure to background TV. Only 49 % of participants had ever thought about the impact of background TV. Believing that background TV is not harmful was associated with higher levels of background TV exposure. Conclusion: Findings suggest that background TV exposure is frequent and caregiver awareness of its potential impact is low in low-income Mexican-American families. Beliefs that background TV is not harmful may predict risk of exposure. Potential targets for interventions focused on reducing background TV exposure in this population include increasing caregiver awareness of the potential negative impact of such TV exposure. The infant mental health field can amplify its effects when it extends its purview beyond the dyad to the larger contexts in which infants and adult caregivers interact and develop over time. Within health, mental health, education, and other human service organizations, the quality of relationships is a critical variable in the individual-level outcomes that such organizations seek. The goals of this work and the means for accomplishing them are highly dependent on human qualities and interactions that are shaped by organizational processes. In communities, too, processes that shape relationships also strongly influence child-, family-, and community-level outcomes. The Touchpoints approach to reflective practice can guide relational processes among professionals, parents, and infants in organizations and communities that influence these outcomes. NEW! The respectful supervisor: integrity and inclusion. 13 min. 2015. (DV0944). This DVD is restricted to HHSC staff and their contractors. This program will help supervisors understand their role in preventing harassment and discrimination while raising their awareness of unconscious bias and microinequities. Supervisors are shown how to avoid being bullies, how to act ethically, and how to be inclusive. Supporting cultural and linguistic diversity in early intervention and early childhood special education. 106 min. 2011. (DD0502). This DVD contains 4 sections that cover issues professionals will encounter as they work with culturally and linguistically diverse children and families. Plan activities and routines that reflect the diversity of children in the class (144 min.): educators work together to embed children\\'s home languages in everyday routines. Partner with families to support dual language learning (30 min.): viewers see first-hand how bilingual parents support children\\'s dual language development at home. Conduct culturally responsive early intervention: four culturally and linguistically diverse early interventionists share their experiences working with families of different backgrounds and give viewers practical guidance on helping families promote early language development. Collaborate with interpreters: viewers learn the characteristics of effective interpreters and how teachers and interventionists can partner with them.\\nFirst-generation\\nCultural\\nThis DVD explores common thinking habits to show how they easily lead to hidden assumptions, bias, and prejudice. Viewers learn that all cultures assume their ways are best and natural. Some simple habits of perception illustrate how \"people typing\" and stereotypes are extensions of how our brains work to make sense of the world. There are all different kinds of families and this book will help teachers feel more comfortable and be more effective in dealing with them. The book discusses honoring and working with diversity, how to deal with holiday issues, working with fathers, communicating and meeting with families, and gives tips for challenging conversations.\\nAnti-bias education for young children and ourselves. Louise Derman-Sparks and Julie Olsen Edwards, 2010. (275 D435 2010. This book aims to support children\\'s full development in our multiracial, multilingual, multicultural world and to give them the tools to stand up to prejudice, stereotyping, and bias. The book begins with a conceptual overview of an anti-bias education approach, then gives basic teacher tools, and finally provides specific curriculum ideas for various anti-bias topics.\\nBilingual language development and disorders in Spanish-English speakers. 2011 . (535 B595 2011. This book is intended to bridge the gap between research and practice by summarizing what is known about language development and disorders in Spanish-English bilingual children and relating it to implications for assessment and intervention.\\nBlindspot: hidden biases of good people. Mahzarin Banaji, 2013. (BF 575 B212b 2013. Blindspot is the author\\'s metaphor for the portion of the mind that houses hidden biases. Banaji questions the extent to which our perceptions of social groups, without our awareness or conscious control, shape our likes and dislikes and our judgments about people\\'s character, abilities, and potential.\\nChildhood bilingualism: research on infancy through school age. 2006. (WS 105.5 C8 C536 2006. This book addresses research on multiple aspects of bilingual development in infants and children, including oral language perception and production and literacy. The book is divided into three major parts: processing two languages; learning two languages; and literacy in two languages. Circles in the nursery: practicing multicultural family therapy. Leena Banerjee Brown, 2007. (275 B878c 2007. This book explains how to understand and practice multicultural infant-family mental health. The book includes clinical case studies with practical applications. A 7-part framework for analyzing family, infant, and clinician cultural perspectives is included. Ideas are given for improving and energizing clinical work with at-risk infants and families.\\nCultural competence for public managers: managing diversity in today\\'s world. Espiridion Albert, 2012. (275 B737 2012. This book offers guidance on how to become a leader in developing cultural competence in your organization. It provides a conceptual foundation and successful examples for developing cultural competence. Terms such as cultural competency are defined. Practical standards and performance measures, coaching and mentoring guides, checklists, and exercises are provided.\\nCultural competency for health administration and public health. Patti Renee Rose, 2011. (WA 18 R797c 2011. This book introduces the topics and tools necessary for the application of cultural competency processes in various healthcare settings. Students who read this book will come away with a systematic approach to the method of achieving cultural competence.\\nWith a focus on a broad spectrum of topics, race, ethnicity, gender, disability, and sexual orientation at the federal, tribal, state, and local levels, this book equips readers to better understand the complex, real-world challenges public administrators confront in serving an increasingly diverse society.\\nCultural reciprocity in special education: building family-professional relationships. Beth Harry and Maya Kalyanpur, 2012. (275 K14 2012. This book provides explicit training on working effectively with all families, no matter how diverse. It explains the concept of cultural reciprocity, which involves examining the provider\\'s own values, learning to respect each other\\'s differences, and collaborating to benefit the children.\\nDeveloping cross-cultural competence: a guide for working with children and their families, 4 th ed. Eleanor W. Lynch and Marci J. Hanson, 2011. (275 L987d 2011. This book offers practical advice for working with children and families of diverse heritage. With insight from their own racial, cultural, and linguistic backgrounds, the chapter authors contribute wisdom about the influence of different cultures on people\\'s beliefs, values, and behaviors. Their knowledge helps professionals learn how to embrace diversity in intervention services and foster respectful and effective interactions with people of many cultures. Diversity in early care and education: honoring differences. Janet Gonzalez-Mena, 2008. (LB 1139.3 G643d 2008. This book explores the rich diversity encountered in programs and environments for children, ages birth to 8, including those serving children with special needs. The emphasis is on the practical and immediate concerns of the early childhood professional and family service worker. It also discusses poverty, family expectations, and appropriate discipline. The diversity training activity book: 50 activities for promoting communications and understanding at work. Jonamay Lambert, 2010. (751 L222 2010. This guide presents a collection of 50 exercises that help participants to examine their assumptions and expectations about other cultures and to become more aware of their own culture and its values. Sections include diversity icebreakers; culture and diversity; change, communication, and conflict resolution; gender at work; and culture and career transitions.\\nDual language development and disorders: a handbook on bilingualism and second language learning. Fred Genesee, Johanne Paradis, and Martha B. Crago, 2010. (535 G327 2010. For professionals working with children who are bilingual or learning a second language during early childhood, it can be difficult to determine whether typical language development or a disorder is present. This comprehensive resource on bilingual and second language acquisition can help. This book dispels many myths about dual language development.\\nFamilies, infants, and young children at risk: pathways to best practice. Gail L. Ensher, 2009. (556 E59f 2009. This book explains the neurological and psychosocial development of children from birth to 8. It covers a full range of issues in early childhood special education including cultural diversity.\\nHealth literacy from a to z: practical ways to communicate your health message. Helen Osborne, 2013. (WA 590 O81h 2013. This book for healthcare professionals gives practical advice on how to communicate with patients and their families so that they all truly understand the message. It discusses the importance of taking into account different cultures and strong emotions. It gives tips on how to communicate using forms, technology, visual aids, and more.',\n       'The authors declare that they have no conflicts of interest.',\n       \"This secondary analysis has several limitations. First, the validity of the ACS question is unknown, although similar ACS questions had acceptable validity. 34,35 Whereas other analytic methods (e.g., propensity score matching) may help reduce bias, these methods cannot take into account the cohort's complex design. 38 ACS was assessed in kindergarten, but not in fifth grade. The contribution of ACS in fifth grade to the primary outcome cannot be evaluated, and the large time interval between kindergarten ACS and fifth-grade BMI z-score may be affected by unaccounted confounders. The ECLS-K lacked an objective measure of distance from home to school, or assessment of the built environment, which have been linked to children's ACS. [58][59][60] There were baseline differences between the active and passive commuting groups, and it is uncertain how these differences may have affected estimates. Regardless, longitudinal cohort and experimental trials are necessary to confirm and more precisely characterize these observational findings and explore associations with other important ACS outcomes.\",\n       'For analysis of island land area change; vegetated area change, and island transgression, Earth observation and ancillary data were acquired from the following sources: • ',\n       'This article examines the role of assessment in the World Bank\\'s Education Strategy 2020. A former World Bank education specialist commented that, \"expanding access has a thousand parents but quality is an orphan-politically speaking at least. We [the World Bank] also try to weigh in on the side of the orphan\" (Collins and Rhoads 2008, 177) . This comment referred to the political palpability of access to education, but the resistance toward considering educational quality. Based on what appears to be a global accountability movement in education, the parent/orphan analogy is no longer applicable. Tuning education standards, assessment, accreditation, the Bologna Declaration, and No Child Left Behind policies all have global applicability. The new World Bank Education Strategy 2020 defines strengthening an education system as reforming \"relationships of accountability\" and ensuring that results of learning are \"measured and monitored\" and ultimately linked to \"financing and results\" (18) .\\nWorld Bank strategies have focused on all sectors of education in recent decades, prioritizing primary education over other sectors at various times. For example, the higher education sector was undervalued when measured by simple individual rates of return (Task Force on Higher Education and Society [TFHES] 2000; Collins 2011 ). As a result, the move toward considering educational quality may be a welcome turn, but definitions and practices surrounding components of accountability have the potential to reinforce mechanisms of inequality, as opposed to liberation. Using discourse analysis, this article examines the language of global accountability as well as the recommended tools used to assess the quality of higher education as noted in the new World Bank Education Strategy 2020. This article concludes that intended learning outcomes often reflect ideological dispositions and when imposed on countries considered \"developing,\" have the potential to replicate the pattern of placing greater value on knowledge produced in \"developed\" countries. ____________________________ *Corresponding author. Address: 1776 University Avenue, Wist 224, Honolulu, HI, 96822, USA.\\nEmail: collins3@hawaii.edu.\\nThis trend may continue to relegate developing countries to the role of consumers in the knowledge economy.',\n       'We mapped the distribution and relative size of gull colonies from 2009 to 2011 with ArcMap version 10.2.2 (ESRI); values for categories of relative abundance were based on natural breaks in the data. Given that many islands on which gulls nest inland are ephemeral, their size or availability for nesting changing from year to year, we generally considered a colony as the whole of a site such as a lake or wetland for purposes of mapping or tabulation of data.',\n       'With the development of deep learning techniques in the big data era, there are opportunities for future deep clinical research. The fusion of heterogeneous data and the reasonable combination of different approaches have come to the foreground as promising ways forward. The integration of clinical data, genomics data, and social behavior data may make precision medicine reality just as we expected, realizing the goal is \"to provide the right treatment to the right patient at the right time\" [147] . Furthermore, various combinations of deep learning models can be expected to achieve better performance. In order to model an interpretable human-like computation system, it can be helpful to join deep learning methods with medical ontologies, rule-based systems, and traditional machine learning solutions. Moreover, developing systems which can perform real-time analysis on continuous vital signs would help medical staff observe life-threatening pathological changes in a timely fashion and provide appropriate treatment as early as possible.\\nIn conclusion, this paper provided a brief overview of deep learning applications on clinical data. It presented the categories of clinical data and their characteristics, an introduction to the common deep learning models used in clinical studies, a summary of the various applications, and a discussion of challenges and the further outlook. The paper\\'s aim was to provide valuable insights for researchers concerning clinical data studies. The characteristics of clinical data and the variety of types of data bring both opportunities and challenges. It is encouraging that deep learning methodologies have improved predictive models in many cases. However, the interpretability of such models remains an elusive goal. With a further understanding of deep learning architecture, there is the hope of better understanding the predictions and recommendations given by deep learning models. Collaboration with other approaches can result in greater achievements in clinical analysis and provide effective assistance to clinical decision making in the foreseeable future. ',\n       'Use a diagram or other model to demonstrate knowledge of science concepts, to illustrate a process, cycle, relationship, or system, or to find solutions to science problems.',\n       'Like all farm operators, most beginning principal farm operators are White, non-Hispanic, and male. Beginning farmers, however, are more likely than established farmers to be female, non-White, or Hispanic. There are distinctions in the relationship between operator demographic characteristics and whether the farm produces any agricultural product. Beginning farms without production are more likely to have a woman operator than those with production. This is true for established farms, as well. In contrast, non-White or Hispanic operators are less likely to be found on beginning farms without production. On average, beginning family farmers and their households earn less income from their farm, but more income from off-farm sources, than more established operators and their households. In fact, most beginning farms lost money farming in 2007. The average income of beginning farm households (from both farm and off-farm sources) is on par with the average income of established farm households ($87,004 compared with $90,866). The average incomes of both beginning and established farm households are signifi cantly higher than that of the average U.S. household ($67,609 in 2007) .\\nIn general, sources of household income vary signifi cantly based on the size of the farm a household operates. The larger the farm, the greater the farm income; the smaller the farm, the greater the off-farm income. This relationship between farm size and household income is true whether the farm is an established farm or a beginning farm. Unlike large farms, most small farm operators (less than $250,000 in gross sales) indicate that their primary occupation is something other than farming. Since a higher share of beginning farmers operate small farms and have a nonfarm occupation, the average beginning farmer household has higher off-farm income, but lower farm income than established farm households. When the value of farm production is less than $50,000, the average farm household loses money farming, regardless of whether they are beginning farmers or not (fi g. 3). Beginning farm households operating large farms, however, have lower household incomes than established farm households on large farms. Since beginning farmers are more likely than established farmers to operate small farms, they produce less of the total annual agricultural product-and less individual commodities-than the size of their population might otherwise indicate. Twenty-two percent of all farms were beginning farms, but they accounted for 10 percent of the value of all agricultural products in 2007 and less than 10 percent of the total land in farm operations. Beginning farmers accounted for more livestock than crop production (12 percent versus 7 percent) in 2007. Beginning farmers produce a variety of commodities, but their poultry production is noteworthy: beginning farmers account for 20 percent of total U.S. poultry production (fi g. 4). Poultry, cattle, and dairy combined represented more than half of the value of production of beginning farms in 2007 (fi g. 5). Nevertheless, more beginning farms producing agricultural commodities in 2007 specialized in beef cattle than any other commodity group. This is the case for established farms, as well (table 3) . ',\n       'We also used the GPCM to calculate two item statistics commonly reported in psychometric IRT studies: item difficulty (decomposed into an overall difficulty and category thresholds) and discrimination estimates (Embretson & Reise, 2000). Overall item difficulty is an average difficulty level that locates the item along the caregiver interaction spectrum. Estimates are on a logit scale, ranging between about −3 and 3, with lower (negative) values indicating easier items (which means that most caregivers are more likely to be rated with the higher categories) and higher (positive) values indicating harder items (which means that most caregivers are less likely to be rated with the higher categories). Discrimination estimates represent the degree to which an item can distinguish between caregivers of higher and lower quality, indicating the amount of information each item provides.',\n       \"The characteristics for men and women are shown in Table 1 . There were significant sex differences for all variables except fasting time (p = 0.35), education (p = 0.12) and APOE*E4 status (p=0.92).\\nUnivariate correlates of cognitive functioning Higher age and shorter time in education were associated with poorer verbal fluency, WLL and WLDR, and with increased RT and VC (Table 2 ). There was a strong inverse correlation between age and years of education (r=-0.51, p<0.0001). Higher HOMA-IR, HbA 1c , systolic BP, BMI, triacylglycerol and BDI scores and lower HDL-cholesterol levels (more consistently among women) were associated with poorer cognitive functioning in both sexes. Higher non-HDL-cholesterol and lower physical activity levels were associated with poorer cognitive functioning in women only (Table 2) .\\nMultivariate correlates of cognitive functioning The final model of our linear regression analysis is shown in Table 3 .\\nHigher HOMA-IR was highly significantly associated with poorer verbal fluency in women (p<0.0001) but not in men (p= 0.56). Higher HOMA-IR was also associated with a slower reaction time on the RT test in the whole study group (p=0.02) (data not shown). HOMA-IR was not associated with the other cognitive tests (i.e. WLL, WLDR, or VC). APOE*E4 genotype was associated with lower scores on the WLL (p=0.004) and WLDR (p=0.005) tests in women, and with a faster response time on the RT test in men (p=0.003).\\nAPOE*E4 modulated the association of HOMA-IR with the verbal fluency and WLDR scores, as the interaction term 'APOE*E4×HOMA-IR' was statistically significant for these tests. The association of HOMA-IR with these test scores was analysed separately in APOE*E4 -positive and -negative individuals. HOMA-IR was associated with poorer verbal fluency in APOE*E4 -negative individuals (p=0.0003), but not in APOE*E4 carriers (p=0.28). HOMA-IR was not associated with WLDR scores in either group (both p>0.26).\\nTo compare the association of long-term glucose homeostasis and insulin resistance on cognitive function, we \",\n       \"As described earlier, the students' classification based on the TIMSS 2011 international benchmarks in mathematics was carried out. Table 7 shows that the frequency distribution of students reaching the different international benchmarks based on the Fig. 3 . Matching of NEPS scores (x t ) and the equivalents on the TIMSS scale using the equipercentile equating method. TIMSS test and based on score equivalents are similar. The maximum difference between TIMSS based results (37.7%) and score equivalents (29.3%) arises to 8.4% on the high international benchmark. Additionally, the classification consistency was calculated. In 55% of the cases the same international benchmarks were classified. The classification consistency on average is 36%.\\nThe k = 0.35 shows that the distribution of the individual linking is a good approximation. Also for the IRT-linking about half of the students are classified into different proficiency levels based on the score equivalents. Therefore, the classification on an individual level should not be used for interpretation. Table 6 provides descriptive statistics for NEPS mathematics scores, TIMSS mathematics scores, the score equivalents of the NEPS test on the metric of TIMSS based on the equipercentile linking, and the score equivalents of NEPS on the metric of TIMSS based on the IRT linking.\",\n       \"As the literature (e.g., Klassen & Chiu, 2010) suggests, teacher self-efficacy should be related to job satisfaction and this should partly explain individual differences in job satisfaction. This relation between self-efficacy and job satisfaction is hypothesized to vary among schools as teachers are nested in schools of varying SES. This study hypothesizes that school-level SES will partly explain the differing effects of teacher self-efficacy on job satisfaction. More concretely, teachers at higher-SES schools are expected to have higher job satisfaction than those with the same level of self-efficacy who work at lower-SES schools. This is because teachers in higher-SES schools (a) teach students who tend to have shared values regarding education and do well academically, and (b) can more easily observe a link between their teaching and students' growth, implying that their self-efficacy in teaching can lead to higher job satisfaction.\",\n       'The general goals of environmental education are to deepen knowledge about environmental problems, to develop cognitive skills for research and to develop awareness and attitudes towards the environment (i.e., environmental literacy). These goals are difficult to achieve only during regular class hours of several teaching subjects. In eco-schools, in which the program was adopted, the full achievement of the general objectives of environmental education also failed. These objectives could be attained by way of realistic, active class work oriented towards problem solving. Therefore, in this paper, a Model suitable for providing students with tools to identify ecological issues, to use existing knowledge of natural sciences in the consideration of an ecological problem and to explain phenomena scientifically was developed and applied. The didactic material \"It Happened, What\\'s the Problem?\" made possible new knowledge of science to add to that existing. The test with non-continuous text \"A Guide through the Problem\" and the following discussion enabled the exercise of applying knowledge of chemistry, giving scientific explanations, generalizations, whereby understanding of the essence of the studied problem was realized. All the achieved results, over 70 % of correct responses, indicated that such a method of work had been accepted. With such an approach, environmental education has a chance to encourage action competence in pupils, which is the basis for the development different behaviors and attitudes.\\nEnvironmental education in practice is completely in the hands of individual teachers, its realization depends on how prepared they are to adopt their subjects to environmental education. The examined model could help the teachers in the preparation and realization of their classes. Considerations of the ecology contents provide great possibilities for classroom knowledge to become applicable in real life.',\n       'Six published manuscripts resulted from the work of the EWG. Each manuscript focused on a different topic and had its own unique selection criteria for included studies (Table 2) .',\n       \"In our published eGWAS [22] , there were three probes on the WG-DASL platform that were used to measure MAPT levels: ILMN_1710903 and ILMN_2310814 that anneal to different regions of the MAPT 3'UTR and ILMN_2298727 that anneals to Exon 4a (Additional file 1: Figure S1 ). Given that the inclusion of exon 4a in tau transcripts in the central nervous system was not reported previously, we generated a quantitative PCR assay against this exon, and were able to successfully measure it in the human brain (data not shown). All three probes passed our QC threshold of detectability in >75% of subjects, with ILMN_1710903 and ILMN_2310814 detected in 100% of all AD brains tested in both the cerebellum (CER) and temporal cortex (TCX) and with ILMN_2298727 detectable in 98.0% of AD CER and 83.7% of AD TCX tissue. We previously estimated intraclass coefficients [28] for all gene expression probes, which represent the percentage of variance in expression between samples over total variance and which reflect the genetic component that contributes to variability in gene expression. We determined that both ILMN_2298727 and ILMN_1710903 had high ICC estimates of 87%, whereas ILMN_2310814 had a low ICC estimate of 18%. The variances of gene expression estimated from all subjects in our eGWAS of cerebellar tissue (n = 374) [22] revealed consistent findings for these three MAPT probes, with both ILMN_2298727 (0.24) and ILMN_1710903 (0.12) having variance estimates that are~an order of magnitude greater than that of ILMN_2310814 (0.03). We thus conclude that ILMN_2310814 is unlikely to be an informative probe.\\nWe previously annotated all our probes for variants in their sequence [22] , given the concern that such variants may result in differential binding of probes with artifactual variance in the expression levels, and therefore could result in false positive associations with genetic variants in LD with probe sequence variants [36, 37] . Our annotation detected two variants within the probe sequence of ILMN_1710903 (rs67759530, rs66561280) that were also polymorphic in our autopsied AD series. ILMN_2310814 did not have any variants within its probe sequence. ILMN_2298727 annotation identified rs73314997 within its sequence, although this variant was essentially monomorphic in our eGWAS subjects [22] . Thus, of the three MAPT probes assessed in our gene expression analyses, ILMN_2310814 is unlikely to be informative and ILMN_1710903 may be prone to artifactual results. We therefore focused on ILMN_2298727 in our MAPT expression analyses (Tables 3 and 4 ), although we show results from all 3 MAPT probes for completeness.\\nEvaluation of the six MAPT SNPs revealed significant associations between ILMN_2298727 and rs1467967, rs242557, rs8070723 and rs7521. The MAPT H2 haplotype tagging rs8070723 was associated with lower MAPT levels in both CER (β = −0.16, p = 0.002) and TCX (β = −0.20, p = 4.9E-04) of LOAD subjects (Table 3) , as we previously reported in this cohort [22] . The other significant variants were associated with higher MAPT levels in both brain regions. Interestingly, the same variants showed associations in the same direction with the ILMN_1710903 probe, although with higher levels of significance.\\nFifteen MAPT haplotypes with frequencies >1% were identified in the autopsied LOAD subjects with complete genotypes for the 6 variants (n = 178). There was globally significant haplotype association with the TCX gene expression levels measured with ILMN_2298727 (p = 0.004) ( Table 4) , that may be a reflection of the significant MAPT H2 association. MAPT H2 haplotype, as expected, was associated with lower CER (β = −0.16, p = 0.003) and TCX (β = −0.20, p = 0.001) MAPT levels. MAPT H1b was marginally associated with higher TCX levels (β = 0.13, p = 0.058), I with higher CER MAPT levels (β = 0.20, p = 0.07), and L with lower TCX MAPT levels (β = −0.33, p = 0.009). Significant associations with similar directions of effect were also observed with ILMN_1710903 and MAPT H2, H1b and I haplotypes.\",\n       'Several sets of synthetic data were created for this modeling effort using SAS-JMP® on a Macintosh® computer. Three cover fractions were created (f1, f2, f3). To insure a high probability of a pure pixel for any of the three surface cover types, and evenly distribute the potential for any one cover type to be either high or low in any given pixel, they were calculated by first generating a random variable, x1, that was between 0 and 1. where the e\\'s represent random variation in the endmembers and \\' sisi represent the bright and dark soil spectra, s wi , and s vi represent standard deviations of water and vegetation, and wi and vi represent the reflectances of water and vegetation. The non-linear model used is a modification of one of the models of Borel and Gerstl (1994). Borel and Gerstl\\'s model incorporated one cover type (soil) beneath the vegetation. This model allows the substrate to vary between 100% soil and 100% water in each pixel. As a first-order approximation, each pixel in this model is treated as though the mixture of soil and water is completely homogeneous and constant across the pixel. The reflectance immediately above the leaf layer is given by: The individual components R v and R g are given by: where ws = w f w /(f w +f s ) + s f s /(f w +f s ). R v and R g represent the reflectances of the leaf layer and the substrate, is the transmissivity of the leaf layer, f v is equal to LAI (Leaf Area Index) for LAI less than 1, ws is the average reflectance of water and soil in any pixel. Borel and Gerstl (1994) also modeled reflectance for LAI 1, but that model was not applied here. Datasets of ten thousand points were created using the non-linear model and unmixed using PCI\\'s unmix algorithm with two different procedures. In one procedure, the principal components were calculated, endmembers selected in PCA space, and then the endmembers were used in TM space to unmix the pixels using bands 3, 4, and 5. This is referred to as the \"PCA\" procedure. The second procedure was to run the NDX transformation on TM bands 3, 4 and 5, use the NDWI and the NDSI to select endmembers and then unmix the data in the NDX data space (referred to as the NDX procedure).',\n       'In summary, the main contributions of this paper are as follows:\\n• A readily available hippocampus segmentation methodology consisting of an ensemble of 2D CNNs coupled with traditional 3D post processing, achieving state of the art performance in public data and using recent advancements from the Deep Learning literature.\\n• An evaluation of recent hippocampus segmentation methods in our Epilepsy dataset, that includes post-operatory images of patients without one of hippocampi. We show that our method is also superior in this domain, although no method was able to achieve good performance in this dataset, according to our manual annotations. This paper is organized as follows: Section 2 presents a literature review of recent Deep Learning based hippocampus segmentation methods. Section 3 introduces more details to the two datasets involved in this research. A detailed description of our hippocampus segmentation methodology is in Section 4. Section 5 has experimental results from our methodology development and qualitative and quantitative comparisons with other methods in HarP and HCUnicamp, while Sections 6 and 7 have, respectively, extended discussion of those results and conclusion.',\n       'It can be hypothesized that silver nanoparticle toxicity is due to the attachment of the AgNMs directly to the viral protein surface. 76 Hence, proper surface modification can be carried out by investigating the exact interacting site. It has been shown that silver nanomaterials are broadspectrum antiviral agents and can efficiently reduce viral infectivity when applied to cultured cells. 76 It has been reported that these nanomaterials are highly cytotoxic to mammalian cells because of their interaction with biomolecules that generate reactive oxygen species by interfering with defensive antioxidant mechanisms, thus posing harmful effects that damage lipids, proteins, and DNA through oxidation. 121 An in vitro study revealed that the toxicity level of such nanomaterials varies depending on the dose. Although the use of nanomaterials is still debatable due to their toxicity or side effects on normal human cells, silver nanomaterials provide a promising means to carry antiviral or other drugs throughout the body. In order to reduce toxicity, surface modifications of the nanoparticles need to be made so that the metal surfaces do not directly attach to cells. Also, their concentration in the interior of a cell should not be high within a particular cell compartment. Although the progression of nanoparticle research is presently ongoing, the detailed mechanisms of nanomaterial actions are not clear yet, which requires improvements with respect to their safety in order to optimize clinical advancements.',\n       'A 1997 EPA report found that roughly 28% of annual pesticide use in the United States is by residential, commercial, industrial or public entities (Aspelin, 1997 The widespread use of the insecticide diazinon on lawns and other urban settings has affected water quality in the Trinity River basin. Wastewater from a number of city sewage treatment plants…can fail monthly toxicity tests because diazinon has reached the system through runoff and is not removed by the treatment plant (p.17)\\nThe report also highlighted the use of pesticides by the Texas Department of Transportation in right-of-way maintenance for state highways, as well as the use of aquatic herbicides by the Texas Parks and Wildflife department, lake managers, golf courses and individual homeowners as other areas for concern.\\nWhile we know that agricultural pesticide usage is highest at the beginning of the growing season and drops off considerably by mid-summer, it is not clear whether this is true for commercial/residential pesticide usage where aesthetic concerns, turf preservation and insect control may result in continued applications into fall. It is difficult to obtain any information on pesticide applications by month and reason for application, and none exists for Texas. California does, however, require registration of pesticide use. In the pesticide data from California, we find that a larger fraction of commercial/residential pesticide use occurs in the fall months compared to agriculture. Specifically, in the California data, 52% of all agricultural pesticide application occurs May through August, but only 16% occurs September through November. In contrast, 44% of all non-agricultural pesticide application occurs May through August, and 24%\\noccurs September through November.',\n       'A single set of targeted recommendations for each watershed was determined by the most frequently occurring practices in each HRU in the final generation of the optimization. The cutoff threshold for frequency of a practice in a given HRU was chosen to be 50% of the final generation in Little Pine, and 25% of the final generation in Little Wea, because these thresholds provided a reasonable number of recommendations to bring to farmers in followup interviews.\\nWe brought a total of 202 targeted conservation practice recommendations on 125 farm fields to ten farmers in follow-up interviews (Table 4) . Twelve of these practices were removed, primarily due to many small parcels modeled as cropland that were not in fact cropland caused by errors in the NASS land use data. An additional 14 targeted recommendations had already been implemented in those lands, but their presence had not been conveyed in the initial interviews. At least one of these had been implemented in the time between the initial interview and the followup interview, and one farmer mentioned that it would have been desirable to have checked back with farmers immediately prior to optimization to obtain the latest information. Some other practices that had not been communicated in the initial interview the farmer referred to as degraded filter strips or grassed waterways, and perhaps they simply had not thought they were worth mentioning at the time. The remaining 176 adjusted targeted recommendations on 103 farm fields were used to assess farmer response to targeted conservation (Table 4) .\\nMost targeted conservation recommendations were filter strips or wildlife habitats, cover crops, and grassed waterways (Table 5 ). Only three instances of no-tillage were present in targeted recommendations brought to farmers, as the model generally found notillage reduced all three water quality constituents less efficiently than the other practices. We recommended creation of only three wetlands to farmers, due in part to the small number of farmers who would consider creating wetlands on their farms and in part to the limited number of locations for placement of wetlands; study watersheds yielded only 47 possible wetland locations but 631 corn and soybean HRUs where other conservation practices could be placed. A fourth wetland recommended to a farmer was found to already exist adjacent to the field it was targeted for, and the farmer remarked that the suggested wetland area was near to the size of that existing wetland, which serves as anecdotal confirmation of the wetland placement method.\\nWhen we asked farmers if they considered a particular practice to be optimal on that land, some were unsure how to answer the question. When they asked \"optimal by what measure?\" the interviewer responded by the measures used in this study: cost and water quality improvement. Some understood \"optimal\" to indicate practicality of use on their farm, and when they asked for clarification, the interviewer replied that \"optimal\" means a best practice for the land regardless of practicality to the farm, since practicality would be captured by the adoption question. Because of this difference of opinions on the meaning of optimal, these results indicate a measure of goodness of fit, but by a variety of measures. Nevertheless, rates of adoption intention and stated optimality clearly tracked with one another (Table 5) . Farmers considered certain conservation practices more optimal than others, and they generally expressed an intention to adopt them in proportion to their stated optimality (Table 5 ). A few farmers receiving recommendations of no-tillage and wetlands consistently considered these practices to be nonoptimal and did not intend to adopt them, yet they were recommended in so few cases that this result is not generalizable to the watershed.\\nCover crops had the highest stated optimality rate (70%) and the highest adoption intention (57%), which was initially surprising, since no-tillage and grassed waterways had higher farmer preferences in initial interviews. However, in the year between initial interviews and follow-up interviews, the study area had seen growing interest in and adoption of cover crops. Indeed, one farmer who had previously given cover crops a \"no\" for future adoption preference (Table 2 ) stated multiple times during the follow-up interview that he had expected cover crop recommendations. His interest in cover crops was also surprising as he had no adoption intention for any of the targeted recommendations in that interview. Following the interview, we explained that cover crops had not been placed on his lands due to his view one year prior, and agreed to send him updated results including cover crops in the optimization for his lands. Such a shift in views on cover crops is likely due to greater adoption by neighbors (which this farmer mentioned), education about growing cover crops, and the severe drought in 2012.\\nGrassed waterways were the second most accepted practice, at 50% stated optimality and 38% adoption intention, including many existing grassed waterways that required reconstruction (these existing grassed waterways were not removed as \"results already implemented\" because farmers agreed they needed reconstruction). Filter strips and wildlife habitats were combined in the interviews because the first interviews showed that farmers were not comfortable with the suggestion of filter strips on lands lacking open waterways, and as they were simulated the same in SWAT, it made sense to combine them to provide greater flexibility to the farmer. Farmers considered only 30% of these filter strips to be optimal on targeted lands, and expressed an intention to adopt only 19%.\\nSome farmers received many more recommendations than others. The farmer who received the fewest recommendations was given just three results on three fields; the farmer who received the most was given 44. This discrepancy was due primarily to the constraint of future preference; those farmers who were unwilling to implement many practices had few options and their land was less likely to be targeted in this adaptive process. Another factor was variability in farm size. Some operations were as small as 30 ha in the study watersheds and others as large as 600 ha. Six farmers considered at least 50% of adjusted targeted recommendations to be optimal, while the two farmers who received the fewest recommendations thought none were optimal. At least one farmer who adopted few recommendations shared that he viewed the practices to be infeasible because as a manager of a research farm he was not always at liberty to make decisions on removing land from production, changing tillage practices, and cover cropping that would interfere with researchers\\' goals. Farmer-specific adoption intention rates varied from 0% adoption intention, with 100% in the \"no\" category, to 71% adoption intention (17 of 24 targeted practices). Seven farmers had greater than 10% adoption intention. Farmers were also given the option to suggest a conservation practice that was more optimal for a given farm field than the recommendation. Farmers suggested cover crops would be more optimal than the recommendation on nine fields, grassed waterways would be more optimal on three fields, and filter strips on one field. ',\n       'In spite of the lack of relation between atherosclerosis and AD pathology, we observed a relation between intracranial atherosclerosis and dementia that was independent of the presence of cerebral infarcts. As shown in Figure 3 and Table 2, increasing intracranial, but not aortic or cardiac, atherosclerosis significantly increased the odds for dementia. Univariate odds for dementia increased by a factor of 2.0 per unit increase in intracranial atherosclerosis grade or increased by a factor of 2.7 for any grade other than 1. The magnitude of the effect was not changed by including age, sex, AD pathology, stroke risk factors, and, most importantly, the presence of cerebral infarcts (including microscopic infarcts) as covariates (Table 2B). To further verify that the effect of intracranial atherosclerosis on dementia was independent of infarction, we analyzed the data from the 110 subjects without any cerebral infarcts (Table 2B) and found the same result. Given the large number of individuals in our cohort with intracranial atherosclerosis grades greater than 1 (136/200), the population attributable risk of dementia related to intracranial atherosclerosis (independent of infarction) is substantial, in spite of the relatively modest odds ratio. The rate of dementia in subjects with atherosclerosis above grade 1 is 82/136; in subjects with grade 1 intracranial atherosclerosis it is 22/64. Given the total incidence of dementia in the cohort (104/200), the percent of dementia that is attributable to intracranial atherosclerosis, independent of cerebral infarcts, in this cohort is 34%. Looking at the same data using a multivariate logistic regression model (Table 2C), a intracranial atherosclerosis grade greater than 1, a composite AD pathology score >3, and any brain infarct were each independent predictors of dementia. While AD pathology was associated with the largest increase in the odds of dementia, intracranial atherosclerosis was still associated with a substantial increase in the odds of dementia, with a significantly higher population at risk than either stroke or high AD pathology score.',\n       'No interference of age or medication adherence in blood pressure control was reported in the prospective cohort of hypertensive patients. In fact, the higher the number of medications in use, the greater was the chance of having blood pressure control out of goals.\\nFemale gender was associated with a 3.1 increase in the odds ratio of poor blood pressure control. Compared with non-diabetic hypertensive patients, the hypertensive diabetic patients had lower chances of poor blood pressure control. A possible reason for this last finding includes higher levels of attention to the control of comorbidities by the multidisciplinary team.',\n       \"We test our hypotheses using mixed-effects multilevel models (Raudenbush and Bryk 2002). These models account for the fact that individual observations in our data are clustered within countries and therefore not independent. We provide a more detailed and formal description of our analysis model in Section A of the online appendix. Here we only summarize its main features. The dependent variable is the numeracy score, and the focal individual-level predictor is a dummy variable for having intermediate qualifications. To assess the impact of country-level factors on the skills gap between less-and intermediate-educated adults, we include cross-level interaction terms between having intermediate qualifications and the country-level variables. Effects of the contextual factors on the mean skills of less-educated adults are captured by the main effects of the country variables (i.e., by their effects on the intercept). The main effect of a country-level variable plus the cross-level interaction term yields the predicted effect on the mean skills of intermediateeducated adults. As individual-level control variables, we include gender, age, and migration/language status. Current applications of mixed-effects models in country-comparative settings tend to specify the effects of most individual-level variables as fixed-that is, as invariant across countries. This is usually implausible, and recent simulation evidence suggests it can lead to substantial efficiency losses (Heisig, Schaeffer, and Giesecke 2014). We therefore specify random slopes for all individuallevel variables. We estimate all models by restricted maximum likelihood in R (R Core Team 2014), using the lmer function from the package lme4 (Bates et al. 2014). All random effects are assumed to have means of zero and follow a multivariate normal distribution with covariance matrix S. We estimate degrees of freedom for confidence intervals and hypothesis tests using Satterthwaite's approximation (as implemented in the package lmerTest; Kuznetsova, Brockhoff, and Bojesen 2015) and Barnard and Rubin's (1999) adjustments for multiply imputed data. Simulation studies (Heisig et al. 2014; Stegmueller 2013) suggest that the standard errors and associated p values of mixed-effects models can be downward biased when the number of higherlevel units is below 20 to 25, especially when there are multiple random effects. We have only 18 cases in our application. To assess this issue, we replicated the main regression models using a two-step procedure that yields correct standard errors but is feasible only for the cross-level interaction terms (details of the method and results are presented in Section B of the online appendix). The two-step estimates of the cross-level interaction effects of external differentiation and vocational orientation do not indicate major problems with the standard errors and p values reported in the main article, suggesting that the significance levels reported in Table 4 are correct (see the Results section).\",\n       nan,\n       'The Newcastle-Ottawa Scale 30 was used to assess the quality of the studies and the mean score was 5 (online-only Data Supplement). The majority of studies [8] [9] [10] [11] [12] [13] [14] [15] [16] 19, [22] [23] [24] [25] [26] [27] 29 used participants that were representative of the population they were drawn from, that is, overweight, obese, had type 2 diabetes mellitus, or chronic kidney disease. Five studies 17, 18, 20, 21, 28 only included men or women and therefore are only somewhat representative of the population they were drawn from. 31 Only 3 studies 9,12,24 were randomized controlled trials and the control groups were drawn from the same population because the intervention group and the baseline characteristics of the groups were comparable. The other 17 studies 8, 10, 11, [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [25] [26] [27] were uncontrolled and reported only before and after intervention data and thus do not control for time-related changes. The randomized intervention trials did not report the method of randomization or blinding. All the studies had a followup time of >1 month. Twelve studies [10] [11] [12] 14, 16, 18, 19, [22] [23] [24] [25] [26] had a follow-up rate of >75% or were able to demonstrate that the characteristics of the participants lost to follow-up were not different to the completers. Five studies 8, 13, 15, 28, 29 had a loss to follow-up rate of >25% and 5 studies did not provide a statement about participant flow. 9, 17, 20, 21, 27 The loss to followup rate was the main difference between the studies and when a sensitivity analysis was conducted to compare studies with a follow-up rate >75% to studies that had a lower follow-up rate or the participants flow was not reported there was no statistically significant difference in the standardized mean difference (SMD).',\n       'We conducted experiments to compare the performance of the SpaCy and Keras-LSTM NER models; compare the performance of the models against humans; determine how training data influences model performance; and analyze human and model errors.',\n       'Aiming to derive the genomic divergence of SARS-CoV-2 across different countries/territories, 540 complete genome sequences with annotated information were retrieved from NCBI-Virus datahub (https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/). The NCBI refseqs are supplied through three distinct pipelines (computed annotation, Entrez genomes, and LocusLink supported pipelines) curated on an ongoing basis [30] . The NCBI viral genome resource curates the submitted viral genome sequences with forced recalibration of the data to better provide extant sequence representations with enhanced reference information. This, in turn, increases the emphasis on leveraging the genome sequence data [31] . Such validated SARS-CoV-2 genome sequences were retrieved in FASTA format as available on 12 April 2020, and the sequence information is provided as supporting information (Files S1 & S2). To date, there have been several studies that have conducted phylogenetic analyses of SARS-CoV-2 based on a varying number of genomic sequences [15] [16] [17] [18] [19] [20] [21] [22] [23] . Phylogenetic analysis of 160 SARS-CoV-2 genomes revealed three central variants distinguished by random amino acid variations [24] . A four-genome phylogeny from Chile revealed two different viral variants coming from Europe and Asia [25] . The characterization and phylogenetic analysis of the first three genomes from Italy revealed a single amino acid variation in the surface glycoprotein [26] . A report from Europe on the phylogenetic analysis of two SARS-CoV-2 genomes revealed the introduction of novel variants describing the initial stages of viral evolution [27] . The Nextstrain database is continuously updating the information on the phylogenetic analysis and genomic divergence of SARS-CoV-2 with concomitant updates on the evolutionary changes [28] . Phylogenetic frameworks are essential tools to identify virus lineages that contribute to their active spread. Rambaut et al. proposed a rational and dynamic virus nomenclature for naming the expanding phylogenetic diversity of SARS-CoV-2 [29] . Their method was made tractable by constraining the number and depth of hierarchical lineage labels and focusing on active virus lineages that are spreading to wider locations. This nomenclature will assist in tracking and understanding the patterns and determinants of the global spread of SARS-CoV-2. In addition to the phylogenetics and divergence, in our current study we aimed to derive the impact of the observed evolutionary changes on the protein functionality and therapeutic interventions. The evolutionary analysis of 540 genomes from 20 different countries/territories described here allowed us to detect a greater number of variations, and correspondingly, the potential impact of such variations on the protein sequences and their implications on vaccine development.\\nAiming to derive the genomic divergence of SARS-CoV-2 across different countries/territories, 540 complete genome sequences with annotated information were retrieved from NCBI-Virus datahub (https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/). The NCBI refseqs are supplied through three distinct pipelines (computed annotation, Entrez genomes, and LocusLink supported pipelines) curated on an ongoing basis [30] . The NCBI viral genome resource curates the submitted viral genome sequences with forced recalibration of the data to better provide extant sequence representations with enhanced reference information. This, in turn, increases the emphasis on leveraging the genome sequence data [31] . Such validated SARS-CoV-2 genome sequences were retrieved in FASTA format as available on 12 April 2020, and the sequence information is provided as supporting information (Files S1 & S2). ',\n       'In this section, we propose a quadratic optimization model for learning the kernel matrix for NCuts type objectives, and then derive its SDP relaxation. First, let us reexamine the multi-ratio optimization problem (7) -observe that (7) can also be (equivalently) written as\\nSince (|X |−1) is a constant (independent of optimization), (8) and (7) have the same optimal solution. Let us create a set of training example \"pairs\",\\nNow, (8) can be modified as\\nRecall that in the single ratio case, the optimal solution of the above problem can be recovered by minimizing a function defined by the gap, say δ, between the numerator and the denominator. We follow its natural extension by minimizing the sum of all gap functions as follows.\\nUsing J gh = (A gh − B gh ), the model can be written as\\nwhere ≥ 0 implies entry-wise non-negativity. If we define J = g =h J gh and let Q = (J + J T )/2, then we can derive the following quadratic optimization problem\\nThe above problem is also called the standard quadratic programming problem (StQP in short) in optimization [23] , and the problem of checking the co-positivity of a given matrix in linear algebra [24] . The problem is NP-hard [25] for Q 0; several algorithms have been proposed [23; 26] that consider various convex relaxations for StQP. Different from [23; 26] where the authors are interested in tightening the lower bounds to improve the relaxation, we will instead pursue a SDP relaxation for the StQP [27] , which will offer certain advantages. Let Z = αα T . It follows easily that \\ns.t.\\nNote that we have removed the constraint on vector α in the above SDP relaxation. We next present a rounding procedure to recover a feasible solution α to the StQP from a solution of the relaxed problem (14) .',\n       \"A third line of research would focus on improving the achievement measures included in the SASS student achievement subfile. The linkages used for the analyses presented in this report were based entirely on the means, standard deviations, and correlations between State NAEP and state assessment school means. The errors in these linkages can be diminished significantly by more detailed analysis of the relationships among the scores. In particular, current research by NCES has found that linkages to NAEP can be improved by considering nonlinear terms and by including demographic indicators. For example, all state reading assessments are sensitive to racial/ethnic differ--tit ences, but some are more sensitive than others. Their sensitivities could be matched to NAEP's measurement of the distribution of racial/ethnic achievement differences by explicitly including that matching factor in the NAEP adjustment step in constructing the SASS school-level achievement score. The result would be increased comparability of within-state variation in the achievement measure across states.\",\n       \"Small-area estimation by synthetic regression was a 2-step process. First, we used generalized estimating equations for logistic regression to analyze the association between recent mammography and the race-age variable, producing a predicted prevalence for each race-age category. The model accounted for the clustering of women by county and the complex BRFSS survey design and included both individual-and county-level confounders:\\nwhere Y ijk is recent mammography for women (i) who reside in counties (j) in states (k); Race ijk *Age ijk is the cross-classification of the race and age categorical variables for women (i) nested within counties (j) and states (k); Hispanic ijk is Hispanic ethnicity for women (i) nested within counties (j) and states (k); V jk is a vector of county (j) level variables nested within the state (k); β 0 is the population-averaged, or marginal, intercept term; β 1 is the population-averaged coefficient for the race*age crossclassification; β 2 is the population-averaged coefficient for Hispanic ethnicity; and β 3 is the vector of population-averaged coefficients for county-level variables. We built separate models for each regional census division to account for regional effects when predicting mammography prevalence.\\nNext, we used the resulting predicted probabilities from step 1 in the synthetic estimation of mammography prevalence to estimate the proportion of women in each county who reported recently having had a mammogram. The synthetic estimate of the prevalence (p j ) of recent mammography for county j equaled\\nwhere p ij is the predicted probability for a specified raceage category from step 1; n ij is the number of women in race-age group (i) who reside in the county (j); and n j is the total number of women aged 40 to 79 who reside in the county (j). The sum of the race-age weighted prevalences equaled the prevalence of recent mammography for the county.\\nWe performed all statistical analyses by using SAS version 9 (SAS Institute, Inc, Cary, North Carolina) and SAS-Callable SUDAAN version 9.1 (Research Triangle Institute, Research Triangle Park, North Carolina) to adjust for complex survey design. We used small-area analysis to estimate the prevalence of recent mammography in each county for women aged 40 to 79. Further, we repeated the procedure to estimate county-level mammography prevalence for each race and age category to determine the extent to which geographic disparities existed for and differed by sociodemographic subgroups.\\nWe mapped results using ArcGIS version 9.2 (ESRI, Redlands, California), which produced a visualization of county-level prevalence to identify geographic disparities in screening. For the maps, the natural-breaks Centers for Disease Control and Prevention • www.cdc.gov/pcd/issues/2009/oct/08_0210.htm\\nThe opinions expressed by authors contributing to this journal do not necessarily reflect the opinions of the US Department of Health and Human Services, the Public Health Service, the Centers for Disease Control and Prevention, or the authors' affiliated institutions. Use of trade names is for identification only and does not imply endorsement by any of the groups named above.\\nclassification method determined the prevalence categories of recent mammography (24) . We hypothesized that county-level analyses would produce similar patterns in prevalence as found at the state level, that counties in the Northeast would report the highest prevalence of mammography, and that counties in the Southeast and Mountain regions would report the lowest prevalence. A prior study found similar geographic patterns when using 2002 BRFSS data to estimate prevalence of mammography for the 9 census regions (5). We posited that some intrastate variation in mammography prevalence would be observed and that the magnitude and location of these geographic disparities would vary by race or age. We further hypothesized that counties with a greater proportion of younger and nonwhite women would have lower prevalence of mammography because these groups have historically had lower prevalence of mammography (25) and are not uniformly represented across US counties (22) .\",\n       nan,\n       \"Conclusions: The results demonstrated that low-income Hispanic children with higher waist circumference have worse physical health and social interactions and fewer school achievements than children with lower waist circumference. These findings indicated health professionals need to consider low-income Hispanic children's body composition when they plan to promote their HRQOL. Given the fact that regular physical activity reduces childhood obesity and is linked to improved HRQOL in children, this study may provide valuable insight for practitioners regarding how to promote Hispanic children's HRQOL and develop effective interventions aimed at the prevention of childhood obesity.\",\n       'In summary, we identify a group of CN persons with emerging amyloid pathology coupled to increased atrophy rates in frontoparietal but not temporal regions, suggesting local effects on brain structure during early stages of Aβ accumulation. Cognitively normal participants and patients having AD with signs of widespread Aβ pathology had increased atrophy rates in classic AD regions. Our results modify previous models of the relationship between Aβ and regional atrophy during the development of AD.',\n       nan,\n       '). 34 Furthermore, we confirmed that it is an eQTL in blood in two independent data sets, significantly reducing gene expression in T-allele carriers. 29, 30 Taking this into consideration with our findings, rs2273647-T carriers may have reduced gene expression and reduced fasting blood glucose levels, which could contribute directly to a protective effect on [ 18 F] FDG decline. Conversely, higher expression of PPP4R3A may result in insulin resistance and reduced brain glucose uptake, which has been linked to AD. 12, 35 Another possibility is that T-allele carriers experience a lower risk for diabetes as a result of their differential glucose regulation, thus indirectly affecting AD risk and brain metabolic decline. 36 Reduced brain glucose uptake is an early marker of neurodegeneration, and not necessarily specific to AD. Therefore, it is possible that PPP4R3A may influence vulnerability to multiple neurodegenerative diseases. 37 Although the exact mechanism of action of PPP4R3A in AD is unknown, abnormal regulation of insulin signaling and glucose metabolism is associated with greater oxidative stress, and thus PPP4R3A may play a role in the predisposition of neuronal cells to oxidative damage and metabolic dysfunction associated with neurodegenerative disease pathology. Interestingly, smk-1 (PPP4R3A ortholog) has been shown to play a direct role in mediating longevity through the insulin-signaling pathway. 38, 39 Furthermore, accumulating evidence suggests that the effective regulation of insulin signaling promotes healthy aging and is protective against toxic age-related protein aggregation, including amyloid-beta. [40] [41] [42] Thus, alterations in PPP4R3A may help slow the onset and accumulation of AD pathology through the modification of insulin-signaling pathways. In a follow-up search for previous associations between PPP4R3A and AD risk, we found that a distinct variant in PPP4R3A passed the suggestive threshold for association with risk of AD in a familybased GWAS, lending additional support for the involvement of this gene in AD. 43 Further investigations will be needed to identify the protective mechanisms by which PPP4R3A affects disease vulnerability. One limitation to our study is that we have not been able to determine definitively whether the true biological effect of rs2273647-T is dominant or additive. As is conventionally done in GWAS studies, we identified this variant in our initial analysis with an additive model. We observed in the follow-up analyses, however, that the effect of genotype on disease risk, progression, and gene expression was stronger when rs2273647 genotype was classified according to a dominant model. We therefore returned to the original GWAS and checked the p value of our SNP assuming a dominant model and found that it remains quite significant at p 5 1.32 3 10',\n       nan,\n       'Temperature in degrees Celsius (°C) may be converted to degrees Fahrenheit (°F) as °F = (1.8 × °C) + 32. Temperature in degrees Fahrenheit (°F) may be converted to degrees Celsius (°C) as °C = (°F -32) / 1.8.',\n       'Competence in reading is an essential developmental achievement that predicts success in almost all facets of contemporary society. In many ways, reading serves as the gateway to children\\'s learning. Students who do not learn to read early in their academic careers experience extreme difficulties when expected to read to learn later on. Juel (1988) reported a .88 probability that children who were poor readers at the end of 1st grade would also be poor readers at the end of their 4th grade year. Decades of early literacy research have shown that young children\\'s early literacy skills predict later academic success (Barnett and Belfield 2006;Barnett et al. 2005;Dickinson and Neuman 2006). An alarming 85-90 % of students with serious reading problems in the primary grades fail to graduate from high school (NICHD 2000). Furthermore, children who read well also read more and thus attain more knowledge across a variety of domains (Cunningham and Stanovich 1998;Echols et al. 1996) than their peers who do not read well. Stanovich (1986) termed this the \"Matthew effect,\" in which poor readers fall further behind their more literate peers in all academic areas. Additionally, literacy skills are closely tied to positive societal outcomes such as employment and participation in society (Heckman and Masterov 2007;Kirsch et al. 1993). Given the predictive nature of early literacy skills, it is imperative that we develop integrated systems of prevention and intervention to assure competence in early literacy for all young children. Learning to read depends on the foundational skills of oral language, phonological awareness, print awareness, and alphabet knowledge (Dickinson et al. 2003;NICHD 2000;Storch and Whitehurst 2001;Whitehurst and Lonigan 1998). A meta-analysis of early childhood literacy research in the National Early Literacy Panel (NELP) report (2008) identified key literacy skills shown to predict later academic success. The skills found to have medium to large predictive relationships with later literacy achievement in decoding, reading comprehension or spelling include (a) alphabet knowledge, (b) phonological awareness, (c) rapid automatic naming of letters or digits, (d) rapid automatic naming of colors or objects, (e) name writing, and (f) phonological memory. These associations hold true for children across community settings, including children who reside in rural communities. Thus, these early literacy skills must be targeted in effective interventions.',\n       \"Information from public care bodies, reviews and guidelines provide for separating the clinical cases according to the seriousness of the clinical photos. The SARS-CoV-2 might be mild, moderate or severe based on the strength of the immune system of the infected individual. Acute influenza, ARDS, sepsis and septic shock are among severe health symptoms. A definite pattern in the bulk of cases tends to reflect the scientific development of the disease. In a proportion that has yet to be identified, after around a week, infected individual health outcomes have unexpectedly worsened, as respiratory failure has declined rapidly. The extreme respiratory failure conditions and medical requirements of sepsis and septic shock should be taken seriously [3] .\\nPatients with uncomplicated (mild to moderate) illness usually have signs that include mild fever, dry cough, sore throat, respiratory irritation, fatigue, stomach aches and malaise, whereas reported dyspnea in patients were asymptomatic [15] . Nonrespiratory signs such as diarrhea are challenging to identify relative to prior HCoV infections. Moderate pneumonia respiratory signs such as cough and shortness of breath (or children's tachypnea, etc.) were reported for patients with some cases [13] . Extreme pneumonia fever is caused by heavy illnesses, respiratory depression or hypoxia (SpO2 <90% in the room). Fever is consistent with extreme pneumonia [30] . Nevertheless, fever signs thought to be correctly recognized as mild or sometimes missing, even in extreme cases of the disease. In children, cyanosis can occur. The description includes a psychiatric condition, and radiological terminology is used to remove complications with clinical and ventilatory requirements that are needed for this diagnosis [13] . This condition indicates a severe new respiratory problem and a deterioration of a respiratory feature that has already been established. The degree of hypoxia in various types of acute respiratory distress syndrome (ARDS) is distinct [31] .\\nFurthermore, sepsis is a life-threatening organ dysfunction due to dysregulated host responses to suspected or proven organ dysfunctions [32] . The clinical pictures of patients with SARS-CoV-2 and sepsis are particularly severe, with a wide variety of signs, symptoms (cardiac disorders, such as extreme dyspnea and hypoxemia, abnormal vomiting, acidosis, altered mental state and functional organ changes), and signs of multi-organ shock presented as hyperbilirubinemia laboratory results [31] .\",\n       'This research is a cross-sectional study because it is carried out at a specific time that corresponds to the period of health contingency in the selected countries (March-June 2020), and analytical because the objective is to analyze the possible association between different variables with Covid-19 mortality.\\nThe following variables were analyzed: A population pyramid, which due to its structure, is classified as regressive; is the type with the largest adult population, stationary; it has a balance of all age groups and progressive; it is the type with the highest young population.\\nIsolation is classified into three types: high; (strict isolation), which is described in the study with the color red, moderate; (isolation for risk groups with authorization of essential activities) described in yellow, and smart; (without isolation and strict hygiene measures) represented with the color blue. Gross Domestic Product and screening used to detect Covid-19 in countries with the highest and lowest mortality.\\nIn the databases of Medline, Elsevier, UpToDate, Cochrane, Pubmed, Ebsco in June 2020, we searched for articles on the mortality due to Covid-19 and types of the population pyramid, type of isolation, gross domestic product and type of screening. Only three articles were found related to the topic. However, these publications made estimates based on the predictions of the World Health Organization and the authors themselves.\\nWhen reviewing journals such as New England Journal of Medicine, Lancet, and JAMA Network in June 2020, 25 articles on research-related topics were analyzed, of which, in the end, information from 14 of these was used to substantiate. Updated information was obtained from the official websites of Johns Hopkins University, University of Oxford, World Health Organization, Our World in Data, Government of Mexico, to make charts and graphics for analysis. 17, 18 A systematic review of documents and statistics expressing mortality in the ten countries most affected by Covid-19 and the ten least affected was carried out. Similarly, information was sought in the media by country, to obtain the type of isolation because this information is not published in indexed articles (links in the webliography).\\nThe variables were analyzed to assess the association between them, to establish statistical inferences, since they were quantitative variables, the difference in means of the groups established was measured through the statistical package for <Excel version Microsoft Office 365.\\nThis research is a study with non-probability sampling since, due to the diversity of information, the objects of study were selected based on the specific variables of the study (convenience sampling). 19 ',\n       'Countries with a regressive population pyramid are more affected by Covid-19, reporting high mortality (per 100,000 inhabitants). The association of the regressive population pyramid and high isolation reflects a higher mortality rate. Finally, there is no significant difference in the mortality of countries that had smart isolation and moderate isolation. In countries with a progressive population pyramid, they present significantly less mortality than those with a regressive pyramid, regardless of the type of isolation performed. However, in this group, if high screening impacts favorably, reduces mortality.\\nThe appropriate protection measures for each country must correspond to their level of risk of contracting the infection and of having an unfavorable result. The risk must be measured concerning the population pyramid, age of the patient, and their comorbidities, to implement the corresponding isolation and screening measures.\\nDuring the following months after the analysis carried out, the outcome of mortality from Covid-19 has been observed in different countries. The variable that had the most impact was screening since isolation was intermittent in most countries with a regressive population pyramid and remained moderate in most countries with a progressive population pyramid.\\nAs can be seen in the coronavirus resource center at Johns Hopkins University, mortality in countries with a regressive population pyramid did not have a significant change, unlike countries with a progressive population pyramid. When comparing mortality and screening in countries with the two types of population pyramid in June 2020 and September 2020, it was concluded that countries with a progressive population pyramid can significantly modify their mortality if they screen the population, since these Countries having low screening increased their mortality. Unlike countries with a regressive population pyramid, screening does not have a strong impact on mortality because their type of population pyramid predisposes them to higher mortality.',\n       'The food supply is usually a residual that makes the supply-utilization commodity table balance. The disappearance method of calculation relegates to the food supply all residual uses for which data are not available, such as miscellaneous nonfood uses, stock changes at retail and consumer levels, and sampling and measurement errors in the estimation of other components of the balance sheet. For example, an increasing proportion of the total turkey supply (especially backs, necks, and giblets) goes into pet foods. But since such use has yet to be officially estimated or entered as a nonfood-use component of the supply-utilization balance sheet, it is included in food disappearance. Thus, this report probably overstates turkey consumption. In contrast, the lack of reliable estimates of game fish supplies means that fish consumption is likely understated.\\nFood disappearance is often used as a proxy to estimate human consumption. Used in this manner, the food supply usually provides an upper bound on the amount of food available for consumption. Food disappearance estimates can overstate actual consumption because they include spoilage and waste accumulated through the marketing system and in the home. (For further discussion of food loss, see Estimating and Addressing Americas Food Losses, FoodReview (Linda Scott Kantor et al., ERS, USDA, January-April 1997, pp. 2-12) .) In general, food disappearance data serve more appropriately as indicators of trends in consumption over time than as measurements of absolute levels of food eaten. This is the case so long as changes in food production and marketing practices or consumer behavior over time do not alter the relative disparity between food disappearance and food actually eaten.\\nThe food disappearance series is becoming a less reliable indicator of change over time in ingestion of food fats and oils. While food disappearance reflects trends in fats and oils sold for human food, it probably does not accurately measure food eaten because the waste portion of fats and oils has increased during the past two decades with the growth in awayfrom-home eating places, especially fast-food places. Foodservice establishments that deep-fry foods can generate significant amounts of waste grease, referred to as restaurant grease. A 1987 study by SRI, International, indicates that used frying fat disposed of by restaurants and processed by renderers for use in animal feeds, pet foods, and industrial operations and for export amounts to about 6 pounds per capita, or about 9 percent of the 1995 disappearance of added fats and oils. A 1993 study estimated that about 50 percent (or more) of deep-frying fat used in foodservice operations is discarded after use and is not available for consumption. For further details on this study, Food supply data are aggregates of food obtained from all sources. Retail-weight equivalents measure food availability as if all food were sold through retail foodstores. Much of this food, however, is consumed on farms where produced, or is sold through wholesale channels to restaurants, hotels, other away-from-home eating places, and to schools, camps, hospitals, and other institutions. The food categories tend to be aggregates according to the basic commodity definitionbeef, for example. Final product forms and market channel flows are not usually known. Most available data are concentrated near the farm and primary processing levels. There are little or no data available for many furtherprocessed products, such as bread, other bakery products, and soup. In short, relatively good data exist for many of the ingredients, but not for final products. Anyone interested in domestic food intake by individuals should use data from USDAs Continuing Survey of Food Intakes by Individuals (CSFII), conducted by the Agricultural Research Service.\\nAnnual per capita estimates of domestic disappearance inherently represent an aggregation, over time, over consuming units, over geographical space, and over various product forms. In any aggregation process, certain information is, inevitably, lost or rendered irretrievable. Consequently, per capita disappearance may mask the influence on consumption of seasonal variation and socioeconomic and demographic characteristics, such as age, sex, ethnicity, family size, household income, and geographic region. Data from the CSFII and the Consumer Expenditures Survey conducted by the Bureau of Labor Statistics are more useful for measuring the effect of socioeconomic and demographic characteristics on food consumption.\\nStocks data are not available for some commodities. Farmer marketings are the only data available for some commodities, and it is assumed that stocks are equal to the proportion of the crop not marketed by the end of the calendar year. For example, the supply-utilization table for dry edible beans uses farmer marketings to estimate stocks. Use of mushrooms for processing is computed without stocks data. The addition of processed mushroom stocks estimates, were they available, probably would have a smoothing effect on food disappearance, making year-toyear changes a little less erratic. In addition, stocks data do not include inventories of wholesalers, retailers, foodservice establishments, and the military because of insufficient data.\\nThe conversion factors used to derive retail weights from primary weights are averages over various varieties and qualities of product and methods of marketing. Though some year-to-year changes have been made in the factors (see Updated Beef and Pork Conversion Factors), most conversion factors are constant since 1970 (table 3) . As a result, many changes in quality and yield of product and in marketing procedures go undetected in the consumption estimates at retail.\\nAnnual food supply estimates are subject to revision in conforming to data from the census of agriculture and the census of manufactures, which are available only in years ending with 2 or 7. For example, estimates of per capita supplies of breakfast cereals for 1988-92 have been revised based on data from the 1992 Census of Manufactures. Current estimates use the annual change in grocery store sales volume of breakfast cereals as statistical movers of 1992 census data. Later in 1999, data from the 1997 census will be used to revise the 1993-97 estimates.',\n       'The Republic of Korea is well known as a leader of gifted science education due to the significant progress it has made in the past two decades. This paper aims to provide a historical perspective of gifted science education in Korea by interviewing a key figure in the Korean science education community. This paper explores the various trajectories of the development of gifted science education via the experience and thoughts of Choe Seung-Urn, who is a founder and developer of gifted science education in Korea. He has participated in gifted science education in Korea since the beginning by building prototypes of gifted education institutes, introducing tools to identify gifted students, and developing an orchestrated and integrated model across both research and education. This conversation with a senior researcher on the brink of retirement is intended to provide a deeper insight for successive generations who will take over the gifted science education system.',\n       \"The results of the current study shed light on the variability of academic achievement growth among Millennials and Generation Z students using a recent national dataset (HSLS:09). The study also clarified the associations among theory-driven covariates, latent achievement-class structure, and college enrollment. In addition to the abovementioned theoretical implication, the current study addressed many practical implications, suggesting solutions to the ramifications of the well-documented decline in GPA growth among high school students (i.e., Henry et al., 2012;Li et al, 2010) and the tendency of students in certain achievement-classes not to enroll in college (Henryet al., 2012;Ma et al., 2016;Moretti, 2007). Initiatives that strengthen the academic growth of high school students in Class 1 (i.e., low-achieving and increasing) and Class 2 (moderate-achieving and declining) are suggested. Educational interventions are recommended particularly for males, blacks/African Americans, and students of both genders in a low SES. The interventions should focus on empowering the students' academic growth and paving the road for future opportunities related to college enrollment. Specific interventions should target academic outcome (e.g., tutoring, study strategies, and setting productive academic goals) and affective traits (i.e., mentoring, empowering students' self-confidence, teaching motivational techniques including grit and behavioral interventions).\",\n       'This study used the SASS dataset to conduct a descriptive statistical analysis of the characteristics and experiences of beginning teachers in the Northeast Region states. These state data are then compared with the regional and national estimates. Tests of difference between estimates for each state and the national estimates are conducted, and statistical significance is noted. 6 The complex sampling process used to collect SASS data (see appendix C) necessitates using special statistical procedures to produce reliable estimates. All estimates and statistical tests were produced with the NCES-recommended balanced repeated replication weighting procedures using the Stata statistical software program. For continuous variables such as age, means and standard errors are presented. For dichotomous and categorical variables, percentages and standard errors are presented. Estimates are produced for each Northeast Region state separately, for the Northeast Region, and for the entire country. All Northeast Region states are also included in the national estimates. Differencein-means tests and Chi-square tests were used to determine whether there is a significant difference between estimates for individual Northeast Region states and the national estimate. Tests of statistical differences were not conducted between estimates for the Northeast Region and the national estimates, as regional estimates are provided for descriptive purposes only. The national estimates should not be considered as a standard or norm to which the states should be compared; rather, they are provided as descriptive data to help states assess their data in the context of findings for other states and the country as a whole. The study applied two NCES guidelines pertaining to SASS data. First, if the standard error of an estimate was greater than 50 percent of the estimate, the estimate is considered not sufficiently reliable to meet reporting standards. Such estimates are not presented in this report. Additionally, estimates with standard errors of 30-50 percent of the estimate are reported but should be interpreted with caution. appendix b. daTa and meThodology 23 Table b1 variables used in the analysis and survey source ',\n       'Methane hydrates, ice-like solids that consist of methane and water that are stable at moderate pressures and low temperatures, are believed to be widespread in Arctic Ocean continental slope and rise sediments [e.g., Kvenvolden and Grantz, 1990;Biastoch et al., 2011;Reagan et al., 2011]. The Arctic Ocean and surrounding landmasses have experienced rapid warming on short-term (decadal) time scales [e.g., Johannessen et al., 2004]. On longer time scales, warming of more than 10°C since the Last Glacial Maximum (LGM) has been linked to permafrost thaw, reduced Arctic Ocean sea ice cover and possibly methane hydrate destabilization [Brigham and Miller, 1983;Allen et al., 1988;Paull et al., 2007;Shakhova et al., 2010]. In the marine system, the impingement of warming ocean waters on continental slopes, which host the most climate-sensitive gas hydrate deposits [Kvenvolden, 1993;Ruppel, 2011], not only leads to breakdown (dissociation) of gas hydrates into constituent methane and water but also can increase subsurface fluid pressures and reduce slope stability [e.g., Kayen and Lee, 1991;Flemings et al., 2003;Hornbach et al., 2004]. Methane that migrates to the seafloor after dissociation may be released into the ocean, enhancing water column methane oxidation that leads to increased ocean acidification and deoxygenation [Kvenvolden, 1988;Dickens et al., 1995;Archer, 2007;Camilli et al., 2010;Biastoch et al., 2011]. Due to the potential for methane destabilization and release, unraveling the connections between climate warming and methane hydrate dynamics on the Beaufort Sea margin has important implications for marine sediment mechanics, Arctic Ocean chemistry, and possibly atmospheric greenhouse gas concentrations. In typical deepwater marine gas hydrate systems, gas hydrates can in theory exist at the seafloor. In practice, outside seep areas, gas hydrate does not usually occur as shallow as the seafloor because anaerobic methane oxidation processes within the sulfate reduction zone that lies within the uppermost meters of sediments consume most of the methane [Reeburgh, 2007]. To first order, the base of gas hydrate stability (BGHS) is controlled by the geothermal gradient and hydrostatic pressure within the saturated and generally highporosity sediments that make up the uppermost hundreds of meters on most continental margins. The BGHS often manifests in seismic data as a strong, reverse-polarity bottom-simulating seismic reflector (BSR) [Shipley et al., 1979;Kvenvolden and Grantz, 1990;Andreassen et al., 1997]. The negative impedance contrast at the BSR reflects the layering of higher-velocity, hydrate-bearing sediments over lower-velocity, gas-charged sediments [Holbrook et al., 1996]. The presence of a BSR is a sufficient condition for the likely occurrence of gas hydrate in sediments, but gas hydrate sometimes exists without an underlying BSR [Holbrook et al., 1996]. BSR depths, combined with hydrate stability models [Sloan and Koh, 2008], have long been used to constrain subsurface temperature regimes [Yamano et al., 1982] and to assess the degree to which the sediments are in steady state thermal equilibrium [e.g., Ruppel, 1997;Ruppel and Kinoshita, 2000;Hornbach et al., 2004;Hornbach et al., 2008;Phrampus and Hornbach, 2012]. Where the gas hydrate stability zone (GHSZ) is out of equilibrium with contemporary ocean temperature and heat flow conditions, the contemporary distribution of gas hydrates may sometimes reflect past conditions. Future adjustments in the distribution of gas hydrates would be expected to bring the system back into equilibrium. In this study, we use the regional 1977 ocean temperature data, long-term ocean temperature data, and heat flow observations combined with numerical models to predict the steady state location of the GHSZ in the U.S. Beaufort Sea. We then compare modeled steady state GHSZ with direct observations of BSRs revealed in regional seismic data. The results allow us to delineate where methane hydrates may be destabilizing on the U.S. Beaufort continental margin.',\n       'The NPSAS and BPS data sets are available as either public-use files or restricted-use files. The NPSAS data set consists of a cross-sectional sample of all PSE students in the U.S., including both undergraduates and graduates. These data were collected in 1989-90 and again in 1992-93. The BPS sample of all first-year/first-time undergraduate PSE students in the U.S. is a subsample of the 1989-90 NPSAS. The BPS sample then became a longitudinal data set, and data were collected on these students every year or so for four years. The data from each wave are available either as separate files or as one cumulative file. Both the NPSAS and BPS samples are national samples, which means they do not provide adequate sample sizes for individual state analysis.24 22See Appendix D for ordering and downloading information. 23For a discussion of some difficulties in using the Analysis file, see the section on time requirements for publicuse files in this chapter. 24The NPSAS and BPS files are on separate CDs, but the process of working with them is the same, so they will be discussed together in this report. NPSAS and BPS public-use files NPSAS and BPS public-use files come with software that can generate most of the possible frequencies, cross tabulations, and correlation matrices in these data sets. However, access to the raw data is not provided. By not providing the raw data, NCES not only protects the privacy of the respondents, but makes the data analysis process easier for the users. Like IPEDS, these public-use files are available on request either on CDs or on the Web. The files include the data, which can only be accessed by the enclosed data analysis software, called Data Analysis System (DAS), the user and technical manuals, and summary reports. The advantages of using the public-use files is that DAS is easy to learn and use. Almost immediately, a researcher can produce weighted frequencies, percentages, means, or correlations on the total sample or on subsets such as public two-year colleges, using any variable as a control variable. DAS can be used to create tables of any specifications, print out the tables, and provide the estimated population sizes and standard errors. Since the NPSAS/BPS data sets have a complex sampling design, the standard errors need to be calculated in a way that takes this design into account, and DAS provides these accurate standard errors. The limitations of using the public-use files are that actual sample sizes and totals cannot be seen, variables from other data sets such as IPEDS cannot be added to the data sets, and variables cannot be recoded, combined, or created.25 For that type of access to the raw data, the restricted-use files are necessary.',\n       nan,\n       'We previously identified KIF5A as a candidate gene for ALS in our prior study that lacked the power to draw a definitive conclusion (Kenna et al., 2016) . KIF5A was also a candidate ALS gene in a previous GWAS, though it similarly failed to reach genomewide significance (McLaughlin et al., 2017; van Rheenen et al., 2016) , as well as a single gene study selected based on the a priori knowledge of its role in HSP/CMT2 and cytoskeletal function (Brenner et al., 2018) . Here, we have confirmed KIF5A as an ALS-associated gene with genome-wide significance through two independent approaches. By performing a GWAS involving $80,000 samples, in addition to replicating five previously published loci, as well as the previously reported locus SCFD1 using a linear mixed model analysis (data not shown), we identified a missense variant within the KIF5A gene that reached genomewide significance for association with ALS risk. It should be stated though, that as with all GWASs, we cannot rule out that other variants in linkage disequilibrium represent the primary risk factor. In an independent line of investigation, we applied RVB to exome sequencing of $21,000 samples and identified an exome-wide significant association between FALS risk and rare KIF5A LOF variants. Analyses of KIF5A in independent replication cohorts both confirmed our initial finding for the p.Pro986Leu variant and revealed three additional carriers of LOF variants in 9,046 ALS cases. a relatively common, but low penetrance, risk allele for ALS, while LOF variants constitute rare, but high penetrance, risk factors. Kinesins are microtubule-based motor proteins involved in intracellular transport of organelles within eukaryotic cells. In mammals, there are three heavy-chain isoforms of KIF5: KIF5A, KIF5B, and KIF5C (Miki et al., 2001) . The three proteins homo-and heterodimerize through their coiled-coiled stalk domain, and create a complex with two kinesin light chains via binding to the tail domain (Hirokawa et al., 1989) . All three KIF5 genes are expressed in neurons (Kanai et al., 2000) and function to transport many cargos by binding to distinct adaptor proteins.\\nThe central role of kinesins in axonal transport leads us to speculate that mutations in KIF5A cause disease by disrupting this process. Indeed, defects in axonal transport are a common observation in ALS patients and are already known to directly contribute to motor neuron degeneration pathogenesis (Chevalier-Larsen and Holzbaur, 2006; Hirokawa et al., 2010; Millecamps and Julien, 2013) . KIF5 mediates the transport of granules containing both RNA and RNA-binding proteins within neuronal dendrites and axons (Kanai et al., 2004) . Among these cargos are the ALS-associated proteins FUS and hnRNPA1 (Guo et al., 2017; Kim et al., 2013; Kwiatkowski et al., 2009; Vance et al., 2009) . Similarly, KIF5 mediates the transport of VAPB through the adaptor protein protrudin (Matsuzaki et al., 2011) , and mutations in the VAPB gene have been identified in ALS and late-onset spinal muscular atrophy (Nishimura et al., 2004 (Nishimura et al., , 2005 . KIF5 is responsible for the axonal transport of neurofilaments (Wang and Brown, 2010) and KIF5A knockout mice display abnormal transport of neurofilaments (Xia et al., 2003) . Abnormal accumulation of neurofilaments is a pathological hallmark of ALS and rare mutations in neurofilament heavy polypeptide (NEFH) are associated with ALS (Al-Chalabi et al., 1999).\\nKIF5 also contributes to the transport of mitochondria (Kanai et al., 2000; Tanaka et al., 1998) and motor neurons derived from KIF5A À/À mice display transport deficits and reduced survival (Karle et al., 2012) . Impaired transport and dysfunction of mitochondria represent another common hallmark observed in ALS patients (Chevalier-Larsen and Holzbaur, 2006; Guo et al., 2017; Palomo and Manfredi, 2015; Smith et al., 2017) . KIF5 also contributes to the transport of AMPA-type (Heisler et al., 2014; Setou et al., 2002) and GABA A receptors (Nakajima et al., 2012) . In keeping with reported ALS genes such as NEK1 (Thiel et al., 2011) and PFN1 (Wu et al., 2012) , modulation of KIF5A expression has been shown to influence the formation of neurite-like membrane protrusions (Matsuzaki et al., 2011) . Given its critical interactions with the cytoskeleton, the identification of KIF5A mutations further extends the list of cytoskeletalrelated proteins implicated in ALS pathogenesis, such as PFN1, TUBA4A, NEFH, and peripherin (Al-Chalabi et al., 1999; Gros-Louis et al., 2004; Smith et al., 2014; Wu et al., 2012 ).\\nAn important question raised by the current study is why variation within the C-terminal cargo-binding domain is associated with ALS, while missense variations of the N-terminal motor domain are associated with hereditary spastic paraparesis and CMT2. Missense mutations within this latter domain have been shown to affect microtubule binding and/or ATP hydrolysis, resulting in a defective KIF5A-mediated anterograde transport of cargo along dendrites and axons. This, in turn, leads to the axonal retrograde degeneration observed both in hereditary spastic paraparesis and CMT2, two length-dependent axonopathies (Ebbing et al., 2008) . In contrast, the primary cellular lesion in ALS is believed to occur within motor neuron cell bodies, where cytoplasmic protein aggregates are consistently observed, and to propagate anterogradely along neurites. We anticipate that LOF variants within the C-terminal domain of KIF5A will disrupt binding with specific cargo proteins. This is supported by a study in zebrafish in which truncation of the C terminus resulted in a dramatic disruption of axonal localization of mitochondria (Campbell et al., 2014) . One possible mechanism is that disruption of binding to cargo may possibly lead to their accumulation and seed aggregation within the cell body, resulting in a deficiency at neurite terminals. Deficiency in KIF5A expression and cargo binding has been associated with accumulation of phosphorylated neurofilaments and amyloid precursor protein within neuronal cell bodies, and subsequent neurodegeneration, in patients with multiple sclerosis (Hares et al., 2017) . While differences in KIF5A kinetics and KIF5A interactions constitute one possibility to explain the phenotypic heterogeneity, it is also possible C-terminal and N-terminal variants act through a common mechanism, but that a difference in the relative extent of loss-or gain-of-function toxicities leads to milder (i.e., hereditary spastic paraplegia or CMT2) or more severe (i.e., ALS) phenotypes.',\n       '-Not available. †Not applicable. !Interpret data with caution. The coefficient of variation (CV) for this estimate is between 30 and 50 percent.',\n       'Oncostatin M 0\\nInterleukin-1 regulation of extracellular matrix 0\\nInterleukin-5 regulation of apoptosis 0 TNF-alpha effects on cytokine activity, cell motility, and apoptosis 0\\nImmune system signaling by interferons, interleukins, prolactin, and growth hormones 0 The predicted pathways in cell lines showed similar significance with selected miRs that mainly target inflammation and virus pathogenesis ( Figure 1 and Table 2 ). Thus, we concluded that the selected miRs, that showed high similarity with human miRs, are also the critical targets, which project the major clinical pathophysiological conditions related to pathway alterations.',\n       'In recent years, increasing attention has focused on the distribution of government payments, especially the share of payments that go to large farms and high-income farm households. Farm commodity program payment limits were first introduced in the Agricultural Act of 1970. The Farm Security and Rural Investment Act of 2002 for the first time supplemented program payment limits with a cap on the income farmers could earn and still receive farm program payments. The 2008 Farm Act tightened payment limitations on some producers and replaced the total adjusted gross income (AGI) limit with separate lower caps for the farm and nonfarm components of AGI. This research uses data from the Agricultural Resource Management Survey (ARMS), a survey of farm operator households conducted annually by the U.S. Dept. of Agriculture, to examine the impact of government payments and changes in payments as a result of changes to farm program policies on income inequality among farm households.',\n       'DECEMBER 2016 | ',\n       'Estuarine water quality, in terms of eutrophication and hypoxia, is greatly affected by multi-stressor interactions. Currently, approximately 250 hypoxic, or low-dissolved oxygen, dead zones exist in U.S. coastal and Great Lake waters (Figure 3-3). Hypoxia may be initiated and/or worsened by high rates of primary productivity spurred by increases in nutrient loading, called eutrophication. After studying a number of watersheds, Howarth et al. (2011) indicated that riverine-discharge increases due to climate change will increase net anthropogenic nitrogen inputs to coastal waters, thereby increasing eutrophication and hypoxia. Climate change, especially warming, may also make systems more susceptible to development of hypoxia through enhanced water column stratification, decreased solubility of oxygen, and increased metabolism and mineralization rates. High-precipitation storms, which are on the increase (see Chapter 2 section 7 this report), increase stratification and organic matter flushed into estuarine and coastal systems from the watershed; algal blooms can result from the nutrient pulses they inject into estuarine and coastal ecosystems (Paerl et al., 2006a) and massive hypoxia can be triggered by such events (Justic et al., 2007;Paerl et al., 2006b). For the Mississippi River basin associated with the northern Gulf of Mexico seasonal dead zone, climate predictions suggest a 20 percent increase in river discharge (Miller & Russell, 1992) that would lead to elevated nutrient loading, a 50 percent increase in primary production, and expansion of the oxygen-depleted area (Justic et al., 1996). All factors related to climate change will progressively lead to an onset of hypoxia earlier in the season and longer periods of hypoxia over time (Boesch et al., 2007) ',\n       'In the present study, we developed a rapid mapping approach for corn and soybeans in the US Corn Belt and adjacent states. Traditionally, the development of timely crop maps of a large area is limited by the tremendous amount of time required by reference data collection. This makes it extremely difficult to finish mapping within the current season. Our approach greatly accelerates the mapping process by using trained classifiers that have been prepared in advance, using reference data from other years instead of relying on current-year ground reference data. Cross-year validation in [2008] [2009] [2010] [2011] [2012] [2013] [2014] showed that the rapid mapping is capable of producing timely and accurate results when using fuzzy classification and phenology-based training year selection. For the middle of the 2015 growing season, we successfully produced corn and soybean coverage maps using trained models selected from 2008-2014, according to phenological similarity. The study also establishes a framework to maintain a historical record of trained classification models for prompt year-by-year mapping in the future. As the record grows longer and covers a diversity of crop progress conditions with an increasing number of trained models, the impact of interannual phenological variability will be reduced and the robustness of resulting map products will be continuously improved. The framework is also applicable to medium-resolution imagery from missions such as Landsat and Sentinel, if the image product is properly standardized.',\n       '[59] With a balanced velocity field, the ocean heat flux across the Arctic Ocean boundary can be calculated. The procedure is analogous that for FW flux calculation (section 3.3), with the addition of a term for latent heat flux, noting that while Arctic Ocean imports liquid seawater, some of the export is in solid form (sea ice). Heat flux (Q) is therefore calculated as where r is water density, c p is seawater specific heat capacity, potential temperature (q) anomaly q′ ¼ q À q and q Figure 15. The FW budget (mSv) for each water mass derived from the box inverse model: net horizontal freshwater transport (dark blue), diapycnal convergence/divergence (green) and residual imbalance (dark red). The Surface layer has additional bars representing the surface FW input and sea ice flux. Positive (negative) values represent FW input (export) for each water mass. For each layer, the sum of all terms (excluding the residual) equals the residual, and the sum of all residuals equals zero. is the area-weighted mean potential temperature equal to 1.159°C, dA is a cross-section area element and L is latent heat flux due to sea ice export. Mobile sea ice is taken into account in a similar way as for the boundary mean salinity calculation. The latent heat flux is calculated using latent heat of fusion of 3.34 Â 10 5 J kg À1 , giving a flux of 15 AE 5 TW. Sensible heat flux requires the ice temperature. The surface seawater temperature is À1.7°C and the average surface atmospheric temperature for August and September 2005 (78-80°N, 17-6°W) is À2 AE 1°C (obtained from the U.S. National Centers for Environmental Prediction/ National Center for Atmospheric Research reanalysis project). The ice temperature is set as À1.9 AE 1.0°C, the mean of ocean and atmosphere temperatures. With a salinity of 6, the specific heat capacity is 2.98 Â 10 4 J kg À1°CÀ1 and sea ice density is 930 kg m À3 , and the resulting contribution to the heat flux is 4 AE 1 TW. See Ono [1967] for seawater heat capacities and Timco and Frederking [1996] for sea ice density. Figure 17 shows the heat flux (actually potential temperature anomaly transport scaled by rc p ) section, and the accumulated heat (scaled temperature anomaly) flux around the section for full depth and for the different water masses. [60] The total Arctic Ocean heat flux is 189 AE 26 TW, comprised of 170 AE 25 TW due to sensible (ocean) flux plus 19 AE 5 TW due to sea ice (latent and sensible heat flux). Almost half the total is carried through the BSO (86 AE 19 TW), with about a quarter entering through Fram Strait (43 AE 16 TW). The BSO dominates because it is warmer; the WSC (25 AE 5 TW) is nearer to the section-mean temperature so provides a lower contribution to the total. Substantial contributions arise from the south-going cold anomalies of Davis Strait and the EGC (28 AE 3 and 18 AE 8 TW respectively) and the north-going warm anomaly of Bering Strait (13 AE 2 TW). The AW layers dominate (69 AE 13 TW), followed by the Subsurface Water (54 AE 7 TW), Upper AW (31 AE 6 TW), and Surface Water (15 AE 1 TW). [61] Figure 18 shows diapycnal volume and potential temperature velocity and associated diapycnal potential temperature transport. Since heat fluxes are not constrained within layers 1-11, a diapycnal potential temperature velocity profile is obtained only for layer interfaces 11-14. Therefore the diapycnal heat flux across each layer interface is estimated using the diapycnal volume flux across all layer interfaces. The diapycnal heat fluxes are estimated as the product of (area of each layer interface) Â (average potential temperature on each layer interface) Â (diapycnal volume velocity across the layer interface) Â rc p . This calculation neglects other heat flux mechanisms such as diffusive diapycnal transport and Reynolds transport, and is likely to be smaller than the real vertical heat transport. Associated with the upwards volume transport of 0.8 Sv across upper AW layer, 0.8 TW heat flux has been obtained through the upper AW layers. [62] Figure 19 summarizes the heat budget of each water mass. As shown in Figure 17, horizontal heat transport in subsurface, upper AW and AW layer brings heat into the Arctic. The total heat input to the Arctic by the ocean is 170 AE 25 TW and by sea ice is 19 AE 5 TW. Therefore the total rate of heat loss from the Arctic Ocean to the atmosphere is estimated as 189 AE 26 TW.',\n       'The genome of 2019-nCoV has overall 89% nucleotide identity with bat SARS-related-CoV SL-CoVZXC21 (MG772934.1), and 82% with human SARS-CoV BJ01 2003 (AY278488) and human SARS-CoV Tor2 (AY274119). The phylogenetic trees constructed using the amino acid sequences of orf1a/b and the 4 structural genes (S, E, M, and N) were shown (Figure 6(A-E) ). For all these 5 genes, the 2019-nCoV was clustered with lineage B βCoVs. It was most closely related to the bat SARS-related CoVs ZXC21 and ZC45 found in Chinese horseshoe ',\n       'The RAVEL correction procedure adapts the linear model introduced in SVA Storey, 2007, 2008 ] to intensity-normalized MRI images. The goal is to remove remaining unwanted variation in the normalized intensities by modeling the residual unwanted variation across subjects. For the optimal performance of RAVEL, we use intensities normalized with White Stripe (see Supplementary Figure S1a) . We model the m × n matrix V WS of registered and White Stripe-normalized voxel intensities, for m voxels and n subjects, as a decomposition of a biological component of interest and an unwanted component as follows:\\nwhere α1 T represents the average scan in the sample, βX T accounts for the known clinical covariates of interest, and γZ T accounts for unknown, unwanted factors. We refer to V WS as the m × n matrix of intensities, α as the m × 1 vector of baseline intensities, X as the n × p matrix of clinical covariates, β as the m × p coefficient matrix associated with X, Z as the n × b matrix of unwanted factors, γ as the m × b coefficient matrix associated with Z, and R as the m × n matrix of residuals. In this model, α, β, γ and Z are unknown parameters that need to be estimated from the data. In the case the unwanted factors Z are known, the problem is reduced to simple linear regression models fit at each voxel separately.\\nAs in RUV [Gagnon-Bartsch and Speed, 2012] , we use a subset of the voxels not associated with disease to estimate the unwanted factors Z T . We refer to such voxels as \"control voxels\". An association between CSF intensities and disease status is highly unlikely [Luoma et al., 1993] , and therefore CSF voxels are good candidates for inferring the unwanted component in the data. We perform a subject-specific tissue segmentation of the T1-w image and choose control voxels as voxels classified as CSF for all subjects in the study. We denote by V WS c the subset of the matrix of White Stripe-normalized intensities V WS confined to the control voxels. For the control voxels, Equation 1 simplifies to\\nbecause of the absence of association between the control voxels and X. To estimate the unwanted factors Z T , we perform a singular value decomposition (SVD) of V WS C as follows\\nand defineẐ T to be the first b right-singular vectors {w 1 , w 2 , . . . , w b } of W. The choice of b is discussed in the next section. Note that for b = 1, the estimatorẐ T will closely estimate the average CSF intensity for each subject. We obtain the estimatesγ i in Equation 1 by performing a linear regression at each voxel separately, using our estimate of Z T in the equation. We define the RAVEL-corrected voxel i for subject j as\\nwhere v WS ij is the White Stripe-normalized intensity for the i-th voxel and for the j-th subject. In summary, RAVEL aims to identify patterns of variation in the control voxels across subjects, and then assess the degree to which this variation explains the brain-wide intensity distributions. In practice, this works well if the space spanned by the unwanted factors estimated from the control voxels also spans the unwanted variation space for all voxels. A schematic of the RAVEL method is presented in Figure 1 .',\n       'Grade 8 Not assessed at this grade. 2a: Find sums, products, and powers of expressions containing variables. 3b: Use formulas to answer questions about given situations. 3c: Indicate whether a value (or values) satisfies a given equation. 3d: Solve simple linear equations and inequalities, and simultaneous (two variables) equations. 3e: Write linear equations, inequalities, or simultaneous equations that model given situations. 3f: Solve problems using equations or formulas. See notes at end of exhibit.',\n       'Imputation addresses the potential concern related to missing values in the data supplied by respondents. Advantages of using imputed values include the ability to use all study respondent records in an analysis (complete-case analysis), which affords more power for statistical tests. Additionally, if the imputation procedure is effective (i.e., the imputed value is equal to, or close to, the true value), the analysis results are likely less biased than those produced with the incomplete data file. (On both the benefits and techniques of imputation, see, for example, Little and Rubin 2002.) To alleviate the problem of missing data from a respondent record, statistical imputation methods were employed for the second follow-up that were similar to those used for the HSLS:09 base year, first follow-up, and 2013 Update. Ten key files but in a recoded form such that the recoded values represented at least 30 respondents. Note that a result of this recoding is that no continuous, restricted-use variables are included in the public-use data files.',\n       'Several studies assessing fish intake and the risk of dementia have been reported, including case reports, case series, controlled studies, and randomized controlled trials. However, no meta-analyses have focused on the correlations between a fish-oriented diet (fish, n-3 FAs, DHA, or EPA) and cognitive decline (e.g., DAT, DAC, or MCI). Thus, this is the first review to evaluate the role of FDI in the risk of cognitive disorders by searching the published literature.\\nSome underlying biochemical hypotheses or other biological mechanisms could play an important role in disease progression or pathogenesis. For example, the high intake of n-3 FAs from fish or fish-related sources could reduce the risk of cardiovascular disease; therefore, it could potentially decrease the risk of DAT or other types of dementia via vascular mechanisms [25] . Furthermore, DAT could also be influenced by a fish-oriented diet via other mechanisms. Insulin resistance was connected to a higher consumption of fish-related fats, whereas high concentrations of insulin might be associated with a high risk of DAT [26] [27] . In addition, a high consumption of fats probably accelerated the oxidation of carbohydrate energy [28] ; this process could contribute to cardiovascular diseases and result in DAT or other types of dementia.\\nMoreover, several findings showed a role for fishrelated fat consumption in the pathogenesis of DAT. The deposition of amyloid-β (A-β) in animal brains could be induced by the consumption of a diet high in cholesterol [29] , whereas the accumulation of neuronal A-β was increased in rabbits with hyperlipidemia [30] . Furthermore, reducing cholesterol concentrations using drugs [31] (e.g. 3 hydroxy-3methylglutaryl-coenzyme A (HMG-CoA) reductase inhibitors) led to a decreased risk of DAT. Additionally, apolipoprotein E (ApoE), a gene relevant to DAT, was closely associated with lipid metabolism [32] , which could dual-directionally match the concentrations of cholesterol in lines with fish-related fats consumptions. In contrast, subjects with the ApoE-4 allele (indicating a higher risk of DAT) had high concentrations of cholesterol [33] . In contrast, subjects with the ApoE-2 allele (which might lower the risk of DAT) had relatively low concentrations of cholesterol [34] .\\nAlthough a series of prospective studies reported correlations between a fish-oriented diet and the risk of DAT, the potential mechanisms behind these correlations were unclear. By following up 980 elderly subjects for 4 years, researchers demonstrated that the risk of DAT was highest in subjects carrying the ApoE-4 allele in the highest quartile of total fat consumption [35] . Another study found that the high consumption of saturated and trans unsaturated FAs was related to an increased risk of DAT regardless of the ApoE genotype [36] . In addition, the intake of fish-oriented fats was correlated with a reduced risk of DAT [37] . Nevertheless, no type of fat consumption was correlated with DAT or other types of dementia in a study performed in over 5000 subjects aged ≥55 years [38] . In summary, inconsistencies focusing on the consumption of a fish-oriented diet and the risk of cognitive decline were found, and specific recommendations could not be made based on the recent studies. However, a diet high in fish-related fats was correlated with a reduced risk of vascular disease, which might be of significance. Therefore, it might be sensible to take these potential benefits into account for the further management of cognitive decline.',\n       \"Teachers in the 10 schools allocated to the attention control arm will be offered teacher professional learning designed to improve their delivery of the NSW Kindergarten-Year 6 Science and Technology curriculum. This program, known as My Science, has been shown to increase teacher confidence and student engagement in science [41] . Teachers who complete the My Science program will receive 10.5 h of BOSTESregistered teacher professional learning credit. They will also have the option to complete the iPLAY program at the end of the study, and earn an additional 14 h of registered professional learning credit.\\nThe primary purpose of employing an attention control intervention is to limit principals' and teachers' disappointment at not receiving the iPLAY intervention, thereby increasing participation during data collection at the post-intervention and maintenance phases.\",\n       nan,\n       'Assuming state and Federal agencies can make the case to fully fund the acquisition of the data to make the National Map and the Digital Coast a reality, this alone would not solve our problem of a seamless interface between the land and sea. The problem of integrating across the zero contour line at the ocean interface is one of vertical datums. Vertical datums come in three categories: those based on a form of Mean Sea Level (MSL), called Orthometric Datums; those based on tidally derived surfaces of high or low water, called Tidal Datums; and three-dimensional (3D) datums realized through space-based systems such as the Global Positioning System (GPS). Topographic maps from the USGS generally have elevations referenced to an Orthometric datum, either the North American Vertical Datum 1988 (NAVD 88) or the older National Geodetic Vertical Datum 1929 (NGVD 29). The NAVD 88 was affirmed as the official vertical datum for the United States on 24 June 1993. Nautical charts have depths referenced to different tidal datums, which can vary from chart to chart. In the United States, mean lower low water (MLLW) is the official NOAA Chart datum. To support harbor and river navigation, bridge clearances are referenced to a mean high water (MHW), and not MLLW. In order for The National Map and the Digital Coast to meet at this continuous surface, the datum issue must be overcome. NOAA\\'s Office of Coast Survey, in coordination with the Geography Discipline of the USGS, has undertaken several pilot studies to develop the procedures to make this possible. These pilots have been called \"topo-bathy.\" They have been successful in developing the V-Datum tool for the datum transformation (Plate 2). V-Datum transforms elevation information from one vertical datum into another datum. Such transformations are necessary when data from diverse sources are to be combined or compared. Informally, a datum can be considered the arbitrary zero level of the vertical coordinate of geospatial data. Artificial steps or discontinuities can appear in maps and charts if they are built from data based on inconsistent datums, and this problem can be particularly acute in coastal areas. In Tampa Bay, the separation of the tidal surfaces and the North American Datum 1983 is in excess of 24 meters. This work must be extended throughout the coastal regions of the U.S. if a seamless integration of the land-sea interface is ever to be accomplished in the NSDI. For more information on vertical datums and the topo-bathy integration work visit http://chartmaker.ncd.noaa.gov:80/bathytopo/vdatum.htm and http://chartmaker.ncd.noaa.gov/bathytopo/, respectively.',\n       'Any course dealing with general topics of agricultural public services.',\n       'This section begins by more formally characterizing the text ranking problem, explicitly enumerating our assumptions about characteristics of the input and output, and more precisely circumscribing the scope of this survey. In this exposition, we will adopt the perspective of information access, focusing specifically on the problem of ranking texts with respect to their relevance for a particular query-what we have characterized as the \"core\" text ranking problem. However, most of our definitions and discussions carry straightforwardly to other ranking objectives, such as the diverse applications discussed in Section 1.1.\\nFrom the evaluation perspective, this survey focuses on what is commonly known as the Cranfield paradigm, an approach to systems-oriented evaluation of information retrieval (IR) systems based on a series of experiments by Cyril Cleverdon and his colleagues in the 1960s. For the interested reader, Harman [2011] provides a nice overview of the early history of IR evaluation. Also know as \"batch evaluations\", the Cranfield paradigm has come to dominate the IR research landscape over the last half a century. Nevertheless, there are other evaluation paradigms worth noting: interactive evaluations place humans \"in the loop\" and are necessary to understand the important role of user behavior in information seeking [Kelly, 2009] . Online services with substantial numbers of users can engage in experimentation using an approach known as A/B testing [Kohavi et al., 2007] . Despite our focus on the Cranfield paradigm, primarily due to its accessibility to the intended audience of our survey, evaluations from multiple perspectives are necessary to accurately characterize the effectiveness of a particular technique.',\n       'Potent therapeutics to combat SARS-CoV-2 infection include virus binding molecules, molecules or inhibitors targeting particular enzymes implicated in replication and transcription process of the virus, helicase inhibitors, vital viral proteases and proteins, protease inhibitors of host cells, endocytosis inhibitors, short interfering RNA (siRNA), neutralizing antibodies, MAbs against the host receptor, MAbs interfering with the S1 RBD, antiviral peptide aimed at S2, and natural drugs/medicines (7, 166, 186) . The S protein acts as the critical target for developing CoV antivirals, like inhibitors of S protein and S cleavage, neutralizing antibodies, RBD-ACE2 blockers, siRNAs, blockers of the fusion core, and proteases (168) .\\nAll of these therapeutic approaches have revealed both in vitro and in vivo anti-CoV potential. Although in vitro research carried out with these therapeutics showed efficacy, most need appropriate support from randomized animal or human trials. Therefore, they might be of limited applicability and require trials against SARS-CoV-2 to gain practical usefulness. The binding of SARS-CoV-2 with ACE2 leads to the exacerbation of pneumonia as a consequence of the imbalance in the reninangiotensin system (RAS). The virus-induced pulmonary inflammatory responses may be reduced by the administration of ACE inhibitors (ACEI) and angiotensin type-1 receptor (AT1R) (207) .\\nSeveral investigations have suggested the use of small-molecule inhibitors for the potential control of SARS-CoV infections. Drugs of the FDA-approved compound library were screened to identify four small-molecule inhibitors of MERS-CoV (chlorpromazine, chloroquine, loperamide, and lopinavir) that inhibited viral replication. These compounds also hinder SARS-CoV and human CoVs (208) . Therapeutic strategies involving the use of specific antibodies or compounds that neutralize cytokines and their receptors will help to restrain the host inflammatory responses. Such drugs acting specifically in the respiratory tract will help to reduce virus-triggered immune pathologies in COVID-19 (209) . The later stages of coronavirus-induced inflammatory cascades are characterized by the release of proinflammatory interleukin-1 (IL-1) family members, such as IL-1 and IL-33. Hence, there exists a possibility that the inflammation associated with coronavirus can be inhibited by utilizing anti-inflammatory cytokines that belong to the IL-1 family (92) . It has also been suggested that the actin protein is the host factor that is involved in cell entry and pathogenesis of SARS-CoV-2. Hence, those drugs that modulate the biological activity of this protein, like ibuprofen, might have some therapeutic application in managing the disease (174). The plasma angiotensin 2 level was found to be markedly elevated in COVID-19 infection and was correlated with viral load and lung injury. Hence, drugs that block angiotensin receptors may have potential for treating COVID-19 infection (121) . A scientist from Germany, named Rolf Hilgenfeld, has been working on the identification of drugs for the treatment of coronaviral infection since the time of the first SARS outbreak (19) .\\nThe SARS-CoV S2 subunit has a significant function in mediating virus fusion that provides entry into the host cell. Heptad repeat 1 (HR1) and heptad repeat 2 (HR2) can interact and form a six-helix bundle that brings the viral and cellular membranes in close proximity, facilitating its fusion. The sequence alignment study conducted between COVID-19 and SARS-CoV identified that the S2 subunits are highly conserved in these CoVs. The HR1 and HR2 domains showed 92.6% and 100% overall identity, respectively (210) . From these findings, we can confirm the significance of COVID-19 HR1 and HR2 and their vital role in host cell entry. Hence, fusion inhibitors target the HR1 domain of S protein, thereby preventing viral fusion and entry into the host cell. This is another potential therapeutic strategy that can be used in the management of COVID-19. Other than the specific therapy directed against COVID-19, general treatments play a vital role in the enhancement of host immune responses against the viral agent. Inadequate nutrition is linked to the weakening of the host immune response, Clinical Microbiology Reviews making the individual more susceptible. The role played by nutrition in disease susceptibility should be measured by evaluating the nutritional status of patients with COVID-19 (205) .',\n       \"In this research, 'science' refers to a system of addressing a hypothesis, using appropriate, replicable methods to investigate, experiment, observe and describe phenomena (Aikenhead, 2001; McKinley, Brayboy & Castagno, 2008) . Depending on the results, a hypothesis can eventually contribute to scientific theory, or a better description about the world, if it cannot be proven false. 'School science' or 'western science' refers to the hegemonic science methodology (as above) currently practised in western education systems to date.\\nSince the late 20th century, science in general has become increasingly focused on accessing traditional Indigenous knowledge systems as part of a suite of solutions needed to deal with global environmental issues (IAEWG, 2012; International Council for Science, 2002) . Recognising the need for Indigenous perspectives, researchers are producing studies that formally engage with Indigenous knowledge (e.g., Davis & Wagner 2003; Grech et al., 2014; Hind, 2015; Norris & Hamacher, 2009 ). In the process, western researchers are illustrating the deep integrative connections between domains of knowledge that have tended to remain isolated in western science, for example, astronomy and ecology coupled with sustainability and environmental management (Bird, Bird, Codding, Parker & Jones, 2008; Prober, O'Connor & Walsh, 2011) .\\nAustralia has not been isolated from these fresh approaches to science in general. In some cases, the nation's researchers and teachers have shown leadership by recognising and valuing Indigenous knowledge systems and the relationship cultural ways of knowing have to science (IAEWG, 2012; PMSEIC, 2003) . While tensions certainly exist between science knowledge systems and Indigenous knowledge systems (Aikenhead, 1996; Ryan, 2008) , the groundwork has been laid to introduce programs that integrate the two systems to produce outcomes, which are both, innovative and beneficial to the people they seek to serve.\\nEmbedding cultural themes into the new National Science Curriculum (NSC) for schools is an approach adopted by the Australian Federal government. The NSC reflects an acknowledgement of Indigenous knowledge as a valuable resource to develop student competencies. This requires teachers to engage all students in science through the development of learning programs within which the students themselves want to be a part.\\nThe need to act is driven by research (IAEWG, 2012; PMSEIC, 2003) that shows, to a large extent, Indigenous students find school science irrelevant and unrelated to their personal lives. This is reflected in the disparity between Indigenous and non-Indigenous engagement and learning outcomes across Australia, (IAEWG, 2012; MCEEYDA, 2010; PMSEIC, 2003) . For example, the Trends in International Mathematics and Science Study (TIMSS) results and learning outcomes across Australia (IAEWG, 2012; MCEEYDA, 2010; PMSEIC, 2003) show that Australian Year 4 Indigenous students attained a lower mean score in science (458) compared with their nonIndigenous Australian counterparts (522) in 2011 (Thomson et al., 2012) . Furthermore, Indigenous students' mean score for science also fell below the international Intermediate benchmark set at 475. Students also responded to items about whether or not they liked learning science. Indigenous students scored significantly lower on this scale compared with their non-Indigenous counterparts. Similar results were reported for the scale measuring students' levels of confidence within school science where a significantly higher number of Indigenous students indicated that they were not confident with science (Thomson et al., 2012) . These trends were also present in the secondary school results.\\nThis disconnect with school science seems to continue beyond the compulsory years of schooling where very few Indigenous students tend to pursue science in further study. In terms of science-related careers for school leavers, statistics show that Indigenous students are less likely to find employment in science-and technology-related jobs compared to their non-Indigenous peers (2% compared with 7%), (IAEWG, 2012) . Given these results, and the fact that the new NSC which incorporates Indigenous knowledge is being implemented in Australian schools, it is timely to investigate the impact of a crosscultural science program on both Indigenous and non-Indigenous students.\\nTo date, few Australian studies have been conducted on the impact of implementing a science curriculum configured by integrating Indigenous knowledge with western science concepts. Significant to the relevance of this research, we were unable to locate any Australian research that targets Year 5 and 6 primary school students interacting with crosscultural science programs about astronomy. Given the importance the NSC places on ensuring Indigenous perspectives are integrated into school science programs, it is anticipated that investigating the impact of such programs would be of value.\\nThe purpose of this research, therefore, is to investigate the impact of a school science program, Indigenous Sky Stories (ISS) that is focused on the astronomy content of the Years 5-6 NSC and which integrates Indigenous knowledge and science concepts, on students' perceptions and engagement of science. The pilot phase of this project involved 19 schools, identified as having high Indigenous student enrolments, drawn from the western region of New South Wales. The current paper focuses on one primary school involved in the pilot program. In-depth accounts of what happened during the implementation of the program at this school are shared and student reactions reported. We also examine students' attendance patterns before and during their involvement in the project. Finally, we consider some of the implications for practice and further research.\",\n       'Since the beginning of reforms under Deng Xiaoping in 1978, China has undergone remarkable development: For about three decades, the economic performance has been outstanding with even double digit growth rates. Today, China not only accounts for the world\\'s largest share in World GDP (17.65% versus 15.71% for the United States (US), see World Economic Outlook, WEO, 04-2016) but is also the world\\'s leading exporter (having surpassed the US in 2007 and Germany in 2009) and the world\\'s second largest importer (WTO, 2015) . 1 However, in 2011, China\\'s growth rate started to decline and amounted to \"only\" 6.9% in 2015 according to official figures; many observers have assessed growth in China as being even lower (which is still quite high compared to other emerging marking economies (EMEs) at the same development stage). A considerable body of literature has emerged (already before 2012) dealing with the concerns that China\\'s growth strategy is unsustainable, arguing that the Chinese economy needs rebalancing 2 (meaning among other things a shift from investment-and export-led to a more consumption-and inward-driven growth path). Hence, China could soon be confronted with a further severe growth slowdown or could even enter a middle-income trap (MIT). 3 The latter term has already emerged in the political debate in China. For example, in his speech at the World Economic Forum in Davos 2015, the Chinese Premier Li Keqiang mentioned the various reforms China has to undertake in order to \"successfully overcome the \\'middle-income trap\\'\" 4 .\\nOur paper deals with the questions of whether China is in the MIT or will enter the MIT in the future. We discuss the relevant (basic and applied) MIT literature and apply various MIT definition and triggering factor approaches to discuss the answers to these questions.\\nThe previous literature on MIT and China can be divided into two branches. There is (A) basic research on the MIT (i.e., cross-country studies 5 and case studies 6 that try to construct MIT definitions and/or find MIT triggering factors in general), often applying its results to China. Furthermore, there are (B) applied studies particularly exploring the development indices (and \"MIT triggering factors\") in China and attempting to derive policy implications to avoid the MIT. 7 While the papers of branch (A) discuss their implications for China rather informally and parenthetically, we focus our attention on China and apply, among others, the 1 In 2014, China\\'s share of the world\\'s trade merchandise exports accounted for 12.33% -the corresponding US level was only 8.53%. 2 For literature on rebalancing in China, see, for example, Blanchard and Giavazzi (2005) , Aziz (2006) , Prasad (2009) , Kawai and Lee (2015) , and Wagner (2015 Wagner ( , 2017 . See also the 12 th and 13 th five-year plans of the Chinese government. 3 According to some studies, China is already in the MIT (e.g., World Bank, 2013) . 4 The whole speech is online available at: https://www.weforum.org/agenda/2015/01/chinese-premier-likeqiangs-speech-at-davos-2015/. 5 For example, Aiyar et al. (2013) , Arias and Wen (2016) , Eichengreen et al. (2014) , Bulman et al. (2014) , Felipe et al. (2012 Felipe et al. ( , 2014 , Han and Wie (2015) , and Woo et al. (2012) . 6 Examples include Cherif and Hasanov (2015) , Daude (2010) , Daude and Fernández-Arias (2010) , Egawa (2013) , Flaaen et al. (2013) , Hill et al. (2012) , Jankowska et al. (2012) , Jiminez et al. (2012) , Jitsuchon (2012) , and Tho (2013) . 7 For example, Cai (2012) , Huang (2016) , Islam (2015) , Lee and Li (2014) , Wagner (2015) , Wen and Xiong (2014) , Wu (2013), Yao (2015) , Yiping et al. (2014) , Zeng and Fang (2014) , Zhang (2014) , Zhang et al. (2012) , and Zhuang et al. (2012) . consensus results of branch (A) for predicting whether China is in the MIT and whether it will fall into the MIT.\\nBranch (B) has not achieved consensus regarding the answers to the questions whether China is in the MIT and/or whether it will get caught in an MIT. In contrast to branch (B), we do not base our discussion only on triggering factors, but also consider the MIT-definition approaches, to analyze whether China is or will fall into the MIT. In this way, we add further arguments to the discussion in the literature of branch (B) . Furthermore, we have a different focus regarding the choice of the main triggering factors in comparison to the branch-(B)consensus, since we base our analysis more strongly on the MIT basic research results (branch (A)).\\nDespite the increasing number of articles dealing with MIT, there is still no clear and generally accepted definition, and some researchers are rather skeptical whether the MIT exists in the sense that middle-income countries are more frequently experiencing a growth slowdown than countries in other income ranges. Obviously, this question is important, since we do not need to worry about China entering an MIT if the MIT does not exist. Therefore, in Section 2, we first take a closer look at the discussion in the literature regarding the existence of MIT, before we further analyze whether China is or will be confronted with it in Section 3. In the latter section, we first analyze according to which definitions China is or will be in the MIT (or not) and then focus on the MIT triggering factors. There we provide an overview of the triggering factors identified in the literature and study the empirical evidence on the development of the most important triggering factors in China for assessing whether China will enter the MIT. Finally, Section 4 briefly summarizes our main findings.',\n       'Importantly, even the most pessimistic scenario (low SSC, high SLR) resulted in projections of a Bay-wide increase in habitat until nearly 2050, indicating that large-scale effects of SLR on tidal marsh may not be seen until near the end of the century. Furthermore, due to the rapidly increasing rate of SLR projected near the turn of the next century, the trajectory of marsh loss is likely to continue at accelerated rates after 2100, with anticipated severe consequences if high rates of SLR continue. This pattern, and the potential for rapid marsh plain loss once marsh drowning begins [42] indicates the importance of proactive marsh conservation planning, via the application of sediment to raise elevations at vulnerable sites before marsh loss occurs, the prioritization of more resilient (high sediment) sites for restoration, and the protection of key upland sites as future marshland. Although our results suggest that sites with low SSC may not be sustainable regardless of starting elevation, the strategic repeated delivery of sediment could potentially be used to sustain a site indefinitely. This requires a shift in sediment management strategies to capture and redistribute excess sediment, especially clean dredge materials. Collaborative efforts to maximize the beneficial reuse of dredge materials are already underway among San Francisco Bay jurisdictions and stakeholders. Because sediment contamination is a major concern [73,74], an approach using multiple lines of evidence to assessing sediment quality has been developed in part to inform sediment reuse decisions and minimize ecological impacts [75]. Due to regional variability in sediment availability, marsh resilience was projected to be much lower in some subregions (e.g., Central Bay) than others (e.g., North and South Bay systems). Thus, when restoration choices are explicit, efforts should be concentrated in sediment-rich areas with better prospects for long-term sustainability. However, highvulnerability (low-sediment) subregions should be closely monitored and may provide early opportunities for validation of marsh sustainability projections. Although it would be easy to dismiss these areas, certain sites may be more amenable to intervention, and could be maintained either by restoring natural sources of sediment or by strategically applying dredge materials [71]. The relative viability of different sites would depend on factors such as wind-wave exposure, proximity to sediment sources, and accessibility, and may also be evaluated with respect to ecological values, e.g., presence of special status and endemic species. Furthermore, future restoration priorities also should be informed by the availability of adjacent upland sites that are suitable for lateral marsh expansion or migration (i.e., undeveloped sites with very gradual slopes). Although our spatial analysis revealed relatively little area naturally available to accommodate future marshes (up to 3,300 ha under high SLR), we found that more than twice as much area (up to 7,000 ha) could be reclaimed by removing levees and other barriers to tidal action. In some of these areas, managed realignment of barriers to tidal inundation could be useful to facilitate marsh expansion while continuing to provide flood control benefits [76,77]. Unfortunately, the large majority of areas with elevations suitable for marsh expansion within the Bay (.13,000 ha) are already urbanized and thus Figure 10. Projected elevation change for the most pessimistic scenario (low sediment, high SLR), using the on-line tool to zoom into the Petaluma River area. Maps assume an absence of levees, roads, and other barriers to tidal inundation. Maps demonstrate the increase in low and mid marsh through mid-century, followed by a decline as SLR accelerates and outpaces accretion rates. Note the limited amount of landward marsh expansion (See Table S3 for area summaries). doi:10.1371/journal.pone.0027388.g010 unavailable. The existing opportunities for marsh expansion into upland areas within particular subregions may be evaluated using our web-based tool.',\n       nan,\n       'We next organised these sequences across two different controls (termed chimeric A/B standards). We 104 partitioned the different targeted regions used by each country into two independent groups and then 105 assembled the regions in tandem (Fig. 1b) . A fragment of the human RNase P gene (RP), which is used as 106 a positive human control, was also added to the chimeric standard B.',\n       nan,\n       \"In order to establish whether parental involvement had an influence on the students' performance on BAS in the different schools, a chi-squared contingency test was conducted between Parental Involvement and Schools for first-/second-graders and third-/fourth-graders separately. No significant results were found in either grade, indicating that parental involvement in math did not differ significantly across the schools.\\nSimilarly, to establish whether outside school mathematics instruction might have an influence on the students' performance in BAS in different schools, a chi-squared contingency test was carried out between Outside School Mathematics and Schools for first-/second-graders and third-/fourth-graders separately. No significant results were found in either grade. Therefore, the students' outside school formal mathematics instruction in math did not differ significantly across the three schools.\",\n       'The majority of teachers (94.9%) participated in trainings that were offered as part of the implementation of TEACHNJ and AchieveNJ. When we examined their self-reported knowledge, however, we found much variation across key concepts. That is, the teachers in this study were more knowledgeable of the most immediate and concrete aspects of the new evaluation system and less knowledgeable of the law TEACHNJ and its implementation platform AchieveNJ. For example, using a rating scale, a majority of teachers said they were knowledgeable (combination of \"very knowledgeable\" and \"somewhat knowledgeable\") about \"student growth objectives\" (84.6%), the \"teacher evaluation system\" (82.5%), and the \"teacher observation instrument\" (71.9%). In contrast, less than half of teachers said they were knowledgeable about \"AchieveNJ\" (46.8%), \"TEACHNJ\" (43.1%), the \"school improvement panel\" (34.4%), and the \"District Evaluation Advisory Committee\" (29.1%). FIGURE 2 -ABOUT HERE Next, we examined teachers\\' evaluation of implementation efforts of the new evaluation system. As seen in Figure 3 , teachers are critical of these efforts. A clear majority (more than two-thirds) of the sample disagreed to some extent with statements describing the implementation efforts. Slightly more than four-fifths (81.8%) disagreed with the statement \"the rollout of the new teacher evaluation system, TEACHNJ, was done at a comfortable pace that allowed for effective implementation,\" and slightly more than three-quarters (77.9%) disagreed with the statement, \"The new laws and changes in teacher evaluation are effectively communicated by NJ Department of Education.\" FIGURE 3 -ABOUT HERE Not only were the teachers in this study critical of the implementation efforts, but the majority of respondents reported pessimistic views of the impact of the new evaluation system on teaching and learning. About three-quarters (74.0%) of respondents said it will not improve teaching (30.1% strongly disagreed and 43.9% disagreed with a positive statement). Slightly more than four-fifths (81.1%) of respondents said the new system will not improve student learning (30.1% strongly disagreed and 51.0% disagreed with a positive statement).\\nFinally, we examined the distribution of the dependent variable: Support for AchieveNJ. As expected, two-thirds of respondents (66.5%) indicated they oppose AchieveNJ (29.5% strongly opposed and 37percent opposed). Only one-in-ten respondents (9.5%) said they favor AchieveNJ. The remainder, one-fourth of the sample (24%) said that they are not sure whether they favor or oppose AchieveNJ.',\n       'Chance and luck are important in my life Table 5.9 shows the correlations among the items, the varimax rotated factor matrix, and some factor statistics. Two factors, explaining 37.0 percent of the variance of the six items, were obtained. The first factor, explaining 28.8 percent of tly item variances, includes items that emphasize obstacles to attempts to control one\\'s life, r one\\'s personal efficacy (e.g., \"Every time I try to get ahead something or someone stops me\"). The second factor, explaining 8.2 percent of the item variances, includes items that emphasize the role of chance and luck in one\\'s life. The two factor solution is similar to a previously found distinction in the literature on Locus of Control, that individuals distinguish between control by other people and control by impersonal forces.47 Table 5.9 -Correlations, varimax rotated factor matrix, and factor statistics for locus of control items Maximum likelihood confirmatory factor analyses were conducted to statistically examine the degree to which a two factor model better explains responses to the six items than a one factor model. Results were first obtained for the entire sample and then for subsamples for which reliability coefficients showed substantial variation. These were black versus nonblack respondents and respondents at the various reading quaitile levels. Results are shown in table 5.10.  (1988). PC -USREL 7.12. Mooresville, Indiana: Scientific Software, Inc. bModels 1 and 2 represent the one and two factor models. CDegrees of freedom me the number of covariances among the variables minus the number of parameters estimated. For the one factor model the number of parameters is equal to the number of factor loadings (6). For the two factor model the number of rerameters is equal to the number of factor loadings (6) plus the number of correlations between factors (1). dMaximum likelihood chi-square goodness-of-fit test statistic. ellie difference between chi-squares for related models is distributed as chi-squaie with degrees of freedom equal to the difference in degrees of freedom between the two models. fThe chi-square for model 1 divided by the chi-square for model 2. The results reported in table 5.10 indicate that for the entire sample and for the various subsamples under consideration the two factor model fits the data substantially better than a one factor model, confirming that the data are not unidimensional. It should be noted that neither model fits the data, as indicated by the large maximum likelihood chi-square statistic:18 This statistic is extmmely sensitive to model deviations in large samples such as the ones used in this analysis. For present purposes, it is not important that either model fits so much as that one model, for example, the two factor model, fits the data better than an alternative model, for example, the one factor model:19 The large difference in chi-square values between the models for the entire sample and for the subpoups indicate the significant improvement in fit of the two factor model over the one factor model. As an indication of relative improvement of the fit across the subgroups, the ratio of chisquares for the two models (model 1 divided by model 2) is presented in the last column. For the entire sample and the two race-ethnicity subsamples this ratio is similar in magnitude, suggesting that the improvement in fit for the two factor model compared with the one factor model is similar for these groups of respondents. This does not appear to be the case when the different reading quartile subgroups are considered. Here the improvement in fit is substantially greater for respondents in the highest reading quartile subgroup than for respondents in the lowest reading quartile subgroup, with improvement increasing steadily for the two middle groups. This suggests that, as reading ability increases, the two factor model becomes a more viable explanation of response: to the Locus of Control items. This is also seen in the bigger differences between the chi-square statistics for the two models for the higher versus lower reading groups (these differences are roughly comparable because the subgroups are made up of nearly equal numbers of respondents). Substantively, these results suggest that respondents with greater reading ability are better able to make the distinction between obstacles to achieving their goals and the role of chance and luck in their lives. Factor analysis of the Self-Concept scale. The second set of factor analyses were conducted on the seven items comprising the Self-Concept scale. The variable names and abbreviated text for these items are listed in table 5.11. Before analyzing the items, values for BYS44A, BYS44D, BYS44E, AND BYS44H were reversed to make the directions of all the items in the scale comparable. WeS44L I feel I do not have much to be proud of Table 5.12 shows the correlations among the items, the varimax rotated factor matrix, and some factor statistics. Two factors, explaining 46.2 percent of the variance of the six items, were obtained. The first factor, explaining 36.5 percent of the item variances, includes items that ask about general evaluations of oneself (e.g., \"I am a person of worth\"). The second factor, explaining 9.7 percent of the item variances, includes items referring to transient self-evaluations or evaluations occurring during specific instances (e.g., \"At times I feel I am no good at all\"). It is no contradiction for respondents to report, for example, that they are generally positive about themselves, but that at times they feel negative.50 However, discerning the dfference is a relatively subtle distinction and, as with the distinction made for the Locus of Control items, should be more pronounced among the students with greater verbal ability. As with Locus of Control, maximum likelihood confirmatory factor analyses were conducted to examine the degree to which a two factor model better fit the Self-Concept data than a one factor model. As for the Locus of Control analysis, results were first obtained for the entire sample and then for subsainples for which reliability coefficients showed substantial variation. Again, these were black versus nonblack respondents, and respondents at various reading quartile levels. Results are shown in table 5.13. The results indicate that for the entire sample and for the various subsamples the two factor model fits the data substantially better than a one factor model, once again confirming that the responses are not unidimensional. As with the Locus of Control data, it should be noted that neither model fits the data. However, the large difference in chi-square values between the models for the entire sample and for the subgoups indicate the significant improvement in fit of the two factor model over the one factor model.  (1988). PC -I.JSREL 7.12. Mooresville, Indiana: Scientific Software, Inc. bModels 1 and 2 represent the one and two factor models. CDegrees of freedom are the number of covariances among the variables minus the number of parameters estimated. For the one factor model the number of parameters is equal to the number of factor loadings (6). For the two factor model the number of parameters is equal to the number of factor loadings (6) plus the number of correlations between factors (1), dMaximum likelihood chi-square goodness-of-fit test statistic. eThe difference between chi-squares for related models is distributed as chi-square with degrees of freedom equal to the difference in degrees of freedom between the two models. file chi-square for model 1 divided by the chi-square for model 2.',\n       nan, 'Andrew L. Pais http://orcid.org/0000-0001-6535-0990',\n       'The anatomical MRI data were analysed using FreeSurfer v5.1 longitudinal pipeline Reuter et al., 2012) . Briefly, each anatomical scan was normalized to an individual within-subject template based on all data and therefore not biased to a single visit (Reuter et al., 2012) . Anatomic regions of interest were then defined in template space, and warped to subject space at each visit. The white matter and pial surface segmentation was examined visually for quality assessment using FreeSurfer tools. In cases where dura or skull influenced the segmentation, voxels were either manually edited or corrected by adjusting the watershed threshold. The preprocessing steps were subsequently re-run on the edited files and re-evaluated. This cumulative quality assessment was iterated until the segmentation results were deemed either sufficient or irreparable for cases with poor T 1 -weighted MPRAGE images. From each of these FreeSurfer parcellations, we extracted hippocampal volume and normalized it by estimated total intracranial volume using simple regression (Mathalon et al., 1993; Buckner et al., 2004) .',\n       'When researchers employed randomized experimental designs, the comparison groups are formed to be only randomly different on all background covariates. However, in studies comparing intact groups or nations, randomization is impossible. Matching methods using propensity scores could then be used to compose comparable samples by equating the distribution of covariates in the comparison groups (Stuart, 2010) . If the pre-existing achievement differences between countries would disappear after matching, it can be concluded that the country differences in achievement can be attributed to the background differences. If the pre-existing achievement differences would remain intact after matching, the conclusion can be drawn that the differences cannot be reduced to the background differences observed. When used this way, propensity matching can be seen as an advanced kind of covariance analysis (Van de Vijver & Poortinga, 1997) .\\nThere are many types of propensity score matching methods. The matching methods that were used in this study are exact, nearest neighbor, and optimal matching. Exact matching is the simplest version of the matching. In this procedure, an individual who has exactly the same values on all covariates is matched with an individual in the comparison group (Ho, Imai, King, & Stuart, 2011) . A problem with exact matching is that when matching is done on several background variables, it is probable to end up with a very small sample that is matched on these covariates, but is very dissimilar on other aspects, which can create an even larger bias (Rosenbaum & Int J Res Educ Sci Rubin, 1985) . As a solution another approach in matching was introduced that constructs matched sets by ensuring similar distributions of the covariates, thereby loosening the need to have exact matches on all the individual variables. Nearest neighbor matching is one of the common matching procedures which selects one individual from a comparison group with the closest matching covariate properties among all available individuals in the group. The unmatched individuals are discarded. As a matched individual is no longer available, the order of matching could change the quality of matches (Stuart, 2010) . Optimal matching takes into account global distance instead of individual distance when performing the matching (Rosenbaum, 2002) . Therefore, nearest neighbor matching could be used when the aim is to create well-matched pairs, whereas optimal matching could give better results when the aim is to create well-matched groups (Gu & Rosenbaum, 1993) . Besides these matching methods, there are other propensity score methods available, such as weighting, full matching, and subclassification. These methods give a weight between 0 and 1 to each individual based on covariates and no individuals are discarded. As the main focus of this study is to form new matched groups and compare results in terms of DIF, weighting, full matching and subclassification methods are beyond the scope of this paper.\\nAfter matching is done and new groups are formed, it is necessary to evaluate the quality of the matching by examining the closeness of the covariate distribution of the resulting matched samples, known as balance. A poorly balanced matching means that groups differ considerably in their distributions of matching variables. In order to evaluate balance of the matched groups, Rubin (2001) recommended that propensity score mean differences should be less than half a standard deviation and propensity score variance ratios should be close to one. A matching result that produced imbalanced samples should be rejected and better balanced sample results should be searched (Ho, Imai, King, & Stuart, 2011; Stuart, 2010) . There might be cases in which comparison groups are too far apart from each other in the background variables that make it hard to produce adequate estimates using matching (Rubin, 2001 ).',\n       'Ky ky=1 b kx,ky B kx (t x )B ky (t y ) for t = (t x , t y ). In the B-spline based scalar-on-image regression model Marx and Eilers (2005) the unknown coefficients b and α are found by minimizing a penalized least squares criterion including a quadratic penalty for penalizing differences in b along the x-and y-axes. This yields a smooth function and -in most cases -an identifiable model Happ (2013) ; Scheipl and Greven (2016) . The penalty parameters can be found e.g. by (generalized) cross-validation Marx and Eilers (2005) or using a restricted maximum likelihood (REML) approach Wood (2011) . The main assumption here is that the unknown coefficient function β(·) can be represented well by the K x · K y tensor product spline basis functions and that it has smooth variation. In the context of neuroimaging this would mean that brain areas that are close to each other have a similar association with the response without abrupt changes in β(·).',\n       'The cDVR at the initial and follow-up PiB-PET was first examined in relation to age at the initial PiB (Figure 1) . Then, annual differences and annual percentage differences were estimated as differences between cDVR at first follow-up and at the initial PiB-PET, adjusted for interscan interval. Similarly, annual differences and percentage differences were also estimated for the 15 ROIs. The annual cDVR and regional changes in the whole group and in those with a minimal and an elevated initial cDVR were evaluated using the Wilcoxon signed rank tests to test whether DVR values increased over time (1-sided tests). In addition, a regression model was used to assess whether age (continuous or dichotomized at age 80 years) was a predictor of longitudinal change in cDVR. Subsequently, we used the Wilcoxon rank sum tests to evaluate whether change in DVR differed between those with minimal vs elevated cDVR at the initial PiB-PET. We also repeated analyses examining whether baseline age (continuous or dichotomized at age 80 years) was an additional predictor of longitudinal change in cDVR, adding dichotomized baseline cDVR as an additional covariate.',\n       'The quality of information gathered is greatly affected by how it is collected and who collects it. Since good decisions are rarely made on the basis of poor information, the task of collecting and processing data (includes editing and scoring) constitutes an integral aspect of the assessment plan. A field survey approach analogous to that used in the ongoing NAEP program is recommended as the most cost-effective way for Minnesota to collect their assessment\\' data. Specially trained survey teams, using a \"one day in--one day out\" approach, will administer exercises and collect background information on the students and schools',\n       'Given a multi-dimensional population dataset, the inference of contrasted pseudotemporal trajectories (and an individual pseudotime value) consists of four main steps:\\nFor high-dimensional datasets (e.g. ~40,000 transcripts), initial selection of features most likely to be involved in a trajectory across the entire population. We apply the unsupervised method proposed by (Welch et al., 2016) , which does not require prior knowledge of features involved in the process or differential expression analysis. Features are scored by comparing sample variance and neighborhood variance. A threshold is applied to select those features with higher score, e.g. we kept the features with at least a 0.95 probability of being involved in a trajectory (i.e. ~3000 gene transcripts).',\n       'This study comprises secondary analysis of data from the first and second cohort measurements of the COOL 5-18 study (Cohort Research on Educational Careers), a large-scale, nationally representative, longitudinal cohort study into the determinants of the cognitive and social-emotional development of children and adolescents in the Netherlands 1 . The COOL 5-18 datasets are available for third-party use, as in the present study. The first cohort measurement included N = 11,609 Grade 6 students from 550 primary schools. The second measurement included N = 21,384 Grade 9 students from 151 secondary schools. A total of N = 2,646 students from 355 primary schools and 143 secondary schools participated in the first measurement when in Grade 6 and in the second measurement when in Grade 9. Participants took several cognitive tests at each measurement, including a math test. They also completed self-report questionnaires\\nthat included scales from externally validated questionnaires on topics including self-efficacy and school functioning. Parents/caregivers completed a demographic questionnaire and schools provided administrative data (e.g., age, sex, educational track). The following paragraphs describe the participants, instruments and data relevant to the present study.',\n       'Volume can be considered as a simple, coarse and intuitive anatomical descriptor, which is independent of the patient position within the scanner. Many previous ROI-based volumetry studies focused on structures such as entorhinal cortex, hippocampus, and amygdala, which are known to present the largest atrophy at the earliest stages of the neurodegenerative process (Laakso et al., 1996; Apostolova and Thompson, 2008) . However, it is known that neurodegeneration spreads over many other regions, in particular over the structures of the limbic system, such as thalami, which are reported less frequently. The statistical techniques to assess significant volume differences are simple univariate hypothesis tests, and correction for multiple comparisons is not an issue. However, volume is an unspecific anatomical descriptor. Recent works show that the shape of a brain structure can be more useful than the volume for population studies (Styner et al., 2004; Csernansky et al., 2000) . Fig. 3 . Illustration of the mean pose of subcortical nuclei for each patient group.\\nMore complex shape descriptors typically involve vectors of large dimensionality. For example, shape analysis of a single structure, such as the hippocampus using coordinates of point sets on the surface as shape descriptor, requires thousands of parameters. Statistical analysis on such high dimensional feature space with relatively small sample size (a few hundreds in the best cases) is problematic.\\nRelative pose information can be regarded as an interesting tradeoff for the following reasons. First, the dimensionality required in pose characterization is not very high, just 7 parameters for each structure. Accordingly, the multiple comparison corrections will not be very severe. Second, the pose information 3 can be considered as a generalization of volume measurements, because in addition to volume, it provides information about the location and orientation of each object. Third, the pose information is complementary to shape, because the relative pose between structures is typically disregarded in the alignment stage performed in single-structure shape studies. In this paper a methodology for analysis of the relative pose information from a set of brain structures has been presented. A general framework allowed us to compare several approaches to perform statistical analysis: pseudo-Riemannian metrics, that were proposed in Woods (2003) in the context of linear transformations and in Park (1995) ; Zefran et al. (1996 Zefran et al. ( , 1999 for SE(3) group; LogEuclidean framework (Arsigny et al., 2006b) ; left-invariant Riemannian metrics on the similarity group, which is, to our knowledge, a novel contribution; and bi-invariant metrics on the group of centered transformations. The first approach can be related to our previous work (Bossa and Olmos, 2006) , while the latter approach to Styner et al. (2006) ; Gorczowski et al. (2010) . The comparison of the geodesics induced us to select the bi-invariant centered transformation approach for the following reasons: it avoids the undesirable effect of the non-monotonic trajectories of the scale parameter (see Fig. 1 and discussion below). Moreover, this approach allows a more clear interpretation of the results because the contribution of each natural category, either rotation or translation or scale, are independent. It should be also noted that, to our knowledge, this is the first work where the pose information provides positive results with clinical data, because the pose was useless in a longitudinal study of autism (Styner et al., 2006; Gorczowski et al., 2010) and only normal subjects were considered in our previous work (Bossa and Olmos, 2006) .\\nThe application of the methodology was performed in order to illustrate the usefulness of the pose information compared to the volume information in a particular case. To our knowledge, this is the first study considering the whole set of pose parameters of the subcortical nuclei as a potential MRI marker of AD. Although the focus of the paper was devoted to the methodological aspects rather than extracting of clinical useful knowledge from the analyzed data, some interesting results were obtained which deserve discussion.\\nRegarding the group analysis, it can be seen from Table 2 that the pattern of significant pose differences was different at each group comparison. At the earliest stage of the disease, represented here by the NOR-MCIs comparison, statistical differences were found only for the scale parameter of bilateral hippocampi and thalami. When comparing NOR-MCIc groups, in addition to the previous differences, an important asymmetry was found in the left hemisphere because all subcortical nuclei showed statistically significant translations. It is interesting to note that this left-hemisphere asymmetry was also recently reported in Cherbuin et al. (2010) . At the latest stage, when comparing NOR-AD patients, a larger number of subcortical structures showed significant differences in the scale parameter, but also interestingly, translations and rotations were significant in both hemispheres. These pose differences were nicely illustrated in Fig. 3 , showing that while some subcortical structures show pose differences along the complete time-course of the disease, such as the hippocampus with an atrophic behavior or caudate nuclei with translations, other structures only experience pose differences at specific stages. Even though pose differences in the MCIs-MCIc comparison were not statistically significant after the correction for 3 When the scale normalization is selected accordingly.',\n       \"This paper reviews the research literature on the relationship between parental involvement (PI) and academic achievement, with special focus on the secondary school (middle and high school) level. The results first present how individual PI variables correlate with academic achievement and then move to more complex analyses of multiple variables on the general construct described in the literature. Several PI variables with correlations to academic achievement show promise: (a) communication between children and parents about school activities and plans, (b) parents holding high expectations/aspirations for their children's schooling, and (c) parents employing an authoritative parenting style. We end the results section by discussing the findings in light of the limitations of nonexperimental research and the different effects of children's versus parents' perspectives on academic achievement.\",\n       \"In this study we demonstrate and dissociate differential patterns of structural and metabolic brain differences related to age, symptom severity and disease progression in AD, cMCI and ncMCI subjects. An important aspect of our study is the focus on detection of the link between the spatial extent of AD related imaging pathology and age, symptom severity and TTC. This focus on spatial extents of pathology contrasts our study from previous research targeting rather the correlational link between these variables and the grade of glucose hypometabolism or atrophy at each particular voxel/region. Both types of relationships are rather independent. For example a reduction of neural function in a particular region under a particular threshold might have an impact on cognitive performance. However, any further increase in AD pathology in the very same region might not have any additional effect on cognition and would be therefore not detected by a correlational analysis. In this case only additional impairment of other brain regions would lead to further cognitive decline. By comparing all patient groups to the same control group and adjusting for healthy aging we further for the first time account for the potential confounding effect of the use of different control groups that was neglected in previous research.\\nThe cMCI and ncMCI differ in the amount of glucose hypometabolism and brain atrophy explained by age and symptom severity with cMCI resembling AD patients. We confirm previous findings of a correlation between TTC and glucose metabolism as well as an inverse relationship between age increase and amount of atrophy and hypometabolism associated with a particular symptom severity in AD (Matsunari et al., 2007; Salmon et al., 2000) . Given that we account for differences associated with healthy aging, our findings of differences in the amount of glucose hypometabolism and atrophy in AD subgroups subdivided by age can be interpreted as pathological age-specific add-ons that relate to a degree of functional impairment. Our study results in three important findings. Firstly, the differential age-related glucose hypometabolism and atrophy patterns between clinical groups have implications for the use of neuroimaging biomarkers for improving diagnostic accuracy and prediction of conversion to AD. While FDG-PET is the dominant biomarker in young cMCI and atrophy is mainly restricted to the hippocampus, the pattern observed in old cMCI is of prominent atrophy associated with equally marked glucose hypometabolism. This observation is consistent with a relative diagnostic superiority of FDG-PET compared to sMRI in younger patients with mild AD symptoms (Kawachi et al., 2006; Matsunari et al., 2007) . The age-related differential pattern of abnormality discovered with FDG-PET and sMRI supports previous findings of better diagnostic performance with FDG-PET only in early-onset AD patients, but equivalent performance for the two in elderly patients (Matsunari et al., 2007) .\\nThe second finding concerns the relationship between symptom severity as measured by classical screening and the progression of changes in FDG-PET and sMRI. We demonstrate for cMCI and AD patients a strong link between symptom severity and the amount of atrophy, which is not evident in ncMCI. Interestingly, no such pattern was seen in glucose metabolism with any of the patient cohorts. This result indicates that changes measured by sMRI are more strongly related to the current symptom severity in AD patients.\\nA third, more surprising result was obtained when MCI converters were split by TTC. We expected that symptom severity and TTC would be strongly linked, as neuropsychological measurements are used to determine AD stage. However, the measurements are only weakly correlated, which suggests that different mechanisms produce current (as measured by MMSE) and future cognitive states (as measured by TTC). Thus, a comparison of TTC duration subgroups with healthy control subjects showed a TTC-dependant amount of glucose hypometabolism, but no relationship with the amount of atrophy. The link between FDG-PET measurements and subsequent cognitive decline is consistent with previous reports showing the superiority of FDG-PET compared to neuropsychological measurements for prediction of subsequent cognitive decline and conversion to AD in MCI patients (Arnaiz et al., 2001; Chételat et al., 2005; Hinrichs et al., 2011) .\\nThe differential results for TTC and symptom severity-related differences observed with FDG-PET and sMRI cannot be explained by different pathologies or other factors because the cMCI subgroups came from splitting the same group of subjects.\\nThe current view of the progression of FDG-PET and sMRI changes in AD is that functional abnormalities occur earlier than those with sMRI (Jack et al., 2010) . Our results suggest a modification of the current model of biomarker changes related to progression from MCI to AD (Jack et al., 2010) . Imaging abnormalities appear at an early stage in sMRI as well as FDG-PET; however, structural changes are restricted mainly to the hippocampus. In contrast, but consistent with the suggested model, glucose hypometabolism progresses rapidly in this early stage of AD. The model suggested by Jack et al. further implies that the magnitude of biomarker changes is equal in both FDG-PET and structural MRI data (Jack et al., 2010) . The assumption behind this idea is that structural changes follow functional changes. In contrast, our data suggest that FDG-PET changes reach their peak already at the MCI stage (Figs. 2 and 3 ) and do not further increase but rather decrease (e.g. because the preserved tissue is maintaining its functional activity level) at later disease stages when adjusting for atrophy effects using PVE correction. In contrast, atrophy further increases at later AD stages in our study population and even extends to regions not showing any significant hypometabolism. For this reason we expect that if one would not use PVE correction, one would observe further reductions in glucose hypometabolism even in AD in agreement with Jack et al (2010) .\\nAlthough the results of our study are relevant to improving understanding of progression from MCI to AD, some limitations need consideration to enable valid interpretation. Firstly, we used the same clinical groups to examine different variables of interest, with splitting into different subgroups depending on expression of the variables of interest. Therefore, the results may be confounded by between-group differences in uncontrolled factors such as functional asymmetries, intelligence quotient and other more specific cognitive domains associated with Alzheimer's disease. Such differences in uncontrolled factors might have contributed to the observed differences in the amount of atrophy and hypometabolism and shall be overcome in future large-scale longitudinal studies. Secondly, although subgroup comparisons for confounding variables were not significant in ANOVAs, they could have contributed to the differential results. We feel this is unlikely to have introduced any systematic bias, as the between-group differences were small and restricted to only one of the subgroups.\\nWhen removing the variance explained by healthy ageing in the imaging data we make the explicit assumption of linearity of age related differences in GM volume and glucose metabolism. This issue has been a major focus of several large scale volumetric and PET studies performed in the past two decades (Gonoi et al., 2009; Good et al., 2001; Loessner et al., 1995; Smith et al., 2007) . All of these studies provided strong evidence that linear as compared to more complex models are sufficient to describe global and local atrophy and glucose metabolism reductions observed in the course of normal ageing (from the age of 20 to 100 years. Our assumption is therefore in line with previous research performed on that topic. Nonetheless, imprecisions of the applied voxel-wise healthy ageing models e.g. due to wrong linearity assumptions are a general possible confounding effect for studying agespecific differences in diseased populations. In this framework it is also important to note that age-related findings we report in our study only reflect cross-sectional differences in sMRI and FDG-PET between differently aged AD/MCI patient groups. They should therefore not be confused with within subject longitudinal changes related to AD progression.\\nA further limitation of our study is that a proportion of control subjects used in our study might have a preclinical AD pathology. The inclusion of such subjects could have lowered the effects observed in subgroup comparisons and therefore lead to an underestimation of the amount of AD pathology. Similarly, one cannot exclude that also some of the ncMCI would have converted to AD (or reverted from MCI to normal range) if followed for a longer time period. As indicated in previous studies on MCI with a follow-up of 10 years more than 80% of MCI are expected to convert to AD in the long term (Visser et al., 2006) . Current ADNI group assignments should be therefore considered carefully and the results need to be validated using cohorts with a longer follow-up than currently available from the ADNI. However, this limitation is common to all imaging studies including not pathologically proven AD patients and control subjects. Furthermore, as we compare all subgroups exactly to the same control group, the potential inclusion of control subjects with preclinical AD would affect all comparisons in the same way without introducing a bias towards a specific subgroup. Also the finding of only a very weak link between TTC and symptom severity in our cohort requires careful interpretation. TTC is only a crude and general estimate of cognitive function. It is therefore likely that some more specific cognitive tests might be more closely related to disease progression as indicated by TTC. It is important to note that all differential results reported in our study are detected by comparing all patient subgroups to the same control group. We therefore provide only indirect evidence for between-subgroup differences. In contrast, the direct subgroup comparisons were not significant. These findings therefore need to be validated in future studies by direct comparisons of patient subgroups using larger data sets than currently available from ADNI and preferably in longitudinal study designs.\\nIn our study we observed a slight decrease in sensitivity of FDG-PET at later as compared to earlier AD stages. We interpret this effect as an interaction of pathophysiology with the PVE correction performed in our study. However, this effect might be also caused by methodical artifacts, e.g. due to intensity normalisation (cerebellar metabolism is known to decline in AD) or due to increasing inaccuracy of PVE correction caused by increasing cortical atrophy and associated problems with accurate MRI segmentation.\\nOur results have potential implications for the selection of biomarkers, MCI staging and for treatment evaluation. While FDG-PET appears to be a more sensitive biomarker of disease progression at the MCI stage, structural differences as measured by sMRI seem to be more strongly linked to the current global cognitive state, potentially reflecting irreversible damage. Therefore, we suggest that for treatment evaluation it may be crucial to use both structural and functional biomarkers.\\nSupplementary data to this article can be found online at http://dx. \",\n       'Although this study is preliminary and based in a small cohort, the obtained results are in agreement with the main known findings regarding AD, such as: a decrease of both GMV and CThk [41] , and an increase of the MD in temporal, frontal and parietal regions [42] , [43] ; an increasingly compromised wholebrain structural connectivity [44] , [45] ; and also a generalized increase in the accumulation of beta-amyloid plaque [46] , [47] . Moreover, our preliminary results showed a low sensitivity of the 18F-AV-45 radiotracer for differentiation between CTRL-EMCI and LMCI-AD; and a reduction of fiber connectivity with the progression of the disease, especially for intra-hemispherical connections.\\nThis study is also in agreement with a very recent study [48] in which a related dataset and connectivity analysis were also used. The authors of this study concluded that the beta-amyloid burden, as measured by 18F-AV-45, was related to changes in structural connectivity metrics.\\nIn addition, it was shown that the MIBCA toolbox is a framework suitable for automated processing and comprehensive analysis of multimodal data, with the potential to facilitate the discovery of new aspects of physiopathology and novel biomarkers of disease.',\n       '\\naverage wave height. Below RM35 the only matches were 12hr and the only other match at RM35 was a 1,200-hr wave period corresponding to the larger upstream-driven flood event. From RM35 to RM48 there were a decreasing number of 12-hr events with decreasing wave height. From RM39 to RM61 both of the noted upstream-driven flood events were matched at each station as were an increasing number of smaller events that were still greater than the 12-hr wave period. Figure 6 provides a summary of the distribution of all the 12-hr waveforms at each station. Of note, there appears to be a trend with two distinct linear or near-linear sections of different slopes. Starting at RM1 there is a decrease of wave height with a gentle slope followed by a breakpoint between RM29 and RM35 and then a rapid decrease to RM48. Also, note that the variability in wave height is highest from RM35 to RM43. Regarding head of tide, RM45 is the last station where the median value is higher than the threshold of 0.2 ft, RM43 is the last station where the entire interquartile range is about 0.2 ft, and at RM48 even the extreme values are below 0.2 ft. Clearly the head of tide exists in this region Volume 6, Issue 1 2019Rosenquist, Hintz but is subject to some variability depending on conditions discussed below.',\n       \"Despite its limitations, the current study addresses significant gaps in our knowledge about the relationships between postnatal mental health, parenting behavior, and child outcomes in a large sample of Australian fathers. The recruitment and retention of fathers in the study is exceptionally high, and such a large, nationally representative sample has not been previously available. Building on research with mothers, this study provides empirical support for parenting behavior as a potential mechanism underpinning the longitudinal associations between fathers' postnatal mental health and later child well-being. These findings are further corroborated by the fact that child outcomes in this analysis were rated by mothers, whereas paternal psychological distress was self-reported by at DEAKIN UNIV LIBRARY on February 28, 2013 jfi.sagepub.com Downloaded from fathers, thus reducing rater-associated bias as well as variance that may be introduced when predictor and outcome ratings are provided by the same individual. This study also identifies several areas for future investigation such as exploring how fathers' mental health changes from the postnatal period across the early parenting period and how this relates to changes in parenting behavior and later child outcomes.\\nFrom a clinical perspective, this study demonstrates that fathers' postnatal mental health difficulties are a potential risk factor for later parenting and child well-being difficulties, underscoring the importance of early postnatal health care for fathers. In Australia and other countries, much work has gone toward the early identification of maternal postnatal depression through routine screening in universal settings and the provision of early intervention and support. There is a need to move toward a postnatal mental health assessment that is inclusive of the whole family, ensuring that appropriate mental health support is provided to all members of the family including fathers, particularly if the relationships observed here are repeated in clinical samples of fathers in the postnatal period.\\nIn addition to access to mental health support, fathers experiencing wellbeing difficulties early in parenthood may benefit from specific strategies and support targeting parenting in the postnatal and early childhood period. For example, psychoeducation about the range of well-being experiences and adjustment difficulties in the postnatal period can be provided, along with information about how fathers' well-being can have an impact on parenting. Given that the strongest pathways between postnatal distress and child outcomes were via parenting hostility, fathers may also benefit from preventative support and strategies that focus on emotional regulation to manage frustration and anger during stressful and challenging interactions with their children. Promoting use of age-appropriate positive behavior management strategies such as praise, positive reinforcement and planned activities for high stress times may also be important. Research is needed to evaluate whether these support approaches reduce the risk of postnatal distress and parenting difficulties for fathers both in the short and long terms. Not only is this critical for improving the well-being and parenting outcomes for fathers but also for the long-term investment in the well-being of children in the early childhood period.\",\n       'Fig. 1. The top row describes the standard prediction setting where the data set is split in a train and test set. A fixed time-delay ∆T is set between the input visits (blue dots) and the target prediction (red dot). It leads to discarding visits in between (grey dots) or entire subjects that do not present sufficient follow up visits (discarded set).\\nThe bottom row corresponds to the procedure with simulated data : the training set is composed of virtual patients that are simulated thanks to the estimation set.',\n       'Although rip currents are a major hazard for beachgoers, the relationship between the danger to swimmers and the physical properties of rip current circulation is not well understood. Here, the relationship between statistical model estimates of hazardous rip current likelihood and in situ velocity observations is assessed. The statistical model is part of a forecasting system that is being made operational by the National Weather Service to predict rip current hazard likelihood as a function of wave conditions and water level. The temporal variability of rip current speeds (offshore-directed currents) observed on an energetic sandy beach is correlated with the hindcasted hazard likelihood for a wide range of conditions. High likelihoods and rip current speeds occurred for low water levels, nearly shore-normal wave angles, and moderate or larger wave heights. The relationship between modeled hazard likelihood and the frequency with which rip current speeds exceeded a threshold was assessed for a range of threshold speeds. The frequency of occurrence of high (threshold exceeding) rip current speeds is consistent with the modeled probability of hazard, with a maximum Brier skill score of 0.65 for a threshold speed of 0.23 m s\\n, and skill scores greater than 0.60 for threshold speeds between 0.15 and 0.30 m s\\n. The results suggest that rip current speed may be an effective proxy for hazard level and that speeds greater than ;0.2 m s 21 may be hazardous to swimmers.',\n       'The first and most novel data source that we make use of provides measures of productivity,Â f k i2011 , by crop k, county i, and field f in 2011. These measures come from the Global Agro-Ecological Zones (GAEZ) project run by the Food and Agriculture Organization (FAO). The GAEZ aims to provide a resource that farmers and government agencies can use (along with knowledge of prices) to make decisions about the optimal crop choice in a given location that draw on the best available agronomic knowledge of how crops grow under different conditions.\\nThe core ingredient of the GAEZ predictions is a set of inputs that are known with extremely high spatial resolution. This resolution governs the resolution of the final grid cells in the GAEZ database and, equally, the resolution of the \"fields\" in our analysis.\\nThere are a total of 114,071 fields in the U.S. counties that comprise our empirical analysis, with 26 fields in the median county.\\nThe inputs to the GAEZ database are data on an eight-dimensional vector of soil types and conditions, the elevation, the average land gradient, and climatic variables (based on rainfall, temperature, humidity, sun exposure), in each field. These inputs are then fed into an agronomic model-one for each crop-that predicts how these inputs affect the \"microfoundations\" of the plant growth process and thereby map into crop yields. Naturally, farmers\\' decisions about how to grow their crops and what complementary inputs (such as irrigation, fertilizers, machinery and labor) to use affect crop yields in addition to those inputs (such as sun exposure and soil types) over which farmers have very little control. For this reason the GAEZ project constructs different sets of productivity predictions for different scenarios of farmer inputs. Throughout we use the scenario that relates to a high level of such inputs (including irrigation). 7 Finally it is important to emphasize that while the GAEZ project has devoted a great deal of attention to testing their predictions on knowledge of actual growing conditions (e.g. under controlled experiments at agricultural research stations) the GAEZ project does not form its predictions by estimating any sort of statistical relationship between observed inputs around the world and observed outputs around the world. Indeed, the model outlined above illustrates how inference from such relationships could be misleading; the average productivity among fields that produce a crop in any given market and time period is endogenous and conditioned on the set of fields who choose to produce that crop at prevailing prices.',\n       '\\n', nan,\n       \"Th e 11 interviews resulted in 141 pages of single-spaced textual data. Th rough a thematic analysis of the interviews, seven themes emerged relating to ranchers' views of vegetation heterogeneity, biodiversity, and ranch management. Each of these themes is described below.\",\n       'n are dealing with serious problems (for instance, mental illness, homelessness, child or domestic abuse, incarceration of a parent, etc.) that make school attendance difficult because family life has been disrupted and public agencies and schools lack a coordinated response? Is chronic early absence an indication that communities... n do not provide adequate support (such as high quality preschool programs) to help young children and families make a positive transition into elementary school? n are severely distressed and suffer from a dearth of formal or informal supports to promote the positive development of children, including regular school attendance? n experience high levels of violence that adversely affect family functioning and getting children to school safely? Since conditions can vary substantially across schools, communities, and families, examining the extent to which any or all of these factors are relevant is critical. While parents are responsible for getting their children to school every day, schools and communities need to recognize and address the barriers and challenges that may inhibit them from doing so, especially when they are living in poverty. Large numbers of chronically absent students could indicate systemic problems that affect the quality of the educational experience and/or the healthy functioning of an entire community.',\n       'In addition to the LOOCV-based parameter sensitivity test that focuses on the ideal performance with all possible parameters, model robustness test is to answer a question whether a classification model is robust so that every time in the nested cross-validation the same parameter(s) was (were) chosen as the optimized parameter(s). Therefore, how many times a specific parameter or a combination of parameters has been selected as the optimal parameter(s) is recorded by the toolbox (namely Parameter Selection Occurrence). If the parameter is selected significantly more than others, the classification model is robust. On the other hand, quite evenly distributed parameter selection occurrence indicates no dominant parameter (or parameter combination), which means that the model could drastically change even with only a few training data being changed (or less robust). Of note, it is not necessary the parameter(s) with the highest occurrence is the same as the suggested parameter(s), but we have observed that a good classification model has the suggested parameter(s) corresponding to the high occurrence.',\n       \"Purpose of review Dementia is a major cause of disability and institutionalization. Apart from age and apolipoprotein E (APOE) genotype, there are currently no established, clinically relevant, noninvasive markers of dementia. We conducted a literature search of recent observational epidemiological studies evaluating the relevance of HDL cholesterol (HDL-C) and apolipoproteins as biomarkers of future and prevalent risk of dementia.\\nRecent findings HDL-C and apolipoproteins, such as apoE have been suggested to play important roles in brain function and have been associated with dementia and Alzheimer's disease in observational studies. However, findings have been inconsistent, especially across study designs. In recent years, modern proteomic approaches have enabled the investigation of further apolipoproteins involved in the deposition and clearance of b-amyloid, a determining factor for subsequent neurodegeneration.\\nAssociations in cross-sectional studies are not always indicative of a prospective relationship. Large studies find that plasma HDL-C and apoE are inversely associated with dementia. Higher apoJ levels might be a marker of prevalent dementia, but were not associated with risk of future dementia. The investigation of HDL-C and apolipoproteins in relation to dementia represents an area of opportunity. Additional prospective studies that account for potential confounding factors and that explore potential effect modifiers such as APOE genotype and sex are needed to fully investigate the potential of these noninvasive measures in disease prediction.\",\n       \"Charles R. Sherman T he National Institutes of Health (NIH) supports various manpower development programs needed to carry out its principal statutory responsibility: to support a high-quality national biomedical research enterprise. In earlier times, NIH had additional responsibility to increase the supply of well-trained manpower in emerging clinical specialties. The information presented here focuses extramurally. There are also training programs and experiences in NIH's own clinics and campus laboratories, the intramural program. But the vast majority of the manpower development is supported and conducted in extramural settings, such as degree-granting universities and affiliated training hospitals. Formal training programs and fellowship and career development applications are evaluated and awarded based on merit by the NIH peer review system. Much training is conducted in the course of research studies supported, similarly, after competitive peer review. The effectiveness of many of NIH's training and career development programs can be evaluated because NIH receives and keeps records of who the individual trainees are. The careers and productivity of the unknown students and postdoctorals supported by research grants-a number estimated to be about twice the number of programmatically supported postdoctorals-cannot yet be examined.\",\n       'To quantify changes in habitat suitability (i.e., the likelihood of species occurrence on the base of environmental FIG. 1. Location of the study area (southern Quebec, Canada) in eastern North America, and its three dominant forest biomes: (1) the deciduous southern temperate forest; (2) boreal forest; and (3) northern coniferous forest. Reference system: NAD83/Quebec Lambert Projection (Lambert Conic Conformal). variables) between 1984-1989 and 2010-2014, we constructed species distribution models (SDMs; Zimmermann 2000, Brotons et al. 2008; see workflow in Fig. 2). A total of 299,302 species occurrences were gathered from the 30,408 point counts that were conducted between 2010 and 2014. From the initial data set of 221 species, we omitted those species with fewer than 30 presences for statistical reasons (i.e., to avoid risk of model overfitting; Wisz et al. 2008, Thuiller et al. 2014; see list of species in Appendix S3: Table S1). We used 19 predictors related to land-use/cover and geographic/topographic information to build the SDMs. Land-use/cover variables were selected to describe the main land uses (intensive and extensive practices) and vegetation types (deciduous, coniferous, and mixed forest with three height classes, tree species tightly linked to water environments, regeneration, and nonproductive forest; see brief description and acronyms in Table 1). These land-use/cover variables consisted of the percentage of area that was occupied by each land-use/cover type within a radius of 500 m for each point count (Appendix S2: Figs. S2, S3). Latitude, longitude, and altitude were also included as predictors to account for geographic and topographic heterogeneity, thereby improving the predictive accuracy of the models (Table 1; Estrada et al. 2016). This set of predictors showed no evidence of collinearity (Pearson coefficient |r| < 0.4; Dormann et al. 2013, see Appendix S2: Fig. S5). All models were trained using three widely used techniques: generalized linear models (GLM); generalized boosted models (GBM); and random forests (RF), which were available in the biomod2 package of R (Thuiller et al. 2009). For each technique, we used the default settings in biomod2 because these settings are optimized for SDMs (see Thuiller et al. 2016). The original bird data set was split into two subsets: 70% of the data was used for training the models and the remaining 30% for testing their performance (hereafter, crossvalidation). We randomly repeated this procedure 10 times to produce predictions that were independent of the training data (Fielding and Bell 1997). The area under the curve (AUC) of the receiver-operating characteristic (ROC) was considered as an estimate of model accuracy (Fielding and Bell 1997). This procedure was repeated for each species (128 species 9 3 modeling techniques 9 10 replicates = 3,840 single-species models) (see Appendix S3: Table S1 for accuracy metrics of individual models). We applied an ensemble forecasting framework by averaging all single model projections (Ara ujo and New 2007), weighted using the AUC values (Marmion et al. 2009). Only models with AUC values above 0.7 were used in the ensemble procedure (see Appendix S3: Table S1 for accuracy of ensemble models). Although species distribution models may suffer from potential biases caused by imperfect detection (K ery 2011, Lahoz-Monfort et al. 2014), we did not account for detection in our modeling approach because: (1) atlas data sets were not collected using the repeated temporal sample structure required for occupancy modeling, and (2) to date, ensemble modeling platforms (e.g., BIOMOD2) do not include algorithms to account for imperfect detection. In addition, (3) \"occupancy,\" after accounting for imperfect detection, is a latent variable and therefore it is impossible to validate on independent data because the \"true\" state of independent data is unknown (Welsh et al. 2013). The ensemble models were projected to the current landcover conditions (2010-2014) and to the land-cover conditions prevailing in 1984-1989 (hereafter, \"hindcasting\") at both square (10-km resolution) and grid cell level (1-km resolution). To evaluate spatiotemporal transferability of the ensemble models (i.e., our ability to extrapolate habitat suitability predictions in space and time), projections at the square level were tested (using AUC as an accuracy metric) against the observed bird data reported in the first and second Breeding Bird Atlases for Quebec, controlling for sampling effort by using only those squares with a minimum of 20 h of observation. The overall AUC values that were derived from each species within a given functional guild are shown in boxplots. The results were compared between functional guilds through a Wilcoxon signed-rank test for paired samples.',\n       'Here we illustrate the random projection approach for a spatial linear mixed model (SLMM)\\nwith an emphasis on dealing with confounding. Banerjee et al. (2012) proposes using random projection for efficient Gaussian process regression. In this subsection we extend their approach so it applies to both SLMMs and the linear restricted spatial regression model. This description also serves as an introduction to our more general approach to SGLMMs.\\nFor the linear case, model fitting is based on the marginal distribution of Y | β, φ, σ 2 , τ 2 .\\nThe main computational challenge is therefore due to the expense in calculating inverses and\\nSubspace Distance α Ω improves the Nystöm approximation to the eigenvectors (left column) and eigenvalues (right column). Letting Φ to be K α Ω with small power α = 1, 2 also provides better approximation for both ν = 0.5 (first row) and ν = 2.5 (second row).\\ndeterminants for large covariance matrices. Random projection may be used to approximate the correlation matrix using its principal components. To fit the full model with random projection (FRP), we apply Algorithm 1 to approximate\\n. We rewrite the model as follows,\\nAnalogously, our RSR model with random projection (RRP) is\\nHereafter, R(φ) will be referred to as R to suppress its dependency on the unknown parameter φ. Fitting the FRP model (6) involves evaluating the inverse and determi-\\nwith cost of m flops, since U m has orthonormal columns. The determinant calculation can also be simplified. By the determinant lemma (Harville, 1997) ,\\nSimilarly, fitting the RRP model (7) involves calculating the inverse and determinant of\\nfor which the dominant cost is tied to the\\nFor the linear case we can simply approximate the correlation matrix of the random effect R with R without explicitly reparameterizing the random effects. However, for SGLMMs, we do not have closed-form marginal distribution. It is therefore necessary to obtain the reduce random effects δ and carry out inference based on π(θ, β, δ | Z).',\n       \"Teachers, like parents, represent a valuable source of information on themselves, the children in their classrooms, and the children's learning environment (i.e., the classroom). Teachers are not only asked to provide information about their own backgrounds, teaching practices, and experience, they are also called on to provide information on the classroom setting for the sampled children they teach and to evaluate each sampled child on a number of critical cognitive and noncognitive dimensions. Special education teachers and service providers of sampled children with disabilities are also asked to provide information on the nature and types of services provided to the child. With the exception of the fall-first grade data collection, teachers complete self-administered questionnaires each time children are assessed. School administrators, or their designees, are asked to provide information on the physical, organizational, and fiscal characteristics of their schools, and on the schools' learning environment and programs. Special attention is paid to the instructional philosophy of the school and its expectations for students. Information is collected from school administrators via self-administered questionnaires during each spring data collection. With the exception of first grade, the administrators of all ECLS-K schools use a single questionnaire. In the first grade two different questionnaires were used, one for original sampled schools and one for new or transfer schools. More information can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135;U.S. Department of Education, National Center for Education Statistics, 2002b). School office staff are asked to complete a student records abstract form and a school fact sheet. The student records abstract form includes questions about an individual child's enrollment and attendance at the school, transfer to another school (if applicable), and verifies whether the child has an Individual Education Plan (IEP) on record. A student records abstract form is completed for each child in the study during each spring data collection. During the third grade data collection, school office staff are also asked to complete a school fact sheet. This form supplements the school administrator questionnaire with basic information about the school, including grade level, school type (public or private), length of school year, and attendance recordkeeping practices. This school fact sheet is only filled out once for each school in the study. Prior to the third grade data collection, the questions were part of the school administrator questionnaire. Exhibit 1-3 summarizes the instruments that were used in each of the data collection periods from kindergarten through spring-third grade. Exhibit 1-4 provides additional detail about the direct child assessments conducted during each of the data collection periods. Separate \",\n       'Overall, the data informing this research indicate that community and cultural factors make treatment in rural areas particularly challenging. Though the importance of these factors cannot be undersold, one of the rural counselors made the provocative suggestion that the work of substance abuse counselors and their rural clientele are intentionally undervalued by policy makers and funding agencies:\\nThere is an undercurrent of intentionality, the more people you talk to on the street the more you will hear this, this isn\\'t by accident that this stuff happens. Let\\'s keep them down in the mountains . . . nobody has made in eastern Kentucky more than a half-hearted effort to really intervene in the disease process that is going on. They took substance abuse dollars, put it into the faith based community where it has not been spent, and cut the programs in each of the communities by that much. And I don\\'t think any of that is by accident. I don\\'t think that I am undervalued by accident. I think my clients are supposed to die.\\nThough these sentiments were met with resistance by some of the other rural counselors, the powerful words represent an undercurrent of frustration common to many of the rural participants. Overcoming these sentiments will only be possible if the treatment environment in rural Kentucky improves. This environment encompasses multiple levels, including a need for community and family recognition of substance abuse problems, as well changes within state-funded treatment facilities. To be effective, these changes must result in offering more diverse services and enhanced cooperation between agencies to meet client needs.\\nUltimately, the results of this study suggest that rural counselors must meet the challenges of working with fewer resources in a context less conducive to recovery. Further, with problems of illicit drug use common in rural America, these key barriers may have even more dire consequences for rural substance abuse counselors and clients (Warner & Leukefeld, 2001) . Given the difficulty of simply getting rural clients \"in the door\" of treatment programs-where they lack anonymity and have limited treatment options-working toward reducing the impact of these factors is a critical goal. Overall, this research suggests that an important component of improving treatment outcomes among rural clients is the recognition of unique cultural characteristics key to establishing clientcounselor rapport and continued treatment. Improved substance abuse treatment among rural populations may also mean recognizing and strategizing how to deal with the layers of contextual factors that problematize recovery, including family histories of substance abuse and community contexts that encourage continued use.',\n       'Alignment of the complete genome sequences of SARS-CoV-2 strains was conducted with MAFFT v7.310 [14] . Phylogenetic tree of all SARS-CoV-2 strains was built with RAxML version 8.2.4 [18] . The tree was edited by iTOL [19] .\\nQuick identification of the types of SARS-CoV-2 genomes in the database All complete genomes available as of 28 March 2020 with 26 March 2020 as cut-off date in the GISAID database were downloaded. Single Nucleotide Polymorphisms (SNPs) calling were performed by Snippy (https://github.com/tseemann/snippy) using Wuhan-Hu-1 as reference. Super-spreader clusters were classified according relative variants. A total of 1539 qualified genomes submitted after 29 February 2020 were included.',\n       nan,\n       'The authors profile facilities converting to critical access hospitals (CAHs) from 1998(CAHs) from -2000 ',\n       'To standardize institution names, fields of study, and course content collected from transcripts, the KCS used three coding taxonomies. Postsecondary institutions at which students were enrolled were coded using IPEDS, developed by NCES (https://nces.ed.gov/IPEDS/). Students\\' fields of study data, captured as majors, minors, and concentrations, were coded using the 2010 Classification of Instructional Programs (CIP) taxonomy developed by NCES (https://nces.ed.gov/ipeds/cipcode/). Course content was coded using the 2010 College Course Map (CCM) (Bryan and Simone 2012). The CCM is a taxonomy for coding postsecondary education courses and it extends the list of codes contained in the CIP. The CIP contains codes for instructional programs, whereas the CCM also contains course-specific codes. For example, the CCM contains a code for abstract algebra because this is a course rather than a program of study, such as computational mathematics. In total, the CCM contains 463 course codes not included in the CIP. The CIP and CCM share a six-digit code structure in which the first two digits identify the general category, the first four digits indicate the subcategory within the general category, and the six-digit code provides the specific definition of the field of study or course content. Figure 5 presents a visual representation of the structure of the codes. This structure includes codes that can be described as general, specific, or other. Eighteen percent of the codes in the CCM are general codes; these typically end with 00 or 01 and represent undifferentiated courses within the main category or subcategory. For example, 27.0101 is \"Mathematics, General.\" More specific subjects are represented with codes ascending from the general code, such as 27.0103, \"Analysis and Functional Analysis.\" In the aforementioned example, 27 represents the general category of \"Mathematics and Statistics,\" the 4-digit subcategory 27.01 encompasses a number of pure mathematics courses, while the full 6-digit code specifically defines \"Analysis and Functional Analysis.\" Eleven percent of the codes are described as other and represent those subjects that are not general and not covered in a specific code. These codes end with 99; for example, 27.0399, \"Applied Mathematics, Other.\"',\n       'Cardiovascular biology and vitamin D ', nan,\n       \"We begin here by highlighting the fact that nearly all students take some vocational coursework-approximately 95 percent, as shown in table 1. We also show that on average students take 3.3 years' worth (the equivalent of 3.3 year-long, one hour per day courses) of vocational coursework, equal to about 12 percent of their 25.5 total credits.\\nNotes: Figures show histograms of credits taken, defined as courses completed, even if it is an F or course that does not accumulate credit hours. One credit is equal to one hour per day for a full academic year. N = 4,414.\",\n       \"The majority of participants in the cohort were either married (70% of HC, 57% of MCI, 60% of AD) or widowed (11% of HC, 21% of MCI, 24% of AD), and most participants primarily spoke English at home (98% of HC, 92% of MCI, 91% of AD). Those who spoke a language other than English at home were nevertheless fluent in English, as lack of fluency in English was an exclusion criterion. Involvement in organized community activities, such as membership of Probus or senior citizen clubs, Returned Servicemen's League clubs and/or sporting clubs was highest in the HC and MCI groups, as was expected (68% of HC, 67% of MCI); however, nearly half (47%) of AD patients remained involved in community organizations at some level. Approximately one third of the cohort reported having at least one pet (33% HC, 28% of MCI, 33% AD). The cohort was well educated, with 47% of HC, 58% of MCI and 42% of AD participants reporting 13 or more years of education. Results of the Wechsler Test of Adult Reading (WTAR) revealed estimated mean premorbid IQ scores of 101 for AD patients, 105 for MCI participants and 108 for HCs, with significant differences between each of the groups (HCs demonstrated significantly higher mean IQ than MCIs, with ADs scoring significantly lower than MCIs). The majority of participants were right handed (88% of HC, 86% of MCI, 87% of AD), in line with the world population proportion of right-handedness (Corballis, 2009 ). Table 2 presents the mean and standard deviation of normed and age-adjusted measures for the neuropsychological tasks, and the results of between-groups one-way analysis of variance (ANOVA) for each measure. These groups differed significantly on all measures (p<0.01). Furthermore, planned comparisons demonstrated that the HC group performed significantly better than MCI participants, and MCI participants significantly out-performed those with AD on all measures (all p<0.01). Table 3 presents the mean and standard deviation of all measures of the CogState battery, and the results of the Kruskal-Wallis significance test for each measure. These findings demonstrate significant differences between the three groups on all measures (p<0.05). Further, Wilcoxon rankedsums tests showed that HC participants performed significantly better than the MCI participants on all measures of the CogState battery (p<0.0001). The MCI participants performed significantly better than the AD participants on the One Card Learning task and the Continuous Paired Associate Learning Task (p<0.05). There were no other significant differences between the MCI and AD participants.\",\n       'To estimate the potential probability of occurrence for the extreme TC-incidence years such as 2015, we examine the empirical probability of exceedance for the frequency (Murakami et al. 2015a ):\\nNo: of years with TCs $ x Total No: of years ,\\nwhere x is the annual number of TCs. For example, P(27) represents the probability of occurrence of a year with 27 or more TCs. For the 1860-(1990-) control experiments, we compute P(x) using all 3500 (500) simulated years. To elucidate intercentennial variability, we compute P(x) for each 100-yr period.\\nFor the multidecadal experiments, P(x) was computed for each 20-yr period from 1941 onward, yielding 700 (520 3 35) samples from which to calculate P(x) (Fig. 5) . Because the simulated internal variability is out of phase among the ensemble members, we can estimate for each 20-yr period the conditional probability P(x j Y 6 ) under any phase of a natural mode of variability (i.e., Y 1 or Y 2 ). We define a positive (or negative) phase of ENSO, PDO, IPO, PMM, and AMO when the May-November index amplitude exceeds one standard deviation (or falls below minus one standard deviation) and estimate the range of P(x) between the two phases.',\n       'In fact, it is illegal to have a labor contract that extends for more than one year into the future. Furthermore, labor contracts have generally been interpreted in a one-sided fashion by obligating the employer but not the employee to stated terms of performance.',\n       'Exonic SNVs were mapped to genes according to annotations based on the RefSeq database 544 [53] . These filtering and mapping procedures were applied to the ADNI and ADSP datasets 545 separately.',\n       'Bowles, Gintis & Osborne (2001; p:1147) note that there is little doubt that those with more schooling have higher earnings largely because they are educated, and not simply because schooling covaries with ability, parental social status, and other traits. They also find that a substantial portion of the returns to school are generated by effects unrelated to cognitive abilities. The question is whether different school types have a different impact on expected future earnings of students. Schools could also shape the perceived views of the costs and benefits associated with higher education. Within the cultural theory of Bourdieu, schooling has its own power to shape consciousness over and above the power of the family. It has an active role in the legitimating of family acquired habits (Nash, 1990) . Tramonte and Willms (2010) also suggest that schools play a prominent role in shaping education and occupational outcomes over and above the role played by peers and parents.',\n       'Six PET scanners were compared for this work. The most relevant characteristics are reported in Table 3 . They are representative of a wide range of available solutions. Reconstruction parameters were standardized across different centres (http://adni.loni.usc.edu/methods/documents/). The reconstruction algorithm used is also reported in Table 3 . Contrast images, representing the differences between the individual patient image and the HC group, generated from each single-subject analysis, were used for the subsequent second level analyses. In particular, two analyses were performed, a voxel-wise analysis and a Volume Of Interest (VOI) one.\\n1. The voxel-wise analysis was performed to evaluate whether the measured patterns were on average the same, independently from the scanner used. In particular, factorial one-way ANOVA analysis was conducted using SPM5, selecting the Bscanner model^as main effect. A threshold of p < 0.05, with an FWE correction for multiple comparisons was applied. 2. The VOI-based analysis was performed in order to evaluate whether the signal extracted from the precuneus and the posterior cingulate gyrus was different among the AD pa t i en t s . T he s e regi ons r ep r ese nt t h e m a j or hypometabolic signatures associated to AD. The volume of interest (VOI) of the precuneus and the posterior cingulate gyrus was obtained from the Automated Anatomical Labelling (AAL) (Tzourio-Mazoyer et al. 2002) . For each patient, we extracted the mean signal in the selected VOI from the contrast images obtained from the SPM single-subject analysis. Then, a one-way ANOVA was performed off-line comparing the extracted mean contrast signals and selecting Bscanner model^as the variable of interest.',\n       nan,\n       \"Phase 2 Agricultural Resource Management Survey (ARMS) data collected at the fi eld level from barley (2003), corn (1996, 2001, 2005), cotton (1997, 2003, 2007), oat (2005), rice (2000, 2007), sorghum (2003), soybean (1997, 2002, 2006), and wheat (2004) producers were used to estimate a relationship between the natural log of pesticide expenditures per acre, a constant, a time index, regional fi xed effects, and a separate latitude coeffi cient for each crop. 1 The time index was included to account for annual weather variation during 1996-2007. National Agricultural Statistics Service (NASS) production-region fi xed effects (not REAP region fi xed effects) were included in the regression to account for spatial variation in weather and production possibilities. 2 The dummy variable for the Corn Belt was excluded. 3 Separate coeffi cient estimates were obtained for each crop by adding independent variables equal to the product of the fi eld's latitude and the crop dummy variables. 1 Pesticide expenditures, which include payments for herbicides, insecticides, and fungicides, were converted to real (2006 US$) expenditures using the Bureau of Labor Statistics (2010) producer price index for pesticides and other agricultural chemicals. 2 In another version of the model, separate annual dummy variables were included for each NASS production region to account for annual weather variation by region and over time more completely. Because the adjusted coeffi cient of determination was slightly lower in that version of the model and because the estimation results were very similar, we used the results reported in appendix table 6 to specify the pesticide-expenditure and yieldloss impacts used in the analysis. 3 Note that the REAP regions are subsets of the major production regions included in the regression.\",\n       'The cheaper computers and greater computing power available today mean that such data can be put to statistical use. Linking different data sources: when it comes to mapping certain areas, such as the innovating behaviour of firms, S&T networks, the mobility of human resources or government S&T policies, more than one source of data is needed in order to obtain a complete picture. Each source of data gives specific information that makes more sense when matched with information from other sources. For instance, a good picture of innovating firms requires knowledge of their innovation activity, skill structure, product range, market shares and more; such information can be accessed only by resorting to several different data sources. Furthermore, in a globalising world, cross-country comparisons of firms are necessary, which implies matching business surveys from different countries. All of this raises few technical problems today, thanks to the advance of computing power, but it does raise legal problems due to concerns about confidentiality which drastically limit access to data. Further thinking about these issues is needed by both statisticians and law makers. Networking statisticians: the increasing diversity of data sources, methods and areas in S&T statistics means that division of labour is progressing in this field as well. It is not the same person, or the same administrative department, which is in charge of all these data in any country: specialisation is becoming the rule. Reaping the rewards from greater division of labour (in terms of more data produced and the higher competence of specialised experts), while keeping some unity (as it is important to keep consistency between the various indicators), is not straightforward. Statisticians in charge of the various aspects of S&T will have to think of new, more open and more flexible ways of co-ordinating with each other, within as well as between countries. Like their colleagues in other domains of knowledge production, they have to work increasingly in networks.\\nFor a correct measurement of total investment in knowledge, all of the above three categories should be taken into consideration in estimating total investment in knowledge. However, lack of data means that spending by enterprises on jobrelated training programmes is excluded from the calculation of the total investment in knowledge. Data on vocational training are available for countries that participated in the Vocational Training Survey of the European Union. However, these data are not fully comparable due to differences in the definitions, coverage and reference periods (OECD, 1998a). Education expenditure can be separated by level of education (primary, secondary and tertiary). Although education expenditures are available for all levels, only tertiary level expenditure (International Standard Classification of Education -ISCED 5 and 6) is included in the calculation of investment in knowledge, since it could be assumed that this expenditure results in the creation of \"sophisticated knowledge\"; higher education (tertiary) expenditure is therefore more comparable to R&D and software expenditures. Although education expenditures for all levels are not taken into consideration, they are reported below to provide some insight into the education priorities of individual countries. Education expenditure are taken from the OECD\\'s Education database. The data used here refer to direct and indirect expenditure on educational institutions from both public and private sources.\\n\\nA preliminary analysis of CLFS data and national labour force survey data provides a number of interesting results; these should be seen as examples of the types of information which could be derived using labour force survey data to analyse mobility: • Mobility rates have risen to around 9% at the EU level. • Significant variations exist between countries, ranging from 15% in Finland to 4% in Italy. • Mobility has increased rapidly in the ICT sector. • Mobility flows are concentrated in own sectors or close sectors Until qualifications and occupations are better recorded in the general information collected on emigrants and immigrants, the possibilities for constructing indicators on international mobility do not look very promising. Information which is partially based on survey results may be available in various countries, although the international comparability of this information is limited. Shares of foreigners in total stocks of highly qualified workers are available from labour force surveys.\\nMember and some non-member countries. At the same time, he made an inventory of available data sources and special studies from which useful information related to HRST mobility could be derived (Rosengren, 1998). The study did not provide an answer to the question of whether it was possible to develop internationally comparable indicators on the mobility of highly qualified personnel. However, it did provide sufficient information to encourage continued efforts to construct at least some rough mobility indicators which could be applied outside the Nordic area. The aim of the study was to examine in detail some of the data sources and special studies identified in the Rosengren study and, if possible, to identify additional sources, and to evaluate their usefulness in the construction of internationally comparable indicators on mobility of highly qualified manpower. The study looked at international sources, such as the Community Labour Force Survey (CLFS) and the European Community Household Panel (ECHP), and at national sources for a set of countries: Australia, Belgium, Finland, France, Germany, Korea, Italy, Spain, the United Kingdom, the United States. A first attempt was made to construct indicators from the international sources and national sources for three countries (the United States, France, the United Kingdom). This article first discusses the types of mobility indicators investigated in the study. The data sources identified as being relevant for the construction of indicators are then evaluated. A presentation of the findings follows, based on international sources and the results for the three countries. This is followed by conclusions and recommendations for future work.\\nMobility always represents a trade-off between the benefits for the recipient of the tacit knowledge embodied in a highly qualified person and the losses for the donor institution. Therefore, a rate of mobility which is too high could have a negative effect. Knowledge flows in and out of a country are of particular interest for policy makers. Their attention is increasingly focused on issues related to \"brain circulation\", encouraging people to spend time abroad and to come back with greater tacit knowledge -to the benefit of the home economy. Today, the statistical information available for policy makers to base policy decisions on is limited. This study attempts to investigate possibilities for improvement. Three types of indicators regarding the mobility of highly qualified manpower are addressed: • Indicators on mobility between firms and other organisations. • Indicators on mobility between the research-producing and research-using sectors. • Indicators on internatinal mobility. It does not look at some of the other types of mobility, such as the flows of university graduates into employment and international flows of university students. For the three types of indicators described above, the focus is on institutional mobility, i.e. change of employer or employment status. Various units allow identification of the employer. Change of employer can be defined as change of establishment (local kind of activity unit). In many cases, a somewhat larger unit is preferable, for example, the local unit. It is also possible to define mobility in terms of change of organisation (enterprise). One possibility is to require both change in enterprise and establishment as the criteria for mobility. In some studies, mobility has been defined as change of industry, which of course is the most restricted definition. The data sources available in different countries will affect the definition used. In order to achieve comparability across countries, it would be useful to be able to apply uniform definitions as far as possible. Mobility can be defined in a narrow sense to only include movements between employers, or in a wide sense to also include movements to and from unemployment or to and from the labour force. This study is limited to mobility of highly qualified manpower. This is a narrower concept than HRST according to the \"Canberra Manual\". It could be defined as: • A combination of educational and occupational criteria, as used in the \"Canberra Manual\" or in the US definition of scientists and engineers. • According to purely educational criteria. • According to purely occupational criteria. If educational criteria are used, a natural definition of the borderline is ISCED 6 and ISCED 7 according to ISCED (1976 version) or ISCED 5A and 6 of the 1998 version. In practice, people with at least a bachelors degree or equivalent are included. For international purposes, the occupations to be included have to be defined according to ISCO. Relevant categories include at least professionals (ISCO 2). A certain part of managers (ISCO 1) might also be relevant, but in practice has to be left out as there are difficulties in defining the appropriate categories according to ISCO, and difficulties in translating national categories into the appropriate categories. The actual limitation is dependent on the data sources available and on the possibilities of translating national classifications into international standards. Differences in the definition of highly qualified manpower across countries will probably not significantly affect the comparability of mobility measures, so there is room for a certain amount of flexibility. Other classifications used in the analysis of mobility are gender, nationality and age of personnel.\\nUsing SESTAT, it seems to be possible to analyse mobility between the following sectors: • Private for-profit (company, business or individual, working for wages, salary or commissions). • Private not-for-profit (tax exempt or charitable organisation). • Self-employed in own not-incorporated business, professional practice or farm. • Self-employed in own incorporated business, professional practice or farm. • Local government. • State government. • US military service. • US government (civilian employee). A separate question was asked about employment in various educational institutions. Sample size permitting, sub-division by the following types of qualifications might be possible: • Bachelor. • Post-baccalaureate certificate. • Masters degree. • Post-masters certificate. • Doctorate. Section V presents some results on the basis of extractions from the SESTAT system.\\nOnly incentives in the second category are credited to government in the regular R&D statistics examined above so this set of data gives a wider picture of government support than is usually available. The Frascati Manual basically prefers R&D data reported by performers to that reported by funders because the former are best placed to assess the amount of R&D activity carried out and, thus, the resources concerned. The guidelines for measuring flows of funds were essentially designed to be appropriate for performer reporting, and were then extended to funder reporting (extramural expenditure and GBAORD) for reasons of comparability. The principles are that there must be a direct transfer of resources and that this transfer must be both intended and used for the performance of R&D (OECD, 1994, para. 368). The Manual is a little more flexible for GBAORD (Box 5).',\n       'Neuroinflammation, in the way of glial activation (especially in the vicinity of amyloid plaques), is a robust but nonspecific feature of AD. A number of reports published in the 1990s and early 2000s describe alterations in the levels of various inflammatory and signaling molecules, as well as markers of oxidative stress (e.g., a1-antichymotrypsin, isoprostane, the interleukins, TNFa, interferon-g, complement C1q, and TGF-b) in AD CSF (Zetterberg et al. 2004; Craig-Schapiro et al. 2009 ). However, results have been very inconsistent, probably owing to methodological differences (e.g., in the procedures for CSF collection and processing, assay differences, and criteria used for subject ascertainment), prevalence of comorbidities in the studied cohorts, and methods of diagnosis. Unbiased proteomics methods have more recently been used to identify molecules that differ between AD and control CSF (and serum and plasma). These studies have consistently identified a plethora of inflammatory markers that differ in abundance between clinical groups (Castano et al. 2006; Finehout et al. 2007 ). However, even in these unbiased screens, the direction of reported difference in abundance has not been consistent. Despite this, one astrocyte marker, YKL-40, discovered in an unbiased proteomic screen, has recently been validated in a large cohort of cognitively normal and AD subjects to be increased in AD and to predict clinical worsening from cognitively normal to very mild dementia (CraigShapiro et al. 2010 ). The recent availability of commercial multiplexed assays should permit analysis of a large panel of inflammatory and signaling molecules in large-scale studies. It is conceivable (and probable) that adding markers of neuroinflammation to the other CSF markers (such as Ab42, tau, and P-tau) will further strengthen diagnostic and prognostic capability (Hu et al. 2010) .\\nAD pathogenesis also includes free radicalmediated injury to neurons. Lipid peroxidation is an important consequence of such damage, and it generates many products, including F2-isoprostanes. These molecules may serve as biomarkers for this pathogenic process. CSF F2-isoprostane levels have been reported to be increased in AD (Montine et al. 2007 ). Recent studies also show an increase in F2-isoprostanes in MCI cases with prodromal AD (Brys et al. 2009a ) and in asymptomatic carriers of FAD mutations (Ringman et al. 2008) . In contrast, studies on F2-isoprostanes in plasma have reported conflicting results, probably because the contribution of brain-derived F2-isoprostanes to plasma is clouded by the much larger contribution of peripherally derived F2-isoprostanes (Montine et al. 2007 ).',\n       'In summary, the genome-wide significance of an association linking resting heart rate and the GJA1 locus previously described in European and Asian populations has now been generalized to African Americans. In addition, this analysis has replicated associations initially discovered in Europeans between common variants within the MYH6 gene and a reduction in heart rate to an African American population. Generalizability across global populations and biological plausibility of the heart rate-GJA1 and heart rate-MYH6 associations highlight the potential importance of these loci in the intrinsic (nodal and myocardial) determination of resting heart rate.',\n       \"In the analysis of 1000 samples, derived from representative countries of all six continents, we detected 313 total mutations, as mentioned earlier, at the nucleotide level in most of the ORFs, except in ORF 7b, 10 and 3'-UTR, (Figure 2 ). The highest mutations (78 mutations; 24.9%)\\nwere detected in Australian samples, (Figure 2 and Supplementary \",\n       'A-8 D',\n       'We also analyze the proportion of users who voted for event time or location in the final decision. The distribution is shown in Figure 9 . More than 70% of the final decisions for both time and location received majority votes to become the final choice. This is understandable since groups tend to agree on the majority votes. For the remaining 30%, we observe some very interesting behavior. In these polls, the final decisions did not receive the majority of the votes. In fact, in a small fraction of cases, there is a non-zero proportion of polls in which the final decision received no votes. In these cases, the group host, who is the only one with the power to finalize the event time and location, decided to override the majority voting results, either by personal fiat or possibly through a discussion with other group members that caused them to change their minds. ',\n       'Alzheimer\\'s disease (AD) is a leading cause of mortality in the elderly. While the coding change of APOE-ε4 is a key risk factor for late-onset AD and has been believed to be the only risk factor in the APOE locus, it does not fully explain the risk effect conferred by the locus. Here, we report the identification of AD causal variants in PVRL2 and APOC1 regions in proximity to APOE and define common risk haplotypes independent of APOE-ε4 coding change. These risk haplotypes are associated with changes of AD-related endophenotypes including cognitive performance, and altered expression of APOE and its nearby genes in the human brain and blood. High-throughput genome-wide chromosome conformation capture analysis further supports the roles of these risk haplotypes in modulating chromatin states and gene expression in the brain. Our findings provide compelling evidence for additional risk factors in the APOE locus that contribute to AD pathogenesis. 1 1234567890():,;\\nNote: CAVIAR analysis results for the major causal variants, defined as a posterior probability ≥ 10%, with a summary for variants frequency in normal control participants from each studied cohort. Regions with transcription factor-binding events annotated by the ENCODE database are marked as \"yes\" in the \"TF binding\" column. The last column displayed the effective allele frequencies of corresponding variants in the normal control populations of given cohorts accordingly BP base position in GRCh37 annotation, Gene nearest genes, EA effect allele, Beta effect size, SE standard error, TF transcription factor, EAF effect allele frequency, NC normal controls NATURE COMMUNICATIONS | https://doi.',\n       \"The sample means thus indicate that overweight and obese children have worse test scores and teacher assessments than normal-weight children. Of course, these differences in means could be due to other characteristics, such as socioeconomic status. The ECLS-K includes data on a wide variety of child, family, and school characteristics that may affect children's academic achievement. The regression model described next adds controls for child, family, school, and teacher characteristics as well as child fixed effects in order to further examine the relationship between weight and academic achievement.\",\n       'We evaluate our results both visually and quantitatively. For the T1w dataset, we use a volume overlap measure, Dice, to quantify the automatic segmentation results [8] :\\n. (17) where s is the predicted segmentation map, and s t indicates the ground truth (FreeSurfer) label at each location. A Dice score of 1 indicates perfect segmentation. We experimented segmenting in the unsupervised setting with standard UNet architectures, using the image MSE and mutual information loss functions. Because of the many structures that share similar intensities, these architectures are not able to produce sensible segmentations that resemble the correct segmentations, and we omit them from these results. Classical unsupervised methods that include sophisticated prior anatomical information take a significant amount time to run, and for T1w we regard FreeSurfer results as an optimistic bound for the T1w data. However, as these methods tend to be focused on specific modalities, there is no annotation tool for cortical and subcortical regions in T2-FLAIR. We evaluate the T2-FLAIR segmentation visually in Figure 5 .',\n       'Weather and tide frequency One major weather event did occur in fall 2003, Hurricane Isabel (Hovis et al. 2004 ). This hurricane made landfall on September 18 approximately 300 km south of UPC near Beaufort, NC and moved northwest. Although it was a category 1 or 2 hurricane at the time of landfall, it produced considerable storm surge along the North Carolina and Virginia coasts. Storm surge overtopped the meteorological station at UPC, so no direct measures of water level were available. At the Chesapeake Bay Bridge Tunnel about 50 km south of UPC, maximum levels were 2.3 m above mean low low water and 1.41 m above mean high high water. A tide gage at Redbank, a few km from UPC and maintained by the VCR-LTER recorded levels of 2.7 m, the highest level for the month by at least 0.4 m. (http://amazon.evsc.virginia.edu/data/ metdata/tide/index.html).',\n       'The goal of this section is to address questions and comments regarding our article.',\n       'Annually, over the 5-year period from 1998 to 2002, teachers were the victims of approximately 234,000 total nonfatal crimes at school, including 144,000 thefts and 90,000 violent crimes. The average annual rate of violent victimization for teachers varied according to their sex, instructional level, and urbanicity (figure 9.1 and table 9.1). Over the 5-year period from 1998 to 2002, male teachers were more likely than female teachers to be victims of violent crimes (34 vs. 15 crimes per 1,000 teachers annually). Senior high school and middle/junior high school teachers were more likely than elementary school teachers to be victims of violent crimes (30 and 26 crimes, respectively, vs. 12 crimes per 1,000 teachers). In addition, annually over the 5-year period, urban teachers were more likely than rural and suburban teachers to be victims of violent crimes (28 vs. 12 crimes each per 1,000 teachers).  ',\n       'The participant countries other than US were selected because they are rapidly developing industrialized countries with an escalation in migration of personnel in STEM areas to US. Further, the ease of data collection by the authors who hail from the respective countries and have ties to US was a key factor. The Trends in International Mathematics and Science Study (TIMSS), and Program for International Student Assessment (PISA) data though available was not used to draw comparisons as countries such as India and China did not participate in these studies. Table 1 summarizes details about science teaching at the schools from which data was collected in the five participating countries. In India, science is taught at all grade levels starting with General Science and Environmental Studies at the elementary levels, differentiating into Physics, Chemistry and Biology at the 8th grade level. Science teaching is mostly passive, generally taught out of a textbook, the teacher doing a few demonstrations at the elementary level. At the 9th & 10th grade level, students visit the science laboratory during practical periods to perform mostly cook-book type experiments.',\n       'The NWS is the lead forecast agency for the U.S., with a 24/7 operational mission. The NWS\\'s National Hurricane Center (NHC) is responsible for forecasting tropical cyclones east of 140 W, the NWS\\'s Central Pacific Hurricane Center (CPHC) for cyclones between 140 and 180 W north of the equator, and the Joint Typhoon Warning Center (JTWC) for cyclones west of 180 W. The NWS forecasters typically require the highest technical level and detail in model output of the user groups considered. This user group has a high level of technical background and training and a comprehensive technological infrastructure, and can therefore deal with complex modeling information. However, they may require specific training with respect to coastal inundation. Forecasters use the output of numerical models (forecast guidance) directly to compile weather forecasts. Their range of responsibilities typically covers the \"weather\" time scale, with a time horizon of around 5 days, and includes both tropical and extratropical events. The models can either be run remotely (e.g., at a NWS national center such as the National Centers for Environmental Prediction (NCEP) or at a NWS regional headquarters) or locally by the forecaster. All of the weather forecast models run in real time, so that all products correspond to the current forecast. A challenge here is that deterministic models can be computationally intensive. There is a need for an adequate and robust computer and data infrastructure and institutional capacity to provide these services (locally or remotely). Therefore, coastal inundation models must be designed appropriately for the IT capabilities available on the Pacific Islands. Modeling expertise is also not uniformly distributed across this region, with most of the technical experience residing in Hawaii. In addition, forecasters need to convey storm surge risks clearly to NWS\\'s customers, including the general public. This is currently done through the NWS bulletin text products, including the Hurricane Local Statement, the Public Advisory and Forecast Discussion. Coastal inundation guidance needs to be designed and implemented to adequately serve these needs.',\n       'Based on the identified landmarks shown in Fig. 2 (b) , we extract multiple patches ( i.e. , instances) from a specific MR image ( i.e. , each bag) for representing each subject (see Fig. 1 ). Here, we extract patches with the size of 24 × 24 × 24 centered at each specific landmark location. The analysis on why this patch size is selected will be given in Section 4.8 . Given L landmarks, we can obtain L patches from an MR image to construct a bag for representing the subject. To suppress the influence of any registration error, for each landmark, we randomly sample different patches on the same landmark location with displacements in a 5 × 5 × 5 cubic (with the step size of 1). Given L landmarks, we can extract up to 125 L bags from each MRI.',\n       nan,\n       'We compared CSF profiles for MCI patients that converted to AD by the 36 month visit vs. MCI patients that did not convert. Three markers had marginal q-values of 0.0508: hemoglobin subunit alpha (HBA), neuronal pentraxin 2 (NPTX2) and poliovirus receptor-related protein 1 (PVRL1 , Table 5 ). Interestingly, the APO-E peptide (LGADMEDVR), which demonstrated excellent differentiation between AD vs. NL, ranked 199/320 for predicting conversion from MCI to AD. These data suggest that individual peptide markers do a poor job of predicting MCI to AD progression on their own; hence the motivation to combine markers in a multivariate analysis to increase their utility (below).',\n       'A firm should find it easier to survive in a sector with increasing demand.\\nIndustry Growth is measured by the annual percentage change in the wage bill for the national two digit industry between 1992 and 2005.\\n14 Data are compiled from 14 We get similar results when we use employment growth as the measure of industry growth.\\nBureau of Economic Analysis, U.S. Department of Commerce. We expect that establishments are more likely to be viable in industries that are experiencing higher growth. ',\n       'Research across the globe has provided evidence on the predictive relationship of early mathematical knowledge and skills on later academic achievement and economic status. Similarly, there has been a growing understanding of the importance of these foundational skills in building and sustaining a democratic society as well as technological capacity at national levels. However, until recently, there have been few efforts to provide valid and reliable population-level assessments of early mathematical knowledge and skills. This manuscript reports on the development and intended uses of the Early Grades Mathematics Assessment (EGMA), an instrument that measures number sense knowledge and skills in the early primary grades. Through snapshots of three implementation studies, we illustrate how policy makers and practitioners can use the EGMA results to evaluate the effectiveness of educational policies, curricular reforms or programs, and instructional practices and interventions.\\nThis paper begins with overviews of research on the increasing importance of early mathematics proficiency and current mathematics assessment policies. We follow this with sections that cover the principles underlying the development of the EGMA including the content domains and assessment framework. We then illustrate the uses of the EGMA results through descriptions of three studies and conclude with recommendations for future research, development, and implementation of the EGMA.',\n       'Accurate numbers for people with disabilities are difficult to obtain and vary with the survey methodology, how disability is defined, and how the questions are phrased. The US Census Bureau 2011-2015 American Community Survey estimates 5.8% of the noninstitutionalized population of US citizens and Karl S. Booksh, professor of chemistry and biochemistry, University of Delaware; kbooksh@udel.edu permanent residents between the ages of 18 and 34 years have a disability. 6 The US Census Bureau 2010 Survey of Income and Program Participation estimates that 12.2% of the noninstitutionalized US population between ages 6 and 14 years have a disability, 7 and 16.6% of those 21 to 64 years working age in the US population have a disability. 8 At the same time, approximately 9% of K-12 students are enrolled in Individuals with Disabilities Education Act (IDEA) programs. 9 The IDEA data are perhaps the most reliable and informative metric because they hinge on the actual percentage of students who receive classroom accommodations for a disability; \"disability\" is functionally defined as an impairment that requires accommodations for the student to reach his/her educational potential. Table I lists the distribution of specific measures of disabilities among grade-school-aged children and highlights the difficulty in quantifying the number of people with a disability. 8 It is unclear how a person with an amputated arm, severe depression, or anxiety would fit in this rubric. This distribution would change, and numbers would generally increase among an older population.\\nThere is a leaky pipeline in STEM degree attainment for students with disabilities. Representation among enrolled STEM undergraduate and graduate students dropped from 10% to 6%, respectively. 10 Ultimately, people with disabilities account for less than 2% of STEM doctorates earned by US citizens or permanent residents. 10, 11 Worse yet, based on the Survey of Earned Doctorates data, the percentage of STEM PhD degrees earned by people with disabilities has not increased since the passage of the Americans with Disabilities Act (ADA) in 1990.\\n* Figure 1 presents data (as a percentage) for US citizens and permanent residents earning doctoral degrees at US institutions. If the data were renormalized for US citizens and permanent residents with disabilities earning STEM doctoral degrees as a percentage of total STEM doctoral degrees conferred at US institutions, the time trend would be negative. That is to say, the number of foreign national students earning US STEM doctorate degrees is alternative phrasing of the question neither assumes inherent limitations on one\\'s capacity to excel nor presupposes a defined set of acceptable pathways toward success. In this light, it is worthwhile to investigate potential societal and structural factors that may be impacting the performance and persistence of people with disabilities in STEM fields, and consider equitable means to support the inclusion of people with disabilities in STEM.',\n       \"Magnetic Resonance Imaging (MRI) data collected in five behavioural and one MRI session in the week before (pretest) and the week after the intervention period (posttest).\\nThe primary cognitive outcomes were: (1) spatial fluid intelligence, as a latent variable (see Statistical Analysis) measured by the Raven's Progressive Matrices, the Wechsler Abbreviated Scale of Intelligence, and the BETA-III matrix reasoning test; and (2) verbal fluid intelligence, as a latent variable measured with the Analogies Task from Berlin Intelligence Structure Test, Syllogisms, and the Verbal Inference Test from the ETS Kit (See Supplement S8 for complete list). The testing battery is described in details in one of our previous publications 55 . Selection of the primary outcomes was motivated by previous literature suggesting a possibility that working memory training can produce improvements in measures of fluid intelligence 24, 56 . Secondary cognitive outcomes were measures of working memory, episodic memory, and task switching ability.\\nStructural brain imaging data were collected as a further secondary outcome. MRI scanning session (MRI Center, Huddinge Hospital) on a 3 Tesla scanner Siemens MAGNETOM Prisma equipped with a 24-channel research head coil was performed at the end of pretest and posttest weeks. The procedure employed a standardised GRAPPA MPRAGE acquisition protocol according to Alzheimer's Disease Neuroimaging Initiative standards (ADNI-3). TR/TE = 2300/2.95 ms, Base resolution = 256, FoV read = 270 mm, Voxel size = 1.1 × 1.1 × 1.3 mm.\\nSix months later, participants were invited back to complete the cognitive assessment once more. Subjects were not unblinded until after the last follow-up visits.\\nAdverse events were assessed according to the most recent guidelines for Good Clinical Practice (European Medicines Agency, December 1 st 2016) and entailed comprehensive evaluation of their severity and possible connection with the drug. For detailed description, see study protocol at https://osf.io/89bcw/.\\nIn order to quantify effective concentrations of the investigated drug, subjects' blood samples were collected in the ethylenediaminetetraacetic acid-treated tubes at the first and last cognitive training visit, approximately 30-40 minutes after completing the training, to evaluate plasma levels of L-dopa and homovanillic acid. Plasma separation was performed within 4-5 hours via a 30-minute centrifugation at 3000 × g. Plasma samples were stored in 1 ml aliquots at −80 °C. The chemical analysis of L-dopa and homovanillic acid was performed with high-performance liquid chromatography analysis (Nexera-i HPLC system, Hertogenbosch, Netherlands; Antec electrochemical detection system, Leiden, Netherlands) and was blinded to the study groups. See Supplement S6 for more detailed description.\\nStatistical analysis. Approximate power analysis was performed prior to the study launch with G*power 3, estimating the required sample size for detecting a group by time interaction (mixed ANOVA, F-test) on the primary outcomes, assuming a net standardised effect size (improvement for active group -improvement for control group) of 0.3 standard deviations, a test-retest stability coefficient of 0.70, and an alpha level (threshold for statistical significance) of 0.05. With these assumptions, we estimated a sample of 56 subjects as sufficient for detecting a true group × time interaction with a statistical power of 0.80. Considering a Bonferroni-corrected threshold for statistical significance (0.05/2 primary outcomes = 0.025), we aimed for a total of 64 subjects, which results, under the premises described above, in a statistical power of 0.786.\\nThe main analysis used an implementation of structural equation modeling with latent change score modeling 57 to test the effect of L-dopa versus placebo on the outcomes of cognitive training (see Supplement S2 and S9). The analysis was implemented in the 'lavaan' package 58 within the R programming language environment, version 3.3.2 (2016-10-31). Latent variables were formed similarly for pre-and post-test assessments based on shared variance from multiple tests measuring each specific construct, and a latent change score, which represented the difference between the assessments, was estimated. Estimating intervention-related changes in this way, by forming a latent variable out of several tests, has the advantage of reducing the influence of measurement error and task-specific variance on the outcome measure, and hence also biases (e.g., regression to mean) that may affect raw change scores. The change factor was regressed on the group predictor (L-dopa/Placebo; dummy coded 1 vs. 0). The regression effect indicates the effect of experimental group on latent change from pretest to posttest (i.e., a time by group interaction). Prior to estimation, we z-standardised all variables, such that the size of this effect corresponds to the difference in gains over time between the two groups expressed in standard deviations. A separate model was estimated for each of the considered primary outcomes (spatial and verbal fluid intelligence) and a Bonferroni-corrected threshold for statistical significance of 0.025 (0.05/2 primary outcomes) were applied. The secondary cognitive outcomes (see Supplement S2 and S8 for a complete list) were analysed in the same way, also applying a Bonferroni-corrected threshold for statistical significance (0.05/6 outcomes = 0.008). The same analytic strategy was employed to exploratively analyse the 6-month follow-up data.\\nBefore model estimation, we cleaned and screened the data for outliers using the outlier labeling rule multiplying the interquartile range by a factor of 2.2. Detected outliers were deleted using pairwise deletion and the resulting missing values were accommodated under the missing-at-random assumption using full information maximum likelihood (FIML) estimation. Non-normally distributed variables were transformed employing applicable transforms until normality assumptions were met. We included all available data. Prior to hypothesis testing, measurement invariance assumptions were evaluated to ensure that the same latent variables are represented on each measurement occasion 59 . Both models that incorporated the primary outcomes, spatial and verbal intelligence, met criteria for strict invariance (see Supplement S2 for all outcomes).\\nTraining progress was analysed employing linear modelling that compared average level reached over the course of training (nested in tasks) between the groups, applying a threshold for statistical significance of p < 0.05.\\nPlasma concentration of L-dopa and homovanillic acid (HVA) were analysed adhering to standard protocols of high-performance liquid chromatography (HPLC) and electrochemical detection (See Supplement S6 Scientific RepoRtS | (2020) 10:5227 | https://doi.org/10.1038/s41598-020-62172-y www.nature.com/scientificreports www.nature.com/scientificreports/ for detailed description). Statistical analyses were conducted employing linear modelling (Group, Group × Visit effects on L-dopa/HVA levels, within-subject, random intercepts), applying a threshold for statistical significance of p < 0 0.05.\\nStructural MRI images underwent standardised steps for bias-field correction, segmentation, spatial normalisation and smoothing (FWHM of 8 mm) as implemented in the CAT12 (http://www.neuro.uni-jena.de), an SPM12 (http://www.fil.ion.ucl.ac.uk) toolbox installed in the MATLAB 2016 environment. See Supplement S5 for more detailed description. Normalised and modulated grey matter probability maps were analysed employing mass-univariate within-subject ANOVA estimating group × time as a primary effect-of-interest. Yielded statistical parametric maps were adjusted for multiple tests employing a family-wise error-correction procedure. This was accomplished by testing the data against an empirical null distribution of maximum cluster size across 10,000 Gaussian noise simulations with an initial cluster-forming threshold of p < 0.005. Clusters with expected false positive rate of <5% of (P FWE < 0.05) were considered statistically significant.\\nSignificance. The results put constraints on the hypothesis of a key role of the deteriorated dopaminergic system in age-related decline of learning abilities, and speak against early pharmacological interventions in older healthy adults to improve cognitive functions by targeting the dopaminergic system. Our findings also raise concerns about usefulness of novel L-dopa-containing supplements that claim to have neuroprotective and learning-enhancing properties. There is need for careful investigation of how cognitive abilities are affected by early L-dopa medication in clinical populations often receiving substantially larger doses of the drug.\",\n       'Excluding the base year ineligible students who were reclassifiLd as eligible in 1990 (and who will be added to the first follow-up data with the secand follow-up data release), nineteen (15 of them from the freshening sample) students completed the Spanish-language questionnaire in the NELS:88 first follow-up. Because of the small number of questionnaires completed in Spanish, a separate flag was not created for these cases. The percentage of questionnaires completed in Spanish --around one-tenth of one percent of the total first follow-up student participants, is similar to the percentage of HS&B sophomores who opted to complete Spanish-language questionnaires in 1980 (36 out of 27,118 participants, or 0.13 percent). NELS:88 First Follow-Up Final Technical Report battery, six forms of the cognitive test battery were produced in the first follow-up, each comprising a different combination of mathematics and reading difficulty levels. Each student\\'s test form was determined by his or her scores on the base year mathematics and reading tests; freshened students and base year non-respondents received the intermediate version of the first follow-up cognitive test battery (Version III). The purpose of the multi-level design of the first follow-up cognitive test battery was to guard against ceiling and floor effects which may occur when testing must span four years of schooling. This adaptive approach tailors the difficulty of the reading and mathematics tests to the ability of the respondent, thereby leading to a more accurate measurement than a single level design. Figure 2-3 illustrates the distribution of test versions to base year retained sample members and defines the test combinations used in the first follow-up. In order to facilitate comparisons with test data from other national studies, NELS:88 borrowed or adapted a number of test items from NAEP and HS&B. Properties of the cognitive tests are discussed in the Psychometric Report for the NELS:88 Base Year Test Battery, and in Chapter 6 of this report.\\nA change that affected a very few Hispanic ineligibles was the provision of a Spanish-language NELS:88 questionnaire in 1990, and again in 1992; a Spanish language student questionnaire was not offered in the base year. 197 157 NELS:88 First Follow-Up Final Technical Report significantly changed, and the likelihood that where change has occurred in a student\\'s eligibility status, that change has been captured. These considerations also support the supposition that a substantial number of students who could successfully have participated were excluded by their base year schools. However, even after a second screening and the passage of two years during which some individuals became more proficient in English or underwent other status changes, about a third of the 1988 NELS:88 ineligibles remained ineligible and could have been surveyed or assessed only indirectly, or through comparatively costly special accommodations to their barrier to participation, or through some form of alternative assessment. NELS:88 First Follow-Up Final Technical Report GLOSSARY OF NELS:88 TERMS Note: Words in the glossary have been cross-referenced. If a word used in a definition has its own entry elsewhere in the glossary, the word appears in italics in its first usage under each entry. Augmentation students: See State augmentation students. Base year ineligible (BYI) study: A NELS:88 First Follow-Up study which sought to locate and survey eligible respondents who were part of the Base Year sample, yet were ineligible to participate in the Base Year owing to mental or physical incapacity or to a language barrier. Bias (due to nonresponse): Difference that occurs when respondents differ as a group from nonrespondents on a characteristic being studied. Bias (due to undercoverage): This bias arises because some portion of the potential sampling frame is missed or excluded. For example, if the school list from which a school sample is drawn is incomplete or inaccurate, school undercoverage may occur. In NELS:88 the most important potential source of undercoverage bias was exclusion of 5.37 percent of the potential sample of eighth graders in the base year. (See entry for \"Base year ineligible study\" and \"Followback study of excluded students.\") Bias (of an estimate): The difference between the expected value of a sample estimate and the corresponding true value for the population. Burden: Formally, this is the aggregate hours realistically required for data providers to participate in a data collection. (Burden also has a subjective or psychological dimension: the degree to which providing information is regarded as onerous may depend on the salience to the respondent of the Ceiling effect: The result of a cognitive test having insufficient numbers of the more difficult items. In a longitudinal study, ceiling effects in the follow-up testings can cause change scores to be artificially constrained for high ability examinees. More information (that is, smaller error of measurement) is obtained with respect to ability level if high ability individuals receive relatively harder items (and if low ability individuals receive proportionately easier items). The matching of item difficulty to a person\\'s ability level yields increased reliability at the extremes of the score distribution where it is most needed for studies of longitudinal change. That is, the measurement problems related to floor and ceiling effects in combination with regression effects found at the extreme score ranges seriously hamper the accuracy of change measures in longitudinal studies. Hence one strategy employed in NELS:88 to minimize ceiling effects was to develop test forms that are \"adaptive\" to the ability level of the examinee. The multilevel tests used in the first and second follow-ups of NELS:88--with test assignment based on prior G-1 NELS:88 First Follow-Up Final Technical Report test performancework to minimize the possibility of ceiling effects biasing the estimates of the score gains. (See entry for \"Floor effect.\") Certainty school: A first follow-up sch^n1 attended by four or more NELS:88 sample members, as determined by tracing and data collection efforts. These schools are included in the sample with certainty (probability = 1). Closed-ended: A type of question in which the data provider\\'s responses are limited to given alternatives (as opposed to an open-ended question. See entry for \"Open-ended.\") Cluster size: The number of NELS:88 sample members attending a particular high school. Codebook: A record of each variable being measured, including variable name, columns occupied by each variable in the data matrix, values used to define each variable, unweighted frequencies, unweighted p :cents, and weighted valid percents. (See entry for \"Electronic Codebook.\") Cognitive test baGory: One of the two parts of the Student Survey (the second part being the student questionnaire). Focr achievement areas (mathematics, reading, science, and social studies [l\\'istoryigeography/civics]) were measured. Cohort: A group of individuals who have a statistical factor in common, for example, year of birth or grade in school or year of high school graduation. NELS:88 embraces three overlapping but distinct nationally-representative grade cohorts: 1987-88 eighth graders, 1989-90 high school sophomores, and 1991-92 high school seniors. Composite variables: A composite variable is one that is constructed through either the combination of two or more variables (socioeconomic status, for example) or calculated through the application of a mathematical function to a variable. Also called a \"derived variable\" or \"constructed variable.\" Confidence interval: A sample-based estimate expressed as an interval or range of values within which the true population value is expected to be located (with a specified degree of confidence). Contextual data: In NELS:88, the primary unit of analysis is the student (or dropout), and information from the other study components, referred to as the contextual data, should be viewed as extensions of the student datafor example, as school administrator, teacher, and parent reports on the student\\'s school learning environment or home situation. Core student: Students who are part of the primary cohort of NELS:88, in contrast to state augmentation or School Effectiveness Study students. The core students include those chosen as eighth graders in the 1988 Base Year Study and those added to the sample through freshening procedures during the First Follow-Up. Core study: The original NELS:88 study, in contrast to the study with additions and follow-up additions like the state augmentation studies and the School Effectiveness Study. Cross-sectional survey: A cross-sectional design represents events and statuses at a single point in time. For example, a cross-sectional survey may measure the cumulative educational attainment (achievements, attitudes, statuses) of students at a particular stage of schooling (for example, eighth grade, tenth grade, or twelfth grade). In contrast, a longitudinal (or repeated measurement of the same sample units) snrvey',\n       'The classifiers trained with individual morphometric modality were combined by a majority vote and subsequently compared with the best model that demonstrated the highest accuracy (the one trained using parcelled thickness and volumetric measurements) on the test set.\\nWe were also interested in assessing the effects of feature selection. For this purpose, all the steps described above (in \"Parameter selection and classification\") were performed without RFE (only mtry-parameter adjustment). The resulting classifiers were assessed using the identical approach and combined together by a majority vote.',\n       \"We first describe couples' socio-demographic characteristics, prenatal attitudes (wantedness), fathers' prenatal involvement, mother-father relationship quality (i.e., mother-reported happiness, father-reported happiness), motherinfant interactions and father-reported engagement (literacy, caregiving, and warmth) in a national sample of Mexican American and other-Latino infants born in 2001 and their parents (see Tables 1 and 2 ). Next, the study hypotheses were tested using hierarchical multiple regression. Four series of hierarchical models with mother-infant interaction quality and fathers' engagement (literacy, caregiving, and warmth) as the outcome variables were constructed. The first models in each regression included the socio-demographic control variables (i.e., infant gender and age, household income, mother and father education, hours employed, age and English proficiency, number of children in the household, and marital status) entered together in step one since they have been associated with mother and father engagement (see Cabrera et al. 2006; TamisLeMonda et al. 2004) . Mother-and father-reported happiness were entered simultaneously in step two. In the second models, the explanatory variables (i.e., mother and father wanting the pregnancy and couple agreement) were added as a set in step three.\\nTo test for moderation, we followed Baron and Kenny's (1986) steps. We first created interaction terms by multiplying each of the dichotomous explanatory variables (mother wanted, father wanted, couple agreement) by mother-and father-reported happiness. Then, in the fourth step of each series of hierarchical regression models we alternatively entered three pairs of interaction terms, one pair for each explanatory variable (e.g., mother wanted x mother-reported happiness and mother wanted x fatherreported happiness) (not shown). For any significant interaction term, we plotted mean scores on the outcome variable for each level of the explanatory and moderating variables.\",\n       'Percentage of 1995-96 beginning postsecondary students at public 4-year institutions who left without a credential and did not return by spring 1998, by change in family and education circumstances by 1998',\n       nan,\n       'This report uses the statistical linking procedures outlined in Johnson, Cohen, Chen, Jiang, and Zhang (2005). One major difference is that this report uses reported statistics from the NAEP 2011, TIMSS 2011, and PIRLS 2011 published reports, and the 2011 NAEP reports in mathematics and reading, rather than recalculating them from the public-use data files and plausible values available for the NAEP, TIMSS, and PIRLS assessments. The international benchmarking in this study is based on data obtained from several publically available reports. Data on mathematics, reading, and science NAEP were obtained from 2011 NAEP reports (National Center for Education Statistics, 2011a, 2011b, 2010. Data on TIMSS mathematics and science results were obtained from 2011 TIMSS reports (Mullis, Martin, Foy, & Arora, 2011;Martin, Mullis, Foy, & Stanco, 2011). Data on PIRLS were obtained from the 2011 PIRLS report (Mullis, Martin, Kennedy, Foy, & Drucker, 2012). In the following discussion, Y denotes TIMSS (or PIRLS) and X denotes NAEP. In statistical moderation, the estimated z score is a transformed x score expressed in the y metric The z is the TIMSS equivalent (or PIRLS equivalent) of the NAEP score x associated with the state performance standard. The NAEP score x is obtained from determining the scaled score on NAEP that is the equipercentile equivalent of the performance standard on the local state accountability test (that is used for federal reporting required by No Child Left Behind). In equation 1, Â is an estimate of the intercept of a straight line, and B is an estimate of the slope of the linear transformation of NAEP to TIMSS or PIRLS defined by ˆŶ In the above equations, ˆX µ and ˆY µ are the national means of the U.S. NAEP and U.S. TIMSS (or PIRLS), respectively, while ˆX σ and ˆY σ are the national standard deviations of the assessments.',\n       \"adverse event falls were tabulated. Baseline characteristics were compared between subjects with and without one or more falls. Cox proportional hazards models were conducted to evaluate the association between subject characteristics and hazard of the first fall. Results Age (p \\\\ 0.0001), Functional Activities Questionnaire (p = 0.035), Beers List (p = 0.0477) and medications for treating cognitive symptoms of Alzheimer's (p = 0.0019) were associated with hazard of fall in the univariate model. In the final multivariate model, after adjusting for covariates, Alzheimer's medication use (p = 0.0005) was associated with hazard of fall. Medication was changed by the clinician after an adverse fall event in 9 % of the falls. About 7 % of the falls were reported as serious adverse events and 6 % were reported to be severe. Conclusion We found a significant association between the use of symptomatic medication treating cognitive symptoms in AD and hazard of fall after adjusting for age and Beers List medication use. Additional pharmacovigilance of the association between falls and Alzheimer's medication use is warranted.\",\n       'Once the parameters for the GPD have been established, the annual damage risk can be estimated for each exposure category for each county. The annual damage risk can be estimated for any scenario of climate change. Hanover County has the second highest annual damage risk, and this is attributed to the fact that the annual occurrence rate of hurricanes is higher for this county than for Galveston County.   or when it is assumed that the frequency increases in addition to increasing hurricane wind. For example, the \"worst case scenario\" (i.e. Climate Change Scenario 9) involves increasing the frequency of hurricanes by 10% and increasing the hurricane wind speed by10%, which results in an annual damage risk ranging from 28% to 45% higher than that of the no climate change scenario for the three counties.  Galveston County, respectively, from the no climate change scenario. From the figure it is evident that the cumulative damage costs for Miami-Dade County are considerably higher than those for the other two counties. This can be attributed to a few factors. First the annual occurrence rate of hurricanes is higher from Miami-Dade County than the other two counties, and the GPD parameters determined for Miami-Dade County are based on wind speeds that are higher than those found for the other two counties. Furthermore, the number of single-family housing units that are exposed to a hurricane event in Miami-Dade County are approximately 452,000 units, while in New Hanover County there are 58,500 units and in Galveston County there are 56,300 units. The number of housing units in Miami-Dade County is considerably higher than that of the other two counties, which results in the significantly higher exposure and damage costs for the region. The number of exposed housing units has a considerable effect on the potential damage cost to a region, and Miami-Dade County has about seven times the number of exposed housing units than both other counties. Therefore, the expected damage cost for Miami-Dade County is significantly higher than those in the other two counties. However, New Hanover County and Galveston County experience a greater percentage of increase because the annual damage risk was initially lower, resulting in proportionally higher increases.   To normalize the damage risk, Figure 3.5 shows the cumulative damage cost after 50 years per 100,000 residents for all three counties under various scenarios of climate change. From this figure it can be seen that New Hanover County has a higher expected cumulative damage cost per 100,000 residents than the other two counties. If no climate change is assumed, the cumulative damage costs is $1,310 million per 100,000 residents in Miami-Dade County, $1,680 million per 100,000 residents in New Hanover County, and $400 million per 100,000 residents in Galveston County. These figures increase to $1,360 million, $1,820 million, and $430 million for Miami-Dade County, New Hanover County, and Galveston County, respectively, for Climate Change Scenario 3, and to $1,500 million, $2,000 million, and $475 million for Miami-Dade County, New Hanover County, and Galveston County, respectively, for Climate Change Scenario 9. The replacement house value is $98,000, $143,000, and $82,000 for Miami-Dade County, New Hanover County, and Galveston County, respectively. The house replacement value in New Hanover County is 45% higher than in Miami-Dade County; therefore, even though the total cumulative damage cost in Miami-Dade County is significantly higher, the cumulative damage cost per 100,000 residents is higher for New Hanover County. ',\n       'Now we turn specifi cally to the relationship between teacher grades, test scores and noncognitive skills. To examine the link we reestimate Equation 1 with the subjectarea grade as the achievement measure, incrementally adding the contemporaneous subject-area test score and ATL score from the previous grade level. 13 So, the estimating equation becomes\\nBecause teachers were unaware of students\\' test scores when they provided their subjective assessments, the test score is exogenous. As in the baseline case, we estimate 12. In kindergarten and fi rst grade, these are \"general knowledge\" test scores and grades. General knowledge questions cover a combination of social science and natural science subject matter. In third and fi fth grade, these test scores and grades refl ect science curriculum only. 13. Proceeding in this way does not treat cognitive and noncognitive skills in a parallel fashion in the sense that lagged test scores are omitted from the specifi cations. This is a potential problem if lagged test scores explain some of the gender gap in grades. However, including lagged test scores either as a covariate or using it as an instrument (like we do with the lagged ATL score in the next section) has no impact on the estimated gender gap in teacher grades. The results of this exercise are available as a web appendix. Notes: Test scores and grades are normalized to have mean=0 and variance=1. All regressions control for family, teacher, and school characteristics. Standard errors are in parentheses; ***, **, and * indicate statistical signifi cance at 0.1, 1, and 5 percent levels, respectively.\\nEquation 2 separately for each race and grade level. 14 Tables 5a-c report these fi ndings for reading, math, and science, fi rst reproducing the baseline results for comparison\\'s sake (Column a), then adding the subject-area test score (Column b) and lagged ATL score (Column c). The kindergarten case is omitted because there is no prekindergarten behavioral assessment.\\nEquation 2 embodies the proposition that students who perform equally well on subject-area tests should receive (roughly) the same subject-area assessment from the teacher. If this assertion holds in the data, controlling for the test score should eliminate the estimated gender gap in grades. If not, then the question remains regarding what accounts for the test-score / grade disparity. We explore the role of noncognitive skills as measured by the ATL score. As evidenced in Table 1 , the average ATL score for boys is roughly 15 percent lower than for girls and the variance in boys\\' scores is greater in every grade. Thus, boys are less likely to sit for long periods of time, participate or demonstrate knowledge in the classroom, or supply effort on assignments and homework. Initially, we employ the lagged ATL score to avoid the possibility of bias that might arise through feedback of the subject-area grade to the behavioral assessments. Bear in mind that the lag entails two years for grades three and fi ve. Nevertheless, to the extent that \"approaches to learning\" behavior is persistent, students with higher lagged ATL scores will be assessed more favorably by their teachers.',\n       \"In North Carolina, Hurricane Fran was a significant shock for many people living inland. This was particularly true for the residents in and around the state capital in Raleigh where a disaster of this magnitude had never been experienced in their lifetimes. For coastal residents of North Carolina, Hurricane Edouard passed off the coast a few days earlier, and Bertha's passage through the area the previous month led to heightened public awareness. Additionally, the long lead times provided by NWS forecasts allowed the emergency management community and the general population to effectively prepare for Hurricane Fran. The NWSO Wilmington, North Carolina, WCM noted that everyone in the area knew what to do for Fran, with Bertha serving as an excellent teacher of respect for tropical cyclones. Eastern Region NWSFOs, NWSOs, and RFCs were prepared internally for Fran with appropriate staffing changes initiated well ahead of the storm. Externally, shelters were opened where needed, and no preparedness problems were noted. Coastal offices had sufficient time to review hurricane preparedness policy and allowed the electronics technicians and staffs to double-check equipment. SKYWARN amateur radio volunteers at NWSFO Sterling, Virginia, raised a temporary HF antenna to use as backup communications to the Richmond EOC and other sites, if needed. SKYWARN at NWSFO Sterling was placed on standby Thursday, September 5, with a plan to fully activate Friday morning. The SKYWARN operators in Pennsylvania solicited rainfall reports from amateur radio operators, filling some holes left by loss of some instantaneous IFLOWS data. In North Carolina, the NWS and state emergency management conducted an exercise of the state emergency response capability after Bertha. This was time well spent. There was also an official hurricane awareness week and a workshop on hurricanes for the media in June. The Raleigh and Morehead City HSAs had provided extensive recent outreach programs to the public and emergency officials. Northern Virginia counties struck hard by flooding in June 1995 and January 1996 had just revised their local emergency plans after these disasters. In northern Virginia, schools and many businesses closed Friday, September 6, before the worst of the storm was forecast, substantially reducing the risk by decreasing traffic on the roads.\",\n       \"In a sample of Medicare beneficiaries with COPD with a hospitalization in 2010, we found that APPs were more likely to prescribe short acting bronchodilators or oxygen therapy and to consult a pulmonary specialist, but less likely to give influenza and pneumococcal vaccinations compared to MDs. Patients receiving care from APPs had lower rates of ER visits for COPD and a higher follow-up rate with a pulmonologist within 30 days of hospitalization for COPD than those cared for by an MD. NP/PAs were introduced in the US in the 1960s; since then, demand for NP/PAs has exceeded the supply. Approximately 205,000 NPs and >93,000 PAs practice in the US [7, 17] . About half are employed in primary care settings (defined as family medicine, general medicine and general pediatrics) [18] . It is estimated that APPs could provide care for 50-90% of patients presenting to primary care [19] . With the increasing number of APPs as primary care providers, they will be more likely to be called upon to manage patients with such chronic conditions as COPD. Large regional differences across US in patients with COPD cared for by APPs are likely representative of state regulations on NP practices [20, 21] . The present study showed no differences in hospitalization or readmission of COPD patients by group. This result is consistent with previous studies of chronic disease management by NPs/PAs. An RCT study showed no difference in blood pressure or total cholesterol control between patients receiving care from NPs and those receiving care from primary care physicians [21] . Similarly, studies of diabetic patients showed no difference in HbA 1 C control and outcomes in patients cared for by NPs or primary care physicians [22] . A recent study showed similar outcomes for in-hospital mortality for patients receiving care from APPs and physicians in intensive care units [23] .\\nOur study showed more use of resources such as pulmonary referrals, oxygen therapy and medication prescription in the APP group, consistent with findings from diabetic care studies, which showed higher use of referrals and resources in patients cared for by APPs than in those cared for by MDs [24] . Similarly, a recent study showed greater use of imaging services by APPs compared to MDs [25] . The more frequent specialist consults with NP care may be due to the recognized need for expertise and skills outside of the NP's scope of practice for complex patients. Lower use of influenza vaccination in the APP group is likely related to the lower age group of these COPD patients under their care.\\nContrary to our hypothesis that patients cared for by APPs have better access to care, we found lower rates of follow-up clinic visits after acute hospitalization in the APP group than in the primary care physician group. However, patients cared for by APPs had more clinic followup visits with a pulmonary specialist than the patients of MDs. Higher follow-up rates with pulmonologist post hospitalization in APP group may partly explain the lower trends in emergency visits and readmission. Studies have shown that early follow up with a pulmonary physician is associated with lower readmission rates [26, 27] . Previous studies including a Cochrane meta-analysis have shown that patients receiving care from APPs have a higher frequency of return visits compared to patients of physicians [28, 29] Higher follow-up rates with a primary care provider in the physician group were likely due to the greater accessibility of physicians compared to APPs. We excluded patients with COPD who received care under a mixed NP/MD model. The lower follow-up rates for APP patients can be explained by the higher number of patients under NP/PA care who may follow up with a physician after hospitalization, thus resulting in lower follow-up rates in the APP group.\\nOur study showed no difference in the 30-day readmission rate after acute COPD hospital admission in patients cared for by APPs vs primary care physicians. No intervention has yet been proven to reduce readmissions in COPD patients. A recent systemic review found inadequate evidence to recommend specific interventions to reduce readmissions in this population [30] . Jennings et al., in a recent randomized controlled trial, showed no difference in 30-day risk of rehospitalization or ED visits after implementation of COPD bundle at discharge. The elements of the bundle were smoking cessation counseling; screening for gastroesophageal reflux disease, depression and anxiety; standardized inhaler teaching; and a 48-hour post-discharge phone call [31] .\\nThis study has several limitations. First, we were not able to distinguish whether APPs were working independently or under physician supervision, as our definition of NP care was based on E&M billing. However, we included only patients for whom all bills for outpatient visits in a given year originated from either APPs only or MDs only. Second, assessing processes and outcomes of care in an observational study is subject to selection bias; for example, severity of COPD was not measured, a factor that can affect outcomes of care. Due to the cross sectional nature of the study, the use of spirometry and vaccinations (specifically, pneumococcal vaccination) were lower than in prior reports [32] We examined only pneumococcal vaccination rates during the study period and missed the opportunity to capture the true rates, given the infrequent recommendations compared to influenza vaccination. Third, we did not look at outcomes and processes of care for COPD patients cared for by both APPs and primary care physicians. Future research should examine the benefits of shared model in managing patients with COPD compare to APPs vs MDs model alone. Complex patients are more likely to benefit from shared model of care than either solo model. Shared models provide easy access to care and expertise needed to manage these patients.\\nFourth, we did not account for cost of care in the two different care models. Previous studies have shown that APP cost of care is the same as or slightly lower than that of a physician [33] . Fifth, we did not compare patient satisfaction for the two groups as in previous RCTs comparing APP vs physician models; however, this is a limitation of the observational study design. Sixth, the results are not generalizable to patients younger than 65 years and those who do not have complete enrollment in Medicare Part A, B and D. Seventh, we reported the adjusted effect estimates but cannot exclude the possibility of false positive findings given the multiple testing. Finally, the proportion of patient with COPD cared for by APPs in the current study is higher than in the general population, as we used the 100% Medicare population with COPD cared for by APPs.\\nIn summary, compared to patients cared for by primary care physicians, patients cared for by APPs were more likely receive short acting bronchodilators, oxygen therapy and being referred to a pulmonologist. Despite lower rates of influenza and pneumococcal vaccination among patients with COPD cared for by APPs, these patients were less like to visit an ER for COPD compared to those cared for by primary care physicians.\",\n       nan,\n       'The Critical Coastal Habitat Assessment is a new long-term monitoring program for Tampa Bay that will assess the status, trends, and ecological function of a mosaic of critical coastal habitats. Its purpose is to detect habitat changes due to natural and indirect anthropogenic impacts, including those resulting from sea-level rise and climate change, and to improve habitat management (Janicki 2013, Sherwood andGreening 2012, 2014). To accomplish this, long-term fixed-transects were established in 2015-2016 to characterize base-   line habitat structure (see full description in Chapter 1). Monitoring will be completed every 3-5 years to detect trends and assess changes in extent and ecological function of those habitats over time. Transects were established at nine sites around the Tampa Bay watershed in areas that have a full complement of emergent tidal wetland communities including mangroves, salt marshes, salt barrens, and coastal uplands. A multimedia training video will also be produced to aid other programs and communities in implementing similar long-term monitoring programs. Updates and documents will be posted at www.tbeptech.org/committees/habitat-partnership.',\n       'At follow-up, 39 individuals were MCI converters (MCI c ), and 88 individuals were MCI non-converters (MCI nc ). Four of the 88 nonconverters reverted to the status normal cognitive control, 84 remained MCI. Groups did not differ with regard to sex distribution, age, years of education, time of follow up, overall cognitive performance as determined using the Mini Mental Status Examination (MMSE) (Folstein et al., 1975) score and episodic memory performance as determined using the delayed recall of the logical memory subtest of the Wechsler Memory Scale-Revised (Wechsler, 1987) and the Auditory Verbal Learning Test (Query and Berger, 1980) (Table 1) .',\n       'About 71 percent of students took at least one mathematics course in college (figure 6). 8 First-generation students, however, were less likely to do so than their peers: 55 percent, compared with 68 percent of students whose parents had some college education and 81 percent of students whose parents held a bachelor\\'s degree or higher. The difference existed at both introductory and advanced levels. Among students who took mathematics courses, first-generation students completed fewer credits in the subject than other students: an average of 8, compared with 10 to 11 credits earned by students in the two groups whose parents had attended college. This discrepancy was mainly due to the difference in credits earned in advanced mathematics courses, because no differences were detected between first-generation students and their counterparts in the number of credits earned in introductory mathematics courses. When examining mathematics coursetaking within major, 9 differences were also evident. For instance, first-generation students majoring in business, social sciences/journalism/communication, humanities/arts, health sciences/services, and human/protective services/vocational fields were all less likely than their counterparts whose parents were college graduates to take 7 The analysis in this section excluded students who earned just 10 or fewer credits during their postsecondary education. This criterion has been applied in many analyses of postsecondary transcripts (e.g., McCormick 1999;Adelman 1999Adelman , 2004aAdelman , 2004b. This group, often referred as \"incidental students,\" accounted for 12 percent of 1992 12th-graders who had enrolled in postsecondary education in 1992−2000 (see Adelman 2004b for more information about their demographic and academic characteristics). 8 College-level remedial mathematics was excluded. 9 Between 1 and 7 percent of students who majored in mathematics or sciences did not take any mathematics course (table 7). This is probably due to students who took only remedial mathematics courses, which were not counted as college-level mathematics courses, students who dropped or stopped out before taking any mathematics course, or students with science majors who used advanced/AP mathematics credits earned in high school to fulfill their degree requirements for mathematics coursework in college. any mathematics courses (table 8). The gaps in advanced mathematics (both in the likelihood of taking courses and credits earned) between first-generation students and those whose parents were college graduates remained, even among those who majored in mathematics or sciences. Finally, the discrepancies held when the analysis was limited to only those with bachelor\\'s degree goals who attended 4-year institutions: first-generation students were not only less likely  than other students to take mathematics courses, they also earned fewer credits if they took any mathematics courses (figure 6).',\n       'Our primary aim was to determine the degree of 5-year overall survival benefit conferred by adjuvant chemotherapy on patients with stage II colon cancer having poor prognostic features. Although these characteristics do distinguish a subgroup of stage II patients with worse survival than the rest of the stage II cohort, they do not predict measurable mortality benefit from chemotherapy in this study population. Our sample size in this analysis of Medicare beneficiaries allows us to detect, if present, a 2% absolute difference in 5-year survival attributable to chemotherapy; our lack of statistical significance implies that any benefit that exists is quite small. Interestingly, chemotherapy use does not differ between stage II patients with or without poor prognostic features, suggesting that these are not central to the treatment decision.\\nOur analysis has several important advantages over prior investigations. In contrast to prior trials that predominantly recruited patients with stage III disease, our study population has sufficient power to detect clinically significant differences in survival for patients with stage II disease, addressing concerns about inadequate sample size. In addition, this is the only study to date that specifically examines the subgroup of stage II patients with poor prognostic features, thereby addressing the population heterogeneity that was thought to confound prior conclusions of minimal survival benefit. [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] 12, 15 The median age of colon cancer diagnosis is 70 years, yet many older adults were explicitly excluded from randomized chemotherapy trials or were otherwise deemed ineligible as a result of age-related comorbidity, provider bias, or patient preference. [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] Our study population importantly presents a more real-world age distribution (68% over age 75 years at diagnosis) and includes patients with greater levels of comorbidity and risk than typically allowed in randomized trials, facilitating better generalizability to the population of senior adults.',\n       'As of 2003, 37 states offered some type of tax credit for research activities. 6 This is up from 1985 when only 7 states offered such a credit. Originally, almost all state credits were modeled after the federal credit but with lower credit rates. In general, the state credits rely on the federal defi nition of qualifi ed R&D activities and operate in a similar manner to the federal credit. There are several characteristics of the state credit that can be used to distinguish one state credit from another. The most prominent feature of the credit is the rate. State credit rates for qualifi ed R&D activities vary from zero to 20 percent in Hawaii and 33 percent in Arkansas. 7 For instance, while some states have retained the use of the 1984-1988 base of expenditures, many states defi ne the base of qualifi ed expenditures for the current year as a function of the expenditures over the previous three or four years. In addition, some states offer a non-incremental credit as opposed to the incremental credit offered at the federal level. Under a nonincremental credit, all R&D expenditures are eligible for the credit, not just those exceeding some base amount. Another potentially generous feature of some state credits is the ability to transfer unused credits to another party. Several states offer such a credit including Pennsylvania, New Jersey, and Hawaii. Many fi rms with R&D activities may not have positive tax liabilities. Since the credit is of no value to a fi rm with little or no tax liability, selling the credits allows the R&D fi rm to generate much needed cash. On the limiting side, several states impose an overall cap on the value of credits awarded annually and many impose limits on the amount of the tax liability that can be eliminated with the credit, such as Georgia which allows fi rms to used the credit against a maximum 50 percent of their tax liability with the remaining credits being carried forward. While there is no single source of information on the value of R&D credits awarded at the state level, several states report the use of the credit. For example, in 2003 California awarded R&D credits of $551 million for 5,086 fi lers. 8 Pennsylvania awarded the total amount allowed by state law, $15 million, but had total claims of $70.2 million by 242 fi lers. 9',\n       \"Academic behavior. Several components of students' academic behavior were related to differences in student test scores, including students' failure to complete homework, their being inattentive in class, and their being frequently absent or disruptive. Television. Increased hours of television watching were associated with lower student test scores.\",\n       nan,\n       '. Fig. 4 shows samples of segmentation of the BrainWeb template with 3% noise, using the adaptive prior (equations (5-6)) compared with a prior that favors homogeneous tissue neighborhoods everywhere [4] . Clear differences between the panels are visible. The boundary between lentiform nucleus and cortical gray is more clearly delineated in the right panel. The middle panel merges the two gray areas whereas the adaptive technique keeps them separated. Similarly, sulci in the vicinity of the insula show greater coherence in the right panel. IV. DISCUSSION Fig. 3 suggests that the GSeg method has achieved, on average, straight-line estimates of one-year longitudinal tissue change, unlike the other methods; this linear change may be more biologically plausible than nonlinear change, and therefore the GSeg method may be superior for quantifying longitudinal change. Examination of the single subject in Fig. 2 suggests that subcortical segmentation is a specific reason for its superiority. GSeg has a more robust and consistent representation of subcortical areas (thalamus and putamen) over time than either FAST or SPM5. Cortical details are also better represented in both GSeg and FAST than in SPM5. Fig. 4 suggests that the adaptive prior may be responsible for this performance advantage.',\n       'In addition to RBD, olfactory dysfunction is one of the most well-known pre-motor symptoms, and multiple studies suggest it is an early sign of PD, 22 consistent with proposed involvement of the olfactory bulb in ',\n       'The Lab\\'s division of labor not only mirrors the NIH\\'s characterization of a typical laboratory given above but also dovetails with Rayna Rapp\\'s (1999) observations that \"laboratories are staffed by an international circulation of highly skilled scientific workers,\" and that their work is often perceived as \"women\\'s work\" (p. 199). By \"women\\'s work,\" I mean to suggest that the labor itself is gendered, i.e., it is marked by \"precariousness, flexibility, mobility, fragmentary nature, low status, and low pay\" all of which are qualities \"historically present in female work\" (Oksala, 2016, p. 281;Standing, 2011). In many ways, the Lab\\'s labor organization and technicians\\' work practices reflect a metaphor drawn long ago by physiologist Claude Bernard (1865) in which he described a physiology laboratory as a \"ghastly kitchen,\" where science resembles a messy kitchen rather than a highly-organized experimental endeavor, a metaphor that Bruno Latour (1992) would later articulate as the \"costly ghastly kitchen\" (p. 300). And yet today, it is not science \\'great men\\' who are \\'cooking\\' down in the laboratory, but rather an amalgamation of science technicians who, precisely because of their economic and political vulnerability as immigrants, bring \"stability\" to the Lab in the course of producing new facts about environmental determinants of obesity and metabolic health (NIH, 2012, p. 10). Technicians at the Lab \\'cooked\\' in several locations across the larger campus. First, the main laboratory area consisted of two \\'wet\\' rooms filled with an array of machines, tools, bottles, flasks, and other items used in the course of everyday work. Second, adjacent to the wet lab, there was a shared computer room with computer stations for four people, but never enough space to occupy everyone on a full day at the Lab. Within this office, technicians strategically combed through scientific papers, investigated protocols, annotated lab notebooks, stored and prepared images and drawings, and utilized software programs to document, interpret, plan, and troubleshoot results. Third, a short walk away from these rooms was the Lab\\'s procedure room. A space, shared with other institute\\'s employees, where animals were sacrificed and early steps of experimentation, like whole animal fixation (perfusion fixation), were performed. Fourth, in another common room, the Lab stored tissue samples in refrigerators and freezers for future use, and lastly technicians shared a common darkroom to expose films that had been treated according to particular visualization techniques. This was a place where \"Did it work?\" was a familiar question because, once films had been developed, technicians eagerly read out a series of smudges that they interpreted as a sign that their earlier experimental work was a success, i.e., a positive result. The organization of empirical work at the Lab typically began with weekly meetings, which provided a time and space for Drs. Harris and Moore to meet with most, if not all, of the technicians to review events over the preceding week and to map out plans for the upcoming one. Lab meetings were held in Dr. Moore\\'s office in a building located adjacent to the laboratory. She led the weekly meetings and her questions to the team were often organized by technician because each one was responsible for specific biomass in a specific experimental system. The overall pace of experimental work was dictated by a need for data for use in imminent grants, conference presentations, and publications. As I was once informed, \"you are always getting data for either grants or publications,\" and \"it takes a long time to get a good story\" (anonymous, personal communication, February 2, 2015). Career technicians. In this section, I provide biographical data and career trajectories for the Lab\\'s three career technicians: Eleanor, Olivia, and Carrie. Eleanor, the most senior and entrusted technician, primarily worked with adipose (fat) tissue and cells in the maternal obesity model. 13 She was also called upon to teach less experienced technicians in techniques, to troubleshoot problems as they occurred in myriad circumstances, and to help establish a new experimental model around programmed appetite pathways in the hypothalamic region of the embryonic mouse brain (discussed in chapter 4). I was told that the Lab \"would be doomed\" without Eleanor\\'s contributions (personal communication, January 4, 2016). Dr. Moore told me that \"to find someone of Eleanor\\'s caliber and passion, at her level, would be difficult.\" Dr. Moore could suggest a protocol for Eleanor to follow on a particular study and, although Eleanor \"has [previously] nailed down the techniques,\" she usually had to navigate unforeseen circumstances in everyday experimental work. Once, for example, in the midst of a protocol disagreement between Eleanor and Olivia, a junior investigator whispered to me that Eleanor has a \"strong personality\" as a way to explain why she seemed so impassioned, but that situation also revealed to me how they each care for particular details of a protocol. Both technicians were pressed continuously with uncertainties and, as a result, they oriented their caretaking practices towards whatever end each one deemed best for the task at hand. In fact, the degree of their personal investments in the Lab\\'s success could be measured by the timing, pitch, and tone of their voices during disagreements with one another. If they did not care, they would not argue about the best way to handle either anomalies or even give suggestions about what particulars of biomatter may be relevant to their investigations. I also found that having a \"strong personality\" in general must have served both career technicians well since the work sometimes entailed long and erratic hours, constant learning, problem-solving, and supervision of others (including me) for which they received relatively low pay, in the range of 50k per year. Eleanor suggests that this type of job is so stressful that many people eschew basic science research altogether and opt for more stable careers in biomedicine.',\n       'Imagery was provided by the USGS EDC from the NLCD01 imagery archive (U.S. Geological Survey, 2006d). Derivation of land-cover and imperviousness datasets was based primarily on Landsat-7 ETM+ 30-meter imagery, but also included IKONOS 4-meter (Space Imaging, Inc.), and U.S. Department of Agriculture (USDA) National Agriculture Imagery Program (NAIP) 1-meter images used as training data for imperviousness regression (table 1). Additional high-resolution (1-foot to 1-meter) Digital Orthophoto Quads (DOQs) or other aerial photography available from the USGS Seamless Server (U.S. Geological Survey, 2006b) were also used frequently as a source of training data or for visual verification purposes. At least three Landsat images were used for every Worldwide Reference System (WRS) Landsat path/row footprint, generally including a leaf-on, leaf-off, and an early spring or late fall image for each. The number of Landsat path/row footprints necessary for coverage of each study area varied: for Denver three WRS footprints were required (fig. 5); for Dallas-Fort Worth eight were required (fig. 6); and for Milwaukee-Green Bay two were required ( fig. 7). For the Dallas-Fort Worth area data-mosaic scenes were delivered based on three time periods: leaf-on, leaf-off, and spring. For the Denver and Milwaukee-Green Bay areas, data were delivered as separate scenes. Landsat data were delivered in two formats: seven band terrain-corrected Digital Number (DN) spectral data and three band (brightness, greenness, wetness) Tasseled Cap transformed data.   ',\n       'In each theory described above, complex dynamics have been at work in determining high school attendance and graduation. The questions one may ask about the potential influence of high-stakes tests on graduation revolve around the relationships among various forces:\\nDo high school students facing high-stakes tests calculate opportunity costs of further education differently from students not facing high-stakes tests?\\nTo what extent might the decisions of students to continue school change depending on existing labor-force conditions? What incentives do school systems have, stemming from within as well as without, that will affect how they respond to students who fail high-stakes tests? Is there independent evidence apart from test results that high-stakes testing has altered the low academic expectations high schools have often set in the past century, as argued by Angus and Mirel (1999) , Powell, Farrar, and Cohen (1985) , and Sizer (1984)?\\nAs argued earlier, these explanations are not necessarily mutually exclusive. While the labor-market explanations suggest a largely external force, the other two theories suggest largely internal dynamics or decisions that shape the opportunities available to teenagers. We need not choose among these explanations at the moment, for they do not explicitly conflict. (Whether fellow researchers may prefer one explanation over another should not preclude consideration of them for policy purposes.) Each provides useful questions to frame further exploration. In particular, the theory of reciprocal labor market-school movement suggests that the effect of high-stakes testing may be indirect. Even if graduation gateway tests do not directly prevent diplomas for many, promotional gates earlier in school may result in a higher proportion of 18-year-olds who are far away from graduation. Will they stay in school at the expense of current earnings, if they will need to stay in school until 20? Advocates of these promotional and graduation gates point out that promoting and graduating students without skills are hollow events, and I have had enough students in my college classes without useful skills to be sympathetic with that argument. The assumption, discussed below, is that the value of a diploma is equivalent to the value of the knowledge and skills one presumably learns in school. However, a teenager is usually not learning academic skills if she or he leaves school for work. Historically, self-education has been a difficult, if virtuous, activity (Kett, 1994) .',\n       \"In the MCI sample, there were no significant sex by APOE interactions on any AD-related marker (Fig. 2 ). There was a main effect of sex on AVLT scores in MCI whereby scores were higher in women than in men (P , .001). There was a main effect of APOE-ε4 for all markers in MCI. APOE-ε4 was associated with smaller HpVR, lower FDG SUVR, higher AV45 SUVR, and poorer AVLT scores (P's , .001). \",\n       'Outer Suburban: suburban schools in a district that touches a rural district, but not the city limit. 2',\n       \"Estimates produced using data from the ECLS-K are subject to two types of error, sampling and nonsampling errors. Nonsampling errors are errors made in the collection and processing of data. Sampling errors occur because the data are collected from a sample rather than a census of the population. Nonsampling Errors. Nonsampling error is the term used to describe variations in the estimates that may be caused by population coverage limitations, as well as data collection, processing and reporting procedures. The sources of nonsampling errors are typically problems like unit and item nonresponse, the differences in respondents' interpretations of the meaning of the questions, response differences related to the particular time the survey was conducted and mistakes in data preparation.\",\n       'This item was considered to belong to three topics, N4 (Properties of numbers), L1 (General logic) and N5 (Powers and exponents).\\nTo compare expected with empirical performance, the question papers of two years, 2006 and 2012, were considered as case studies. The anticipated level of difficulty as envisaged by the experts who set the question papers was compared with the actual performance of contestants in the papers.',\n       'Introduction. The application of digital monitoring biomarkers in health, wellness and disease management is reviewed. Harnessing the near limitless capacity of these approaches in the managed healthcare continuum will benefit from a systems-based architecture which presents data quality, quantity, and ease of capture within a decision-making dashboard.\\nMethods. A framework was developed which stratifies key components and advances the concept of contextualized biomarkers. The framework codifies how direct, indirect, composite, and contextualized composite data can drive innovation for the application of digital biomarkers in healthcare.\\nResults. The de novo framework implies consideration of physiological, behavioral, and environmental factors in the context of biomarker capture and analysis. Application in disease and wellness is highlighted, and incorporation in clinical feedback loops and closed-loop systems is illustrated.\\nConclusions. The study of contextualized biomarkers has the potential to offer rich and insightful data for clinical decision making. Moreover, advancement of the field will benefit from innovation at the intersection of medicine, engineering, and science. Technological developments in this dynamic field will thus fuel its logical evolution guided by inputs from patients, physicians, healthcare providers, end-payors, actuarists, medical device manufacturers, and drug companies.',\n       'ERS uses a perpetual inventory method to construct stocks. This methodology is also widely used by others. However, Sliker (2014b), as discussed below in the depreciation section, questions whether the aggregation procedure over individual assets is internally consistent.',\n       'We evaluate our algorithm with extensive experiments on liver CT datasets and brain MRI datasets. We compare our algorithm against state-of-the-art traditional registration algorithms including ANTs [4] and Elastix [5] , as well as VoxelMorph [17] . Our algorithm achieves state-of-the-art performance while being much faster. Our experiments prove that the performance of our unsupervised method is improved as more unlabeled data are used in training. We show that cascading subnetworks significantly improves the performance, and that integrating affine registration into the method is effective.\\nWe evaluate the performance of algorithms with the following metrics:\\n• Seg. IoU is the Jaccard coefficient between the warped liver segmentation and the ground truth. We warp the segmentation of the moving image by the predicted deformable field, and compute the Jaccard coefficient of the warped segmentation with the ground-truth segmentation of the fixed image. \"IoU\" means \"intersection over union\", i.e., |A∩B| |A∪B| , where A, B are the set of voxels the organ consists of.\\n• Lm. Dist. is the average distance between warped landmarks (points of anatomical interest) and the ground truth.\\n• Time is the average time taken for each pair of images to be registered. Some methods are implemented with GPU acceleration, therefore there are two versions of this metric (with or without GPU acceleration).\\nOur model is defined and trained using TensorFlow [28] . We accelerate training with nVIDIA TITAN Xp and CUDA 8.0. We use the Adam optimizer [29] with the default parameters in ',\n       'Previous research provided a comparison of injury rates for farm women in Kentucky and Texas (Browning et al., 1997) . The rates for injuries sustained while doing farmwork were similar for farm women in both states. Work involving animals was the leading cause of farmwork injuries for women in both Texas (25% of the injuries) and Kentucky (21%). Sprains and strains were the most frequently reported types of injuries. These injuries were related to bending, twisting, lifting, and repetitive motion. Bites, stings, and burns were also reported. When the rates of farmwork injuries were examined for women self-described as homemakers, Kentucky farm homemakers had a lower farmwork injury rate (1.7/100 farm homemakers) compared with other Kentucky farm women (3.0/100 farm women). Texas homemakers also had a lower farmwork injury rate (2.3/100 farm homemakers) compared with other Texas farm women (3.6/100 farm women). The results suggest a relatively modest risk of injury resulting from farm chores for women in both Texas and Kentucky, with perhaps a slightly decreased risk of a farmwork injury among women classified as homemakers in both states compared with other farm women. However, given the relatively small numbers of farmwork injuries reported in both the Kentucky and Texas women, the number of injury events was not sufficient to determine whether these modest injury rate differences were significant.',\n       'There are strong seasonal patterns of DO within the Yaquina Estuary. Oxygen levels are stable in the estuary during the wet season, but decline during the dry season. The wet season DO (Zone 1 and Zone 2 combined) has an overall median value of 9.8 mg l À1 Table 1 Summary of sampling details for recent discrete dissolved oxygen sampling. The location of the sampling stations is presented as distance from the mouth of the estuary.',\n       nan,\n       \"The apathetic subsyndrome consists of apathy, eating abnormalities, and aberrant motor behavior. However, existing literature consist of [ 18 F]FDG PET studies either in the apathetic subsyndrome or apathy only. On the whole, there is correlation between apathy and hypometabolism in the orbitofrontal cortex (OFC) and cingulate cortex in dementia subjects; while in MCI, apathy appears to be correlated with an AD-specific pattern of hypometabolism in the PCC.\\nIn a cohort of 53 AD patients with mean disease duration of 28.7 months and Mini-Mental State Examination (MMSE) score of 22.5, apathy was associated with hypometabolism in the left OFC (Holthoff et al., 2005) . In 41 AD patients, hypometabolism in bilateral ACC and bilateral medial OFC were reported (Marshall et al., 2007) . Ballarini et al. examined the associations between regional metabolism, functional connectivity and neuropsychiatric subsyndrome clusters in early onset AD (EOAD). In 51 EOAD subjects, 27 had NPS, of which apathetic subsyndrome was the most common (74%). Hypometabolism was found in bilateral middle orbitofrontal and middle frontal gyri of subjects with the apathetic subsyndrome (Ballarini et al., 2016) .\\nThere are fewer [ 18 F]FDG PET studies in prodromal AD. A small study of 24 MCI subjects showed no significant association between apathy and regional glucose metabolism (Marshall et al., . A larger study of 65 MCI individuals from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database showed an AD-specific pattern of PCC hypometabolism in MCI subjects with apathy (Delrieu et al., 2014) . This was corroborated by a subsequent ADNI study including 422 cognitively normal, MCI, and early dementia subjects, demonstrating correlation between PCC hypometabolism and higher apathy scores (Gatchel et al., 2017) . Baseline hypometabolism of the supramarginal gyrus was also found to predict the increase of apathy over time (Gatchel et al., 2017) . In AD dementia, the association between apathy and hypometabolism in the OFC and anterior cingulate cortex (ACC) is consistent with their role in recognition of salient stimuli, reward-based decision-making, drive, and motivation (Holthoff et al., 2005; Wallis, 2007; Kouneiher et al., 2009 ). This is supported by a number of studies using other imaging modalities such as magnetic resonance imaging (MRI), diffusion tensor imaging and single-photon emission computed tomography (Stella et al., 2014) . Indeed, the ACC has been recognized as a key node in the salience network (SN) (Seeley et al., 2007; Menon, 2015) .\\nIn MCI with apathy, the finding of PCC hypometabolism mirrors the early metabolic dysfunction characteristically seen in amnestic MCI subjects reflecting underlying AD pathology (Drzezga et al., 2003; Nestor et al., 2003) . The reason for sparing of frontal lobe metabolism is less certain. Firstly, the degree of apathy may be below the threshold for detection of hypometabolism in the OFC and ACC (Delrieu et al., 2014) . Secondly, the frontal and parietal regions are interconnected, and dysfunction in one or more parts of the network may give rise to apathy (Gatchel et al., 2017) .\",\n       'At a high level, our prototype system consists of two stages: data preprocessing and visualization (see Figure 5) .\\nWhen visualizing isosurfaces of a binary 3D segmented image, it is often necessary to perform smoothing to reduce aliasing artifacts and facilitate 3D rendering and shading. First, we perform this smoothing in a two-step preprocessing stage. In the first step, the binary partitioned image is antialiased using an iterative relaxation process. 11 Next, a small amount of mesh smoothing is performed on the isosurface mesh generated from the antialiased binary image. All visualization preprocessing operations occur on the 3D volume (and corresponding codimension one isosurfaces) prior to cut-plane extraction.\\nThe second stage includes some visualization strategies to facilitate the perception and navigation of the rendered 3D objects. To improve shape perception in our application, we include interactivity with renderings of 3D objects as part of the visualization system. In our settings, the user can rotate the object displayed on the screen using a standard trackball interaction mechanism. The system lets the user select cutting planes, which clip a portion of the volume displayed on the screen, to render cross-section views of surfaces embedded in 3D. The user can also interactively orient and translate the cutting plane. Additionally, the system provides the flexibility of having one or multiple cutting planes and interactively adjusting their position and orientation. The system Figure 5 . Overview of prototype system designed for shape alignment evaluation using ensemble visualization. The prototype system consists of data preprocessing and visualization stages.\\nFeature Article interface lets the user interactively select various features of interest for rendering in order to focus on particular features of interest. For example, the user can select specific ensemble members to be rendered individually.\\nIn the case of 3D contour boxplots, we performed the analysis on the 3D binary segmented volumetric data (in the preprocessing stage) and rendered the results interactively. Although the analysis was performed on the volumetric data leading to volumetric 50 and 100 percent bands, we rendered the visualization of the statistical summaries only on chosen cut planes to deal with the issue of occlusion. For instance, in the absence of a cut plane, the 100 percent band entirely occludes the median shape and the 50 percent band.',\n       'This paper addresses an emerging issue of vulnerability of older adults to accelerating coastal flooding. Coastal hazards such as tidal inundation, sea level rise, storm surge, and high winds are capable of exerting direct and indirect impacts on places where older people live and places where they need to go to meet their health and well-being needs. They can aggravate physical health of older adults by reducing their mobility and access to health services, as well as by putting them in situations where they have to operate in unsafe conditions or engage in actions that are not compatible with their physical capabilities (e.g., wade through the floodwaters, drive in the dark, or move heavy objects to minimize property damage). Hazard exposure may also affect psychosocial well-being of older adults by, for example, promoting isolation due to disruption in communications, power grid, and accessibility to public facilities. The role of well-being on one\\'s overall psychosocial health has been increasingly recognized in the literature as an important contributor to the overall health of older individuals. It has been associated with positive relationships with others; purpose in life; realization of potential and self-acceptance [122]; individuals\\' perception of their current situation and their aspirations [123]; and lack of distress and dysfunction [124]. The exposure to recurrent flooding and consequent strain of dealing with repetitive damages, loss of belongings, changes in the demographic community profile, restricted accessibility to gathering places, and other impacts can all affect the well-being of older coastal residents. Even though some studies suggest that older residents cope better with disasters and are more resilient during the recovery process [49,[125][126][127], Eisenman et al. [128] observed after Hurricane Katrina that \"social isolation, even in the midst of a large community, prevents many older people from receiving warning signals or asking for help, rendering them invisible to rescue teams\". Thus, it is unclear how this protective effect would be challenged by recurring hazard events and where the threshold would be beyond which it would diminish. Similarly, less is known how life experiences and acquired skills contribute to the sense of self-sufficiency and confidence in coping capacity that may prompt many older adults to stay in place regardless of the risk. In this study, we found that some states on the U.S. Eastern shores have a higher population of older adults living in areas prone to coastal flooding. Namely, Delaware, Florida, Maine, New Jersey, North Carolina, South Carolina, and Virginia all have more than 20% of the coastal counties that directly border the Atlantic Ocean and/or its waterways with elderly populations making up more than 20% of the population. These higher concentrations may form pockets of vulnerability: even though New Orleans, Louisiana had only 15% of older population age 60 and over prior to Hurricane Katrina, they represented over 70% of all victims with many more experiencing a health decline due to disruption in medication treatment or lack of access the medical equipment necessary for independent living [29]. The examples from earthquake, tsunami, and heat wave disasters show that the majority of disaster fatalities do occur among individuals older than 60 years with those over age 74, being at the highest risk due to frailty, cognitive impairment, and limited mobility [129]. Even though chronic nuisance flooding is less likely to lead to immediate fatalities, it will still pose persistent complex challenges for older adults. For example, in our paper, we identified that many counties with the highest percent of older population also have an older housing stock that is more prone to damage and more expensive and difficult to retrofit. Further, we found that a number of counties affected by future sea level rise are located in rural areas where the older population is already predisposed to limited availability of health care and support services, isolation, and access to information. In two rural case study counties, we found that older residents tend to live not only closer to the waterfront, but often have a lower per capita income and older homes. In urban settings, many older adults also live along the coast and in older homes, but generally have higher income than the same population group in rural counties. Our analysis suggests that urban census tracts have a higher percent of older population, higher occupancy and renting rates, and higher population without health insurance and vehicle. On the contrary, coastal rural census tracts have lower educational attainment and higher disability rates. A more comprehensive understanding of these general predispositions among rural and urban coastal populations may help identify specific support needs that could increase the coping capacity, engagement in disaster preparedness, and overall resilience of this population group. For example, even though renting is often identified as an indicator of lower vulnerability, it may also support mobility and allow tenants to find alternative housing options in response to progressive flooding. In urban environments, the new resiliency-building programs should focus on accessibility and reliability of public transportation during hazard event and on policy changes that will allow older residents to adjust their circumstances in synergy with their evolving age-related health and well-being needs and with hazard propagation. In rural coastal settings, adaptive interventions should focus on risk communication and innovative financial mechanism such as microloans, tax deductions, or rebates that would support people\\'s self-sufficiency and capacity for self-organization in resilience-building efforts. In some cases, that may mean only providing technical assistance and non-monetary support such as labor, materials, or equipment. Considering that the extent of SLR and storm surge impacts will vary greatly between different locations, the decision-making should take a two-pronged approach to adaptation planning, one that addresses the acute but less predictable episodic events and another one that uses the long-term visioning and scenario planning to support adjustments in land use, delivery of services, and sitting of critical facilities. Both rural and urban counties have high rates of ambulatory and independent living disabilities, with many more nursing homes/living assisted facilities located in urban centers. Flooding in coastal communities is often caused by a complex set of circumstances, many of which involve an unsustainable legacy of land use and development, aging infrastructure, increased risk of precipitation and flooding from the sea and unique socioeconomic dimensions. Thus planning for adequate health and well-being support of older residents should be comprehensive and explore multi-hazard scenarios that also include cascading events and secondary impacts, such as loss of power and access to health facilities. Lastly, we found that while significant research has been conducted outlining increased short and long-term health risks following disasters, very little research has been conducted on recurrent nuisance flooding that had not resulted from a major weather event. It is unclear whether results from major disasters can be extrapolated to lower level of coastal flooding and whether preparedness strategies for major events will also lead to better outcomes following chronic flooding events. In addition, while much research has been devoted to characterizing and quantifying the health risks associated with natural disasters, effectiveness of intervention strategies are more difficult to study due to the episodic and unpredictable nature of extreme weather events. Future research in this area could take advantage of areas prone to coastal flooding to evaluate intervention strategies over shorter periods of time that may be applicable to larger flooding events associated with more rare extreme weather events.',\n       'To examine the relative merits of both the western and eastern dike sections in suppressing storm surge propagation into the bays, all the storms in Table 1 were simulated for a third coastal spine alignment that is shown in Figure 16. This alignment is quite similar to the alignment recommended by the GCCPRD in their final report. This alignment has neither the western nor eastern dike sections; it has only the middle section, which extends from the west end of Galveston Island to High Island. To quantify the additional storm surge that enters the Houston-Galveston region by not having an eastern section, the peak surge field for the alignment that is similar to the coastal spine included in the likely USACE TSP (comprised of middle+eastern sections), which was discussed previously and is shown in Figure  17, is subtracted from the peak surge field for the alignment shown in Figure 16 (middle section only). Results are shown in a series of figures below, Figures 18, 19 and 20. There is one figure for each of three storms, Hurricane Ike (Figure 18) and the two hypothetical storms which best replicated the 100-yr and 500-yr water levels within Galveston Bay, Storms 033 ( Figure 19) and 036 ( Figure 20) in Table 1, respectively. Of all the storms simulated, these three produced the highest peak surges in the vicinity of the eastern dike section. Therefore, of all the storms simulated, these three would be those in which having an eastern section would be most beneficial. All results shown are from simulations made using the future sea level scenario. For each storm, the upper panel in each figure shows the increase in peak surge that occurs without a western section of the coastal spine concept; and the bottom panel shows the increase in peak surge that occurs without an eastern section. Results for all three storms show that the western dike section provides much greater flood risk reduction benefits throughout both West and Galveston Bays than does the eastern dike section. The eastern section primarily provides peak surge reduction benefits in the vicinity of the eastern dike section; whereas, the western section provides peak surge reduction benefits throughout the Houston-Galveston region that lies behind the coastal spine, even communities located along the eastern shoreline of Galveston Bay. Results are quite similar to those for the simulations made for present sea level.   These observations are primarily due to the fact that high storm surge conditions develop at the location of the eastern dike section much later during the storm, just before landfall. Flanking flow around the eastern end of the coastal spine at High Island, in the absence of an eastern section, commences at this time, relatively late in the storm. And, when flanking of the eastern side occurs, the eastern side of the bay is severely set down by hurricane force winds that blow from east to west within Galveston Bay. Water that flows around the eastern end of the barrier and into the eastern portion of the Bay, flows into an area where water levels are already significantly depressed, creating minimal influence on peak surge levels at locations in the bays away from the eastern section. Whereas, in the absence of a western section, storm surge steadily propagates through the open \"back door\" beginning with the surge forerunner several days before landfall. Flow into the bays continues while the storm transits the continental shelf, and approaches and makes landfall, as storm surge builds at the entrance to San Luis Pass. Flanking flow around the western end of the barrier in the likely USACE TSP occurs for a much longer duration and it influences peak surge levels throughout both bays.',\n       'After linear registration to the higher-quality T 1 -weighted image, the rest of the images in the DTI image set, one at a time, are input along with T S−A1 into STAMP (as shown in Fig. 3 ) to 7 For the DTI images, T 2 maps, and maps STAMP will automatically choose the T 2 -weighted image in A1 space for alignment if it is available, otherwise it will use the T 1 -weighted image in A1 space. 8 Higher quality means better tissue contrast and higher resolution. 9 Since the skull signal intensity in a typical DTI B0 image is usually on the level of the background noise or removed during DTI processing (prior to STAMP), the proper reference image to be used here is the skull-stripped T 1 -weighted image i.e. I T 1 ,brainonly,A1 . generate outputs as described by Sec. 2.3.1. For example, if the DTI image is DTI FA (I DTI_FA,S ), the output images will include I DTI_FA,A1 (which is both in A1 space and aligned with the T 1 -weighted image), the labeled image I DTI_FA,labeled,S , and the image I DTI_FA,H upon HAMMER registration.',\n       \"As indicated previously, obtaining information from student records was a sequential three-stage process. The first stage, implemented for the first time in NPSAS:96, involved an electronic data interchange (EDI) with the ED CPS database of electronic SARs). The second stage involved collection of information from student records at the postsecondary institutions in the NPSAS sample using a CADE software system3; and the third stage involved EDIs with the ED Student Pell Summary records and ED's NSLDS database. Outcomes for these three activities are considered in separate subsections below. The CADE operation was implemented by either staff at the NPSAS institution or contractor field data collectors (FDCs). NPSAS:96 METHODOLOGY REPORT C 3-3 CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES 3.2.1 CPS SAR Data Table 3.2 summarizes results of matching and downloading data for SAR 96 and SAR 97, in total and by selected student classifications. Obtaining a match was determined by whether or not: (1) the student was listed on the CPS files (i.e., had applied for Federal financial aid during the 1995-96 academic year and entered on the file by the time the request was made) and (2) a valid CPS ID and name4 could be determined . While application for federal aid is one of the factors (and probably the principal one) affecting the match rate, differences shown in Table 3.2 should not be over-interpreted, since the percentages shown are unweighted. From Table 3.2, it should be noted that the SAR 96 matching attempt involved only 61,932 of the total; specifically, those for whom a CPS ID had been determined from information on the institution's enrollment lists. (A total of 23 institutions failed to provide sufficient information to construct a CPS ID; other institutions provided no information or inaccurate information for differing numbers of students. No matches were obtained for 16 additional institutions.) Matches were obtained, and some SAR data obtained, for 30,821 of those submitted (about 50 percent).6 SAR 96 matching rates were lowest among students at public institutions offering less than 4-year programs and among graduate students; they were greatest among students attending private for-profit institutions and first-professional students. These results are not particularly surprising. Federal aid applications at public community colleges and technical institutions are expected to be proportionately less than other sectors, and federal aid applications at private for-profit institutions proportionately greater. Moreover, firstprofessional students tend to rely more on federal aid (primarily loans) whereas graduate students generally rely more on institutional aid (teaching and research assistantships). application and the time that student lists were prepared, could create a non-match. s Recall that both CADE and CATI data collections were conducted only after the initial SAR 96 matching attempt; consequently the CPS IDs for the additional 1,148 students (obtained from either CADE or CATI data) were not available for the SAR 96 request. 6 For purposes of comparability, all percentages shown in. Table 3.2 are based on the full set of 63,080 students with apparently valid CPS IDs; consequently the SAR-96 rates are depressed from the values obtained using only the subset of 61,932 actually submitted. Original plans called for resubmitting these students for SAR 96 data following CATI; however, at that time CPS processing of the 1995-96 year had been discontinued. Only sampled students for whom an apparently legitimate social security number was available were submitted to CPS for matching. Of the 63,080 with valid CPS Ids, 3,643 were determined to be ineligible for NPSAS:96. cThese matching rates are somewhat depressed, since the 63,080 students include 1,148 students for whom CPS IDs were not determined until after CPS processing for the 1995-96 year had been discontinued; such students did not have the opportunity to match to SAR 96. d Student level is based on the student's last term of enrollment at the NPSAS institution during the NPSAS year.\",\n       \"Sex steroid hormones play fundamental roles in the development and function of CNS. Estrogens, progestins and androgens are able to induce several effects in brain areas of the CNS, by binding with specific receptors. The action of sex hormones is not limited to the regulation of endocrine functions and mating behaviour: the identification of estrogen, progestin and androgen receptors outside their classical CNS regions, such as the pituitary and hypothalamus, justifies their role in controlling different brain functions (Genazzani et al., 1996) .\\nThese findings highlight the importance of sex steroids in the development and the regulation of the CNS. Marked differences are found in the structure and function of the brain of male and female animals and humans (Kruijver et al., 2000) . Indeed, in humans and in rats, several areas of the brain show gender dimorphism, as indicated by differences in structure (such as different numbers of cells in specific areas). The different organization of brain areas in males and females appears to be largely dependent on the action of sex steroid hormones as demonstrated by the differential expression of steroid receptors in sexually dimorphic nuclei (Kruijver et al., 2001) . These 'organizational/developmental' effects are permanent and are acting during development, mainly in the fetal-neonatal period when estrogens and aromatizable androgens modulate neuronal development and the formation of neuronal circuits. As a result, several areas of the CNS become sexually differentiated. This is also suggested by the evidence that the levels of circulating and locally produced steroids control the structure and activity of sexually dimorphic nuclei. Therefore, exogenous sex steroids could cause differences in the structure/ function of specific brain areas with measurable clinical effects. Although abundant morphological and functional evidence exists for sex differences in brain development, much less is known regarding the underlying developmental mechanisms that direct these differences (Beyer, 1999) . Because neurons are limited in their proliferative life and there is considerable programmed cell death after birth, the effect of steroids appears to involve the regulation of neural cell kinetics and apoptosis. Sexual dimorphism is also present in other cellular compartments such as the glia, and the sex-steroid-related developmental differences seen in both animals and humans are real and complex (Cooke et al., 1998) : gender differences show up, among others, in dendritic structure, organization of the neuronal membrane, neuronal connectivity patterns, as well as in opiate receptor numbers (Pfaff, 1980) . Brain plasticity is most apparent during early development with the formation of the nervous system, but it continues through puberty, reproduction and adult life (Keefe et al., 1994; Cooke et al., 1998) . These 'activational/neuroplasticity' effects are transient and fluctuate considerably as the hormonal milieu changes, thus affecting almost potentially every aspect of brain physiology in different biological phases. Estrogen-induced synaptic plasticity is clearly seen during puberty and with seasonal changes as well as during the ovarian cycles. Estrogen appears to be important for the regulation and maintenance of network integrity of several brain areas related to cognition . In addition to changes in the cortex accompanying cognitive tasks, estrogen regulates the anatomy and connectivity of the hippocampus and associated structures (Polcz et al., 1998; Genazzani et al., 2003) . In post-menopause, neurotransmitters, neuropeptides and neurosteroids undergo important changes as a consequence of the failure of gonadal hormone production at a time when many CNS activities deteriorate, particularly those associated with hippocampal functions such as memory, attention, cognition and autonomic control (Genazzani et al., 2005) . This neuroendocrinolgical ageing process represents a unique opportunity to investigate the actions of gonadal hormones on their specific receptors in the nervous system.\",\n       'The specifics of pendulum slosh modeling for a sphere and an upright cylinder, which can then be combined in the hybrid approach just described, are defined in the SwRI reference on slosh modeling. They are briefly reviewed in this paper. Not included here are their derivations, though those also exist in the reference. 24 For a simple spherical tank, a plot of the pendulum length over tank radius, as a function of fill level for the first slosh mode is given in Ref. 24 . In the same reference is the participatory mass fraction as a function of fill level for non-sloshing mass as well as the sloshing mass. Once pendulum length and the longitudinal acceleration is defined for a given tank geometry and fill level, the natural slosh frequency (in rad/s) can be estimated with the simple generic pendulum equation:\\nwhere L is pendulum length and a X is the longitudinal acceleration. For the spherical tank, the center of pendulum rotation is assumed to be the center of the tank.',\n       'AGA: appropriate for gestational age CI: confidence interval ECLS-B: Early Childhood Longitudinal Study-Birth Cohort GWG: gestational weight gain IGF: insulin-like growth factor LMP: last menstrual period OR: odds ratio SGA: small for gestational age',\n       \"The hippocampus has been widely studied using neuroimaging, as it plays an important role in memory and learning. However, hippocampal subfield information is difficult to capture by standard magnetic resonance imaging (MRI) techniques. To facilitate morphometric study of hippocampal subfields, ADNI introduced a high resolution (0.4 mm in plane) T2-weighted turbo spin-echo sequence that requires 8 min. With acceleration, the protocol can be acquired in 4 min. We performed a comparative study of hippocampal subfield volumes using standard and accelerated protocols on a Siemens Prisma 3T MRI in an independent sample of older adults that included 10 cognitively normal controls, 9 individuals with subjective cognitive decline, 10 with mild cognitive impairment, and 6 with a clinical diagnosis of Alzheimer's disease (AD). The Automatic Segmentation of Hippocampal Subfields (ASHS) software was used to segment 9 primary labeled regions including hippocampal subfields and neighboring cortical regions. Intraclass correlation coefficients were computed for reliability tests between 4 and 8 min scans within and across the four groups. Pairwise group analyses were performed, covaried for age, sex and total intracranial volume, to determine whether the patterns of group differences were similar using 4 vs. 8 min scans. The 4 and 8 min protocols, analyzed by ASHS segmentation, yielded similar volumetric estimates for hippocampal subfields as well as comparable patterns of differences between study groups. The accelerated protocol can provide reliable imaging data for investigation of hippocampal subfields in AD-related MRI studies and the decreased scan time may result in less vulnerability to motion.\",\n       'In which of these positions does gravity act on the rocket? A. position 3 only B. positions 1 and 2 only C. positions 2 and 3 only D. positions 1, 2, and 3 Key: D Source: TIMSS 1999, Grade 8.',\n       'In science, assessment of developmental processes is generally based on indicators which present reality in terms of numerical relations (Hornbostel, 1999) . Indicators can achieve different levels of complexity ranging from simple figures to relative numbers and complex indices (Meyer, 2004) . Input figures such as material equipment or human resources are correlated with measurable outcomes, e.g. prizes, publications, doctoral degrees, stipends, informing on activity, structure and quality of a field of research (Hornbostel, 1999) . Several factors bear an impact on the validity of indicators: type and scope of available data, research approaches and characteristics of the matter under investigation. Depending on the approach taken, the number of funded projects might serve as an indicator for research achievements, such as success in competitively acquiring funding. From another perspective, external funding can be interpreted as simple input of financial resources. Moreover, assessment of an external funding indicator needs to consider in how far the acquisition of external funding is common to a research discipline: Block, Hornbostel and Neidhardt (1992) have demonstrated that external funding is far more wide-spread in natural sciences than in social sciences, hence, external funding has a different meaning in the disciplines, which should be reflected in a comparison of research domains. The relevance of indicators is furthermore affected by characteristics within the disciplines. Hornbostel (2001) characterises educational science as a discipline that is comprised of humanities, social-scientific and empirical traditions and a part specialised in delivering practical services. In each of these parts within the discipline, a particular indicator plays a different role and it bears a different meaning.\\nTo analyse the research projects we selected such indicators that cover the structure as well as the content of a research project. Existing data did not allow for construction of complex indicators. Against this background, we perceive indicators as metadata that according to their respective character describe different features of a field of research. \"Research activity\" (Forschungsaktivität) models the development of a field of research as a basic indicator. Taking into account that since the 1990s research funding is predominantly governed by external sources (Schubert & Schmoch, 2010) and, thus, the acquisition of research funding is increasingly gaining importance, the indicator for \"research funding\" (Förderart) reflects the development in educational research. Development regarding obtainment of degrees, subsumed in the indicator \"qualification\" (Qualifizierungsarbeiten) demonstrates the state of training for academic research which is highly relevant for the continuity of a discipline and plays a pivotal role in strategies for strengthening educational research (Hauss et al., 2012) . The indicator \"disciplinary area\" (Disziplinbereich) models the subject discipline a project is assigned to, it serves to ascertain what disciplines are active in educational research and reflects the diversity of access to the field. Beyond these indicators, for which an implementation in the web prototype is exemplified below, we examined other indicators such as cooperation, research methods and objectives, biographical aspects and target groups.',\n       \"We included 68 pediatric participants with ASD, 34 with OCD, and 61 healthy controls. Participants (aged 8-13 years) were recruited across four sites in Europe (Radboud University Medical Center and the Donders Institute for Brain, Cognition and Behavior, Nijmegen, The Netherlands; Brain Center Rudolf Magnus, University Medical Center Utrecht, Utrecht, The Netherlands; King's College London, London, UK; Central Institute of Mental Health (CIMH), Mannheim, Germany). The measures used here were part of a larger test battery including questionnaires, neuropsychological testing, and MR scanning, as described in Naaijen et al (under review) . Exclusion criteria for all participants were the presence of any contraindications for MRI, an IQo70, and the presence or history of neurological disorders. In addition, the ASD and OCD patients were not allowed to have comorbidity of the other disorder of interest. For the healthy comparison group, no DSM axis I disorders were allowed in any relatives up to two generations back. Ethical approval for the study was obtained for all sites separately. After description of the study, parents gave written informed consent, children aged 12 years or older gave written informed assent and children younger than 12 years gave oral informed assent.\",\n       \"To understand farmers' decisions about practices that affect land quality and productivity, it is necessary to take a longrun perspective. One such approach is to examine farmers' choices using a dynamic economic analysis. For simplicity, some previous simulation studies of degradation and productivity assumed that current practices continued into the future and generated a range of estimated erosion-induced productivity losses. Pierce et al. (1984) estimated productivity losses of 1.8-7.8 percent over 100 years, while Alt et al. (1989) estimated losses of 3.5 percent over 100 years. Improved models (e.g., Burt, 1981 andVan Kooten et al., 1990) allow for both farmer response to resource conditions and resourcequality change in response to management practices in a single-dimension (topsoil depth) framework. Hopkins et al. (2001) extend Burt and Van Kooten et al. in several ways. First, they allow farmers to consider economic incentives under all resource states, rather than just the steady state. They also analyze a two-dimensional definition of soil degradation rather than just a single dimension-thereby incorporating both irreversible soil erosion and reversible nutrient depletion. Finally, they determine how optimal levels of two practices-fertilizer application and residue management-vary with the two dimensions of soil degradation. The Hopkins et al. model chooses levels of fertilizer application (F) and residue management (R) to maximize the expected present value of net returns over time from corn production, recognizing that yields (Y), soil nutrient condition (N), and topsoil depth (D) are determined jointly. Yields are determined jointly in any period by the interaction of fertilizer, soil nutrient condition, and topsoil depth, based on specifications derived from earlier research (Johnson and Shepherd 1978;Schumacher et al., 1994). Soil nutrient stocks may be built up or drawn down relative to initial levels (at least for potassium and phosphorus), depending on removal in harvested crops, fertilizer application, and changes in topsoil depth. Topsoil depth in any given period depends on soil depth in the previous period and on the level of residue management (in conjunction with the inherent erosion potential of the soil based on physical soil properties, landscape position, and climate condition). Costs of residue management are assumed to increase exponentially in\",\n       'Using all available time-points per participant, we investigated atrophy rates and rates of clinical decline using a linear mixed effects model [50] . We estimated the sample size required to detect 25% slowing in mean rate of decline for a hypothetical disease-modifying treatment versus placebo for a 24 month, two-arm, equal allocation trial, with a 6-months assessment interval, with the requirement that the trial have 80% power to detect the treatment effect using a 2-sided significance level of 0.05. The power calculations, modeling linear change over time for each participant, were based on the mean rate of decline for the patient cohorts relative to the rate of decline experienced by the control group of diagnostically stable Ab -HCs [50] . This represents maximal estimates for the disease (or treatable) effect, since therapies aimed at AD are unlikely to affect rate of change experienced by healthy older individuals. We assessed estimated sample sizes per risk group using rates of change in CDR-SB and in various brain measures as outcome variables.\\n',\n       'As discussed in section 3.1, this study focuses on K cohort children because test scores are more widely available for them. Furthermore, among students who took any test in any test grade, the focus is on about 96% of those who completed all five test subjects. Moreover, the sample is restricted to students without missing information on a list of important explanatory variables. To keep the results comparable over time, specifications that use variables which are available in all waves of the LSAC and contain the least missing information (see Table 1 and Section 4 for a list of variables included in our baseline models) are used. These variables are commonly used in studies which employ a popular and comparable US data set from the Early Childhood Longitudinal Study Kindergarten cohort (Fryer Jr and Levitt, 2004; Fryer 5 Unreported results for writing, spelling and grammar are largely similar to the results of reading reported in this paper. The results for other non-numeracy test subjects are available upon request. 6 The differences in test dates and survey dates in the empirical models are addressed by including dummies for survey months and test and survey years (see Section 4). See Appendix Table A1 for variable description and  summary statistics. and Levitt, 2010; Sohn, 2012; Bertrand and Pan, 2013) to study a gender test score gap of school students. 7\\nThe original sample sizes for the K cohort in Waves 2, 3, and 4 are 4464, 4331 and 4169, respectively. The above restrictions result in final samples of 2471, 3225, and 2801 students in Waves 2, 3, and 4, respectively. Appendix Table A2 suggests that sample attritions are mainly attributed to the fact that students\\' NAPLAN test scores are not linked to the LSAC data. Reasons for original sample attrition are discussed in (Norton and Monahan (2015) and seasons for not having NAPLAN test scores linked to the LSAC data are discussed in detail in a technical report by Daraganova et al. (2013) . Note that there is a slightly smaller number of students in Wave 2 in this sample because the grade 3 NAPLAN tests were first introduced in 2008 when some K cohort students might have attended higher grades, and as such did not take the tests. Additionally, Table A2 reveals that, conditional on having NAPLAN test scores linked to the LSAC data, sample attritions are mostly due to missing information on pre-school cognitive skills (i.e. PPVT and WAI) and household income. We dropped individuals with missing information on control variables rather than using the \"dummy variable adjustment\" method because deletion has been found to produce less biased estimates (Allison, 2001) .\\nWe investigate whether our sample selection criteria led to sample selection issues. One particular concern relating to our research design is that the child\\'s gender may affect the probability that an individual child is included in the final sample. Therefore, we ran a probit model where the dependent variable is equal to one if the child is in our sample and zero otherwise. The explanatory variables are basic demographic characteristics, including the child\\'s gender. Regression results (reported in Appendix Table A3 ) suggest some evidence of statistically significant selection on some observables. For instance, children in our sample are more likely to come from more advantageous households with non-Aboriginal or native backgrounds or come from two-parent households or live in owned homes. However, the pseudo-R 2 values are relatively small, indicating that selection on observable characteristics is quantitatively weak. More importantly, in 2 out of 3 regressions by test grades, -values from a test for statistical significance of the gender dummy included in the regression are greater than 0.05, alleviating concern that our results may be driven by sample selection. 7 To examine the impact of other important variables and check the robustness of the results, a richer list of variables is included in extended specifications, where possible. The data contain father information including age, education, work status, and ethnicity. However, due to a large number of missing data (13% of the final sample has missing data), father information is not used in our baseline specifications like US studies (Fryer Jr and Levitt, 2004; Fryer and Levitt, 2010; Bertrand and Pan, 2013) .',\n       'The mistake detection video classroom design originated from my experiences as a learner who had trouble with math mistakes. My greatest challenge in learning math was knowing what to do when I got stuck on a problem. As a doctoral student learning statistics, I knew that I was making mistakes, but felt powerless to identify them. In response to Copyright © 2017 by the International Journal of Designs for Learning, a publication of the Association of Educational Communications and Technology. (AECT). Permission to make digital or hard copies of portions of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page in print or the first screen in digital media. Copyrights for components of this work owned by others than IJDL or AECT must be honored. Abstracting with credit is permitted.\\nhttps://doi.org/10. 14434/ijdl.v8i2.23073 2017 | Volume 8, Issue 2 | Pages 124-136 my frustration, I sought to adopt a new approach by mounting a digital camera on a tripod over my work and recording my problem-solving processes (see Figure 1 ). I watched the videos and reflected on the paths I took to solve the math problem. I then attempted the problem again; applying any new insights to incrementally improve my performance (e.g., try a different strategy or retrace steps).\\nIn retrospect, the idea to record myself emerged from my teaching experiences. I have taught public speaking for over ten years and my instruction frequently involved video recording students delivering speeches. In a typical assignment, students reviewed these videos and wrote reflection papers describing ways to improve their public speaking. By applying this video recording method to mathematics, I could see where I was making mistakes in my work, enabling me to better articulate to a tutor or teacher when I sought help. Thus, reflection and help-seeking were a key part of the process (see Figure 2 for a description of the personal recording process). My math performance did not improve overnight, but through the video recordings I was able to decrease the number of mistakes I made on subsequent attempts. I became comfortable openly discussing mistakes with others; signifying a positive change in my beliefs and behaviors related to mistakes. Following these discussions and adjustments, I experienced an improvement in my performance and mathematical understanding.\\nThe video recording design emerged from a personal need and was not originally intended to become a research project or instructional design. Rather, it was an attempt to identify my mistakes, build confidence, and improve my performance. However, as a graduate student working towards a degree in education, my own success with these mistake recovery recordings inspired me to adapt these videos into a design that would suit the context of math learners in traditional K-12 classroom settings. As could be expected with creating a design while doing academic work, a number of research studies influenced our design decisions.',\n       \"Data in macroeconomics generally possess strong deterministic trend especially when there is a sufficiently long time series. The variables in such cases are generally nonstationary (that is they do not have constant mean and variance over time). In time series, when variables are non-stationary, conventional estimation techniques, such as ordinary least squares, are expected to be driven by spurious correlation (Phillips 1986 ). Engle and Granger (1987) show that linear combination of two or more I(1) (nonstationary) variables could be I(0) (stationary) in which case the series are said to be cointegrated. In other words, non-stationary variables are said to be cointegrated if the residuals from their relationship are stationary. By using cointegration, one can use full information embodied in the variables and also use the attractive properties of cointegration techniques such as super consistency when n goes to infinity (Stock 1987) . Estimates generated by ordinary least squares, however, do not follow asymptotic Gaussian distribution, therefore standard testing procedures are invalid unless they are significantly modified. Fully Modified OLS (FMOLS) and Dynamic OLS (DOLS) are generally considered as an alternative to simple OLS in presence of cointegration. Since our data contains relatively large macroeconomic time series dimension of 16 years, we test our variables for unit root, the presence of which motivates the test for cointegration.\\nIn time series, Engle and Granger (1987) cointegration test is used on I(1) variables to test for cointegration. If the residuals from the regression are I(0) then the variables are said to be cointegrated. On similar principle, Pedroni (1999) , Pedroni (2004) and Kao (1999) propose cointegration tests for panel data. Pedroni test consists or several tests under different assumptions on constants and trends across cross-sections. Consider following regression:\\nThe variables x and y are assumed to be I(1). The individual constant and trends are represented by α and δ, respectively. Null hypothesis of the test is 'no cointegration'. In case of no cointegration, residuals are integrated of order 1. If is I(0) then the variables are said to be cointegrated. Formally, null hypothesis of no cointegration implies ρ = 1 in equation 11\\nPedroni proposes two sets of hypotheses for between and within dimension. Under the test for between dimension, the test allows for different cointegrating relationships across cross-sections while under the test for within dimension the cointegrating relationship is assumed to be homogenous across cross sections. Eleven statistics are calculated for Pedroni test under the assumptions described above. For decision rule, however, there is no concrete guideline for how many tests out of eleven should show cointegrating relationship. In this study, we reject the null of no cointegration if six out of eleven 30 statistics of Pedroni reject the null of cointegration. Kao (1999) uses the similar approach as of Pedroni but allows for cross section specific constants and homogenous coefficients in the first stage regressions. Null hypothesis, similar to Pedroni test, is no cointegration. For robustness of the results, we have used both Kao and Pedroni tests for cointegration. Model (2) Model (3) Model (4) Model ( Pedroni Cointegration Test 4 out of 11 5 out of 11 6 out of 11 6 out of 11 5 out of 11 6 out of 11 6 out of 11 \",\n       'In this paper, we generalize the separable spatiotemporal Gaussian process (STGP) to model the temporal evolution of spatial dependence (TESD) of the data. Instead of treating the space variable x and the time variable t as an extended variable z = (x, t) in the traditional non-separable STGP (Marco et al., 2015; Datta et al., 2016; Hyun et al., 2016) , we generalize STGP by varying the eigenvalues of the spatial kernel in the Mercer\\'s representation. Two Bayesian nonparametric models are introduced and compared based on the generalized STGP with the joint kernels structured as the Kronecker product and the Kronecker sum respectively . The Kronecker sum structure is proved to be superior to the Kronecker product in characterizing TESD, from both modeling and computing perspectives. The advantage of the Kronecker sum kernel is demonstrated by a simulation study of spatiotemporal process. The generalized STGP model based on this structure is then applied to analyze PET brain images of Alzheimer\\'s patients recorded for up to 3 years to describe and predict the change of the brain structure in these patients and uncover TESD in their brain regions in the past and for the future. The numerical evidence verifies the effectiveness and efficiency of the proposed model in characterizing TESD.\\nThere is room to improve the current method. For example, it requires to have full data on the grid of the space and time and can be relaxed to handle missing data. The model can also be generalized to include covariates to explain the response variable (process) (Hyun et al., 2016) . Therefore, the estimation and prediction, e.g. of the brain images, can be done for each individual. We can further incorporate such information in the covariance and investigate the effect of covariates on TESD. They will be our future work.\\nFor the longitudinal analysis of AD patients\\' brain images, subjects who are followed up for the whole study are limited in number. There are more dropped in the middle or missing scheduled scans from the ADNI (ADN) study thus discarded in the paper. Therefore, there could be large variance in the estimation e.g. of TESD, due to the insufficient data. As ADNI continues collecting more data, we hope they can facilitate more accurate description of TESD of AD brain images to aid the exploration of the mechanism behind this disease. Another important topic is the diagnosis of AD. It would be interesting to investigate TESD of the subjects\\' brains before and after being diagnosed as AD, which could shed more light on the reason of such disease. Figure 11 : Predicted correlation between the brain region of interest and a selected point of interest (red cross) for CN (left), MCI (middle) and AD (right) respectively. Sarkka, S. and Hartikainen, J. (2012) . Infinite-dimensional kalman filtering approach to spatio-temporal gaussian process regression. In Lawrence, N. D. and Girolami, M., Sarkka, S., Solin, A., and Hartikainen, J. (2013) . Spatiotemporal learning via infinitedimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering. IEEE Signal Processing Magazine, 30(4):51-61.\\nSenanayake, R., O\\'Callaghan, S. T., and Ramos, F. T. (2016) . Predicting spatio-temporal propagation of seasonal influenza using variational gaussian process regression. In AAAI.\\nShen, X., Papademetris, X., and Constable, R. (2010) . Graph-theory based parcellation of functional subunits in the brain from resting-state fmri data. NeuroImage, 50 (3):1027-1035.\\nSingh, A., Ramos, F., Whyte, H. D., and Kaiser, W. J. (2010) . Modeling and decision making in spatio-temporal processes for environmental surveillance. In 2010 IEEE International Conference on Robotics and Automation, pages 5490-5497.\\nSmola, A. J. and Kondor, R. (2003) . Kernels and regularization on graphs. In Schölkopf, B. and Warmuth, M. K., (19) and (20) are well defined positive definite kernel on Z = X × T .\\nProof of Proposition 2.1. We first prove both series (19) and (20) converge in L 1 (Z × Z).\\nNote for (19) we have\\nThe convergence of series (19) and (20) follows by the dominated convergence theorem. Now we prove the positive-definiteness. ∀f (z) ∈ L 2 (Z), denote f (t) := X f (z)φ (x)dx. Then we have\\nwhere the convergence can be shown as above.\\nSimilarly we have\\nTherefore we complete the proof.\\nFor the dynamic spatial kernels\\nFor λ(t) ∈ 2,s (L ∞ (T )), we can bound the Hellinger distance, Kullback-Leibler distance and variance distance V (p 0 , p 1 ) = E 0 (log(p 0 /p 1 )) 2 in the following lemma with · k,s ,∞ for s > s. Notation ( ) means \"smaller (greater) than or equal to a universal constant times\".\\nLemma A.1. Let p i ∼ N n (0, C i (t)) be Gaussian models for i = 0, 1, with {λ 2 i, (t)} being the eigenvalues of C i . Assume for i = 0, 1\\nThen we have\\nFirst we calculate the Kullback-Leibler divergence\\nConsider m i ≡ 0. By the non-negativity of K-L divergence we have for general C i > 0,\\nTherefore we can bound K-L divergence\\nwhere we use\\n3) Now we calculate the following variance distance\\nConsider m i ≡ 0 and we can bound it by the similar argument as (A.3)\\nIt is easy to see that the centered variance distance V 0 = Var 0 (log(p 0 /p 1 )) 2 can be bounded\\nLastly, the squared Hellinger distance for multivariate Gaussians can be calculated\\nConsider m i ≡ 0. Notice that 1 − x ≤ − log x, and by (A.2) we can bound the squared Hellinger distance using the similar argument in (A.3) zero-mean tight Gaussian random element in 2,s (L ∞ (T )) and P (n) λ = ⊗ n j=1 P λ,j be the product measure of Y (n) parametrized by λ. If the true value λ 0 ∈ Θ is in the support of λ, and ε n satisfies the rate equation ϕ λ 0 (ε n ) ≤ nε 2 n with ε n ≥ n − 1 2 , then there exists Θ n ⊂ Θ such that Π n (λ ∈ Θ n : d n,H (λ, λ n,0 ) > M n ε n |Y 1 , · · · , Y n )) → 0 in P (n) λ n,0 -probability for every M n → ∞.\\nProof of Theorem 2.1. We use Theorem 1 of Ghosal and van der Vaart (2007) and it suffices to verify two conditions (the entropy condition 2.4, and the prior mass condition 2.5) as follows:\\nsup ε>εn log N (ε/36, {λ ∈ Θ n : d n,H (λ, λ n,0 ) < ε}, d n,H ) ≤ nε 2 n (A.4) Π n (λ ∈ Θ n : κε n < d n,H (λ, λ n,0 ) < 2κε n ) Π n (B n (λ n,0 , ε n )) ≤ e nε 2 n κ 2 /4 , for large κ (A.5)\\nwhere B n (λ n,0 , ε) := {λ ∈ Θ : 1 n n j=1 K j (λ n,0 , λ) ≤ ε 2 , 1 n n j=1 V j (λ n,0 , λ) ≤ ε 2 }, with Θ = 2,s (L ∞ (T )), K j (λ n,0 , λ) = K(P λ n,0 ,j , P λ,j ) and V j (λ n,0 , λ) = V (P λ n,0 ,j , P λ,j ). For each 1 ≤ ≤ N , define the coordinate rate function ϕ λ 0 , (ε n, ) = inf h∈H : h−λ 0, ≤ε n, Now let ε n, = 2 − −s ε 2 n for = 1, · · · , n. Set Θ n = {λ : λ ∈ B n, }, and N (ε n , Θ n , d n,H ) = max 1≤ ≤n N (3ε n, , B n, , · ∞ ). By Lemma A.1 and (A.7) , we have the following global entropy bound because d 2 n,H (λ, λ ) ≤ λ − λ 1,s ,∞ ≤ ε 2 n for ∀λ, λ ∈ Θ n .\\nlog N (ε n , Θ n , d n,H ) ≤ 6Cn(2 − −s ε 2 n ) 2 ≤ Cnε 4 n ≤ nε 2 which is stronger than the local entropy condition (A.4). Now by Lemma A.1 and (A.9) we have Π n (B n (λ n,0 , ε n )) ≥ Π n ( λ n,0 − λ 1,s ,∞ ≤ ε 2 n , λ n,0 − λ 2 1,s ,∞ ≤ ε 2 n ) = Π n ( λ n,0 − λ 1,s ,∞ ≤ ε 2 n ) ≥ exp n =1 log Π ( λ − λ 0, ∞ < 2ε n, )\\n≥ e − n 4 n =1 ε 2 n, = e −nκ 2 ε 4 n /4 , κ 2 = n =1 2 −2 −2s Then (A.5) is immediately satisfied because the numerator is bounded by 1. Therefore the proof is completed.\\nRemark 3. This theorem generalizes Theorem 2.2 of Lan et al. (2017) where the spatial domain has fixed dimension D. Therefore the Hellinger metric, KL divergence and variance are easier to bound (Lemma B.1). Note we do not have the complementary assertion as in Lemma 1 of Ghosal and van der Vaart (2007) thus the resulting contraction is only on Θ n , weaker than that in Theorem 2.2 of Lan et al. (2017) .\\nonly requires log-posterior density and works well for scalar parameters, Figure 12 : Estimated variance of the brain images for CN (top row), MCI (middle row) and AD (bottom row) respectively.',\n       \"All co-authors have seen and approved this submission. There are no conflicts of interest including any financial, personal or other relationships with other people or organizations, by any of the co-authors, related to the work described in the paper. The submission is not under review by any other archival journal. An oral and poster presentation of this work has been made at the International Conference of Alzheimer's disease, in Madrid, July 2006.\",\n       'In 1989, for the first time, per capita consumption of all farm foods except fluid milk and cream were reported on a U.S.-total-population (including Armed Forces overseas) basis. Earlier estimates had reported animal product consumption on a civilian-population basis. Fluid milk and cream estimates use the U.S. resident population. This bulletin no longer adjusts for military consumption in the supply and utilization balance sheets since data on military food use do not reflect all military food purchases or consumption. The data include purchases by the Defense Departments central purchasing office for troop feeding, but exclude local purchases for troop feeding and purchases through commissaries, clubs, exchanges, and civilian distribution channels for personal or household use. The incompleteness of the data tended to distort both military and civilian per capita consumption estimates. For most years, changing the statistical series to represent the total population results in very small changes in per capita consumption. The main exception is the war years of the 1940s, frequently deleted from studies of consumption because of abnormalities created by the war. Adding the table to these publications facilitates the comparison of the quantity and value of imports with domestic production and consumption.',\n       'Because of the limitations on memory storage, a single GFDL coupled model run requires a parallel computation environment [e.g., a minimum number of processing elements (PEs) is 20 on the Silicon Graphics Inc. Intel-Altix cluster] and the ensemble filter demands a so-called superparallelization technique to guarantee that model ensemble integrations and the filtering computation are conducted iteratively online. First, a large number of PEs (where K is the total PE number) are loaded and regrouped to form a global PE list and M sub-PE lists, each of which has K/M PEs (where M is the ensemble size). The analysis domain decomposition is done on the global PE list in which K analysis domains [each containing a core domain and a halo; Zhang et al. (2005) ] are formed. Within each sub-PE list, the model domain decomposition is first conducted and a certain ensemble member model integration is then advanced in parallel, in which each PE works for a subdomain. In this process, these M sub-PE lists work independently and the whole ensemble of model integrations is forwarded synchronously. Then, when the model ensemble reaches an observational time, a data transfer process from the model domains (sub-PE lists) to the analysis domain (global PE list) is activated so that an ensemble vector is formed in each analysis domain where a specific PE updates the ensemble vector by assimilating observations independently. Once the analysis process is done, data in the ensemble vectors over core domains are transferred back to the model domains for each ensemble member on a certain sub-PE list for initializing the next cycle of ensemble model integrations. A flow chart illustrating the iterative procedure specifically for a six-member ensemble is shown in Fig. 3 in which each member uses 30 PEs to carry out the model integration (left panels) while the daily filtering analysis uses 180 PEs (right panels).',\n       'The likelihood of being poor also increased with rurality for both men and women. Within each level of rurality, however, a greater percentage of women than men were poor. For example, 16 .8% of women in the most rural non-MSA counties had poverty-level incomes (income-to-needs ratio < 100%) compared with 12.4% of women in MSA counties. The corresponding percentages for men were 12.3% in the most rural non-MSA counties and 8.3% in MSA counties. The proportion of men and women with less than 12 years of schooling increased significantly with levels of rurality (P < 0.01). Across all levels of rurality, there was no significant difference in employment rates among women and men.\\nConsistent with other studies discussed above, most rural non-MSA residentswhether men or women-were more likely to be uninsured than their counterparts in metropolitan areas. An interesting finding was that a substantially greater fraction of men in the 2 types of non-MSA counties had coverage through public insurance than their counterparts in MSA counties.\\nA key result was that for both men and women, reported mental health deteriorated slightly but significantly as rurality increased (P < 0.01 for men and P < 0.05 for women). For instance, the percentage reporting \"excellent\" mental health declined from 42.9% for women in MSA counties to 39.2% in the least rural non-MSA counties and to 38.2% in the most rural non-MSA (7-9) counties. For men, there was also a comparable decline in reported mental health across levels of rurality. Similarly, men and women in the most rural non-MSA counties reported poorer general health than did their counterparts in more urban counties. Across gender, men generally reported better mental and general health than did women at each level of rurality.\\nEstimates, by level of rurality and within gender of (1) the percentage receiving any type of mental health treatment in a calendar year, (2) the percentage receiving specialized mental health treatment in a calendar year, and (3) the mean number of visits (of any mental health treatment type in a calendar year), appear in Table 3 . Our results show that women residing in the most rural non-MSA counties received significantly less of any treatment and of specialty treatment than did women living in urban areas. Treatment rates of women in the least rural non-MSAs were similar to those of women in MSA counties; both were higher than the rate for women in the most rural counties. Just 7.5% of women in the most rural non-MSA counties received any type of treatment compared with 10.1% of those in MSA counties and 11.1% of those in non-MSA, less rural counties. Likewise, just 4.4% of the most rural women received specialized treatment compared with 6.6% and 5.9% of women, respectively, in MSA and non-MSA, less rural counties. It is interesting to note that among women who did obtain mental health treatment, women in the least rural non-MSA counties had significantly fewer visits (mean = 5.12) than their counterparts in either MSA counties (mean = 8.54) or the most rural non-MSA counties (mean = 7.35).\\nA second finding, consistent with previous research, is that in each type of county men received significantly less treatment than did women, but the difference between men and Number of visits in year NS NS NS * Standard errors are in parentheses. The last column reports P-values for difference across level of rurality for each measure separately for men and women. The last 3 rows report gender differences for each measure by level of rurality. MSA indicates metropolitan statistical area.\\nwomen was smaller in the most rural non-MSA counties. For any treatment, across all levels of rurality, there were no differences in rates of treatment. The pattern was different, however, for specialty mental health treatment; men in both nonmetropolitan areas received less specialty care than did men living in MSAs. Likewise, among men who obtained some type of treatment, the mean number of visits in a calendar year was lower in non-MSA counties than in MSA counties. Self-reported mental health was significantly related to receipt of treatment for both men and women. Figure 1 focuses on differences in rates of any type of treatment by level of reported mental health. There was a sharp increase in treatment rates as mental health deteriorated from \"excellent\"to \"poor\" for both men and women. For men with mental health ranging from \"excellent\" to \"good,\" there were only small differences in treatment rates by level of rurality. However, among the group that reported needing treatment the most-those with \"fair\" and \"poor\" mental health-far fewer men in the rural non-MSA counties obtained treatment than did their counterparts in MSAs or least rural non-MSA counties.\\nThis figure also illustrates the sharp difference in treatment rates between women in the most rural non-MSA counties and other types of counties, with the obvious exception of those with \"poor\" mental health. This finding may be due to the very small sample of women in this category; the estimate for women with poor mental health in the most rural counties was based on just 32 personyear observations. Among women who reported \"fair\" mental health, for example, only 25.6% of women in the most rural non-MSA counties obtained mental health treatment compared with rates greater than 40% in both MSA and least rural non-MSA counties.\\nMultivariate regression results estimated separately for women and men are shown in Table 4 ; the full set of estimates is presented in Appendix A. This table also presents predicted values for women and men across each level of rurality. These values were obtained Table 1 ). The sample used in the \"number of visits\" regression is restricted to respondents with one or more visits. Predicted values were computed with all covariates set at their mean for the full sample of women in Panel A and for men in Panel B. Absolute value of t ratio in parentheses. ‡ Significant at 5%. † Significant at 1%.\\nby substituting the means of the covariates for the entire sample of either men or women into the corresponding regression model. As a whole, multivariate regression results are consistent with those reported above. First, we found substantial differences in treatment rates across levels of rurality that persisted even when reported mental health and sociodemographic variables were controlled in the analysis. All else equal, the odds of any treatment for women in either MSA or the least rural non-MSA counties were more than 1.5 times the odds of treatment for women in the most rural non-MSA counties. We found that women in the most rural non-MSA counties were significantly less likely to obtain any type of mental health treatment and especially specialized treatment than were their counterparts in metropolitan counties or less rural non-MSA counties. The predicted treatment rates of 10.18% in MSA and 10.39% in least rural non-MSA counties were significantly higher than the predicted rate of 7.24% for women in the most rural non-MSA counties. Mental health specialty treatment rates for women were also significantly higher in MSA counties (6.56%) than the rates of their counterparts in the most rural counties (4.63%); among women receiving treatment, the predicted number of visits of any type of treatment was slightly lower (β = −2.274) in least rural non-MSA counties. The results in Table 4 for men also show a statistically significant difference in specialty treatment (but not any type of treatment or number of visits) across level of rurality. In particular, the predicted rate of mental health specialty treatment in the most rural non-MSA counties was nearly half the rate in MSA counties, 2.34% compared to 4.13%.',\n       \"Researchers may not always know at the outset of an initial study if they will be able to conduct subsequent rounds of data collection, for a variety of reasons including tenuous funding (Nicholson et al., 2002) . Likewise, the research literature may prompt new questions that were not under consideration as part of the original research, but which could be addressed by re-recruiting original participants. Indeed, this was the case for us. Even in these situations, there are several steps researchers can take to facilitate potential future rounds of data collection. First, researchers may partner with local organizations, including schools (Language Reading Research Consortium et al., 2016) . These relationships can assist with both initial recruitment (Striano, 2016) and re-recruitment, depending on the initial language used in establishing said partnership and the nature of the study itself (see the discussion on locating participants below for more information).\\nSecond, researchers can create a database with detailed participant contact information, including information such as participants' and their family members' full names and aliases (e.g., nicknames, maiden names), phone numbers, emails, mailing addresses, educational and employment information (if relevant), contact information for friends, neighbors and/or relatives, birthdates, plans to move or change names, physical descriptions, and favorite hangouts (Ribisl et al., 1996; Cotter et al., 2002; Haggerty et al., 2008; Barakat-Haddad et al., 2009; Masson et al., 2013) . Researchers have the ethical responsibility to gain informed consent for the foreseeable uses of such information, as well as to ensure participants that providing such information is voluntary (National Association of the Education of Young Children, 2011). Researchers should also take reasonable efforts to store participants' personal information securely (e.g., password protected or encrypted files, stored on a non-wide area network (WAN) connected computer server). In addition, principal investigators should also limit access to sensitive information so as not to violate participant anonymity (Hartmann, 1992; American Educational Research Association, 2011) . During our original data collection, we collected names, phone numbers, email addresses, and employment information for up to two parents; full names, birthdates, and preschool/kindergarten names for children; and mailing addresses for the entire family. This helped ensure high participation over the course of the original intervention, and, as described in more detail below, was invaluable to subsequent re-recruitment efforts.\\nThird, researchers can adopt a blanket policy of including an optional element on all consent forms seeking permission for future contact about later research opportunities (e.g., Masson et al., 2013) . This easy addition to consent forms provides an opportunity for future outreach should the need arise. In the absence of this initial consent, unexpected followup contact may be perceived as a privacy violation, and prevent former participants from choosing to dedicate more time to a research project. Further, IRBs may have an ethical obligation to prevent non-consensual follow-up contact. Indeed, this seems to be a common standard across universities in the U.S. (e.g., J. Hecht, personal communication, July 1, 2016). It is important researchers take these initial safeguards, because children are special vulnerable populations (Hartmann, 1992) .\\nFourth, researchers should attempt to build rapport with both parents and children across research activities (Cotter et al., 2002) and establish clear branding via the use of university or other logos (Ribisl et al., 1996; Haggerty et al., 2008; Striano, 2016) . Depending on the level of anonymity of the study, researchers could even establish a Facebook or similar social media group for participants. Having a blog or newsletter to update families about study findings also can be a way to maintain connections with families who have participated in prior studies. Further, providing information and resources that parents will find useful can help to keep them engaged (Ganz, 2016) . In prospective studies, researchers sometimes send participants regular newsletters and birthday/holiday greetings (Ribisl et al., 1996; Language Reading Research Consortium et al., 2016) . Such techniques can help ensure the research experience is positive for participants and help them identify with the research study (and perhaps sign-up for other cross-sectional studies with researchers even if a team does not attempt to follow up with a specific study).\\nFinally, researchers with sufficient foresight in certain sub-domains may wish to gain consent from parents to contact their family members, friends, neighbors or other professionals in their lives (e.g., clergy; case workers) for help locating them (i.e., participating parents and children) at a later point, and to have parents prepare notes for these individuals consenting for them to provide current family contact information (Ribisl et al., 1996; Passetti et al., 2000; Cotter et al., 2002) . These permissions could later be leveraged to assist with locating study participants for later waves of data collection. As a caveat, this approach may not be appropriate for all topic areas. For example, seeking this information might be reasonable as part of a lengthy intervention but could be intrusive in a one-time lab session.\",\n       'The following words are bolded and marked by an asterisk (*) wherever they appear in the questionnaire. Please use these definitions as you respond. Active shooter -an individual actively engaged in killing or attempting to kill people in a confined and populated area; in most cases, active shooters use firearm(s) and there is no pattern or method to their selection of victims. At school/at your school -activities happening in school buildings, on school grounds, on school buses, and at places that hold school-sponsored events or activities. Unless otherwise specified, this refers to normal school hours or to times when school activities/events were in session. Bullying -any unwanted aggressive behavior(s) by another youth or group of youths who are not siblings or current dating partners that involves an observed or perceived power imbalance and is repeated multiple times or is highly likely to be repeated. Cyberbullying -occurs when willful and repeated harm is inflicted through the use of computers, cell phones, or other electronic devices. Diagnostic assessment -an evaluation conducted by a medical or mental health professional that identifies whether an individual has one or more medical and/or mental health diagnoses. This is in contrast to an educational assessment, which does not focus on clarifying a student\\'s diagnosis. Evacuation -a procedure that requires all students and staff to leave the building. While evacuating to the school\\'s field makes sense for a fire drill that only lasts a few minutes, it may not be an appropriate location for a longer period of time. The evacuation plan should encompass relocation procedures and include backup buildings to serve as emergency shelters, such as nearby community centers, religious institutions, businesses, or other schools. Evacuation also includes \"reverse evacuation,\" a procedure for schools to return students to the building quickly if an incident occurs while students are outside. Firearm/explosive device -any weapon that is designed to (or may readily be converted to) expel a projectile by the action of an explosive. This includes guns, bombs, grenades, mines, rockets, missiles, pipe bombs, or similar devices designed to explode and capable of causing bodily harm or property damage. Gender identity -means one\\'s inner sense of one\\'s own gender, which may or may not match the sex assigned at birth. Different people choose to express their gender identity differently. For some, gender may be expressed through, for example, dress, grooming, mannerisms, speech patterns, and social interactions. Gender expression usually ranges between masculine and feminine, and some transgender people express their gender consistent with how they identify internally, rather than in accordance with the sex they were assigned at birth. Lockdown -a procedure that involves occupants of a school building being directed to remain confined to a room or area within a building with specific procedures to follow. A lockdown may be used when a crisis occurs outside of the school and an evacuation would be dangerous. A lockdown may also be called for when there is a crisis inside and movement within the school will put students in jeopardy. All exterior doors are locked and students and staff stay in their classrooms. Mental health disorders -collectively, all diagnosable mental disorders or health conditions that are characterized by alterations in thinking, mood, or behavior (or some combination thereof) associated with distress and/or impaired functioning. Mental health professionals -mental health services are provided by several different professions, each of which has its own training and areas of expertise. The types of professionals who may provide mental health services include psychiatrists, psychologists, psychiatric/mental health nurse practitioners, psychiatric/mental health nurses, clinical social workers, and professional counselors. Physical attack or fight -an actual and intentional touching or striking of another person against his or her will, or the intentional causing of bodily harm to an individual. Gang -an ongoing loosely organized association of three or more persons, whether formal or informal, that has a common name, signs, symbols, or colors, whose members engage, either individually or collectively, in violent or other forms of illegal behavior. Hate crime -A committed criminal offense that is motivated, in whole or in part, by the offender\\'s bias(es) against a race, religion, disability, sexual orientation, ethnicity, gender, or gender identity; also known as bias crime. Rape -forced sexual intercourse (vaginal, anal, or oral penetration). This includes sodomy and penetration with a foreign object. Both male and female students can be victims of rape. [Counts of attempted rape should be added to counts of rapes in your reporting of item 26a.] ▲ ▼ ▼ ▲ §,#&¤ 110205',\n       'The results of our survey of male and female economists show that the area of largest disagreement between men and women lies in views on equal opportunity. These differences reveal themselves not only in views of gender equality in the economics profession but in society in general. Large disparities in average responses between male and female economists emerge in response to the statement, \"Job opportunities for men and women in the U.S. are currently approximately equal.\" The estimation results show the mean response of female economists is one point (a full standard deviation) lower than that of male economists after controlling for degree vintage and employment type. The average response of males is closer to \"agree\" with the statement while the average response of females is closer to \"neutral.\" The linear probability model, however, indicates that 58 percent of female economists with the average characteristics of the sample either \"disagree\" or \"strongly disagree\" that job opportunities are equal. Women are 42 percentage points more likely to display this opinion than men. As our previous results indicate, male economists appear more likely to prefer market solutions to economic problems than do women. We are interested to learn if male economists are also more likely to see differences in outcomes, such as variations in wages between men and women in the U.S., as an outcome of choice or the free market versus other institutional factors. When we ask if respondents agree with the statement that the gender wage gap is largely explained by differences in human capital and voluntary occupational choices, the results of our survey indicate a significant difference between male and female economists on this issue. When comparing economists with the same employment status, who received their Ph.D. in the United States in the same decade, we find that the mean response of males is more than a full standard deviation higher than that of females. On average, women disagree that the gender wage gap is due to choices and productivity difference; the average man is more likely to agree. The probability of disagreeing or strongly disagreeing with the statement is 51 percentage points higher for female economists than for male economists. Rent seeking, male preferences for market solutions, and women\\'s views on gender equity may be reflected in the gender gap in responses. Because one of the most visible and controversial solutions to the problem of discrimination and equal opportunity has been affirmative action, we ask two questions about affirmative action. Question 26 asks whether affirmative action programs for women are a good idea and Question 27 asks whether affirmative action programs for African-Americans are a good idea. Women\\'s average responses to both questions are statistically significantly more favorable to the use of Affirmative Action than men\\'s;; however, the magnitudes of the gender differences in opinions are only about one third of a standard deviation. Finally, we examine views on equal opportunity in the economics profession and examine perceptions of equal opportunity for women as graduate students and as faculty. The question about graduate education asks respondents if they believe that opportunities for women graduate students in economics in the U.S. currently favor men or women. After controlling for degree vintage and type of employment, we find that the mean response of female economists is closer to the \"favors men a bit more\" choice and the mean response of male economists is closer to the \"is approximately equal for men and women\" choice. According to the point estimates, 52 percent of women and 15 percent of men believe graduate education favors men either a bit more or a lot more. On the issue of gender equality as it relates to women faculty in economics we ask respondents if they believe that opportunities for economics faculty in the U.S. currently favor men or women or neither sex. We find that the average female economist believes opportunities favor men more than women and this view reflects a full standard deviation difference in opinion from the average male economist. The average male response indicates a belief that current opportunities for economics faculty in the U.S. are equal, favoring neither men nor women. Women are 55 percentage points more likely to believe that opportunities for economics faculty favor men a bit or a lot more than women. On four of the five questions concerning gender equality, male and female members of the AEA have reached the opposite conclusion, with statistically significant and large differences of opinion. More than 50 percent of female economists believe that opportunities in the economics profession or in the job market in general favor men whereas more than 75 percent of male economists believe they are either equal or favor women.',\n       'The financial performance of the illustrative farms will be evaluated by examining several key financial characteristics of the farm business. The mean as well as the distributions for the following financial measures will be discussed: net income, change in net worth, debt-to-asset ratio, term debt and capital lease coverage ratio, working capital, cash balance, return on equity (ROE), operating line utilization, term debt payment delay, and land liquidation.',\n       \"Because current and historical practices have led to high leaching losses from cropland soils, reductions will be needed in order for agriculture to comply with California law, which stipulates that groundwater cannot be degraded. To provide a broad reference point of what the source loading numbers mean with respect to potential groundwater pollution, it is useful to introduce operational benchmarks that indicate whether NO 3 -leached in recharge to groundwater exceeds the NO 3 -drinking water standard at the field and study area scales. Our benchmark for low intensity vs. high intensity of NO 3 -leaching is 35 kg N ha -1 yr -1 and is based on average groundwater recharge rates (~300 mm yr -1 ), the maximum drinking water level for NO 3 -(45 mg L -1 ), and evidence that there is only limited denitrification occurring in the vadose zone of many California regions (Green et al., 2008) . Few cropping systems (e.g., alfalfa and grapes) in the study area leach less than 35 kg N ha -1 yr -1\\n. Even where no excess manure N is applied, significant synthetic fertilizer N reductions would be required to lower groundwater N loading to 35 kg N ha -1 yr -1 while maintaining yields: in cotton, nuts, tree fruit, subtropical, and vegetable crops, necessary reductions range from 30 to 60% (Table 2 ). Aggregated across the 1.4 million ha of CAML cropland (not including alfalfa), the benchmark for total annual NO 3 -loading in the study area is 50 Gg N yr\\n, 30% of simulated current loading rates. The large total NO 3 -loading to groundwater relative to this benchmark indicates a high potential for past and current regional groundwater degradation.\\nIn some systems, especially high-intensity, double-cropped systems with N inputs typically exceeding 400 kg N ha -1 yr -1\\n, it is difficult to develop management practices that could meet this threshold, as shown in a meta-analysis by Zhou and ButterbachBahl (2013) . These cropping systems would need to reach over 90% partial nitrogen balance (N inputs-N exports). Currently, few California cropping systems exceed 50% partial nitrogen balance on average . While theoretically plausible and even practically sustainable, it would mean that many production systems operate near N equilibrium. Achieving this level of efficiency would require transformative changes in how these systems were managed, which is often at great cost to the farmer (Medellín-Azuara et al., 2013) .\\nIn 2007, California adopted regulatory measures for Central Valley dairies requiring that all N inputs total at most 140 to 165% of harvest. If all cropland in the study area were under such regulations, the total allowable N application to cropland, at today's crop harvest output, would be on the order of 195 to 230 Gg N yr -1 , which is 55 to 64% of the current total N inputs to cropland (360 Gg N yr -1 ). Total groundwater NO 3 -loading (after accounting for atmospheric losses) would then indeed be on the same order as the above benchmark if yields were sustained at the current level, which indicates the potential value of such guidelines.\\nConsidering the cropland N mass balance, another underlying challenge is that many inputs cannot be reduced, such as atmospheric deposition, irrigation water NO 3 -, application of urban and industrial organic waste, and the manure generated by the native dairy herd. These now total 127 Gg N yr -1\\n, which is over half of the amount of synthetic fertilizer N applied (230 Gg N yr -1 ; Table 3 ). The issue is similar California-wide, as irrigated cropland is about 2.5 times larger and the statewide dairy herd is about two times larger than the study area. Therefore, addressing this issue requires a fundamental shift in crop nutrient and waste management practices. Nutrient management practices need to be developed that account for and use a large amount of N inputs that come from nonsynthetic fertilizer sources. Additionally, waste management practices are needed that convert dairy, municipal, and food processing organic wastes into more effective, transportable, and accountable fertilizer.\\nFinally, our analyses provide average estimates of the magnitude and intensity of NO 3 -loading from a wide range of land use categories. Average annual NO 3 -loading for specific categories are based on simplified assumptions and on limited data with varying degrees of accuracy. The numbers given represent a best, albeit uncertain, approximation of the actual NO 3 -loading from specific sources. Furthermore, agricultural discharges of NO 3 -to groundwater may vary widely between individual fields, farms, or facilities of the same category due to differences in operations, management practices, and environmental conditions. We currently lack data to take such variability into account. Since our estimates do not account for edaphic differences in production, we caution that actual local groundwater NO 3 -loading at any location within the study area is likely to vary from those projected. With such inherent heterogeneity, complexity, and uncertainty, and given the additional uncertainty of the groundwater system itself, it is tempting to lose sight of the overarching effect of intensive agriculture on groundwater quality. ). Assuming recharge rates are similar over the historic period, concentrations of NO 3 -in recharge have likely doubled over the past 30 yr. For the study area, statistical error analysis furthermore indicates that the 95% confidence interval for 2005 cropland N loading to groundwater (not including alfalfa) ranges from 100 to 220 Gg N yr -1 , which is well above the benchmark of 50 Gg N yr -1\\n. The amount of groundwater NO 3 -loading is therefore of such magnitude that no matter the uncertainty the overarching finding is that agriculture has and continues to significantly degrade groundwater quality in the study area.\",\n       \"The lower incidence of risky behaviors-among young people who are involved in extracurricular activities may be wholly or partly due to the fact that, as shown above, students who participate in these activities are more likely to have gotten good grades in school in the past and to be currently enrolled in college-preparatory programs. Participators are also more likely than non-participators to come from families that are well-educated, prosperous, and involved in school-related activities themselves. It may be these related factors, rather than extracurricular participation as such, that help to protect the young people from potentially detrimental behaviors like droppIng out, smoking, and drug use. Multivariate analysis methods were used to control at least partly for the 'effects of family, student, and school characteristics and to reveal whether any association between extracurricular participation and risky behaviors remained after these related influences were taken into account.' Family characteristics entered into the analysis were the education level of the student's more educated parent,. and*whether the faMily's socioeconomic status fell into the first (lowest) quartile, or the 2nd, 3rd, or 4th (highest) quartile, as estimated from information about the parentsf occupations and household possessions. Also entered were whether the family had received welfare in the last year, and whether the parents spoke a language other than English as their main language at home. Family structure was taken into account by coding whether the parents or guardians present in the home with the student were the biological or adoptive mother and father (reference group), the mother only, the mother and a stepfather, the father only, the father and a stepmother, or if neither biological parent was present. The level of parent involvement in school-related activities was assessed from student responses to a series of questions about the parents meeting with teachers, going to PTA meetings, etc., and ertered into the analysis. Based on their responses; students' families were categorized as displaying low, moderate (reference group), or high levels of Multiple logistic regression was the analytic method used. The regression analyses are summarized in Tables 1 through 18.   42 C involvement, or they were put into a missing information category on the in/gement variable to avoid throwing out the substantial number of cases for whom this was true. Student characteristics entered into the analysis were the student's sex, and whether the student's racial and ethnic background was black, Hispanic, Asian or Pacific Islander, or white non-Hispanic (reference group). Also included were whether the student's grade point average in the eighth grade was in the first (lowest) quartile, or the 2nd, 3rd, or 4th (highest) quartile, and whether, in the 10th grade, the student was enrolled in a general high school program, a vocational/technical program, an academic or college-preparatory program (reference group), or some other type of program. Two school characteristics were taken into account in the analysis: whether the-school was a private-religion affiliated school, a private school that was not affiliated with a religion, or a public school (reference group); and whether the school had a high proportion (50 percent or more) of minority students, a medium proportion (25-49 percent), or a low proportion (reference group). The results of the multivariate analyses showed that both hours per week of extracurricular participation and the control variables were associated with the occurrence of the problem behaviors (Figure 23 and Tables I and 2). Most of the family, student, and school characteristics were significantly related to the probability that a student would engage in problem behaviors like dropping out, becoming a teen parent, or using drugs. Although the pattern of relationships was fairly similar from one problem behavior to the next, the strengtfi of specific predictors varied somewhat across the different behaviors. Dropptg out was most strongly related to the combined Set of participation and background characteristics, whereas binge drinking was least strongly related. Becoming a teen parent, getting arrested, becoming a smoker, and using drugs fell in between these poles in terms of the strength of tl:e overall association with the predictor set. When the related characteristics were taken into account, the strength of the association between hours per week of extracurricular participation and the occurrence of a problem behavior was generally diminished. However, a significant negative relationship between participation and risky behavior remained in almost all cases. The one exception was with respect to binge drinking, which did not show a significant relationship with hours of participation after the other factors were controlled (Figure 23). The most consistent finding was that sophomores who spent no time participating in extracurricular activities had a higher chance of exhibiting risky behaviors than those who ..Spent low-to-moderate amounts of time (less than 5 hours per week) in such activities. The non-participators were half again as likely to drop out or use drugs, a third again as likely to become teen parents or smokers, and a quarter agahi as likely to be arrested, compared to the low-to-moderate participators. Sophomores who spent 5-19 hours per week in extracurricular activities had at least as low a likelihood of engaging in the risky behaviors as thoie who spent only 1-4 hours per week in extracurricular activities. With respect to some of the problem behaviors, the former grouP had a significantly lower likelihood of engaging in the behaviors than did the latter group. For example, high participators.were only half as likely as low-to-moderate partic:pators to drop out and 80 percett as likely to become smokers. Sophomores who wete extreme extracurricular participators (those who reported spending 20 hours or more per wek in the activities) were not significantly 'higher nor lower than the low-to-moderate participators in terms of their chances of engaging in risky behaviors. Differences in the effects of extracurricular participation among males and females Parallel analyses were carried out for only male and only female students, to discover whether extracurricular participation operated similarly or differently for the two sexes. The pattern of results was generally similar (Figures 24 and 25; Tables 3 through 6). One notable eAception involved the criterion event of becoming a teen parent. Among females, the link between extracurricular non-participation and a greate likelihood of teen parenthood was stronger than the relationship found when the sexes were combined. Female non-participators were half again as likely as low-to-moderate participators to have given birth by their senior years. Very heavy participators were also more likely to have given birth than low-to-moderate participators, but this difference was not statistically reliable. The reason for the lack of statistical significance was the small size of the subsample of females who spent 20 or more hours per week in extracurricular activities (Figure 25). A similar pattern of differences was observed among males, but none of the differences was large enough to be deemed statistically reliable. No significant relationship between extracurricular non-participation and teen parenthood remained after controlling for related factors (Figure 24). The difference between the sexes with respeclinhis relationship may be because the individual male has less direct control than the individual female over . whether or not he becomes a parent. Hence, chance plays a greater role in the occurrence of early parenthood among males. It may also be that male students are less candid in reporting their parenthood because the evidence is not as inescapable as it is for females. Effects of the number of activities in which the student participates Thus far, the analyses of the relationship between extracurricular participation and the occurrence of risky behaviors have focused on the amount of time students spent in extracurricular activities in a typical week. The number of different types of activities in which the student participated during the school year is another measure of involvement. The relationship between this measure and the qcurrence of risky behaviors was examined, controlling for the influence of the same family,/ student, and school characteristics that were employed earlier. As before, separate analyses! were conducted for male and female students. Once again, the most consistent finding was that students who were involved in no activities were significantly more prone to risky behaviors than those who were involved in at least one activity (Figure 26; Tables 7 through 10). Students who reported involvement in no activities during a year were a smaller and more extreme group than students who reported spending no time in extracurricular activities in a typical week. It is not surprising, then, that the former group showed even more pronounced tendencies to engage in risky behaviors than did the latter. For example, males involved in no activities were twice as likely to drop out,. and two-thirds more likely to smoke, than male students who participated in one activity. Likewise, females who' participated in no activities during the year were sixty percent more likely to smoke or use use drugs than females who participated in one extracurricular activity ( Figure 26). Participating in two or three or more extracurricular activities --as opposed to only one --was generally not associated with a further reduction in the risk of exhibiting problem behavior, at least not a statistically significant reduction. However, females who participated in three or more activities were three-quarters as likely to drop out or become teen parents as females who participated in only one activity ( Figure 26). In terms of bivariate associations, most of the school-sponsored activities for which participation data were'collected in the NELS showed negative relationships with at least some risky behaviors; i.e., those who participated in the organized activity were less likely to engage in the risky behavior. Noteworthy exceptions were participation in intramural sports (as opposed to interscholastic sports), which was largely unrelated to risky behavior, and participation in vocational clubs, which, if anything, was positively related to some problem behaviors, such as dropping out and becoming a teen parent. The latter may have been the case because students who participated in vocational clubs were apt to be in the vocational/technical or general programs in their higli -schools and, hence, had a higher risk of engaging in early school departure and childbearing to begin with.\",\n       'Supplementary data related to this article can be found at http://dx.doi.org/10.1016/j.jalz.2016.06.2358.',\n       \"Prior research suggests that the large increase in public funding for compensatory education programs and means-tested subsidies in the 1990s may have increased low-income families' use of child care, particularly of formal care in child-care centers or early education programs. To date, the size of this effect has not been empirically estimated, perhaps in part because doing so is difficult. States do not collect comparable data over time on formal care enrollment or program quality. In addition, few studies with large and nationally representative samples track young children's experiences in formal care. Most studies that collected detailed information from parents about early education and child-care arrangements were fielded only at one or two time points during the 1990s. The only study that repeatedly surveyed parents about early education and formal center-based care programs is the U.S. Census Bureau's annual October CPS, which includes basic questions about the enrollment of 3-and 4-year-old children in school or school-like arrangements.\\nThe analyses in this article make use of repeated cross-sectional data from the annual October CPS (from 1992 to 2000) to examine whether and to what extent increases in public spending for compensatory education programs and means-tested child-care assistance predict increases in low-income children's participation in formal care. With rates of enrollment in center-based child care and early education increasing among low-income children, to what extent do increases in Head Start and child-care funding explain these increased levels of enrollment? Do effects differ according to children's ages or their mother's marital status?\",\n       'Question Response future adaptation methods are proposed and made relevant to guide agents and farmers in beneficial agricultural management practices (Haden et al., 2012; Mase and Prokopy, 2014) . Others have found that focusing on local impacts of climate change can enhance communication (CRED, 2014). There were some important differences identified for priority level when compared within different demographic groups. One of the largest differences occurred with gender where female agents were more likely to identify high or very high priorities for climate change compared with low or very low priorities under the personal (44 vs. 28%), federal (64 vs. 20%), and extension (40 vs. 24%) categories (Table 4 ). This was opposite the male agents, who were more likely to identify low/very low priorities than high/very high priorities for the personal (46 vs. 18%), federal (32 vs. 27%), and extension (47 vs. 17%) categories. Though not as large a contrast, McCright (2010) found that concern for global warming by US women (35-37%) was greater than men (28-29%) between 2001 and 2008, which corroborates our gender differences.\\nAmong age groups, the priority level with the greatest number of responses in the personal category was \"medium\" for most groups (35-43%); however, the 30 to 39 and 40 to 49 age groups had more responses for the \"low\" priority level (38 and 31%, respectively) ( Table 4 ). In comparing responses for the combined \"low/very low\" and \"high/very high\" priority levels, the 20 to 29 year old group was the only group with more responses in the \"high/ very high\" level compared with the \"low/very low\" level (36 vs. 21%). Prokopy et al. (2008) found that age generally had a negative relationship with adoption rates of best management practices, which may explain the results for the youngest age group in our study. For the federal category, the 20 to 29 and 40 to 49 age groups indicated a greater response to the \"high\" priority level than other levels, whereas the remaining groups identified more with the \"medium\" priority level. This trend is also found in a comparison of responses to the combined \"high/very high\" and \"low/very low\" priority levels, where the 20 to 29 and 40 to 49 age groups had more \"high/very high\" level responses than \"low/very low\" level responses (64 vs. 0% for 20-29, 45 vs. 31% for 40-49), but the 60+ age group did have an equal response level (24 vs. 24%). The trend in the \"extension\" category was the same as for the \"personal\" category for highest response and \"high/very high\" and \"low/ very low\" priority levels for each age group, although the 20 to 29 age group had similar \"high/very high\" and \"low/ very low\" responses (28 vs. 29%). This indicates that the \"extension\" category may reflect personal beliefs, because the gender group also had similar trends between these two categories. Lemos et al. (2014) observed that advisors\\' inclination to deliver climate-related information was partially dependent on individual factors such as behavior and perception, which may explain the link between the \"extension\" and \"personal\" categories in our study.\\nThe amount of extension experience also related to the trends observed for age groups for the \"personal\" and \"extension\" categories where those with 5 or less or 21+ years of experience had the most responses for the \"medium\" priority level and those in the other groups had the most responses in the \"low\" priority level. The 6-to 10-year group, however, was spread equally between Table 4 . Extension agents\\' responses related to their perceptions of the importance of climate change personally, at the federal level and for Extension across gender and age groups. \"medium,\" \"low,\" and \"very low\" for the \"extension\" category (Table 5) . As stated previously, age and experience are likely to be well-related. There were greater responses in the \"low/very low\" level than \"high/very high\" for the \"personal\" and \"extension\" categories. For the \"federal\" category, the most responses for each group were found in the \"medium\" priority level, although the 6-to 10-year group was evenly divided between \"very high,\" \"high,\" and \"very low.\" There were more responses for the \"high/very high\" priority levels than \"low/very low\" priority levels for all groups except agents with 11 to 20 years of experience. Again, these results may indicate that the \"extension\" category may align closely with agents\\' personal beliefs.\\nIn general, there were very few regional differences within the state when it came to priority levels for climate change (Table 5 ). The most responses were found for the \"medium\" level, although the Eastern region had greater variability around this level and the Central region had more responses for the \"low\" priority level for the \"extension\" category. Comparing responses for the \"high/very high\" and \"low/very low\" priority levels, state regions had greater responses for the \"low/very low\" priority level for the \"personal\" and \"extension\" categories and more responses for the \"high/very high\" priority level for the \"federal\" category, although the Central region did have an equal proportion of responses for both combined levels.\\nVery high  4  13  20  7  13  10  3   High  22  25  5  12  11  7  30   Medium  44  0  20  38  32  37  30 Low 19 that some midwestern farmers were adapting by investing in technology to help them track weather, purchasing larger equipment to work faster in the field, installing more irrigation pivots and drainage equipment, implementing notill practices, spending more time scouting fields to identify emerging issues, and purchasing seeds that matured faster and had better resistance to disease, pests, and drought. A different study of midwestern farmers found that the most popular (64%) adaptation strategy farmers were engaged in was in-field conservation practices with fewer involved in new technologies (43%), edge-of-field conservation practices (35%), and diversifying production (10%) (Mase et al., 2017) . Farmers involved in the study also indicated drought, disease, heat stress, insects, and weeds as their areas of greatest concern. A majority of producers in Mississippi, North Carolina, and Texas believed that, in the future, the types of crops being grown in their areas would be different due to climate change (Rejesus et al., 2013) . When compared across different demographic groups, there were some differences that were similar to the trends for climate change perceptions. Female agents differed from male agents where the higher priority for female agents was agronomic decisions (13%), followed by weather variability (12%) ( Table 7) . This was opposite to male agents, who identified weather variability (22%) as the higher priority followed by agronomic decisions (15%). Also, those in the youngest age group (20-29) and with the least extension experience (5 years or less) identified greater training needs for agronomic decisions (13 and 14%, respectively) than for weather variability (11 and 10%, respectively) (Tables 7 and 8). The youngest age group also identified soil loss (11%) and crop insurance (11%) as a need of similar importance to weather variability. The older and more experienced groups had similar trends as the male agents. There was little difference across region where the greatest needs were for training related to weather variability (16-19%) and agronomic decisions (10-16%), although the Eastern region also identified weather forecast information as one of the higher needs (10%) ( Table 8 ).',\n       'The Adopt-A-County program assigned to WFO Baltimore-Washington staff improved relationships and communications between the WFO and emergency managers.',\n       'Thrombocytopenia. An increment of per 50 × 109/L in platelets was associated with a 40% decrease in mortality (hazard ratio: 0.60, 95% CI: 0.43, 0.84). Table 4 Laboratory parameters determining prognosis in COVID-19 patients showed no differences, prealbumin levels were found to be lower in HBsAg+ patients (81) . In solid organ transplant recipients with COVID-19, a biphasic pattern was observed with initial increases in inflammatory markers, followed by an increase in WBC, CRP, ferritin and D-dimer (82) .\\nTo assess the efficacy of treatment, the primary tool for analysis have been the trend shown by Laboratory parameters. Lymphocytopenia improved after Convalescent Plasma transfusion. C-reactive protein (CRP), alanine aminotransferase, and aspartate aminotransferase decreased after treatment (83) . Table 5 depicts the laboratory parameters which are associated with complications and monitoring of response to treatment in COVID-19 patients.',\n       nan,\n       'Step 1 of the test of measurement invariance establishes a general confirmatory factor analysis (CFA) model that fits the individual groups (i.e., boys ineligible for FRL, girls ineligible for FRL, boys eligible for FRL, and girls eligible for FRL). If the model-data fit is adequate for all the groups, then Step 2, which establishes a baseline model without cross-group constraints, is conducted, or else no further analysis is pursued. If the model-data fit is adequate, then Step 3, which is a test of complete measurement invariance and equality of factor loadings and intercepts for the overall group, is conducted. If the model-data fit is inadequate for the baseline model, then there will be no further analysis. A chi-square difference test and a comparative fit index (CFI) difference test are conducted to compare the measurement invariance model (i.e., constrained model) and the baseline model (i.e., unconstrained model). If the chi-square difference test between the two models is not significant, then the invariance of factor loadings and intercepts (i.e., measurement invariance) is supported. However, if the chi-square difference test is significant, the invariance of loadings and intercepts is not supported. A cutpoint of CFI difference test of less than .01 was chosen to decide whether there was a substantial decrease in model fit between the baseline model and the constrained model (Cheung & Rensvold, 2002; Hirschfeld & von Brachel, 2014) .\\nAs a first step in evaluating measurement invariance, the model for each of the four groups (i.e., boys ineligible for FRL, girls ineligible for FRL, boys eligible for FRL, and girls eligible for FRL) was individually fitted. Our judgment regarding good model-data fit indices were indicated by the following: (a) non-significant (Browne & Cudeck, 1993) ; and (e) standardized root mean square residual (SRMR) values less than .08 or .10 (Hu & Bentler, 1999) . The models of each of the four groups converged to an admissible solution. The modeldata fits were not statistically significant across all four groups. Table 2 presents the modeldata fit indices for the stepwise approach undertaken. The model-data fit for the boys ineligible for FRL was: χ 2 (6) = 4.94, p = .55, CFI = 1.000, TLI = 1.000, RMSEA = 0, 90% confidence interval (CI) = .00 to .13, and SRMR = .02; the model-data fit for the Note: All values were significant at p < .01 except those marked n ; FRL = free or reduced-price lunch; a n = 78; b n = 65; Educational Research and Evaluationgirls ineligible for FRL was: χ 2 (6) = 6.76, p = .34, CFI = .996, TLI = 0.990, RMSEA = .04, 90% CI = .00 to .17, and SRMR = .04; the model-data fit for boys eligible for FRL was: χ 2 (6) = 9.30, p = .16, CFI = .994, TLI = 0.985, RMSEA = .06, 90% CI = .00 to .12, and SRMR = .02; and the model-data fit for the girls eligible for FRL: χ 2 (6) = 6.79, p = .34, CFI = .998, TLI = 0.995, RMSEA = .03, 90% CI = .00 to .12, and SRMR = .02. These results suggest that the model-data fits for all four groups were adequate.\\nSubsequently, a baseline model comprising all the groups with no constraints on the loadings was derived in the second step of the stepwise approach. The baseline model converged to an admissible solution. The model-data fit was not significant: χ 2 (33) = 36.21, p = .32, CFI = .998, TLI = 0.996, RMSEA = .03, 90% CI = .00 to .08, and SRMR = .04. The fit indices revealed very good model-data fit.\\nIn the third step, a test of complete measurement invariance on factor loadings (i.e., constrained model) for the overall group was conducted. The model-data fit for the measurement invariance model was excellent: χ 2 (42) = 52.41, p = .13, CFI = .993, TLI = 0.990, RMSEA = .05, 90% CI = .00 to .08, and SRMR = .05. The Satorra-Bentler scaled chisquare difference test between the final measurement model and the baseline model was not significant (Δ S-B χ 2 = 16.12, Δ df = 9), which suggests that complete measurement invariance was established. Similarly, the CFI difference test (Δ CFI) between the baseline model and the constrained model was less than .01. The parameter estimates for the complete measurement invariance model with constrained factor loadings are reported in Table 3 .\\nFor the measurement invariance model with constrained factor loadings, we report the unstandardized estimates when comparing across groups because the unstandardized factor loadings are set to equality across groups (Kline, 2011) . Conversely, for within-group comparison, we report the standardized estimates (Kline, 2011) . The factor variances of alphabet knowledge (i.e., LNF and LSF) across both gender groups were substantially larger than the participants who were not eligible for FRL. For example, the mean unstandardized variance for the boys and girls eligible for FRL was 238.62 but 142.04 for the boys and girls ineligible for FRL. The variance for the boys ineligible for FRL on spelling was larger than the other three groups.\\nThe standardized loadings on most of the factors were very high (i.e., above .80) except for the factor loading on CTOPP Elision for two groups who were not eligible for Note: AK = alphabet knowledge; LN = DIBELS Letter Name Fluency; LS = AimsWeb Letter Sound Fluency; PA = phonological awareness; BL = CTOPP Blending; EL = CTOPP Elision; SP = spelling; RW = decodable real words; PW = pseudowords; FRL = free or reduced-price lunch; SE = standard error; unstd. = unstandardized; std. = standardized. All parameter estimates, p < .01.',\n       'At the end of 2007, 65 percent of farmers reported having no debt outstanding on their farm business balance sheet (although some of these farmers may have used and repaid a line of credit during the year to finance farm activities). These self-financed, debtfree farms are the smallest in terms of land operated, averaging 258 acres. ARMS data show that 7 of 10 debt-free farms had less than $10,000 in sales. Yet, not all debt-free farms are small. For example, over 14 percent of farmers with annual sales between $1 million and $5 million reported owing no debt at the end of 2007. Nonetheless, on average, farms that reported outstanding debt or that had an established line of credit were larger than farms that operated debt-free.',\n       'The X-rays and computed tomography (CT) scans of the COVID-19 patients are being used to train various computational models and neural networks to aid in the diagnosis of COVID-19. 7 In this way, AI can be used to detect severely ill patients as well as predict the prognosis. In addition, AI can help distinguish COVID-19 infection from the community acquired pulmonary infection. 8 ',\n       '). Performance deterioration (slowing down) was assessed during a 400-m long-distance corridor walk (LDCW), done as quickly as possible. 15 The walk included ten 40-m lap segments for which split times were recorded. Participants whose Lap 9 time was at least 6.5% slower than the Lap 2 time or who were unable to complete the 400-m walk were classified as having high performance deterioration. 15 Mobility was defined as the fastest (m/s) of two usualpaced 6-m walking tests, with slow gait speed classified as less than 1.0 m/s. 21 Physical function was measured according to time needed to complete five chair stands, with worse physical function defined as the highest quartile (>13.58 seconds). Time to complete the 400-m walk from the fast-paced component of the LDCW was used as a measure of fitness, with participants in the highest quartile (≥307.1 seconds) classified as having lower fitness.',\n       'Two initiatives in particular have caused this. The first has been concerned with the range of fees charged by universities. In order to make sure that students were offered a range of prices for higher education places, the government made 20,000 places available in 2012 to institutions charging £7,500 or less. These places were meant to act as an incentive to universities to drop their prices to £7,500 or less and to colleges to offer courses at degree level and thereby draw in more money from government. However, the incentives did not work. Of the 9,600 places allocated to universities, 4,200 were unfilled, and of the 10,400 allocated to Further Education colleges, 2,800 were left empty (i.e., over a third were unused).\\nThe second initiative has formed more serious effects, creating uncertainty and, for institutions, a high level of risk. In 2012, the government allowed universities in England to recruit as many extra students as they wished-with the grades AAB (the highest grades)-in the university entry examinations. This appeared to be advantageous to the universities in the most-highly selective group (the Russell Group). However, the overall numbers of applicants for 2012/13 showed a fall of 5 percent for those aged 18 and a fall of 15-20 percent for those aged 19 and older. The pool of those applicants achieving AAB shrank, which left several universities unable to enroll the numbers of students they expected. Liverpool, Sheffield, and Southampton-all in the Russell Group-failed to meet their targets, though the University of Bristol grew by 28 percent. Among those other universities charging less than the full £9,000, there were wide variations. While Staffordshire University',\n       'The challenge of managing manure nutrients in the Chesapeake Bay watershed is essentially one of nutrient flows and balance-large imports of animal-feed nutrients supporting an increasingly concentrated animal industry, and only a limited amount of available cropland to assimilate excess manure nutrients. Where excess nutrients cannot feasibly or economically be addressed through dietary regimes, improved management practices, increased landowner acceptance of manure, or expansion of non-cropland uses-all of which must be part of an integrated solution-reductions in the scale of animal production may be viewed as a reasonable approach. The Florida buyout of dairy operations in the northern Everglades basin suggests that voluntary, compensated reductions in animal herd size to improve water quality and environmental resources may represent a viable policy option. Here we consider the effect of reducing the scale of the animal sector in the Chesapeake Bay watershed on (1) off-farm manure hauling costs and (2) returns (above operating costs) to animal production. Using WTAM50 scenario as a benchmark, we consider three levels of reductions in manure-N excess-5, 10, and 15 percent-roughly equivalent to reductions of 2, 4, and 6 percent of animalunits 15 basinwide. Excess manure is removed on a uniform percentage basis across all manure types in the watershed, and reductions in animal-units are assumed to be applicable only where AFO manure exceeds onfarm crop requirements. Figure 11 shows the effect of a reduction in animals on manure hauling costs and returns (above operating expenses 16 ) to the animal industry. Basinwide, a 2-percent reduction in animal-units results in a loss of approximately $52 million to the animal sector, measured as weighted net returns across animal species types. Reducing nutrient excess by 15 percent would require roughly 6 percent fewer animal-units in the basin, valued at more than $150 million. The number and total value of animal-units forgone to meet manure-nutrient excess targets would vary over sub-basins, reflecting both the concentration of excess manure and the species composition of animals. The potential savings in off-farm hauling costs-ranging from $16 to $44 million depending on the level of nutrient excess targeted-are well below the forgone returns to animal production. Moreover, as forgone returns increase linearly with higher targeted levels of excess, the increase in sector savings decline as less costly hauls are eliminated. While operators facing exceptionally high manure hauling distances would have increased incentive to participate in a voluntary herd/flock reduction program, savings in manure hauling costs alone may not be a significant driver for many operators. ',\n       'Organizers: Baruch Schwarz and Paolo Boero Hebrew University, Jerusalem and University of Genova Argumentation has been used in mathematics by many researchers as a vehicle of shared understanding through which learning emerges. Argumentation is rooted in divergent philosophical traditions, though. Also, it has been used by psychologists with different theories on learning and development. Research in mathematics education and argumentation is mature enough to reflect on central issues on which there is agreement and disagreement. Leading researchers have agreed to take part in this discussion group in two 90 minutes long sessions. The discussion will focus on several issues 1) The definition of argumentation: Several divergent definitions have been given by argumentation theorists such as Perelman, Toulmin, van Eemeren and Antaki in the last decades. Mathematics educators have used some of these definitions and neglected others. A thorough discussion is needed to understand the definitions and to reflect on some conflicting results that the choice of definitions has implied. 2) The goals and functions of argumentation: accommodating divergent views, understanding, convincing, wining, etc. We will 3) The role of argumentation in construction of knowledge and in learning in general. For example, we will focus on classroom discussions and small group collaborative problem solving in which different types of argumentative talk may emerge. An overview of empirical results in mathematics education will be presented. This overview will be undertaken in light of a reflection on the definition of argumentation adopted in the studies reviews and the goals and functions of argumentation. 4) The relation between argumentation and rationality (according to Habermas\\' definition of \"rational behaviour\"), especially related to \"proving\", but in the perspective of more general issues (concerning intercultural studies). We will stress the gap between argumentation in informal settings (conversations, dinner, etc.) and argumentation at school 5) Methodological issues concerning the study of the role of argumentation in construction of knowledge, learning and development. Since argumentation is generally a social activity during which participants 6) Argumentation and the design of activities. Since argumentation cannot be considered as a manipulation imposed on students but rather as a natural commitment to accommodate divergent or different views, we will discuss the issue of the design of activities that invite students to engage in argumentation: confronting students with different mental models, the use of hypothesis testing devices, the role of the teacher in classroom discussions, the presentation of controversies, etc. In 2005 we had a lively discussion group that centered on three areas of interest: intervention strategies that might be used in countries such as Korea with large extant gender differences in achievement; how to study linkages among gender, ethnicity and socio-economic status; and setting a research agenda for future work on gender and math. The group felt that our work on these important topics had barely begun, so we propose to continue the discussion group with the aim of setting some agendas for research in various parts of the world where such work is most pertinent relative to these three themes. Two students, Crystal Hill and Beverly Bower Glienke, at UNC Chapel Hill, did a search for articles in mathematics education that included gender and interaction between gender and factors such as ethnicity in a collection of eighteen peerreviewed and research-based journals from 1990 to present. Their results were staggering: out of eighteen peer-reviewed and research-based journals, only twentysix mathematics articles focused on gender differences. Studies that study specifically the interaction of gender and ethnicity were more scarce. Out of twenty-six articles, there were five articles that focused specifically on African American students. Despite that their emphasis focused on African American students, they noticed that other ethnic groups had limited representation. Their results was presented at the annual meeting of the NC Association for Research in Education, in March 2006. Activities: We will begin with brief introductions and a short presentation on the paucity of work that has studied the interaction of gender, ethnicity and socioeconomic status to stimulate discussion. Depending on the size of the group, we may break into small groups to discuss critical questions such as those posed below or others that emerge from the participants. Small and large group discussions will be synthesized into key ideas for continued discussion, possible joint research, or future action. How do we develop research alliances that allow for the study of gender, ethnicity, and socio-economic status? What inhibits or encourages data being collected across all of these dimensions? Who has influence at the state and/or national level on the mathematics curriculum and/or the assessment program? Is gender a factor here? How can we impact such programs? What methodological approaches and theoretical framework(s) would enable us to investigate difficult and unresolved issues concerning gender especially as they relate to ethnicity and socio-economic status? Vygotsky and Piaget made a similar distinction between scientific concepts and spontaneous concepts. It is learning of the former that poses the challenge for mathematics education. Learners seem to learn mathematics through their own mathematical activity and through participation in discourse with other learners and more knowledgeable others (e.g., teachers). How do we understand these processes? How can we plan for learning opportunities that will maximize their learning of important mathematics? This discussion group will focus on how students learn from their own mathematical activity. It is in understanding better the processes by which learners learn through their activity that mathematics educators can develop a more scientific approach to the design, selection, sequencing, and modification of mathematical tasks for student learning. The coordinators originally approached this issue from different theoretical orientations. Simon and his colleagues worked on this problem based on the Piagetian constructs of assimilation and reflective abstraction. Dougherty and her colleagues based their work on Davydov\\'s approach to teaching primary-school mathematics grounded in activity theory (deriving from the work of Vygotsky). Despite the different theoretical starting points, Simon and Dougherty have noticed an important confluence of ideas. This discussion group can provide a venue for further exploration of these issues. The aims of the discussion group are to: • Foster among an international group a discussion of work that is contributing to or has the potential to contribute to a collective understanding of the processes by which learners learn from their own activity. • Discuss implications of this emerging understanding for elaborating a pedagogical framework on the design, selection, sequencing, modification, and implementation of mathematical tasks for student learning. • Consider the confluence and contrasts with respect to these issues that result from different theoretical perspectives, including but not limited to activity theory and constructivism. Each of the co-ordinators will lead one of the sessions, providing a brief set of ideas and questions to be considered and an example or two from recent research data to provide a common focus for discussion. This discussion group will be an important opportunity for PME participants to meet and discuss work in a broad set of domains which until recently have not been within the remit of PME but which many PME members have worked for some time. With the change in aims of PME at PME29, it now becomes possible for PME conference activities to engage with research in the social and political dimensions of mathematics education. Mathematics education is a key discipline in the politics of education acting as a gatekeeper to employment and further education. Thus, managing success in mathematics becomes a way of controlling the employment market. Mathematics education also tends to contribute to the regeneration of an inequitable society through undemocratic and exclusive pedagogical practices which portray mathematics and mathematics education as absolute, authoritarian disciplines. There is a need for discussing widely the social, cultural and political dimensions of mathematics education; for disseminating research that explores those dimensions; for addressing methodological issues of that type of research; for planning international co-operation in the area; and for developing a strong research community interested in this view on mathematics education. The First International Conference on Mathematics Education and Society took place in Nottingham, Great Britain, in 1998; subsequent Conferences were held in Portugal (2000); Denmark (2003); Queensland (2005). On all four occasions, participants from around the world had the opportunity of sharing research, perspectives and theoretical orientations concerning the social, political, cultural and ethical dimensions of mathematics education and mathematics education research. This discussion group aims to bring together such researchers to offer a platform on which to build future collaborative activity within PME and beyond. Drawing on work and collaboration already undertaken at the four previous conferences, the MES Discussion Group will focus on a central discussion theme: \"Power, politics and research\". There is much interest in many countries over the ethics of educational and social research. There is a sense of needing to be ethical in research, but there is an uncertain line between \\'ethical action/thought\" and compliance. As conservative agendas take over more and more of educational practice and policy, the implications for the conduct of research are becoming more profound. How does one \\'ethically\\' research aspects of power, disadvantage etc. In addition, the group will consider the future of the MES group both outside and inside PME. \"Freudenthal claimed that mathematical practice often led to a didactical inversion in which the genetic sequence was reversed in exposition. While the psychological processes are those that create the product, the ordinary conventions of lecturing offer the product first and then expect the genetic processes to take place as the succeeding exercises are attempted\" (Burn, 2002, p. 21). In several parts of the world, there has been a trend towards the creation of learning environments which attempt to reverse Freudenthal\\'s inversion by engaging students in a manner similar to that experienced by active research mathematicians. Examples of this include NSF sponsored programmes for undergraduate students, and the French \"Math.en.Jeans\", which runs programmes involving schools at various grade levels. Fundamentally, the offered experience is intended to expand student awareness of mathematics as a discipline, and of its practice. The aim of this working group is to assemble researchers interested in investigating the educational outcomes of these experiences, and in developing criteria for their design and evaluation, geared towards students at various levels of schooling. The proposed sessions will begin with the enactment of an exemplary situation, representative of the learning experience under discussion, offering participants a direct experience of mathematical research as practiced by professional researchers. In the remaining available time, the situation will serve as the basis for a forum on educational considerations stemming from such experiences. These considerations will include the heuristic strategies that were used in the research situation, the construction of mathematical knowledge that can emerge from these experiences, and epistemological and didactical considerations of programmes that propose these experiences. Participants will be invited to establish a community of scholars interested in an ongoing dialogue focusing on these issues and propose contexts for the experimental evaluation of the resulting theoretical frameworks.',\n       \"The High School Transcript File describes the coursetaking behavior of 15,941 sophomores of 1980 throughout their four years of high school. Data include a six-digit course number for each course taken, along with course credit, course grade, and year taken. Other items of information, such as grade point average, days absent, and standardized test scores, are also contained on the file. The Offerings Vile contains school information, course offerings, data for 957 schools. Each course offered by a sci 1 is identified by a six-digit course number. Other information, such as credit offered by the school, is also contained on each record. The Updated School File contains base year data (966 completed questionnaires) and first follow-up data (956 completed questionnaires) from the 1,015 participating schools in the HS&B sample. First follow-up data were requested only from those schools that were still in existence in the spring of 1982 and had members of the 1980 sophomore cohort currently enrolled. Each high school is represented by a single record that includes 230 data elements from the base year school questionnaire, if available, along with other information from the sampling files (e.g., stratum codes, case weights). The Postsecondary Education Transcript File for the HS&B seniors contains transcript data on dates of attendance, fields of study, degrees earned, and the titles, grades, and credits of every course attempted at each school attended, coded into hierarchical files with the student as the highest level of aggregation. Although no survey forms were used, detailed procedures were developed for extracting and processing information from the postsecondary school transcripts that were collected for all members A-4 F2: School Component Data File User's Manual of the 1980 senior cohort who reported attending any form of postsecondary schooling in the first or second follow-up surveys. (Over 7,000 individuals reported over 11,000 instances of school attendance.) The Postsecondary Education Transcript File for the HS&B sophomores includes transcript data for over 6,000 members of the 1980 sophomore cohort who reported in the follow-up survey that they had attended a postsecondary institution. The data file created for this study includes detailed information about program enrollments, periods of study, fields of study pursued, specific courses taken, and credits earned. An updated transcript file is being prepared as part of the 1992 HS&B fourth follow-up. The Senior Financial Aid File contains financial aid records from postsecondary institutions respondents reported attending and federal records of the Guaranteed Student Loan Program and of the Pell Grant program. The Sophomore Financial Aid File includes data on postsecondary financial aid experiences for 1980 sophomores who attended a postsecondary institution. Financial aid data were collected from federal records of the Guaranteed Student Loan and Pell Grant programs, and GSL disbursement data from guarantee agencies participating in the Guaranteed Student Loan program. The HS&B HEGIS and PSVD File contains the postsecondary school codes for schools HS&B respondents reported attending in the first and second follow-ups. In addition, the file provides data on institutional characteristics, such as type of institution, highest degree offered, enrollment, admissions requirements, tuition, and so forth. This file permits analysts to link HS&B questionnaire data with institutional data for postsecondary schools attended by respondents.\",\n       'Our study is 1 of many that support the use of structural MRI for providing valid surrogate markers in clinical drug trials. MRI is also useful for detecting factors that affect structural changes in anatomical regions involved in AD. CSF biomarkers, despite their value for early diagnosis, might not be so effective for tracking disease progression over time or even for evaluating therapeutic interventions in MCI and AD. For example, their n80s -measures of sample size requirements to detect a fixed percent reduction in the rate of progression -are 1000 -10,000 times larger than those from structural MRI (Table 2) .\\nTBM-derived maps of atrophic rates, coupled with voxel-based statistics, offer an easy-to-implement process to investigate factors that exert negative or positive influences on aging and AD. Full 3-dimensional maps are used in these correlations, as opposed to only 1 biomarker measure per individual. This type of map-based method may offer more information and spatial detail on the profile of effects, and may offer better statistical power if effect sizes are not constant across the brain.\\nEach AD biomarker, derived from structural MRI, clinical, or CSF measures, can be used independently to evaluate drug treatment effects, providing a surrogate outcome measure to track the rate of disease progress. As a result of using different biomarkers, the sample size estimates (n80) should be interpreted with care. For example, a 25% reduction in the atrophic rate (measured by MRI) may have a different functional significance for a patient than a 25% reduction in the rate of decline for clinical or cognitive test scores; similarly, it may also have a different biological significance than a 25% reduction in the rate of change in CSF biomarkers. For example, there may be important and relevant biological events that do not have an immediate imaging correlate. Future efforts will focus on combining multiple biomarkers that measure different aspects of disease progress to reduce the sample size even further. This study has some limitations. The age and sex effects on atrophic rates, which were still significant here after controlling for education, BMI, and ApoE4, need to be replicated in future independent studies. A more complete dataset from a large number of subjects with MRI, PIB-PET, [18F] fluorodeoxyglucose (FDG)-PET, diffusion tensor imaging (DTI), resting-state functional MRI, and arterial spin labeling is now being collected to explore the complementary value of each of these neuroimaging markers. Future longitudinal ADNI studies will make use of more than 2 serial scans, allowing acceleration hypotheses regarding age effects to be tested in the same subjects. More advanced statistical designs, such as random effects or mixed effects models, may then be used to estimate intra-subject variance and group effects with repeated measures (Fitzmaurice et al., 2004; Frost et al., 2004; Schuff et al., 2009 ).',\n       'Supplementary data are available at The Journals of Gerontology, Series A: Biological Sciences and Medical Sciences online.',\n       \"As part of its mission, NIH plays a significant role in training the biomedical research workforce. This study found, in FY 2009, that NIH supported 29,702 postdoctoral researchers as well as 31,487 graduate student and 5,788 undergraduate researchers from all grant mechanisms ( Table 4) .\\nThe figure for NIH-supported graduate students exceeds that reported by the National Science Foundation (NSF) in the results of the NSF-NIH Survey of Graduate Students and Postdoctorates in Science and Engineering (GSS), an annual survey of academic institutions in the United States that grant researchbased master's degrees or doctorates in science, engineering, or selected health fields. In 2009, the GSS reported 16% fewer graduate students supported by NIH funding (N = 26,506) than found in this study (N = 31,487). About 26% of the graduate students in science and engineering fields reported in the GSS as receiving NIH support in 2009, were associated with NIH training grants or fellowships (19) . However, the GSS uses different methods and captures only students' primary sources of support, reporting this information only for full-time students at degree granting institutions. In addition, the GSS enumerates students at 1 point of time in the year. In contrast, the All Personal Report includes all of NIH's support for part-and full-time graduate students for the entire fiscal year, perhaps accounting for this study's higher counts than the GSS.\\nThe NIH flagship program for research training is the Ruth L. Kirschstein NRSA program, which offers support to undergraduates, graduate students, postdoctoral researchers, and senior fellows in both institutionally (training) and individually administered (fellowships) awards. These awards provide tuition support, stipends, and training-related expense funding at levels that are set annually. NRSA funding is available only to U.S. citizens and permanent residents. In addition, undergraduate training programs are only provided at a limited number of institutions. In FY 2009, 21% of postdoctoral, 34% of graduate, and 12% of undergraduate research positions supported by the NIH were funded through NRSA awards. As shown in Table 4 , the NRSA training programs represented a larger fraction of the postdoctoral, graduate, and undergraduate PYE than unique persons, because, in contrast to non-NRSA postdoctorates and students, NRSA trainees and fellows are required to devote their full-time effort to the NRSA grant and are generally supported for 12-mo continuous periods (20) .\\nMost postdoctoral, graduate, and undergraduate research positions were supported through mechanisms other than the NRSA training programs (Table 4) , including non-NRSA training-related NIH funding opportunities [such as the Fogarty International Research Training Award programs (21) and the NIH Summer Research Experience Programs (22)], as well as through positions working on NIH research grants. Because, according to the GSS, ;51% of postdoctorates in biomedical sciences departments at U.S. institutions were on temporary visas in 2009 (19) , the positions on research grants-which do not have citizenship restrictions-are commonly used to support those researchers.\\nUsing the information from IMPAC II and the SED (http://www.nsf.gov/statistics/srvydoctorates/), citizenship was determined for the subset of non-NRSA postdoctorates who reported their Commons ID and received an academic doctorate in the United States. The SED is an annual census of all persons who received an academic doctorate from a U.S. institution, and thus it excludes those who received their academic doctorate outside of the United States or lack such a degree (e.g., M.D.-only degree holders). Citizenship status was preferentially obtained from IMPAC II, and supplemented by the information in the SED when IMPAC II citizenship status was unavailable. Individuals were classified as temporary residents if their citizenship status was recorded as a temporary resident in IMPAC II, or their citizenship status was unknown in IMPAC II and they were either not captured in the SED or reported having a temporary U.S. visa in the SED. In this sample, almost 65% of non-NRSA postdoctorates were estimated to be on temporary visas ( Table 5) .\\nFrom the available demographic information, there were differences in the educational background of the NRSA and other grant-funded postdoctorates. The proportion of NRSA postdoctorates with medical or other health professional doctorates was .35%, whereas ,10% of the non-NRSA grant-funded postdoctorates had clinical degrees (Fig. 8) . This result aligns with NIH policy, which encourages doctoral-level health professionals to receive formal research training through the NRSA program (20) . However, because of the larger overall number of postdoctorates on research grants (which are mostly Ph.D. holders), the number of all clinicians (medical doctors, as well as those with other health professional doctorates) on NRSA grants and other grants was about equal (2319 and 2210, respectively).\",\n       \"Students' response refers to either student's question or student's statement. In this study, sequence of verbal interaction after student's response looked into sequence of verbal interaction after student's question as well as sequence of verbal interaction after student's statement.\",\n       '♦ African-American preschool and kindergarten boys score relatively lower (about one-fifth of a standard deviation) in math assessments (see Figure 4). 42 However, once we control for SES, financial resources and demographic characteristics, the gaps disappear.',\n       \"Previous studies that have examined relationships between δ 15 N of wetland plants and watershed land use have largely focused on comparisons of single plant species across diff erent sites. For example, the δ 15 N values of Spartina alternifl ora were positively correlated to human N inputs in New England salt marshes (McClelland and Valiela, 1998;Valiela and Cole, 2002;Cole et al., 2004). Similar correlations have been reported for Iva frutescens, S. patens, and Phragmites australis (Wigand et al., 2007). However, in Hawaii, plant community structure diff ers considerably among wetlands (Bantilan- Smith, 2008). Th is is largely a result of the isolated nature of Hawaii's wetlands and the introduction of exotic plants. Th us, intersite comparisons of the same plant species can be diffi cult if not impossible because one species does not occur across all coastal wetland sites. However, in this study we have provided evidence that the δ 15 N of mixed plant communities, compared with the δ 15 N of individual plant groups (i.e., grasses, sedges) or individual plant species, shows promise as a potential indicator of N sources to these wetlands.\",\n       'We performed a longitudinal study of 465 Baltimore Longitudinal Study on Aging (BLSA) participants at first visit and an average of 13 years follow-up (ranging from 7 to 18 years) under an IRBapproved protocol. Demographic characterization of these participants is summarized in Table S1 in Supplementary Material. At each visit, fasting blood was collected. Sera were isolated from blood stored in a −80°C freezer, and genomic DNA was isolated from PBMCs using the Qiagen kit (QIAamp DNA mini Kit) and stored at −80°C.',\n       \"All five fathers completed all of the intervention sessions. There were two boys and three girls in the study; all the children had siblings. At the first session, the children's ages ranged from six months (the four youngest being born within six weeks of each other) to 15 months. Fathers' ages ranged from 36 to 53 years. All fathers lived at home with the respective infant's mother. The three fathers who declined to participate offered brief explanations for this choice. Two reported a lack of interest in participating in research, and the third that he did not have enough time to spare.\\nSessions predominantly took place in the evenings or at the weekend. A few sessions were held early in the morning prior to normal working hours. Four fathers had all sessions at their home, and one father had some of the feedback sessions at his place of work and the rest of the sessions at home.\",\n       \"To fully represent the 3D marine environment beyond bathymetry requires a bathymetric model to be populated with 3D marine objects. Currently, urban and terrestrial environments are well represented in 3D GIS and virtual landscape environments. Landscape visualization software and some GIS software maintain their own library of 3D objects that can be imported into 3D scenes. These objects include, for example, city buildings, street features, terrain, and trees contributing to realistic renderings for urban and terrestrial planning (e.g., Esri CityEngine and VNS) (Fritsch and Kada 2004;Lloret et al. 2008;Yano et al. 2009). In addition, the Google SketchUp 3D Warehouse provides online access to a 3D object library for users to download and insert into a scene. Google SketchUp's 3D Warehouse library includes 3D marine-related objects-in particular, coastal infrastructure and marine vessels; however, marine plant and benthic 3D models are nearly absent. A shared online library of 3D marine objects and substrate textures would facilitate the enhancement of marinescape geovisualizations.\",\n       nan,\n       '(3)     6 Appendix A: Summary statistics   (3)   (3)  (3)   (3)  (3)   (3) ',\n       \"Les données disponibles sur la R-D dans les pays de l'OCDE sont de très bonne qualité, dans la mesure où la plupart des pays Membres les recueillent depuis longtemps en suivant les principes directeurs énoncés dans le Manuel de L'investissement dans le savoir\",\n       \"A graduate's first position after earning the doctoral degree may reflect broad economic conditions and can shape later career opportunities and choices. Over the longer term, the early career patterns of doctorate recipients may influence the decisions of future generations of students considering careers as scientists, engineers, researchers, and scholars.\",\n       \"For protein-based immunization, repeated vaccine administrations due to the clearance and indication may be required to maintain a therapeutically effective level [41] . Furthermore, some non-specific effects may result from these typical vaccines, such as cross-reactive TCRs and antibodies, bystander activation of pre-existing effector or memory cells, and classical cell-mediated immunity may function as either beneficial or unfavorable to the public health of the population [42] . Also, although there have been previous studies on the safety of live attenuated or killed vaccines, and the safety of these vaccines used in clinics is very reliable, the potential for inducing infection still exists [43] .\\nThere are four main safety and efficacy advantages of the use of mRNA-based antiviral vaccines over traditional approaches. First, mRNA-based antiviral vaccines minimize the potential risk of infection and insertion-induced mutagenesis due to natural degradation of mRNA in the cellular microenvironment [44] . Second, the immunogen's high efficacy due to engineered mRNA structural modifications improves its stability and translation efficacy. Third, the high potency of mRNA-based vaccines capable of generating potent antiviral neutralizing immunoglobulins with only one or two low-dose immunizations [23, 45] may induce strong immune responses by activating both CD8+ and CD4+ T cells [46] . Fourth, engineered mRNA production facilitates large-scale production of sufficient vaccine doses required to treat mass populations [29, 47] . All these factors make the mRNA vaccine more suitable for a rapid response to the emerging COVID-19 pandemic.\\nAlthough these beneficial features of mRNA vaccines provide some hope for the development of the first clinically applicable SARS-CoV-2 mRNA vaccine, recent reports regarding rare cases of moderate or severe reactions for different mRNA vaccines have raised concerns about safety and immunogenicity, including in the primary outcome findings of the phase I trial on mRNA-1273 [20, 48] . Therefore, it is important to clearly understand the potential risks of this type of mRNA-based vaccine, which include local and systemic inflammatory responses, the biodistribution and persistence of the induced immunogen expression, possible development of autoreactive antibodies and toxic effects of any non-native nucleotides and delivery system components [48] [49] [50] .\\nGlycosylation of viral envelopes is a very common biological phenomenon. Glycosylation refers to the process in which proteins or lipids are linked to sugar chains by the action of enzymes. Glycosylation is one of the important forms of posttranslational modification of proteins and is an essential way to regulate protein localization, function, persistence, and e924700-5 diversity [51, 52] . For viruses, their replication and invasion into host cells are closely associated with the glycosylation of their structural proteins [53] . The main purpose of viral vaccine development is to evoke an immune response to fight the virus. However, the high glycosylation of proteins on the surface of the viral envelope works like invisible camouflage, which can help the virus to successfully escape the detection of the human immune system and achieve the purpose of survival [54, 55] . Therefore, the higher the degree of glycosylation, the greater the probability that the virus will escape the immune response and the lower the success rate of vaccine development.\\nTherefore, overcoming the highly glycosylated viral envelope is another challenge of SARS-CoV-2 vaccine development. SARS-CoV-2 includes highly glycosylated spherical particles and has a large structure containing at least 66 N-linked glycan sites, of which 54 sites show similarity with SARS-CoV [56] . In contrast, the glycan sites of HIV are between three times and six times that of the influenza virus, which is one of the major reasons why an HIV vaccine has not been successfully developed at this time [57] . However, SARS-CoV-2 has more than twice the glycan sites of HIV [58] . This unusual degree of glycosylation means that SARS-CoV-2 may mutate quickly, making the development of a vaccine extremely difficult. However, vaccine development failures caused by glycosylation remains significant, because vaccine design is based on the traditional inactivated or attenuated live vaccines, and different traditional vaccines may induce various glycosylation patterns of antibodies that subsequently affect the protective role of the antibodies [59] . However, the use of mRNA-based vaccine technology and targeting only the S protein, rather than the entire virus particle, may lead to the human immune system producing S protein antibodies without the influence of viral glycosylation.\",\n       'This chapter highlights the essential role of the ocean data assimilation systems for ENSO monitoring and SI forecasting. Considerable efforts to improve the data assimilation systems and increased ocean observation data have brought about the recent brilliant development of ocean data assimilation systems, contributing to the improvement of SI forecasting.\\nAlthough ENSO and seasonal forecasting have been realized and sophisticated enough to be used operationally now, further improvements are earnestly desired to reduce the damage of climate disasters and to increase the efficiency of industry, agriculture, and fisheries. Further development of ocean data assimilation systems is a possible factor for such improvement. Although the innovative progress of the development thus far has followed material changes of the ocean observing system as described in Section 2, no other material change is likely to occur in the next few years. Instead, we assume that imposing the ocean-atmosphere balance relationship to mitigate the coupled shock is the key to further improvement of the assimilated fields and SI forecasting, just as imposing the T-S balance relationship was the key to progress from the first generation to the second generation of ocean data assimilation systems. In order to achieve such development, the use of a coupled atmosphere-ocean model is essential; thus, it is necessary to develop a system in which observation data are assimilated into a CGCM. In Section 5, we demonstrated a benefit of this strategy. However, the quasi-coupled data assimilation system introduced in section 5 is insufficient because it relies on adjustment in the coupled model integration for establishing atmosphere-ocean balance. Developing a scheme to assimilate atmospheric data is also desirable. Atmospheric data have an inherent potential to improve assimilated fields, as demonstrated in Balmaseda & Anderson (2009) , although the difference in the major time-scale between the atmosphere and ocean makes the assimilation rather difficult. To adress these issues, several institutes, including JMA/MRI, have currently been developing truly coupled data assimilation systems, in which oceanic and atmospheric data are assimilated into a coupled model imposing the ocean-atmosphere balance relationship. We expect those systems to form the third generation of data assimilation systems for ENSO monitoring and SI forecasting. It should also be noted that a solid ocean observing system is essential for issuing reliable information on ENSO and seasonal forecasts. Therefore, sustaining current observing platforms, including the TAO/TRITON array and Argo floats, is crucially important, as well as proposing innovative and potential platforms. In order to sustain current platforms securely, it is necessary to demonstrate to the administrators the essential effects of those observation data in a readily visible manner as attempted in Section 4. In that sense, the activity of the GODAE Ocean View observing system evaluation task team (see the last of Section 4) should be seriously supported. In particular, we hope that the observation impact statement will have a substantial effect for the secure development of the ocean observing system, resulting in further improvements of ENSO monitoring and SI forecasting.',\n       'Since 1970, an exploding U.S. population has migrated to the coast. Population on the coast has generally followed national trends. The problem, however, is coastal population is concentrated within a limited land area (Crossett et al., 2004). A densely populated coast places a significant burden on local and state governments who need to assess environmental and economic challenges imposed by potential hurricane damage. Hurricanes are the most common natural hazard in east coast communities, regardless of climate change (Pielke et al., 2008). The effects of climate change are expected to intensify hazardous conditions. Growth in coastal communities is driven by economic activity related to recreation, tourism, water-reliant industry and commerce, and employment (The Heinz Center, 2000;Crosset et al., 2004;U.S. Commission on Ocean Policy, 2004). Economic activity along the coast supports not only the year round coastal population, but also the more than 180 million seasonal visitors to coastal regions each year. Tax revenue from tourist activities alone encourages policies that support even more growth. Commercial activities surrounding natural resources in coastal areas generate billions of dollars each year (Marlowe, 1999;U.S. Commission on Ocean Policy, 2005). Fisheries generate more than $32 billion in income and over 1 million jobs in the U.S. (National Marine Fisheries Service, 2014). In 2010, the Bureau of Economic Analysis estimated that coastal shoreline counties generated 45% of gross domestic product (GDP) for the U.S (NOAA, 2013b). Shoreline counties generate over 54% of GDP for coastal states. More than 50 million jobs and $2.7 trillion in wages were generated in coastal shoreline counties, including the Great Lakes Region (NOAA, 2013b). All of this robust economic activity encourages even more development that in turn contributes to a continuing migration to coastal communities. Population density in coastal areas has been expanding at a rapid pace for nearly half a century. In 2010, population density for the U.S. was 87 persons per square mile (NOAA, 2013a, p. 3). During the forty years between 1970 and 2010, the U.S. added 36 persons per square mile overall (Crowell et al, 2010;NOAA, 2013a, p. 3). In contrast, coastal shoreline counties added 125 persons per square mile during the same forty year period resulting in a density of 446 persons per square mile in coastal counties (NOAA, 2013a, 3;Crossett et al., 2004). Population density of this magnitude puts more pressure on the natural resources and economic stability essential to a sustainable existence. The current U.S. coastal population is estimated at 123.3 million or 39% of the national population and growing (Crowell et al, 2010;NOAA, 2013, p. 5). Coastal population growth is expected to increase another 8%, meaning that by 2020, another 10 million people will reside on or near the coast (NOAA, 2013, p. 4). Coastal counties encompass roughly 10% of land area in the United States, excluding Alaska. Population density of this scale imposes significant risks on those that live, work, and operate businesses on the coast, particularly when disaster strikes. The effect of disaster places an even greater burden on coastal states that must allocate limited resources while attempting to project the potential impact of coastal disaster on housing and essential services.',\n       'Crowd Out or Opt Out: The Changing Landscape of Doctorate Production in American Universities',\n       'Additional file 2: Table S2 presents detailed results from the final mixed-effect model for cortical projection zone subregions. In summary, there was a significant main effect for TCV (P < 0.001) but not for scanner upgrade status (P = 0.32). The interaction between age and subregions was significant (P < 0.0001), indicating that subregions grew at different rates, but there were no significant age by diagnosis or age by sex interaction effects (both P > 0.19), indicating that the growth rate did not differ between diagnosis and sex. There was a significant three-way interaction effect between diagnosis, cortical projection zone subregion, and sex (P = 0.004). Simple effects for subregion-specific diagnosis and sex differences are detailed in Table 2 . Subregions that differ by diagnosis and sex include the orbitofrontal, anterior frontal, and superior frontal regions. Differences are depicted in Figure 2 . Specifically, the orbitofrontal fiber region area is decreased in males with ASD relative to TD males (estimated difference = −6.98, P = 0.02) but did not differ between females with ASD and TD females (P = 0.83). In contrast, the anterior frontal and superior frontal fiber regions are significantly decreased in females with ASD compared to TD females (anterior frontal: estimated difference = −20.45, P = 0.01; superior frontal: estimated difference = −17.49, P = 0.01). In males, there were marginally significant differences in these regions with differing patterns. In the anterior frontal region, males with ASD were increased relative to TD males (estimated difference = 8.18, P = 0.09), opposite to the pattern observed in females. In the superior frontal region, the pattern was similar to females; males with ASD were decreased relative to TD males (estimated difference = −7.92, P = 0.07). There was also a marginally significant difference in the posterior parietal fiber region area, with females with ASD decreased relative to TD females (estimated difference = −13.34, P = 0.07) but no difference between males with ASD and TD males (P = 0.22). There were between sex differences for males and females with ASD in orbitofrontal, anterior frontal, and posterior parietal fiber region areas (all P < 0.05). There were only marginally significant differences between TD males and females in the anterior frontal and superior parietal regions (P = 0.08).\\nA mixed-effect regression model fitted in secondary analyses to test for the effects of DQ revealed no effect for baseline DQ (P = 0.53).',\n       'In order to define a cutoff for amyloid positive scans, we used cortex-wide florbetapir SUVR values from a reference subgroup of all 661 subjects: 99 control subjects with a normal CSF Aβ level (> 192 pg/ml) (Shaw et al., 2009) . We computed mean and standard deviation of the global florbetapir SUVR in these subjects and used these values to compute a Z-score for global amyloid burden in all subjects. We considered a Z-score of 1.65 or more to be an indication of a positive amyloid scan. This cutoff corresponds to a P-value of 0.05 in a one-sided test. In our study sample, this Z-score translates into a cortex-wide florbetapir SUVR threshold of 1.263. Rabinovici et al. (2010) reported a disappearance of regional associations once the study group was restricted to subjects with the same diagnosis. Thus, we repeated the test for association between regional florbetapir SUVR and regional FDG SUVR in the subset of subjects (n = 267) with amyloid positive scans.',\n       'This article is motivated by the fact that the last more or less systematic review of medical multimedia retrieval reports on articles already six years old and many developments have changed the research domain of large-scale multi-modal retrieval. Deep learning has been a development of only the last 3-4 years that has totally changed the techniques most commonly employed in scientific challenges and that obtains very good results in many competitions. In combination with GPUs (Graphical Processing Units) larger training data sets and data augmentation techniques have become possible. Scientific challenges allowed to create large data sets by sharing efforts of annotation. Such challenges allow comparing different techniques based on the same grounds. Many funding organizations such as the American NIH (National Institutes of Health) now require the data of funded research to become available for the community and this will likely change future research, as large-scale research on medical image data becomes available.\\nBy reviewing existing work and missing parts the text should give multimedia researchers several possibilities to work on available medical data and to respond to current research challenges. It is clear that most medical data sets used are not really large scale but data production in medical institutions is extremely is high. Also the multimodal nature of medical data is often not fully exploited, but possibilities are enormous when the full available data are used.',\n       'The genome sequences of SARS-CoV-2 are available in the GISAID repository, https://www.gisaid.org/ and the MAFFT software used to align them is freely available for download (https://mafft.cbrc.jp/alignment/software/). Predicted protein models and molecular dynamics simulations are available at https://compsysbio.ornl.gov/covid-19/covid-19structome/. Experimentally solved protein structures used are available in the Protein Data Bank, https://www.rcsb.org/. The PDB IDs of these structures are as follows: 6vxx, 6xr8, 6vsb, and 6m17 (spike glycoprotein); 6yyt (replication complex); 2 k87 (nsp3); 6jyt (nsp13); and 6vyo (nucleocapsid). The iterative Random Forest code is available at https:// github.com/Jromero1208/RangerBasediRF and https://www.osti.gov/biblio/1560795 with DOI:https://doi.org/10.11578/ dc.20201001.84, under the GNU General Public License. Random Intersection Tree code is available at https://CRAN.Rproject.org/package=FSInteract, under the GNU General Public License. The R code used to calculate the success metric is included as Additional file 2.\\nEthics approval and consent to participate Not applicable.',\n       'Establish centers responsible for the secure archival of NRT marine observations from the GTS in their native formats.\\nBuilding on the successful examples, such as the Argo Data System (ASVs) 4 , establish international data centers for all marine observation types including for ASVs to ensure that all new data types are managed by domain experts and can be efficiently integrated into more general archives such as ICOADS.\\nWork with data providers to establish the inclusion of originators unique identification tags with all marine observations to enable the efficient linking of reports derived from the same original observation.',\n       'We began by assessing the limit of detection (LoD) of N1-STOP-LAMP under optimum conditions using a 10-fold dilution series of purified RNA, prepared from a titred SARS-CoV-2 virus stock. Although LAMP assays are not strictly quantitative, a positive correlation between Tp and RNA concentration was evident. This was indicated by an absolute detection threshold between 50 and 500 viral genome copies per reaction. We achieved a reliable detection of 5/5 replicates at 500 viral genome copies and detection of 4/5 replicates at 50 viral genome copies per reaction (Fig. 1a) . Of note, this threshold is likely an underestimate of the true detection limit, as we assumed all RNA yielded from the viral stock generated from the Vero cell supernatant was of viral origin. Using TCID 50 , the absolute LoD of N1-STOP-LAMP was between 0.001 and 0.01 TCID 50 per reaction (equivalent to 1-10 TCID 50 ml −1 ).',\n       'All datasets generated for this study are included in the manuscript and/or the Supplementary Files.',\n       'The 1989 SDR was a mail survey. The questionnaire was first mailed in March 1989. A follow up mailing to nonrespondents occurred in early May, and a second in September 1989. Of the 72,555 individuals in the sample, 2,160 were deceased or otheiwise out-of-scope of the survey, including those who indicated on a previous survey that they held a doctorate from a foreign institution and were foreign citizens living outside the United States. This resulted in an active sample of 70,395 scientists and engineers. Analysis of undeliverable questionnaires showed an estimated 61,982 individuals were contacted. A total of 38,799 survey responses was returned. The survey overall response rates were 55.1 percent for the active sample and 62.6 percent of those contacted. The overall response rate for the 1989 active sample reflects respective declines of 3.0 and 8.3 percentage points below 1987 and 1985 levels. Information on sample sizes and response rates for selected items are found in tables A-1 through A-11.',\n       'Baseline cognitive scores (a), changes in cognitive scores (b), and conversion from MCI to AD (c) and voxel-wise correlation results We give the 25th, 50th, and 75th percentiles for these variables to give the reader an impression of their distribution. A ↓ implies that lower numeric values of the variable are associated with greater atrophy, whereas an ↑ implies that higher numeric values of the variable are associated with greater atrophy.',\n       \"The results of path analysis showed that the model fit the data well with CFI of .974, TLI of .963, and RMESA value less than .028. As a result, the parameter estimates of the models were interpretable. The significant unstandardized parameter estimates of the model are reported in Figure 1 . The findings of the present study demonstrated that students' domain-specific self-efficacy and intrinsic motivation significantly predicted their identification with academics in math and science respectively. Students' utility value in science also significantly predicted their science identity. On the contrary, students' utility value in math was not a significant predictor their math identity. In turn, students' math identity significantly predicted their plans to enroll in an AP calculus course and students' science identity significantly predicted their plans to enroll in an AP science course (see Figure 1) . Interestingly, the path coefficient from students' science identity to their plan of taking an AP science course was much stronger compared to the path coefficient from students' math identity to their plan of taking an AP math course. Also, SES appeared to be a significant controlling variable for students' plans to take an AP course both in calculus and science. Prior math achievement also appeared to be a significant controlling variable for students' plan to take an AP calculus course, but not an AP science course.\",\n       'In the equatorial Pacific, the Alk* increases from the west to the east by 9.1 μmol kg À1 (Table 1 ). There is a significant difference in Alk* (greater than the uncertainty in Alk*: 3.02 μmol kg',\n       'Physical activity was measured using a standardised questionnaire to determine estimates of activity based on a 24 h history. A physical activity score was calculated from the number of hours spent doing specific activities that were categorised (sleep, sedentary, slight activity, moderate activity and heavy activity) and weighted according to oxygen consumption required to perform them (metabolic equivalent task (MET)-h per d) (26) . Additional covariate information included age, sex, smoking dose (0, 1-15, 16-25, or . 25 cigarettes/d) and years of education.',\n       \"Lessons from the past. Early research on the cellular composition of the brain culminated in the development of atlases of the human cerebral cortex, which were pioneered by Brodmann 6, 23 . These studies continued until the 1960s . The cortex was segregated into numerous structurally defined areas, based on regional cytoarchitecture (identified mainly by the number of cortical layers, laminar patterns of cell packing density and the shape of neuronal cell bodies) or myeloarchitecture (identified mainly by the degree of myelination and the presence or absence of myelinated fibre bundles in the cerebral cortex). These types of architecture were studied by visual inspection of Nissl-or myelin-stained histological sections in single brains. However, when maps by different authors of this classical period are considered, a number of problems become apparent. For example, the drawings do not provide the sulcal pattern of a single real brain or a welldefined 'average' brain but of an imagined 'ideal' brain (for example, Brodmann's schematic drawing of a brain 6 ); so, it is impossible to compare different maps in a common reference space as they are not registered in a stereotaxic system, and brain shape and sulcal contours vary greatly between maps. The number of cortical areas also varies between the different maps, as the delineation procedure for areal boundaries was highly observer-dependent and not assessed quantitatively or statistically. Furthermore, with few exceptions, the drawings or descriptions do not show the positions of areas and their borders in the cortical sectors that are hidden in the sulci. This amounts to nearly two-thirds of the total surface area of the cortex 27 . Any desig nation of cortical areas in the sulcal part of the cortex 5 is pure supposition, and not supported by original observations. Finally, most classical maps 9, 10 do not match the high degree of cortical segregation that has more recently been shown by functional imaging -particularly in the multimodal association cortices [28] [29] [30] [31] . Indeed, architectonic brain mapping based on visual inspection has been severely criticized 24, 32 because of its lack of clearly stated and objectively verifiable criteria. This led to a decrease in the general interest in architectonic atlases for decades. More recently, axonal tracing techniques have been combined with architectonic observations to produce atlases of selected cortical regions in non-human primates and other mammals 28, 33 . Unfortunately, this experimental approach cannot be used in studies of the human brain. Meanwhile, stereotaxic atlases of the human brain, based on imaging data, began to be published (mostly in book form), primarily in response to the needs of neurosurgeons. These atlases compensate for some of the problems of the classical brain maps -particularly the lack of a spatial reference system. However, stereotaxic atlases tend to focus on subcortical structures and provide only sparse information about the cortex 14 . Furthermore, they lack quantitative information on the intersubject variability of areal boundaries 11 , intersubject variability in gross anatomy, left-right asymmetries and the criteria to identify architectonic areas 5 .\\nThe driving force for modern brain atlases. The current resurgence of interest in brain atlases that provide architectonic maps of the human cerebral cortex has been largely motivated by the introduction of functional imaging techniques such as positron emission tomography (PET) and fMRI. Researchers using these techniques invariably want to define -as far as possible with the limited resolution of PET or fMRI -the location of neural activity and determine whether the focus of activity is associated with a portion of a specific cortical area, an entire cortical area or overlapping areas.\\nThe development of functional imaging techniques has therefore relied on the evolution of structural imaging. These advances have included both in vivo, structural MRI and high-resolution imaging of threedimensional reconstructed histological sections. An example of the use of these techniques has been the development of the concept of a 'cortical area' , which has an important role in functional brain mapping. Cortical areas can be defined structurally on the basis of either macroscopic or microscopic (architectonic) criteria, although architectonic borders vary considerably in relation to macroscopic landmarks [34] [35] [36] [37] . The concept of 'brain mapping' benefits from the combination of functional imaging of a distinct and well-defined function with a microstructurally defined architectural atlas, because the correlation between function and its underlying cyto-or myeloarchitecture can then be tested by using the architectural map for the definition of regions of interest in functional imaging.\\nThe correlation between the (micro)structure and the function of an area has been established for many cortical regions, in particular for primary sensory and motor areas 7, 8, [38] [39] [40] [41] . Many functional units in the cortex are consistently found in humans and other primates, such as primary visual area V1, or the motion-sensitive area MT/V5, in the middle temporal gyrus. The spatial layout of these regions in the cortex is also somewhat consistent across individuals 38, 39, 42, 43 . Visual areas V1-V5 were initially defined electrophysiologically, based on their highly selective responses to visual stimuli with specific intensities, orientations or directions of motion 29, 30, 38, 44, 45 . These classifications allow functional imaging studies to relate their observations to known anatomical units that were initially defined using electrophysiology or direct histological mapping.\\nRequirements for modern brain atlases. Based on the criticisms of architectonic brain mapping 24, 32 , the requirements of basic and clinical brain research and the current progress in structural and functional brain imaging techniques, the criteria for ideal brain atlases can be established. They should include a multimodal microstructural approach that combines different independent methods, such as cytoarchitectonic, myeloarchitectonic, chemoarchitectonic and modern fibre-tracking approaches along with macroscopic, in vivo and tomographic imaging methods, and use observer-independent methods [46] [47] [48] [49] to define cortical boundaries -so-called architectonic parcellation. Brain atlases should contain data on interindividual variations in the geometry and topology of architectonic areas and should allow the use of algorithms that perform adequate linear (such as scaling, rotating, translation and shearing) or nonlinear (based, for example, on an elastic model 50, 51 ) alignment of individual brains and their structures to a standard spatial reference system. Nonlinear transformations, which apply local contractions or dilations, are often required (as well as linear transformations such as local rotations and shearing) to equate, or deform, an individual subject's anatomy to match the shape of a brain atlas 21 . In addition, efforts should be made to establish population brain maps (probability maps), which define the probabilistic position and borders of cortical areas based on studies of a large number of subjects across various population groups. Finally, these atlases should use a database format that provides easy access to the original data on which the atlas is based (for example, anonymized images from individual subjects, or models of specific brain structures). Alternatively, the statistical maps and anatomical delineations in the atlas should be readily importable into software and tool boxes that are commonly used when analysing new brain images. Once imported into other image analysis software, probabilistic maps of cortical regions or fibre pathways can be used to define regions of interest on new images, to provide a priori constraints or search regions for new statistical analyses, or simply to provide visual overlays of data from an alternative modality.\",\n       'Intended for educators and researchers in the field of early childhood education working with children from immigrant families, this article first briefly addresses the relationship between home literacy environment and English language learners\\' literacy development in both their heritage language and English. Second, through surveying the literature, I identify three different areas in which a home literacy environment influences English language learners\\' literacy development: (a) through language attitudes and parental beliefs; (b) through identity formation; and (c) through literacy behaviour of immigrant parents. Some helpful strategies learned from the literature are provided for educators to use with newcomer families in support of children\\'s literacy development.\\nLiteracy development is essential to a child\\'s school performance and future success. Yet, literacy is not a single, monolithic, and autonomous construct (Street, 2000) . Here, literacy is defined as a social practice that is socially constructed in educational and cultural contexts, including skills in dealing with printed and nonprint-based texts and in critical thinking (Kahn & Kellner, 2005) . This definition is in contrast to a singular, autonomous notion of literacy in which literacy development emphasizes decoding a text and studies involve the analysis of literacy rates, comprehension levels, ages, and reading and writing skills (Kahn & Kellner, 2005) . Multiple literacies consider literacy to be a social practice (Street, 2000) , where context and meaning in groups of different cultural, linguistic, and socioeconomic backgrounds need consideration. Just as cultural and linguistic backgrounds in families vary, literacy practices vary between and within cultures.\\nIn addition, rapid development in technology has changed how we look at literacy; the idea of multiliteracies shifts our thinking about literacy from privileging the printed text to acknowledging various ways that literacy is practiced in a society (Cope & Kalantzis, 2009) . With the increasing use of technology, literacy is no longer restricted to an ability to deal with printed texts, but has expanded to include electronic and multimedia modes. Nonetheless, as Cope and Kalantzis (2009) argue, \"whichever way we look, written language is not going away. It is just becoming more closely intertwined with the other modes\" (p. 182).\\nIn this paper, an English language learner (ELL) will refer to preschool and early elementary grade children whose first language is not English and who are learning English as a second language in a North American setting (Shi, 2011) . Learners of English as a second language may include children from Africa, Bangladesh, Hispanic regions, China, Laos, and many others (Shi, 2012) . Although different terms appear in the literature to describe such learners, English language learner is increasingly utilized because it highlights the learning process instead of a deficiency in nonnative English-speaking students (Gere, 2008) .\\nBecause literacy is a socially constructed practice, children who are learning English and whose parents speak another language and come from a culture different from that of the mainstream culture will inevitably be influenced by literacy practices at home. Therefore, I have prepared a representative (not comprehensive) review of published research in the past decade that is focused on preschool to early elementary grade children and addresses the relationship between a home literacy environment (HLE) and ELL literacy development. I will:\\n1. define the concepts of HLE and home literacy practices and briefly review relevant research regarding the relationship between home literacy practices, heritage language maintenance, and the acquisition of English as a second language. 2. identify three areas in which a HLE influences ELL literacy development: (a) language attitudes and parental beliefs; (b) identity formation; and (c) literacy behaviour of immigrant parents. 3. identify strategies that educators can use to work with newcomer families in support of their children\\'s literacy development.\\nA child\\'s literacy development involves home, school, and community support. In this paper, I focus the discussion on the literacy environment at home (Burgess, Hecht, & Lonigan, 2002; Sénéchal & LeFevre, 2002; Teale, 1986) . A HLE consists of a variety of attitudes, resources, and practices in the home that influence children\\'s literacy practices and development (Burgess, Hecht, & Lonigan, 2002) . In the literature, researchers define the HLE in a variety of ways. Teale (1986) , for example, categorize the HLE as a physical and social environment, defining it as (a) the physical environment where print exists; and (b) the social environment where children, siblings, and parents interact with print. However, this construct emphasizes the role of print in literacy development. Cope and Kalantzis (2009) suggest that it is important to consider multiliteracies, with \"linguistic, visual, audio, gestural and spatial modes of meaning becoming increasingly integrated in everyday media and cultural practices\" (p. 166). Alternatively, Burgess, Hecht, and Lonigan (2002) define the HLE as either passive or active. In a passive environment, parents indirectly model behaviours such as parental leisure reading, parental literacy beliefs, the number of books at home, and public library visits. In an active environment, parents engage children in literacy activities, such as shared reading activities. Similarly, Sénéchal and LeFevre (2002) define home literacy activities as informal or formal. Informal experiences focus on information in a storybook, such as the meaning of a story, while formal literacy experiences centre on print, such as talking about the letters or providing the names and sounds of specific letters. However, the definitions proposed by Burgess, Hecht, and Lonigan (2002) and by Sénéchal and LeFevre (2002) are from studies with English native speakers rather than English language learners. In addition, these definitions ignore the concept of multiliteracies.\\nOf these different definitions, I have adopted that of Teale (1986) ; however, I extend the idea of a physical literacy environment to include multiliteracy elements, such as the multimodal texts in Pokemon and Yugio characters presented in television, film, and game cards (Pahl, 2003) . Here, I focus on linguistic, visual, and audio elements of literacy behaviour without considering gestural and spatial modes of meaning in literacy activities. I maintain that a HLE consists of a number of activities that children observe at home and activities in which parents participate actively. A HLE also includes parental beliefs regarding literacy, the parental education level, the family socioeconomic status, the number of books at home, and daily life activities in the social domains (Auerbach, 1989; Teale, 1986; van Steensel, 2006) .\\nWith the understanding that literacy practices are socially constructed, children\\'s literacy experiences in daily life will inevitably influence their literacy development. Many different terms are used interchangeably in the literature (Street, 2000) to refer to literacy experiences, such as literacy events, literacy activities, literacy patterns, literacy strategies, and literacy situations. I use the term literacy practices to denote children\\'s multiple literacy experiences in home settings. The idea of literacy practice refers to a broad \"cultural conception of particular ways of thinking about and doing reading and writing in cultural contexts\" (Street, 2000, p. 22) and offers the potential to understand observable behaviour within different cultural contexts. That is, the concept of a literacy practice is broader than that of literacy events as the latter is used primarily in a descriptive way without offering any possibility of understanding how meaning in literacy is constructed (Street, 2000) .\\nTo attain an understanding of how a HLE influences ELL literacy development, current research has taken two approaches. One line of research examines the effect of the HLE on heritage language maintenance. A heritage language is a language other than English that is associated with an individual\\'s ethnic or cultural background (Chinen & Tucker, 2005) . In terms of order of acquisition, this is the first language for an individual; however, an individual may not completely acquire this language because of a transfer to the dominant language, such as English, in a host country (Valdés, 2000) . The role of heritage language maintenance in promoting second language development is well documented in the literature; heritage language maintenance is directly associated with English proficiency and academic achievement (Suarez, 2007; Yeung, Marsh, & Suliman, 2000) . Bilingualism is not a disadvantage for children who are acquiring literacy in a second language (Dickinson, McCabe, Clark-Chiarelli, & Wolf, 2004) . Cummins (1981 Cummins ( , 1983 proposes a common underlying proficiency model to explain this effect, where literacy-related skills are transferable across languages. Cummins (1981) argues that, given adequate exposure to a second language, concepts developed in the first language can be transferred. However, it may be noted that ELLs have diverse backgrounds in terms of heritage language proficiency. Some learners might develop their first language literacy in formal educational settings while others start and develop the language at home.\\nResearchers have identified a number of practices of a HLE that positively influence heritage language maintenance Farruggio, 2010; Lao, 2004; Wu, 2005) . These practices may include, but are not limited to, sending a child to a heritage language school, speaking a heritage language at home (Liao & Larke, 2008) , emphasizing the value of learning the language (Chinen & Tucker, 2005; Lynch, 2003) A second line of research uses a variety of methods and designs (e.g., ethnography and case study) to examine how home literacy practices influence ELL literacy development in second language acquisition (Garcia, 2008; Li, 2006a; 2006b; Menard-Warwick, 2005; Perry, Kay, & Brown, 2008; Reese & Gallimore, 2000; Zhang, 2007) . Given the complex and heterogeneous population of ELLs, qualitative researchers in the last decade have employed interviews, observations, focus groups, and documents in the study of the HLE (Shi, 2012) . These studies include analyses of how the HLE affects the literacy behaviour of Chinese children (Liu & Vadeboncoeur, 2010) , how immigrant parental beliefs affect literacy acquisition (Garcia, 2008) , and how Chinese immigrant family human and social capital affect literacy (Li, 2006a) . These studies examine the HLE of learners from many different ethnic backgrounds and demonstrate that a HLE shapes the development of English language literacy (Shi, 2012) .\\nIn summary, current research helps us to understand the scope and complexity of the HLE of immigrant families from diverse socioeconomic and ethnic backgrounds. However, there is a lack of consideration in the literature of how HLE influences ELL children\\'s literacy development in specific areas. It is important to connect both heritage language maintenance and learning English as a second language to ELL literacy development because these two areas may embody distinct HLEs in literacy development. Therefore, a review of the literature in both areas is of paramount importance.',\n       \"We designed a household questionnaire to capture the different dimensions of responsible consumption described above. Exploratory ethnographic work provided an opportunity to adapt the questions to the understandings and context of households in Quito (Maas, 2017 ; see also Chapter 7 , this volume). The questionnaire as a whole consisted of 78 questions, which also addressed topics other than responsible consumption: ten questions about general household characteristics, 22 questions about household food acquisition practices, and 36 questions about individual dietary practices and knowledge. Interviewers were trained by the lead authors in two-day workshops, followed by one day of practice interviews. The training included how to select the respondents within the selected households, how to ask each question, and how to record the data on Android tablets. For all data collection, interviewers used Android tablets with a pre-coded interview guide that was constructed using ODK ( https:// opendatakit.org/ ). The latter obviates a separate data entry step and permits daily monitoring of incoming data as soon as data are uploaded to a cloud-based server. Assessing responsible food consumption 201 2 0 1 In each of the three study counties, a two-staged, random sample of households was selected to represent both urban (64-74 per cent) and rural (26-36 per cent) populations. First, census sectors, subdivisions of counties defi ned by the Ecuadorian National Institute of Statistics and Censuses (INEC), were selected randomly. Within each manzana (roughly translates to 'neighbourhood') of the chosen census sector, ten dwellings were chosen randomly. As necessary in multi-household dwellings, one household was chosen randomly within that dwelling. Within each household, we explained the project objectives, sought written consent (authorized by the Bioethics Committee of the San Francisco University of Quito), and interviewed two people: a principal adult respondent who answered questions on food provision in the household, and a second adult respondent of the opposite sex. When there was more than one eligible principal or second adult, we randomized by selecting the one with the most recent birthday. Response proportions were high: Ibarra (1282/ 1475, 87 per cent), Quito (775/ 860, 90 per cent), Riobamba (858/ 896,96 per cent). For surveys in agroecological locations, the same team of interviewers visited agroecological fairs, markets, stores, and food basket distribution points. Interviewers approached shoppers as they were exiting after their purchases. They explained the study and, when consent was obtained, conducted the interview immediately, except for a few cases where arrangements were made to visit the shopper later in their homes. After the completion of a survey, the interviewers would repeat the process, approaching the next shopper who had completed shopping. The number of agroecological locations was greater in Quito (37) and Riobamba (11) than in Ibarra (6), resulting in larger numbers of respondents in the fi rst two counties (551, 299, and 48 respectively).\",\n       'The majority read data (the decision from at least three of five blinded readers; brain region either positive or negative) were used as the output data in this analysis. Data from each of the two population groups were tabulated in two ways: first as the percentage of the samples positive in all five read regions followed by the percentages positive in four, three, two or one region, and second as the percentage of samples positive in a particular region (e.g. frontal). Descriptive analysis of the data where appropriate was adequate for the assessment of the regional reads of the positive scans in both cohorts, as well as for the descriptions of the read characteristics of the clinical cases and the comparisons between visual and quantitative SUVR measures. Additionally, the Mantel-Haenszel test was used to assess the association between the two populations studied and the patterns of regional amyloid positivity. In these cortical regions, the intensity is higher in the abnormal (positive) image (>60% of maximum) than in the normal image (<60% of maximum), and in the abnormal image the intensity radiates to a sharply defined convex edge, but in the normal image the intensity tapers to the periphery of the tissue in these regions. Striatal gaps (s) between the thalamus and frontal white matter are visible in the normal image but not in the abnormal image as [ 18 F]flutemetamol shows consistent binding to striatal amyloid [12] . b Parasagittal images touching the medial surface of one hemisphere. Right The abnormal image shows increased tracer uptake (>60% of maximum) in the posterior cingulate/precuneal (pc) region, superior and posterior to the corpus callosum (cc). Left The normal image shows <60% of maximum intensity in the same regions. The pons (p) shows high intensity in this sagittal view (approximately 90% of maximum). c Coronal images (slices posterior to the corpus callosum). Left Normal images show a white matter sulcal/gyral pattern. Right the abnormal image loses this pattern and also shows increased uptake in the posterior cinguli (pc) and an increased radial extent of high intensity uptake to the lateral surfaces of the temporoparietal lobes as well as the insula region. The inferior parietal (ip) region in particular shows increased intensity and is a robust read region as is less susceptible to atrophy [12] Results',\n       \"Introduction: There are two groups of methods to visualise water motion using MRI, namely, phase contrast methods and spin labelling methods. The phase contrast methods measure each voxel's average velocity, which may become a significant drawback when measuring flow where large intra-voxel velocity gradient exists, like the aqueduct.\\nIn the spin labelling techniques, the observation duration is limited to several seconds due to T1 relaxation. Here we propose a new technique that can visualise relatively slow flow continuously. Methods: The pulse sequence is based on SSFP (steady-state free precession), and positively utilises the dark band artifacts of the SSFP sequence. A relatively large field gradient is introduced along one direction to make equally spaced dark bands appear. When the CSF moves, the dark bands follow the motion of the water. We developed a modified version of SSFP, where the direction and the interval of the bands are freely controlled by the operator. A numerical simulation was also performed to investigate the precise relationship between the band motion and the actual flow. Results: CSF flow was clearly visualised with healthy volunteers. The imaging speed was 9 frames/s, and the in-plane spatial resolution was about 2 mm. It was possible to acquire images continuously for 60 s.\",\n       'Objective To investigate the effects of genetic risk of Alzheimer disease (AD) dementia in the context of β-amyloid (Aβ) accumulation.\\nWe analyzed data from 702 participants (221 clinically normal, 367 with mild cognitive impairment, and 114 with AD dementia) with genetic data and florbetapir PET available. A subset of 669 participants additionally had longitudinal MRI scans to assess hippocampal volume. Polygenic risk scores (PRSs) were estimated with summary statistics from previous large-scale genome-wide association studies of AD dementia. We examined relationships between APOE e4 status and PRS with longitudinal Aβ and cognitive and hippocampal volume measurements.\\nResults APOE e4 was strongly related to baseline Aβ, whereas only weak associations between PRS and baseline Aβ were present. APOE e4 was additionally related to greater memory decline and hippocampal atrophy in Aβ+ participants. When APOE e4 was controlled for, PRS was related to cognitive decline in Aβ+ participants. Finally, PRSs were associated with hippocampal atrophy in Aβ− participants and weakly associated with baseline hippocampal volume in Aβ+ participants.\\nGenetic risk factors of AD dementia demonstrate effects related to Aβ, as well as synergistic interactions with Aβ. The specific effect of faster cognitive decline in Aβ+ individuals with higher genetic risk may explain the large degree of heterogeneity in cognitive trajectories among Aβ+ individuals. Consideration of genetic variants in conjunction with baseline Aβ may improve enrichment strategies for clinical trials targeting Aβ+ individuals most at risk for imminent cognitive decline.',\n       'The CAFE study compared the effect of beta-blockers on the central pressure for a similar peripheral BP, and the atenolol/ thiazide group showed higher aortic central pressure values when compared with the amlodipine/perindopril group. 66 Nebivolol (a beta-blocker with a vasodilatory effect) and carvedilol (an anti-hypertensive with alpha-and beta-blocking effects) compared with atenolol promoted a greater reduction in central aortic pressure and pulse amplification. 72, 73 Nebivolol reduces central aortic pressure and the augmentation index in mildly hypertensive patients after 3 months of treatment. 74 ',\n       nan,\n       'Bottom stress in the 2DDI version of ADCIRC is generally expressed as: Depending on the form used for τ* the result is either a linear, quadratic or hybrid function of depth-averaged velocity. For most coastal applications, quadratic friction should be used with a drag coefficient, C f ∼ 0.0025. In very shallow water, hybrid friction may be useful with C fmin ∼ 0.0025, particularly when wetting and drying is included since this expression becomes highly dissipative as the water depth becomes small. Linear friction is primarily useful for model testing or when a totally linear model run is desired. In this case the magnitude of τ* should be consistent (at least in order of magnitude) with a value that would have been computed using the quadratic friction expression and not with the value of C f that would normally be used in the quadratic expression. The description of formulation based on the form of τ* is given here. Linear friction: τ* = C f where C f = constant in time (may vary with space), read in model as input, unit s -1',\n       \"A validation study has been conducted on the brain MRI data drawn from the Alzheimer's Disease Neuroimaging Initiative database (http://adni.loni. usc.edu). As a pilot study, 100 brain images were randomly picked: 34 healthy subjects, 33 with mild cognitive impairment (MCI) and 33 AD patients, with a demographic profile presenting no statistically significant difference on age and MMSE score from the whole database (p − value > 0.1). All images were bias-corrected and linearly aligned in the widely-used MNI152 template space. Each image also includes a reference manual segmentation of the hippocampus. A sample image superimposed with its label map is shown in Fig. 1 .\",\n       \"All the participants were asked to list those features of the SciPack that they felt were most valuable in helping them learn the content. Twenty-seven of the respondents mentioned that the visuals, animations, and simulations were well done and complemented/clarified the text for them. Five of the respondents indicated that the immediate feedback associated with the practice and quiz items was the feature that helped them the most. Four participants indicated that the on-demand nature of the entire experience helped them the most because they could access what \\nOnly the scores of participants who completed both the pretest and the posttest were included in the analyses *p \\\\ 0.01, **p \\\\ 0.05 (difference between pretest and posttest scores on a paired sample t-test) Total n = 13 n = 0 n = 29 n = 18 n = 3 n = 27 28.9% 0% 64.4% 40% 7.4% 60%\\n''How would you rate your confidence in your ability to teach physical science concepts related to Newtonian force and motion?''\\nOverall n = 45 [completed both pre-and post-survey] they wanted, when they wanted in order to help them learn what they needed to learn. And three of the respondents felt that the audio feature (the ability to have the text read out loud) was the most valuable feature.\",\n       'The familial AD forms are relatively rare, less than 10% and have an autosomal dominant pattern. The debut occur at an early age (Early Onstet AD or EAOD), characterized by cognitive impairment associated with other neurological signs such as spasticity, motor control disorders, ataxia and seizures. ',\n       'The Program Manager will provide the following hardcopy reports to the NHEP: • Interim Report #1 (due 6/30/03): An update on all activities between 4/1/03 and 6/30/03, including completion of a QA Project Plan. • Interim Report #2 (due 9/30/03): A one page update on all activities between 7/1/03 and 9/30/03. • Final Report (due 12/31/03): The final report will contain ArcInfo files of eelgrass distribution throughout the Great Bay Estuary, all necessary documentation/metadata for the ArcInfo files, and a final report describing the results and any deviations from the protocols established in the QA Project Plan.',\n       'It is not unreasonable to assume there may be significant correlation among the farm-level variables. For example, larger farm operations may be likely to have more higher revenue per acre or at least lower expenses per acre due to economy of scale advantages. In addition, higher input costs are likley associated with higher revenue, after all, more nitrogen results in higher yields up to a certain point on the yield curve. Further, as just discussed, many of the variables included could possibly be correlated with coverage level selection. Thus multicollinearity and engodeneity are persistant issues when using farm level variables. Wooldridge (2015) states that the problem of multicollinearity cannot be clearly defined, however all else equal, for estimating coefficients it is better to have less correlation between our variables. The presence of multicollinearity inflates the variance and standard errors of the estimated parameters and can therefore reduce the signicance of the coefficients. For a given dataset, multicollinearity can be reduced by dropping indpendent variables from the model. However, this can introduce bias if the dropped variables belong in our model. In order to counter this, the scope of this thesis was reduced in term of variables of interest, with the acres leased and several financial variables which exhibited significant correlation with other controls removed. The consistency of the statistical significance of the each of the variables of interest proved particularly robust however, for the most part staying consistent regardless of the model specification that was run and the variables that were included. Correlation matrices between each of the variables included in the models are provided Figures 7.1, 7.2, and 7.3 in order to assist in determining potential multicollinearity concerns. The variation inflation factors (VIF) for the models were also computed, which serve as an indicator of multicollinearity. A VIF of 8 for a particular variable would imply that the standard errors are 8 factors higher than if there were no multicollinearity between that variable and the remaining independent variables. Commonly a value of 10 is recommended as a maximum (Kennedy, 1992). However lower recommended values can be found in the literature. When the VIF were calculated for each of the final models, the majority of the VIF for both the variables of interest and controls were between 1 and 2. The largest VIF tended to be for Gross Income per Acre and Variable Costs per Acre, which ranged between 3 and 5. This was unsurprising after reviewing the correlation matrices shown below. We found these to be relatively reasonable given the correlation typically found between farm level variables and the consideration that both of the variables belong in the model and thus left the final models unchanged.   Table 8.1 include six regression specifications. From left to right, the results in column (1), the OLS BR&SOB the (Ordinary Least Squares with Base Rate and Summary of Business data) model, and column (2), the FE BR&SOB (Fixed Effects with Base Rate and Summary of Business data) model, include the two base rate (BR) variables, BaseRate70 and BR slope, as well as the two Summary of Business (SOB) variables, the 5-year loss ratio (LR) and the 5-year earned premium rate (EPR). Column (1) does not include fixed effects, while column (2) includes county fixed effects. The results in columns (3) and (4), the OLS BR (Ordinary Least Squares with Base Rate data) (3) and FE BR (Fixed Effects with Base Rate data) (4) models, do not include the SOB variables LR and EPR, while the results in columns (5) and (6), the OLS SOB (Ordinary Least Squares with Summary of Business data) (5) and FE SOB (Fixed Effects with Summary of Business data) (6) models, do not include the two BR variables. To clarify, models (2), (4), and (6) are identical to models (1) and (3), and 5respectively, but include state fixed effects. Models 5 and 6 include additional observations and states, due the larger dataset they are run on, since the first four models include the base rate variables which limit them to counties and states where the Area Yield Protection plan of insurance is offered. Though the AYP counties do account for the majority of the production and thus most of the corn belt, the observations outside of the AYP offer area are not included except in columns (5) and (6) due to the unavailability of the base rate data. Results are attained across all farms in the annual ARMS sample across the United States for all corn growing states.',\n       nan,\n       \"Since, the program does not include any evaluation of pupils' achievements, it was necessary to build an ad hoc method to estimate the program's impacts. The method includes a control group and consists in surveys administered before and after the project. Since the survey was computerized, only pupils from 2nd to 5th grades were included. The questionnaires used include several indicators as well as questions about individual characteristics.\\nTo measure the impact of the project, several types of indicators were considered: the attitude toward reading (taste of reading, practice of reading, knowledge about books and authors, etc.), the attitude during school life (attitude during class, school life activities, self-evaluation, etc.), and academic abilities. For reading and school attitudes we re-used some questions from PIRLS. We will report two aggregated scores (on 10): the Student's Attitude Toward Reading (SATR) and the Student's Reading Self Concept (SCRC).\\n2 Measuring academic abilities is done using exercises issued from French national evaluations 2 SATR is based on students' agreement with the following statements: I read only if I have to; I like talking about books with other people; I would be happy if someone gave me a book as a present; I think reading is boring; I enjoy reading. The SCRC is based on students' agreement with the following statements: reading is very easy for me; I do not read as well as other students in my class; and reading aloud is very hard for me.\\nthat are set at the beginning of 3rd and 6th grade. We use the 3rd grade evaluation exercises for 2nd and 3rd grade pupils and the 6th grade evaluation exercises for 4th and 5th grade pupils. Three types of skills have been studied: identifying the nature or the type of a text, processing information, and making inferences. These three skills represent 10% of the national evaluation of reading, which is marked out of 100 and thus we use a score out of 10 for these skills.\\nThe collection of the individual characteristics was limited since it was not possible to send a questionnaire to families. A few individual characteristics were gathered directly from pupils: sex, age, month of birth, language spoken at home (the variable French principally says that the pupil 'always speaks' or 'almost always speaks' French at home, the variable African languages says that the pupil knows a sub-Saharan African language and similarly for Arabic and Asian languages), housing conditions (the variable Own bedroom says that the pupil has his/her own bedroom). Furthermore we have some overall data on the social composition of each school and this indicator is a good measure of pupils' social environment.\\nThis data collection has been done with a set of three questionnaires completed on-line during school time. The timeline was the following: pupils replied to the first questionnaire one week before the implementation of the project, to the second in the week following its execution and to the third about two months later. Our analysis is focused on schools which followed the project between November 2007 and March 2008. Six schools were concerned and we gathered data on more than 400 pupils with around 100 pupils for each grade. In order to take into account this time gap in the data collection, we have used a variable time of passage which indicates the month during which the data was collected: it takes a value of 1 for September and 12 for August; furthermore if the date of passage was t for the first questionnaire, it takes the value t + 1 for the second and t + 3 for the third. The same timing has been respected for participating and control groups.\\nThe first questionnaire was the longest, with 40 questions and 3 exercises. The second was the shortest with only 8 questions and 2 exercises, and the third contained 27 questions and 1 exercise. The exercises were different in each round, and to take into account differences in difficulty, the order of passage was randomized such that half of each class had the first order and the other half the second order.\\nLet us precise now how we select the treatment schools and the control schools. To benefit from an Action Lecture program, schools apply voluntarily then a selection committee chooses which schools to admit. Application to the program is open to all nursery and primary schools in Paris. The head teacher and his colleagues have to propose a project that is consistent with the Action Lecture guidelines (2 weeks without classes, intervention of external professors,). During the year of the evaluation, the number of applicants was very low and all applicant schools were admitted into the program. Thus for the selection of the school, it was not possible to apply a standard randomized process to select the treatment schools and the control schools (see Duflo, Kremer, and Glennerster (2008) for the randomized methodology in evaluation). The control group was constituted by classes in non concerned schools, from which we had to seek agreement. As this evaluation was quite intrusive for the class, the control group was relatively small. We were limited to three classes (3rd, 4th and 5th grades) that we chose in three different schools that were similar to the treated schools in terms of socio-economic characteristics.\",\n       'For classification we used two different types of classifiers: (i) a multivariate Support Vector Machine (SVM) [29, 30] with a radial basis function (RBF) kernel and (ii) a univariate Naïve Bayes (NB) classifier [28] as baseline. SVM performed highly accurately in former studies, e.g. in Klöppel et al. [31] , Plant et al. [27] , Cuinget et al. [32] , and Grañ a et al. [19] . In contrast, NB is simple and efficient but relies on the naïve assumption of statistical independence of the features. Under this assumption this algorithm is statistically optimal regarding the minimal error rate. Preceding results, e.g. Plant et al. [27] , showed that NB performs well for the detection of AD vs. HC even if this assumption of statistical independence of different features is not correct.\\nFor the SVM we needed to define two parameters including the complexity or cost constant C and the radial basis function kernel width (c.0). The parameter .0 determines the trade-off between margin maximization and training error minimization for the soft margin SVM. To estimate suitable values for C and c we used a grid search in the range of C = 2',\n       'Genotype and Neuropsychological Characteristics of Different Regional Patterns of MRI Atrophy The comparative demographic, clinical and cognitive features of different MRI patterns of atrophy were examined for CN, aMCI and AD subjects, contrasted with a control group of CN subjects with scans showing no atrophy (CN MRI− ) (n = 144) ( Table 3 ). There were no statistically significant differences with regards to age [F (5,442); p = 0.056], although there was a trend for the NeoC-2 Only 18.3% (L = 11.4%) (R = 13.4%) 12.6% (L = 9.9%) (R = 9.6%) 6.2% (L = 8.2%) (R = 5.5%) Table 2 , it is evident that, as compared to those who had no regional atrophy (\"MRI Normal\"), the Limbic pattern had significant atrophy in the limbic structures (hippocampus, Limbic structures only). NeoC-1 subjects to be the youngest group and the Limbic + (NeoC-1/NeoC-2) subjects to be the oldest group. The NeoC-2 subjects had the lowest percentage of females (25%), whereas the CN MRI− subjects had the highest percentage of females (50%). There were also group differences with regards to APOE ε4 frequency ([χ 2 (df = 5) = 56.84; p < 0.001] with NeoC-1 (70.4%) and combined Limbic + (NeoC-1/NeoC-2) subjects (66.1%) having the highest ε4 frequencies, which were significantly higher than CN MRI− (25.7%) and NeoC-2 subjects (39.2%) who had the lowest APOE ε4 frequencies.\\nIn Table 3 , it can be seen that the control group, namely (CN MRI− ) subjects, scored higher on all neuropsychological and functional measures than subjects with any pattern of atrophy including NeoC-2. NeoC-2 subjects generally had the least impairment on all tests scores, with the exception of scores on Trails A and B and Category Fluency tests, whereas subjects with a predominant Limbic + (NeoC-1/NeoC-2) pattern generally had the most impaired scores on all tests, with the exception of Trails A and B tests. Subjects with a predominant NeoC-2 pattern had significantly better scores than participants with the NeoC-1 and the Limbic + (NeoC-1/ NeoC-2) regional patterns of atrophy on CDR sum of box scores, MMSE, delayed memory for passages and AVLT total scores. In addition, the NeoC-2 subjects had the least impaired scores on tests of memory (WMS and AVLT, overall cognition (MMSE score) and functional ability (CDR score) relative to NeoC-1, NeoC-1 + NeoC-2 and Limbic + (NeoC-1/NeoC-2) subjects.',\n       \"Spyridoula Vazou, Iowa State University-Ames; Collin A. Webster F , Chelsee Shortt, University of South Carolina-Columbia (svazou@iastate.edu) Background/Purpose: In the context of comprehensive school physical activity (PA) programs, movement integration in the academic classroom has been considered effective for the promotion of PA among children. However, efforts for the professional development and motivation of classroom teachers to successfully implement this strategy are vague. Preservice education provides a promising platform for early intervention with classroom teachers to increase their desire and competency to engage in PA promotion. Online communities of practice (CoP) have been used extensively in education for professional development. An online CoP can provide training, support, and a platform for professional interactions related to movement integration. Incorporating online CoP in preservice education constitutes a new strategy that hasn't been evaluated before in the context of classroom PA promotion. The purpose of this study was to provide evidence on the efficacy and effectiveness of an online CoP as a potential tool to motivate, educate, and support preservice teachers to integrate movement in their future classrooms. Specifically, this study examined the effect of an online CoP within a PA promotion course for preservice teachers (intervention), compared to a traditional course (control), on perceived competence, values, barriers, and intentions for movement integration in their future classrooms. A secondary aim was to assess the feasibility of the CoP in preservice education. Method: A total of 71 preservice classroom teachers (M age =22.47±4.55) from two universities were randomly assigned to the intervention (n=36; 2 course sections) and the control (n =35; 2 course sections) group over a semester. Participation in the online CoP (named Move for Thought) lasted for 10 weeks and was implemented as part of an assignment in the course. Before and after the intervention a survey measuring perceived competence, value, barriers, and future intentions on movement integration from existing valid and reliable questionnaires was administered to both groups. Additional data on the experience from the CoP (value, enjoyment and intentions to visit the CoP) were collected from the intervention group. Analysis/Results: Internal consistency was acceptable for all measures (α>.70). Repeated-measures analysis of variance showed a significant increase in perceived competence [F(1,69)=9.18, p=0.003 η 2 =.12] and value [F(1,69)=7.36, p =0.008 η 2 =.10] for movement integration in the intervention group, compared to the control group, which either remained stable or decreased over time. The intervention group also had a significant decrease in perceived barriers for learning new content related to movement integration [F(1,69)=5.28, p=0.025 η 2 =.07], compared to the control group. No interaction effect on intention emerged. The experience from the CoP was highly valued (M=5.87±0.87) and enjoyable (M=5.01±1.20) for the participants who also reported they intended to visit it in the future to get additional knowledge on movement integration (M=5.34±1.17). Participation in the CoP included, on average, 9.45 active days, and ranged from 6 to 42 posts (M=23.80 ±6.78) per participant who received, on average, 12 comments per post they shared. Conclusions: This study provides evidence on the effectiveness of an online CoP within preservice education for the promotion of classroom PA as well as recommendations to easily implement online CoPs to preservice teachers. Background/Purpose: Physical Education (PE) teachers hold a unique position in schools as being one of only a few teachers in a school building that have multiple opportunities to build relationships with students over consecutive school years. For the students in high-poverty schools whom oftentimes grow up without a consistent supportive adult in their lives, PE is uniquely positioned to provide consistent opportunities for personal wellness and social-emotional development to occur over many years. Little is known about PE teachers with resilient dispositions and the psychological factors that promote resilience. The purpose of this study was to investigate PE teacher resilience by examining teacher workplace psychological flexibility relative to intention to remain teaching in highpoverty schools. Method: A random national sample of physical educators working in high-poverty schools representing each SHAPE America regional district participated in a survey measuring resilience, workplace psychological flexibility, and teachers' intention to remain teaching in high-poverty schools. In total, 540 teachers (25% response) completed electronic questionnaires. Analysis/Results: A random national sample of physical educators working in high-poverty schools representing each SHAPE America regional district participated in a survey measuring resilience, workplace psychological flexibility, and teachers' intention to remain teaching in high-poverty schools. In total, 540 teachers (25% response) completed electronic questionnaires. Frequencies and percentage of total responses to questions regarding resilience, psychological flexibility, and intent to remain teaching in high-poverty schools were calculated. The survey items were analyzed through factor analysis and structural equation modeling as part of procedures to establish the survey as a valid and reliable instrument to measure resilience and psychological flexibility and predict teacher likelihood to remain in high-poverty schools. The majority of teachers in this study (76.9%) were considered to have elevated resilience according to the Connor-Davidson Resilience Scale10 (CD-RISC 10). Related to the measure of workplace psychological flexibility (WAAQ) a high-percentage of teachers (82.8%) were found to have increased workplace psychological flexibility. More than half of teachers surveyed (60.3%) reported that they intended to remain teaching in their current high-poverty schools for the next three school years. Conclusions: School districts can create environments that either support or inhibit teachers' attitudes about their jobs by the organizational structures and cultures they create in schools and through the relationships they foster. Physical education teachers possessing the individual dispositions toward increased resilience and psychological flexibility are more likely to remain teaching in high-poverty schools. Because teacher attrition has a negative influence on the educational system-especially in high-poverty schools, providing resources to build resilience and psychological flexibility in teachers is critical to their professional success and development as well as for the success of high-poverty schools.\",\n       \"International Journal of Gender and Women's Studies, Vol. 5(1), June 2017\\nThe survey was conducted once every two years from 1995 to 2010, and is a rich source of information on careers of doctoral recipients. The results indicate that the family wage gap does exist among women with Ph.Ds and is about 6.8 percent. However, most of the existing wage difference can be explained by one, human capital characteristics such as: field of majors, school quality, and years since Ph.D. Two, maternity leaves and other selfreported-career breaks adjusted experience.\\nThree, current job characteristics such as: extent to which job is related to the highest degree, authority level, tenure status, occupation major group, choice to work part-time, working outside field of research, employment type, experience in current job, average number of hours worked, and average number of weeks worked in a year. This wage gap increases with the number of children. Higher wage penalties are observed for married women, women with younger children, and women who have children before age 36. Though there is extensive literature on the family wage-gap, this study adds to the existing literature by examining a group of women with doctoral degrees. Estimating the FWG and the factors explaining the existence of FWG among these highly skilled women is interesting from the policy perspective. As, the former provides information on the cost of choices that are related to child bearing, and the later helps in formulation of family policies that may make motherhood lest costly for highly skilled female scientists, doctors and engineers.\",\n       nan,\n       'The copyright holder for this this version posted December 9, 2020. ; https://doi.org/10.1101/2020.12.08.20245803 doi: medRxiv preprint',\n       '\\n',\n       'According to modern expectancy-value theory, students\\' motivation in school subjects begins to vary at the very beginning of their school careers, showing a task-specific pattern of motivation. However, there is no clear evidence in the literature on how students\\' value beliefs are formed and interact with each other in early elementary schools. Using the longitudinal structural equation modeling, this study examined relations between science-related task values (i.e., intrinsic value and cost), self-concept of ability, and future occupational aspirations based on first graders and 1-year follow-up from seven schools in Helsinki (N = 332; ages = 7 and 8 years; girls = 51%). Results showed that the students who had a high science-related self-concept of ability and intrinsic value tended to perceive low cost of science learning. Science-related self-concept of ability was the most stable construct, while in intrinsic value and cost, there were significant levels of fluctuation across the first and second grades. A high science-related self-concept of ability in the first grade predicted a lower cost value in the second grade, and a high science-related intrinsic value was a marginally significant predictor of future occupational aspirations in science, technology, engineering, and mathematics (STEM). Mean-level differences revealed that the girls\\' science-related self-concept of ability, intrinsic value, and cost remained the same in both grades, while the boys\\' self-concept of ability decreased. The girls\\' mean levels in science-related intrinsic value were higher than those of the boys, while students\\' self-concept of ability and cost were similar across gender in both grades. A cross-lagged panel model revealed that the girls reported more STEM occupational aspirations than the boys in the second grade, while controlling for the motivational beliefs. In summary, the results indicate that a high-level of science interest in young students predicts STEM occupational aspirations; high girls\\' intrinsic value in early science education does not steer them away from STEM occupations; boys\\' task motivation might be at greater risk of decline during early science education.\\nKeywords: expectancy-value theory, intrinsic value, cost, self-concept of ability, STEM occupational aspirations, gender differences, elementary students (OECD, 2015) , stimulating research into possible explanations. Some findings suggest that stereotypes might play a role in students\\' future occupational aspirations and that women have not stereotypically been seen as scientists (Chambers et al., 2018; Miller et al., 2018) . Female students\\' self-concept of ability in science is lower than that of male students, despite the fact that girls often outperform boys in school science (Watt, 2016) . There are also gender differences in attitudes to science among Finnish students, with boys showing higher enjoyment and girls holding higher instrumental value in science learning (OECD, 2016) . Still, the gender gap favoring boys in bachelor\\'s degrees obtained in science remains significant in Finland (OECD, 2015) . High academic motivation in science has been positively linked to deeper engagement, persistent learning, better knowledge acquisition, and higher aspirations in that domain, which prepares individuals to pursue further education and careers in STEM fields [see Wang and Degol (2013) for a review].\\nUntil now, studies of science motivation and STEM aspirations and their development have mainly focused on middle and high school students. In order to form a more comprehensive picture of the phenomenon, and to understand middle and high school students\\' science-related motivation and aspirations regarding STEM, we need to examine the point at which decline in students\\' science motivation begins to emerge, how stable their motivational beliefs are, and what factors relate to this development. Thus, in order to investigate the trajectories of STEM motivation from the beginning of elementary students\\' school careers, we need to address their science-related motivation. In addition, most previous studies of young students\\' STEM motivation have been conducted in the United States; therefore, more research is needed in other educational contexts.\\nIn this study, we draw on the modern expectancy-value theory (EVT) framework (Eccles, 2009) . EVT (Eccles et al., 1983; Eccles, 2009 ) posits that achievement-related performance and choices are most directly influenced by students\\' expectations of success on achievement-related tasks and their subjective assessments of the relative value of different achievement-related tasks. Eccles\\' EVT of achievement-related choices is a major theoretical framework for studying achievement motivation. It has been widely used to tackle both individual and gender differences in educational and career choices [see Wang and Degol (2013, 2017) and Watt (2016) for reviews]. Within the academic domain, Eccles et al. (1983) operationally defined expectancies of success as children\\'s beliefs of how well they will do on an upcoming task. Expectancies of success are children\\'s evaluations of their current abilities as well as how they think they compare to other students (Wigfield et al., 2016) in a given task. Thus, we use the term task-specific self-concept of ability. Wigfield and Eccles (1992) and Eccles and Wigfield (2002) also distinguished between multiple components of subjective task values: intrinsic value (enjoyment or liking), utility value (the usefulness of a task for helping to fulfill personal goals), attainment value (relevance of a task to one\\'s sense of self, identity, and core personal values), and costs (perceived negative aspects of making a specific choice).\\nIntrinsic value refers to the extent to which an individual gains enjoyment from performing an activity (Eccles, 2009 (Eccles, , 2011 . Cost refers to the things that students perceive they are investing or giving up in order to engage in a task (Flake et al., 2015) , including the degree of potential loss of time; effort demands; the loss of valued alternatives, such as spending time with friends; and additional negative experiences, such as stress. According to EVT (Eccles et al., 1983; Eccles, 2009) , people are most likely to select those tasks for which they hold the highest expectations for success and the highest levels of subjective task value.\\nStudents\\' achievement-related beliefs and attitudes play an important role in academic environments by directing their behavior and effort in learning situations (Eccles, 2009; Marsh and Martin, 2011) . Students who have a positive self-concept of ability and intrinsic value in specific academic subjects are likely to perform better and be more engaged in school than those who have a less positive self-concept of ability in a given subject. Previous studies have shown that students hold high beliefs in their abilities at the beginning of elementary school and that they are highly optimistic about their competences in different areas and domains (Stipek and Mac Iver, 1989; Wigfield et al., 2016) . The trajectories of students\\' self-concept of ability decline from the elementary to the middle and high school years, although some domain-specific features can be identified in these emerging trajectories (Jacobs et al., 2002) . Vinni-Laakso et al.\\nStudies have provided several reasons for the decline in students\\' self-concept of ability, including children\\'s developmental changes, increasing social comparisons among students, and environmental changes in the school context, for example, increasing numbers of subjects and teachers and greater emphasis on grades (Stipek and Mac Iver, 1989; Marsh et al., 1995; Harter, 2012) . Intrinsic motivation is well known to be associated with ability beliefs, i.e., those with a higher self-concept of ability are more willing to engage in learning processes and enjoy learning. Thus, it is no wonder that, upon entering school, elementary students have high academic interest (e.g., Viljaranta et al., 2016) . However, this interest soon starts to decline across domains (Wigfield et al., 1997; Gottfried et al., 2001; Jacobs et al., 2002) . Furthermore, previous EVT studies have tended to show moderate to large correlations between academic self-concept of ability and the value components [see Wigfield and Eccles (2002) and Wigfield et al. (2009 Wigfield et al. ( , 2016 for reviews]. Self-concept of ability is more highly correlated with intrinsic value than with the other value components within a specific domain (Wigfield et al., 2009 (Wigfield et al., , 2016 , and the positive correlation between task motivation and self-concept of ability has been found to strengthen with age (Wigfield et al., 1997; Fredricks and Eccles, 2002; Jacobs et al., 2002) .\\nA general downward shift in students\\' interest from elementary to middle school is also evident in the science domain (Gottfried et al., 2001) . In general, academic interest is rather stable, and this stability increases with age across subject domains (Gottfried et al., 2001 ). This kind of development poses a challenge for children who begin their school careers with low science motivation. The trend of declining motivation affects students\\' achievement and increases the risk of dropout from science education later on and, as a consequence, from STEM careers.\\nIn addition to low interest, students might also experience costs that affect their motivation as a negative valence of a task . Cost is the least studied component of EVT, and it is distinct from other components of the EVT model (see Flake et al., 2015) . In addition, students have been found to report different types of cost, some of which include the requirement of too much effort, emotional/psychological demands, and loss of other valuable opportunities. It has been shown that perceived cost can detract students from engaging in a task or activity and that it can be a powerful predictor of career and education-related outcomes in middle school, high school, and college students (e.g., Battle and Wigfield, 2003; Wigfield and Cambria, 2010; Perez et al., 2014; Flake et al., 2015; Jiang et al., 2018) . However, research on elementary students\\' EVT motivation including perceived cost is lacking.\\nTo sum up, in general, self-concept of ability and intrinsic motivation decrease as students proceed from the first grade onward. Alongside this development, interest may play a role in the formation of self-concept of ability. However, little is known about elementary students\\' experience of cost, particularly in the context of science. First graders would probably engage in a task that is interesting and fun rather than a task that they evaluate as useful for their future or important for their personal selves (Wigfield and Eccles, 1992; Eccles and Wigfield, 2001) . In a similar vein, students would most probably disengage from tasks they perceive as overly demanding or emotionally exhausting. In this study, our primary aim is to explore the stability and associations among first graders\\' science-related self-concept of ability, intrinsic value, and perceived cost during a 1-year follow-up.\\nResearch based on EVT has demonstrated that self-concept of ability and value beliefs represent the most proximal precursors of academic achievement, effort, school engagement, and educational aspirations (e.g., Marsh et al., 1995; Eccles, 2009; Watt et al., 2012; Guo et al., 2015a) . Several studies showed that interest in a certain subject domain is related to academic achievement in that domain (e.g., Harackiewicz and Hulleman, 2010; Guo et al., 2015b Guo et al., , 2017 . Positive associations of intrinsic value and self-concept of ability with achievement have also been found among elementary school students (Denissen et al., 2007; Viljaranta et al., 2014) and in the context of science (Guo et al., 2018b) . According to EVT, the cost component of task values is assumed to dampen students\\' motivation, and it is strongly and negatively related to expectancy and moderately and negatively related to value, long-term interest, course grades, and overall motivation (Flake et al., 2015) . Thus, it is important to differentiate and consider cost, self-concept of ability, and intrinsic value among elementary school students to further disentangle the relationships between self-concept of ability and positive and negative value beliefs in achievement-related outcomes of future occupational aspirations.\\nThe recent study on elementary students\\' career aspirations demonstrates that children\\'s aspirations are shaped from a young age (Chambers et al., 2018) . Students\\' attitudes shape their interests and later behavior. Thus, student motivation determines the choices students make about their educational pathways. Highly motivated students are more likely to choose courses and aspire to careers that correspond with the subjects in which they are motivated (Simpkins et al., 2006; Chow et al., 2012; Wang, 2012) . Students\\' intrinsic value and academic self-concept in mathematics and science have been found to predict their STEM aspirations in middle and high school (Wang and Degol, 2013; Guo et al., 2015b) . Moreover, longitudinal tracking has showed that students who do not express STEM-related aspirations at the age of 10 years are unlikely to develop STEM aspirations by the age of 14 years, and consequently are less likely to pursue science subjects (Archer et al., 2013) . In contrast, perceived cost is a negative predictor of interest and performance outcomes (Perez et al., 2014; Barron and Hulleman, 2015; Flake et al., 2015; Jiang et al., 2018) . These findings further underline the importance of student motivation for long-term academic and career success. However, whether these motivational beliefs are related to elementary students\\' future occupational aspirations has not Young Finnish Students\\' STEM Motivation been tested. In fact, we know very little about the factors that influence early career aspirations, despite the fundamental role of aspirations in individuals\\' career choices and development throughout the lifespan. Of particular relevance is that no previous study has integrated science-related self-concept of ability, intrinsic value, and cost to determine the extent to which these beliefs and emotions relate to future occupational aspirations among early elementary students. Thus, we have chosen to focus on self-concept of ability and positive and negative aspects of task value -namely, intrinsic value and cost -to examine the associations of these constructs among first and second graders in science learning, and how these constructs are related to students\\' occupational aspirations.\\nPrevious findings show that students\\' self-concept of ability and intrinsic value become gendered, especially in relation to mathematics and literacy (see Eccles et al., 1993; Wigfield et al., 1997; Jacobs et al., 2002) . Girls\\' self-concept of ability in mathematics is found to be lower than that of boys, but girls show a lower decline over time (Fredricks and Eccles, 2002) , indicating that the gender gap decreases over time. Meanwhile, boys\\' self-concept of ability in language and arts is lower and declines more than that of girls (Jacobs et al., 2002) . According to a recent meta-analysis (Miller et al., 2018) , the last five decades have witnessed a developmental change in children\\'s gender-science stereotypes. In a draw-a-scientist study, children in the 1960s almost exclusively depicted scientists as males; in 2000, significantly more children depicted female scientists than their 1960s counterparts. In another study, Bian et al. (2017) observed that not only gender-science stereotypes were still prevalent, but they also started to emerge early. Bian et al. (2017) also found that children perceived males as more intellectual than females, which had a clear influence on their interests in selecting tasks that are described to be easy or difficult, even at the age of 6 years. However, no gender differences in science-related self-concept were found in preschool and early elementary school-aged children (Leibham et al., 2013) .\\nBoys have been shown to hold higher interest in science in early education, although high science interest in preschool predicted higher self-concept and achievement for 8-year-old girls (Leibham et al., 2013) . Particularly in the case of the early elementary school years, it appears that interest helps build a higher self-concept of ability. Various studies have found that, as early as elementary school, boys hold higher intrinsic values in mathematics, while girls hold higher intrinsic values in language (Eccles et al., 1983; Jacobs et al., 2002) . These gendered value beliefs also feature among secondary school students (e.g., Gaspard et al., 2015) .\\nAs noted, cost is salient in student motivation and is linked to several educational outcomes (Flake et al., 2015) . Watt (2016) investigated adolescents\\' gender differences in science and found different types of cost to be differently gendered. For example, girls experienced greater psychological cost (e.g., \"It frightens me that math/science courses are harder than other courses\"), while boys experienced more social cost (e.g., \"I\\'m concerned that working hard in math/science classes might mean that I lose some of my close friends\") in their science learning in Grade 10. In terms of effort-related cost, no gender differences were found. In this study, we are interested in studying whether the experience of cost emerges from the first school years and whether this experience is gendered. This might provide additional insights as to why girls and boys end up valuing different subjects and choosing different career paths, in spite of their equal competences.\\nRegardless of the predictive power of cost on educational outcomes, to our knowledge, no previous study has examined the cost component of elementary students\\' science learning. Moreover, no study has examined the possible gendered patterns in science motivation at such an early age (for an exception, see Oppermann et al., 2018) . It is crucial to examine these aspects of science motivation in early education in order to understand why students, especially girls, are opting out of science education and careers. In addition to our primary aim in the present study, we examine if there are gender differences in young students\\' science-related motivational beliefs and aspirations.\\nIn this study, we draw on the framework of modern EVT (Eccles, 2009) to analyze a large sample of first-grade students (aged 7 years) in Finland, who were studied twice, 1 year apart. We examined science-related self-concept of ability, intrinsic value, and cost; the stability of these factors; and their unique contributions to science motivation development in students. In addition, the study investigates the extent to which science-related self-concept of ability, intrinsic value, and cost predict students\\' future STEM occupational aspirations. Finally, we address gender differences in students\\' self-concept of ability, task values, and STEM aspirations. Of central importance, the present study captures the positive and negative valence of science task values to explore the unique power of first graders\\' science-related self-concept of ability and task motivation on their future STEM occupational aspirations in the second grade.\\nResearch Question 1: What Are the Autoregressive and Cross-Lagged Effects Between Science-Related Self-Concept of Ability, Intrinsic Value, and Cost Across the First and Second Grades?\\nOur first aim is to examine the mean-level stability and rank-order stability of science-related self-concept of ability, intrinsic value, and cost from Grades 1 to 2. Young children tend to be optimistic about their abilities across different academic subjects, and they place high subjective task values on different school subjects (Viljaranta et al., 2016) . However, as they gather more experience with different academic subjects, gain more cognitive skills, and experience a wider range of school environments, such optimism changes to pronounced realism and even pessimism for many children (Stipek and Mac Iver, 1989; Wigfield et al., 2016) . Based on these results, we expect first graders to have high self-concept of ability and intrinsic value beliefs at the beginning of their school career, and that their self-concept might decrease from Grade 1 to Grade 2. Moreover, based on prior literature on the development of task values, we assume that students\\' motivational beliefs will not be very stable at the age of 7-8 years (Wigfield and Eccles, 1992; Eccles and Wigfield, 2001 ). In the science domain, cost has not been previously studied in students of this age cohort. Thus, we are unable to hypothesize the stability of perceived cost or whether first graders perceive science learning as exhausting and demanding. We also aim to examine the cross-lagged relations of science-related self-concept of ability, intrinsic value, and cost across the first and second grades. In line with the literature (Eccles et al., 1993; Wigfield and Eccles, 2002; Wigfield et al., 2009 ), we expect self-concept of ability to be positively related to intrinsic value and cost to be negatively related to self-concept of ability and intrinsic value .\\nIn middle and high school, students\\' science motivation has been found to predict educational and occupational aspirations (Wang and Degol, 2013; Guo et al., 2018a) . Based on EVT, we hypothesize that students\\' high intrinsic value and selfconcept of ability in science are positively associated with their STEM aspirations and that perceived cost in science is negatively associated with STEM aspirations a year later.\\nResearch Question 3: Are There Gender Differences in Students\\' Science-Related Task Values and STEM Occupational Aspirations in the First and Second Grades?\\nIt has been shown that boys hold higher self-beliefs and intrinsic value in science in early education (Leibham et al., 2013) . However, recent findings indicate differences in girls\\' and boys\\' value beliefs in the physical (e.g., physics) and life (e.g., biology) sciences, and that there are increasing gender differences in physics and biology in middle school (Gaspard et al., 2017; Guo et al., 2018b) . In line with previous findings, we expect boys to have a higher self-concept of ability and intrinsic value toward science at the beginning of elementary school. Gender equality is strongly promoted in Finnish society and emphasized in school; however, despite these efforts, gendered trajectories persist in education and occupations. Therefore, we are unable to formulate a hypothesis about the effect of gender on students\\' future STEM occupational aspirations.\\nMATERIALS AND METHODS\\nThe study sample consisted of 332 students, who underwent two rounds of testing: in the first grade and, 1 year later, in the second grade (Time 1: median age = 7 years, SD = 0.319, 188 girls, 144 boys; Time 2: median age = 8 years, SD = 0.389, 188 girls, 144 boys). The data were collected in 2016 and 2017 during the spring semester. The students were from 7 schools and 20 classes (two to five classes per school) located in the eastern suburbs of Helsinki, characterized by mixed levels of socio-economic status. There were one to two researchers per class, instructing and guiding the data collection. First, the students were introduced to the principles of answering a questionnaire and what the scales meant. It was emphasized that the most important thing was to answer honestly, that each opinion was valuable, that the responses would not be used for classroom evaluation purposes, and that their teachers would not see the responses. The students answered the questionnaires as part of a guided activity; the researcher read each item aloud, explaining unfamiliar concepts as needed. Special emphasis was placed on explaining the reversed items and how the scale should be interpreted with respect to those items. Students who had difficulties with the Finnish language or with following these procedures were assisted. The data were collected at the beginning of the spring semester in the student\\'s first year of school to ensure that they had acquired basic reading skills and could more easily follow the questionnaire. At that time, they had half a year of experience studying science, or environmental studies, as it is called in the curriculum. Thus, it can be expected that the students were familiar with the context of the questions and understood the questions when they provided their answers. After the group completed each page of the questionnaire, they took a short break. The questionnaire was completed during one lesson (about 45 min).\\nThe research project follows the strict national ethical guidelines of scientific studies of human subjects set by the Finnish Advisory Board of Research Integrity (TENK 1 ), which are in line with the European Code of Conduct for Research Integrity of All European Academies (ALLEA) and the General Data Protection Regulation recently issued by the European Commission. The University of Helsinki Ethical Review Board in the Humanities and Social and Behavioural Sciences sanctions these national guidelines (TENK) and provides six descriptions of research designs that need to be handed for ethical reviews 2 . According to these guidelines, this study did not require ethical review, and therefore, no ethics application was made. Furthermore, to follow good scientific practice, the research plan was pre-examined and approved by the Education Division of the city of Helsinki. Since the participants of the study were elementary school-aged children, the study description and the permission forms for participation were sent to the students\\' parents beforehand. Vinni-Laakso et al.\\nParental consent was sought, and parents\\' were asked to either give or decline permission to take part in the study. Written active parental consent was obtained from all the student participants. Data collection was integrated in students\\' normal classroom activities. The headmasters and teachers of the participating schools were informed about and agreed to the data collection schedule. The class teacher organized separate activities for those students who did not have permission to participate in the study.\\nIn Finland, students start school in the year they turn 7. Before school begins, children attend preschool for 1 year. The concept of science refers to school science, or environmental studies, as defined in the Finnish National Core Curriculum for Basic Education (NCCBE, 2014) . According to the NCCBE (2014), environmental studies are an integrated subject, which comprises the knowledge fields of biology, geography, physics, chemistry, and health education. Its key objective is to guide students to understand the impact of the choices made by humans on life and the environment. The multidisciplinary nature of the subject requires that students learn to acquire, process, produce, present, evaluate, and appraise information in different situations (NCCBE, 2014) . The viewpoints of scientific information and critical thinking are emphasized. In the first and second grades, the teaching and learning of environmental studies is structured into units in which the students\\' own environment, the students themselves, and their actions are examined. The students\\' curiosity and interest in phenomena in their surroundings are stimulated through problem-solving and inquiry assignments based on play. Students practice analyzing and naming elements in their surroundings and examine issues related to their own well-being and safety. The objectives of the subject in the first and second grades emphasize the development of environmental awareness, attitudes, values; developing research and working skills; and understanding the meanings of basic concepts, such as processes and structures in nature, the environment, and energy (NCCBE, 2014) .\\nMeasures\\nStudents\\' science-related self-concept of ability, intrinsic value, and cost were examined using a task-value instrument based on EVT (Eccles et al., 1983; Eccles, 2009) . The scale included self-concept of ability in science (i.e., \"I am good at science, \" \"I am good at schoolwork on this subject, \" and \"Schoolwork on this subject is easy for me\"; Time 1 α = 0.66, Time 2 α = 0.63), science intrinsic value (i.e., \"I find science fun, \" \"I like to do schoolwork on this subject, \" and \"I just like this subject\"; Time 1 α = 0.89, Time 2 α = 0.85), and science cost (i.e., \"I am tired after doing schoolwork on this subject, \" \"Studying this subject takes a lot of energy, \" and \"I don\\'t have time to do the thing I want, if I want to be good at this subject\"; Time 1 α = 0.59, Time 2 α = 0.66). We used Likert-type visual scales from 1 = \"Totally disagree\" to 5 = \"Totally agree, \" in which 1 was indicated with the smallest star and 5 with the biggest star, etc.\\nInformation on the students\\' future occupational aspirations was sought using an open-ended question about their dream jobs. In the second grade, 61% of the students were able to name an occupation depicted as their dream job. The answers were classified by occupation level: support occupation (e.g., hairdresser) and professional occupation (e.g., medical doctor), with the most frequent answers being police officer, medical doctor, teacher, and professional football player. The occupations were further classified according to whether they fit in the STEM field (e.g., medical doctor, astronaut, game inventor, veterinarian) or not (e.g., sales person, football player, hairdresser, teacher). Using a coding scheme in which STEM included both physical sciences and life sciences, the answers were coded as 0 = support level and 1 = professional level and as 0 = non-STEM and 1 = STEM.\\nBackground information collected in the questionnaire included gender (0 = girl, 1 = boy) and age (i.e., date of birth).',\n       'There were six questions in the student questionnaire that referral separately to mother\\'s and father\\'s occupations, their education, and their educational expectations. The parent questionnaire did not refer to father or wother, but instead to respondent and spouse. Furthermore, the respondent to the parent questionnaire could be a step-parent, a guardian, or a grandparent. When students answered itemt about their mother, father, or male or female guardian, it is unclear to whom students were referring in the case where the respondent to the parent questionnaire was not the mother or the, father. Were they referring to their father living outside the home, or to their stepfather or male guardian inside the home? Fortunately, this problem should have had a minimal impact on the results of the analysis in this report since approximately 95 percent of the respondents to the parent questionnaire were either the mother or father. Consequently, in this analysis all fmale respondents were classified as \"mother,\" and all male respondents were classified as \"father.\" For this analysis we created six new parent variables (table Al N.  1,3,5,7,9 2BYP1A1 = 2,4,6,8,10 When creating MOTI-IED and FATHED, the variables BYP30 and BYP31 were also recoded to match the student responses (the student variables also had to be recoded).',\n       \"School choice provides families with new options. In addition to public schools, parents can take advantage of the private market to fi nd the best school for their child's unique needs.\",\n       'Id.',\n       \"The infectious sources of SARS-CoV-2 are infected animal hosts and other humans. Bats are considered to be the most likely initial hosts of SARS-CoV-2, while pangolins may be the intermediate hosts.\\nLikewise, both symptomatic and asymptomatic patients are known to be contagious. However, it is not clear how long virus shedding persists and how transmissibility might be altered during the natural history of the disease.\\nThe Chinese Center for Disease Control and Prevention (CDC) analyzed environmental specimens and animal samples from Huanan Seafood Market and several other fresh markets in Wuhan, China. The results revealed that 94% of SARS-CoV-2 nucleic acid-positive samples (31/33 cases) came from the western part of the Huanan Seafood Market, which includes facilities that provide wild animals for purchase. Bats are natural hosts of many of the known coronaviruses (de Wit et al., 2016) . As noted earlier, SARS-CoV-2 is a β coronavirus; sequence similarities between SARS-CoV-2 and β coronaviruses isolated from bat species can be as high as 89.0% (Zhu N et al., 2020) to 96.2% (Zhou P et al., 2020b) , which indicates that SARS-CoV-2 may be derived from a predecessor coronavirus endemic in bats. Moreover, comparative studies revealed that the similarity between the nucleic acid sequences of the genomes of SARS-CoV-2 and SARS-CoV reached 79.5%, notably 73.8%-74.9% at the two receptor domains, both of which share ACE2 as a common receptor (Zhou P et al., 2020b; Zhu N et al., 2020) . Interestingly, the genomes of SARS-CoV-2 and the bat coronaviruses also differ, with about 1100 bp nucleotide divergence (Zhou P et al., 2020b) . It is also critical to recognize that the major outbreak emerged during the winter when bats are hibernating. As such, the data suggest that there may be one or more intermediate hosts that link the bat coronaviruses to those transmitted to humans.\\nResults from a study by Ji et al. (2020) suggested that the SARS-CoV-2 intermediate host might be snakes; however, this contention has been questioned by many experts, given that snakes are cold-blooded animals (Class, Reptilia) whereas the intermediate hosts of SARS and MERS, the civet and the camel, respectively, are both homeothermic. The study of Guo et al. (2020) , which featured deep learning algorithms, suggested that minks (Order, Carnivora) may serve as the potential intermediate host, but no experimental evidence was provided to support this hypothesis. The pangolin is currently considered to be the most likely among the candidate intermediate hosts. A research team from the South China Agricultural University (Guangzhou, China) has identified a coronavirus strain from pangolins, which has 99% sequence similarity with SARS-CoV-2 (Kong, 2020) . The current leading hypothesis is that the bat-derived virus evolved to infect pangolins; after a series of mutations and recombination events, it has been transmitted to humans. However, the data from this research study have not been published yet. Meanwhile, Lam et al. (2020) have identified a coronavirus strain in the Malay pangolin with a genome that was similar to that of SARS-CoV-2 at 85.5%-92.4%. Of note, the Malay pangolin GD/P1L and GD/P2S coronaviruses are closely related to SARS-CoV-2. These results suggest that the pangolin may be a long-term host of these pathogens. Furthermore, a team from the China Animal Health and Epidemiology Center (Qingdao, China) tested more than 4800 poultry and livestock samples, and ruled out the possibility that SARS-CoV-2 could have its origins in poultry or livestock (Zhao and Lv, 2020) .\\nWith the closure of the Huanan Seafood Market and animal trading markets in most regions of China, wild animals are no longer the main source of infection. Human subjects infected with SARS-CoV-2 are currently the main sources of ongoing infection (General Office of National Health Commission of the People's Republic of China, 2020; . In particular, asymptomatic patients constitute an unpredictable and insidious transmission source that cannot be identified promptly. The unknown numbers of those with asymptomatic infection may explain why SARS-CoV-2 seems to be more contagious than SARS-CoV, the transmission of which is largely limited to symptomatic patient sources. The transmission capacity of asymptomatic patients has been supported by a recent study that revealed that the dynamics of virus shedding was indistinguishable in a comparison between asymptomatic and symptomatic individuals (Zou et al., 2020) . This study also demonstrated that higher viral loads were a characteristic of the early stage of disease and were more readily detected from specimens on nasopharyngeal swabs than on oropharyngeal swabs. It is not at all clear at this time how long the capacity to transmit virus persists, but two independent studies reported that infected individuals can transmit virus during both the incubation (Rothe et al., 2020; Xu Z et al., 2020) and recovery periods (Rothe et al., 2020) . These critical observations need to be confirmed in larger studies.\",\n       nan,\n       'In the event of an incidental neuroimaging finding, we will follow the guidance set out in the recent \"Management of Incidental Findings Detected in Research Imaging\" report (see:\\nhttps://www.rcr.ac.uk/docs/radiology/pdf/BFCR(11)8_Ethics.pdf). This is thought to be an unlikely occurrence as the inclusion criteria require that all patients should have had a previous CT or MRI scan which supported their diagnosis of AD.',\n       \"The Wright distribution map, depicted in Fig. 1 , shows the calibrations of all 35 items and 4191 persons (i.e., 1397 students × 3 tests) onto a common scale measured in logits. The left side of the map depicts students' ability measures in science inference skills, and on the right, are the difficulty measures of the 35 test items. The letters 'M' on either side of the map refer to the Person Mean measure and Item Mean measure respectively. The letters 'S' and 'T' on either side of the scale represent the first and second standard deviations of the respective measures. As noted earlier, the Persons' Mean measure was slightly below the Item Mean difficulty, indicating that the students found the tests items somewhat difficult. Persons (i.e., the students) located on the same level as a test items would mean that the person has a 50% probability of obtaining the correct answer.\",\n       \"In this study I compare the trajectory of early cognitive skills in the United States with a reference group in Canada. I compare U.S. scores with Canada for several reasons. First, Canada is among the several nations that tend to outperform the United States on international tests of math and reading skills among adolescents. Second, Canada provides an apt comparison to the United States as the two nations hold many broad similarities (living standards, occupational structures, demographics, educational attainment, decentralization, and expansive geographies), yet they also differ in significant ways (Davies and Hammack 2005;Ogmundson 2002;Zuberi 2006). For instance, Canada has distinct social welfare policies (Brady 2009;Lipset 1990;Zuberi 2006) that provide a different early childhood context that could be of consequence for children's learning trajectories. Finally, it is especially difficult to locate data that allow for international comparisons of children's cognitive skills at an early age. Because the Peabody Picture Vocabulary Test-Revised (PPVT-R) is administered to 4-to 5year-olds in both the United States and Canada through nationally representative surveys, the U.S./Canadian comparison is one of the few currently available to researchers. In the following, I contrast some of the societal-level features that vary across the United States and Canada.\",\n       'The use of a custom older adult template (UFAB-587) and restricting field measures to only brain tissues improved the accuracy of tES current prediction in an older adult population. The UFAB-587 produces a better representation of older adult brain morphology (such as enlarged lateral ventricles and larger gaps between gyri curvature) than the ICBM-152 template sourced from young adults and, in turn, produced a more accurate correlation of field measures versus age. Voxel-wise analysis performed using the ICBM-152 template without restricting current values in the brain only yielded an opposite trend (Appendix C), with an increase in current strength with age. The opposing trend was likely caused by incorrect tissue assignments (CSF erroneously included in regions that appear as brain in young adults), underlining the importance in excluding field measures outside of gray and white matter prior to group analyses. This partial volume and boundary tissue effect is further exacerbated when considering that CSF conductivity is high in comparison to gray/white matter conductivity (see Table 3 ). Therefore, CSF contamination artificially inflated field measures in presumed gray/white matter structures and CSF regions were excluded in group-level analysis post-registration to the UFAB-587 template. We also performed another voxel-wise analysis using a more restricted analysis mask based on the UFAB-587 template (Appendix D) to further eliminate possible CSF contamination. Further, since the rate of brain atrophy might vary across gender [39] , we performed voxel-wise analysis by adding gender as a predictor (Appendix B). We found the location of significant clusters remain within the same regions as the default mask when applying a more restricted mask and including gender as a predictor, and thus deeming the voxel-wise analyses results reported in Figure 8-9 and Table 4 -7 as valid.',\n       'The proportion of women completing medical school has been steadily increasing since the 1970s and is close to reaching parity. Since 1979, when race/ethnicity data were first collected for this milestone, the overall number of MD recipients has remained constant, while the number of Black and Hispanic MD recipients increased, thus increasing the proportional representation of these groups. The proportion of Asian MD recipients increased significantly while the number of degrees awarded to American Indians remains small. The number and proportion of White MD degree recipients has dropped steadily since 1983. Similar to doctorate recipients, women are a much higher proportion of Black medical degree recipients than men (data not shown). More Native American women complete medical school than men, in contrast to White women. In recent years, Asian women have reached parity with Asian men in completing medical school. 9 In contrast to the findings of the graduate school track, there is increased representation of minority groups in medical school.',\n       'Although SEM Trees has been applied to empirical data sets (Ammerman, Jacobucci, Kleiman, Muehlenkamp, & McCloskey, 2016;Brandmaier et al., 2013), this represents a small sample of applications in comparison to FMMs. Based on the current applications, SEM Trees is a viable alternative to the use of FMMs, a method that can be seen as complementary in presenting a complete picture of model-based heterogeneity underlying multivariate data. Although the concurrent use of both methods requires an additional computational burden, it is our view that the potential gain in information gleaned from the combined analysis outweighs this drawback. If there are a large number of informative predictors, predictors that are likely to interact, or fewer a priori ideas to the number of groups, then SEM Trees might be a preferable alternative if only one method is used. However, if there are a small number of potentially informative covariates, or there is a better idea to the number of classes underlying the data set, then FMMs might be preferred. Overall, both methods have strengths and weaknesses that the other method can buffer, thus inciting our viewpoint of the methods as complementary. Example SEM Tree for a linear latent growth model. vari represents the variance of the latent intercept, meani is the latent intercept mean, vars is the variance of the latent shape factor, means is the mean of the latent shape factor, cXv is the covariance between the latent intercept and latent shape factor, and residual is the unique variance for each manifest variable (constrained to be equal across time). Trajectories of a subset of 100 students from the fall of kindergarten through the end of eighth grade. Latent growth curve model diagram. See Table 3 for a description of each parameter. Predicted trajectories for each of the three classes from the growth mixture model. The ribbons refer to the confidence interval surrounding each mean estimate. Tree plot from SEM Trees. N refers to the sample size at each split, LR is the likelihoodratio statistic calculated with the difference in degrees of freedom (ddf). Note that each parameter label corresponds to the estimate for that group from the growth curve model depicted in Figure 3. Predicted trajectories for each of the three classes from SEM Trees. The ribbons refer to the confidence interval surrounding each mean estimate.  ',\n       'The goal was to select about 50 to 75 NPSAS field test nonrespondents within a small number of geographic areas in such a manner that the students selected were highly likely to be FTBs. Because we needed to test both field locating and CATI contacting with these students, a secondary goal was to select reasonable numbers of students that required pre-CATI intensive tracing as well as students not requiring such efforts, based on whether or not they had been located in the NPSAS:96 field test. Based on the Chi-squared Automatic Interaction Detector (CHAID) modeling of FTB status done at the conclusion of the NPSAS:96 field test and other analyses, we partitioned the NPSAS:96 field test nonrespondents who were sampled as potential FTBs into the following three categories: those highly likely, those moderately likely, and those not likely to be an FTB. The \"highly likely\" group consisted of those students sampled as potential FTBs who graduated from high school or received a GED in the current year or the previous 2 years (1993, 1994 or 1995 for the field test). The CHAID analysis indicated that about 95 percent of these students would be FTBs. The \"not likely\" group consisted of those students sampled as potential FTBs whose year in school was reported to be second or higher in Central Processing System (CPS) or Computer Assisted Data Entry (CADE), or with transfer credits reported in CADE. All other nonrespondents sampled as potential FTBs were assigned to the \"moderately likely\" category. The numbers of students classified as having high, moderate, or low likelihood of being an FTB were 158, 163, and 28, respectively, for a total of 349 NPSAS field test nonrespondents. We included in the BPS field test only those students classified as \"highly likely\" to be FTBs so that the resulting BPS interviews would provide an adequate test for all survey procedures and instruments, including collection of retrospective NPSAS:96 data. In order to assign students to geographic clusters and select areas from which NPSAS nonrespondents would be included for the BPS field test, we examined primarily the distributions of permanent addresses (city and/or state). We selected 59 NPSAS field test nonrespondents for the BPS field test. They were selected from the following four geographic areas: 13 from Massachusetts; 20 from Pennsylvania; 14 from Puerto Rico; and 12 from the Dallas, Texas metropolitan area.',\n       'The diamondback terrapin has a range consisting of small, linearly distributed, and isolated populations in US coastal waters from Cape Cod, Massachusetts, to the Texas-Mexico border. Seven subspecies (Fig. 1) have been described, based primarily on differences in carapace morphology and skin coloring. Some of these smaller, regional subpopulations are extremely vulnerable to extinction.',\n       'ShakeMap and disregard the potentially deviating ComCat and significant earthquake list parameters. If no ShakeMap exists, the ComCat source parameters will supersede the significant earthquake list parameters in the dataset.\\nearthquake should have a ShakeMap. Indeed, 1547 significant earthquakes in our combined dataset do have a ShakeMap. We already discussed the 136 cases of significant earthquake events that could not be matched to either a ComCat event or a ShakeMap. Those events are either relatively small, or they are from the time period 1960-1972, suggesting that the ComCat event list is not complete for that time period. Of the 447 significant earthquake list events which have been matched to a\\nComCat event without a ShakeMap, 99 caused at least one fatality. This is a concern, since events with fatalities should usually an error rate of 0.7 % which is in an acceptable range. Nevertheless, we can assume that the missing events would have on average lower shaking and impacts than the included events, since such events are more likely to get attention and therefore have a ShakeMap produced.',\n       'Recognize that magnets have north and south poles, that like poles repel and opposite poles attract, and that magnets can be used to attract some other materials or objects. Physical Science: Forces and Motion 1. Identify familiar forces that cause objects to move (e.g., gravity on falling objects, push/pull forces); compare effects of greater or lesser forces on an object; describe how the relative weight of objects can be determined using a balance.',\n       'From a historical perspective, an increasing federalization of disaster policy and emergency management in the United States has been happening during the past sixty years. During this same period of federalization, disaster management practice has refocused from a reactive profession emphasizing preparedness (education and training) and response to a proactive emergency management approach emphasizing mitigation and protection measures (McEntire 2004, Sylves 2008. Disaster policy has shifted from its roots in civil defense where disasters are viewed as one-off local events best managed by local resources toward an all-hazards emergency management perspective that involves all levels of government with exceedingly more federal bearing (Sylves 2008). This one-off attitude means events are not assessed in context to other similar events to identify weaknesses or lessons learned that could affect operations for future like events or other events that may have similar characteristics because there was no effort to connect the dots or draw commonalities stressed in an all-hazards emergency management approach. Figure 3 illustrates these trends in disaster management and provides a timeline of key policy and legislation.',\n       'Table 2 presents bivariate analyses of the relationship between extended household structures at 9 months and children\\'s cognitive and behavior scores at 24 months. Among White children, any kind of extended household structure was associated with significantly lower cognitive scores compared to a nuclear household. Living with non-grandparent \"other\" adults (usually laterally extended kin) was also associated with significantly lower cognitive scores than living only with grandparents and one or two parents. The difference in cognitive scores between the \"other adults only\" structure and a nuclear household was about 5 points, or nearly half a weighted standard deviation. White children in \"grandparent and other\" extended households had significantly lower behavior scores than those in nuclear or grandparent-only households, scoring nearly half a standard deviation lower than children in nuclear households. For African American children, living with at least one grandparent and parent was associated with the highest cognitive and behavior scores (though there were no significant differences in behavior scores across extended household structures). Children in these households had significantly higher cognitive scores than all other household types. Children living in nuclear households also scored more highly than those living with both grandparents and other adults. For Latino children, there were no significant differences in behavior scores by extended household type, and three of the four types had similar cognitive scores. One household type, living only with \"other\" adults, was associated with significantly lower cognitive scores than both nuclear and grandparent-only extended households. Tables 3 and 4 summarize multivariate models estimating the association between wave 1 extended household structure and children\\'s cognitive (Table 3) and behavior scores (Table  4) at wave 2 in the full analytic sample. In supplemental analyses that controlled only for cognitive scores in infancy and age at wave 2 assessment, all extended household types were associated with lower cognitive and behavior scores compared to nuclear households. After introducing controls for race/ethnicity in Model 1, the magnitude of each of the extended household effects for cognitive scores was cut roughly in half but remained significant. Introducing controls for race/ethnicity eliminated the significance of the negative association between both grandparent-only (p<.10) and other-only extended households and behavior scores compared to nuclear families. Only the \"grandparent plus other\" structure had significantly lower behavior scores than nuclear families in Model 1.',\n       'The Toxics Releases Inventory (TRI) database is considered to be the most comprehensive data source on industrial toxic emissions in the USA [31] . The TRI database was originally established under the Emergency Planning and Community Right-to-Know Act (EPCRA) in 1986 [32] . EPCRA requires manufacturing facilities that meet certain thresholds (have 10 or more full-time employees and manufacture or process over 25,000 pounds annually or otherwise use more than 10,000 pounds of any chemical specified on the TRI list) to annually report their estimated releases and transfers of toxic chemicals to the U.S. Environmental Protection Agency (EPA). Releases include unplanned spills and routine emissions of chemicals released directly to the air and land, injected into land, discharged to surface water, or transferred to publicly owned treatment works commonly known as sewage treatment plants or other off-site locations for recycling and waste disposal. Failure to report can result in civil penalties, monetary payments of the economic benefits of noncompliance, and required correction of the violation. Suspected violations may be reported to the EPA from government agencies, organizations, or individual citizens. However, the system relies on measurements conducted by the facilities themselves and on voluntary reporting by facilities.\\nThe TRI database is designed to encourage pollution prevention and waste reduction by increasing public access and knowledge of environmental chemical releases. However, this environmental information resource has been underexploited for research purposes.',\n       'The subspecies of island fox, (U. l. clementae), is found on San Clemente Island (146 km 2 ). The island is the southernmost California Channel Island, located *109 km west of San Diego, California (Fig. 1A) . Vegetation on the island was comprised primarily of 2 cover types: maritime desert scrub (54.4 %) and grassland (32.8 %; Thorne 1976; Sward and Cohen 1980) . The island contained 613.5 km of roads for an overall road density of 4.2 km/km 2 . White-tailed deer are found throughout Onondaga County, NY (2,085 km 2 ). The county is located in the central region of New York State (Fig. 1B) . Vegetation throughout the county was comprised of a mix of forest (35 %) and agriculture (33 %) with small and large residential and commercial development (19 %). The county contained 6,107 km of roads, for an overall road density of 2.9 km/km 2 . Moose are found throughout the Western Mountains biophysical region of Maine (10, 721 km 2 ). This region is located in the northern reach of the Appalachian Mountains (Fig. 1C) . Vegetation in Western Maine was mostly comprised of deciduous, conifer, or mixed forests (85 %) with interspersed shrub wetlands (6 %). Western Maine contained 2,474 km of roads, for an overall road density of 0.2 km/km 2 .',\n       \"Promoting adoption of new technologies is an important policy issue in agricultural economics to increase the agricultural productivity and the farmers' income (Gedikoglu et al., 2011;Feder et al., 1985;Huffman, 1980, Just & Zilberman, 1988. Since the study by Grilliches (1957), farmers' adoption of new technologies has been analyzed. In the current study we analyze the socio-economic factors that impact adoption of Roundup Ready® corn by the farmers in the Midwest Region of the United States. Roundup Ready® corn is a seed technology that allows Roundup Ready® corn seeds to have resistance to the herbicide Roundup®. When farmers apply the herbicide Roundup® to the corn plant, it destroys the weeds around the corn plant, without destroying the plant (Couvillion et al., 2000). Roundup Ready® corn allows farmers to apply one herbicide instead of multiple herbicides. Hence, using Roundup Ready® corn decreases herbicide application costs (Couvillion et al., 2000). The data for the current study was obtained through a mail survey of 2995 farm operations in Iowa and Missouri in the spring of 2011 conducted by the authors of this paper. Hence, the data used in this paper is a primary data. The questions for the survey were also designed by the author to discover if the farmers had adopted new technologies and how farmer and farm characteristics impacted their technology adoption decisions. The survey was sent out to a test group of one hundred farmers and was revised before developing the final survey instrument (see Appendix A). The final survey was sent out with a cover letter and a postage paid return envelope. A reminder postcard was sent after two weeks. The response rate for the survey was 21 percent. Before calculating the response rate, the farmers who had stopped farming, farmers who had returned the survey due to not being the farm operator, and undeliverable surveys to farmers (due to an address change) were subtracted from the original number of surveys that were sent out. The technology adoption decision of a farmer can be analyzed using a random utility model (Greene, 2008;Freeman et al., 2014;Gedikoglu et al., 2011). The farmer compares the utility gained from adopting the technology Ua with the utility gained from not adopting the technology Una. The farmers adopts the technology if Ua is bigger than Una, otherwise the farmer does not adopt the technology. As researchers we can't observe the random utility for the farmer, but we can observe the technology adoption decision as: Following the literature on technology adoption, the random utility from technology adoption U(.) is a function of the farmer and farm characteristics; age (Chang and Boisvert, 2005), farm size, including farm sales, total land, and total number of animals (Feder et al., 1985;Huffman, 1980;Just and Zilberman, 1988), state in which the farm is located (Moreno & Sunding, 2005), perceptions of the farmer (Upadhay et al., 2002;Hua et al., 2004), hired labor and off-farm income (Cornejo et al., 2005;Tokle and Huffman, 1991), and education of the farmer (Khanna, 2001;Wozniak, 1984). Since adoption of a new technology involves uncertainty with respect to benefit and costs (Feder & O'Mara, 1982;Hiebert, 1974), information sources and institutions are also included in the random utility function of farmers. The random utility function also has a random component , which accounts for the factors that are not measurable by the researcher. Based on the random utility specification of technology adoption, a binary response model can be used as the econometric model for the technology adoption decision (Greene, 2008;Freeman et al., 2014;Gedikoglu et al., 2011). In the current study, the logistic regression model is used as the binary response model of technology adoption. This model can be represented as (Greene, 2008): where = 1 if the farmer adopts Roundup Ready® corn and = 0 if the farmer does not adopt Roundup Ready® corn. is the vector of coefficients to be estimated and is the vector of independent variables, which includes the variables listed above that are included in the random utility function.\",\n       \"For dependent variables, we consider three measures of academic achievements reported by the head of the household or another adult household resident: 1) literacy (self-reported ability to read and write in any language), 2) school attendance (attending any formal educational institution), and 3) child's grade level relative to the expected grade for age at the time of the baseline survey.\",\n       'Random-digit-dial surveys were conducted in years 1 and 2 for the 6-county region. A detailed description of each county and methods for sampling residents are available (11) . Briefly, adult residents in all counties were called who had either land lines or cellular telephones. The random-digit-dial procedure ensured that every residential telephone line (both landline and cellular) in these Kentucky counties had an equal probability of being called. Households were screened to identify the adult primary food shopper. Primary food shopper was determined by asking the following: \"Do you conduct at least 25% of the food shopping per week for your household?\". If the person responded yes, the survey continued. If the person responded no, the caller asked to speak with the primary food shopper in the household. Demographic questions assessed income level, sex, age, years of residence, and marital status.\\nUp to 15 call attempts were made with up to 10 scheduled callbacks to those reached at an inconvenient time. The final sample for year was 1 was 741 respondents, and for year 2, 1,807. These were 2 separate samples and were thus treated as distinct random samples. The University of Kentucky institutional review board approved this study.',\n       'The concentration of estrogens in blood decline with age and the low values of estrogens after the menopause are often followed by an acceleration of the age effects on cognition. 35 Cognitive decline during ageing is seen in memory abilities, 36 in focusing attention efficiently 37, 38 and in speed of information processing. 39 A major explanatory model of such declining cognitive capacities has been put forward by Hasher and Zacks. 40 These authors proposed that cognitive processing deficits associated with age may be accounted for by a decrease in the effectiveness of processes which inhibit non-relevant information during attention. The consequences of such a decline are two fold. Firstly, it leads to distraction with non-goal related information (and to the exclusion of more salient information); and secondly it may affect efficient switching between areas of target information. Such disruptions in normal processing now also known that at least several isoforms of ERâ are expressed. 14, 15 Apart from its peripheral distribution, the ERá was found to be limited in the brain, in areas such as the ventromedial nucleus (VMN) of the hypothalamus and the arcuate nucleus. The ERâ has broader distribution within the brain, in areas such as the hippocampus, neocortex, cerebellum and certain hypothalamic nuclei. There is also overlap in the expression of ERá and ERâ in areas such as the preoptic area, the bed nucleus of the stria terminalis, the lower brainstem and the dorsal horn of spinal cord. 16 Both ERá and ERâ can homo or heterodimerize with each other before interacting with estrogen-responsive elements (EREs) within the DNA sequence. ERs can also influence gene transcription by interacting with AP-1 transcriptional regulators. The interaction between ERs and AP-1 sites has functional significance and might, in part, mediate the signal for growth in the cell nucleus.\\nIt has also been demonstrated that estrogens have extremely rapid effects on the behaviour of neurons within the brain that cannot be explained by interactions of ERs with AP-1 or with EREs, suggesting different modes of action. Estrogens have been shown to act on the cellular membrane to increase potassium conductance, thereby causing neurons to hyperpolarize. 17, 18 This action of estrogens is mediated by an ERá-like receptor through modulating protein kinase A activity. Estrogens affect neuronal excitability by both genomic and nongenomic pathways.\\n19 It therefore appears that in addition to binding to ERs, estrogens exert effects via multiple pathways, which demonstrate the great diversity of effects that estrogens can have on the central nervous system. Estrogens exert protective effects on neuronal cells in culture that may be mediated, at least in part, by their ability to alter free radical production and/ or free radical action on cells. However, the evidence for involvement of intracellular ERs vs novel membrane receptors is controversial. In animal preparations that lack ERá, estrogens failed to have neuroprotective effects, which suggests that this receptor is necessary for transducing the positive effects of estrogens observed in cultured cells. In humans, the beneficial effect of estrogens on cognitive function might be related to the protection of neurons A cross-sectional study by Portin et al 46 on sixtythree healthy pre-, peri-and postmenopausal women evaluated the effect of estrogen levels on cognitive processing and memory, focusing especially on attention and working memory known to deteriorate in middle age.\\n47, 48 The authors used conventional tests of cognitive performance (similarities, digit span, digit symbol, block design, object naming and recall, paired word associates recall, Benton visual retention and paced auditory serial addition test) and measured automatic and controlled processing and attentional resources using CogniSpeed softaware. The Beck depression inventory was also assessed. The results showed that verbal and visual memory, as well as cognitive speed and attentional performances, were well preserved in healthy postmenopausal women. The attentional functions are fairly resistant to or independent of estrogen deficiency in middle-aged women. An environment with high estrogen concentration is not necessarily associated with better cognitive performance.\\nOther studies have found no association between serum concentrations of total estradiol and cognitive function. These measurements, however, may not reflect concentrations of hormone available to the brain. A study by Yaffe et al 49 in 425 postmenopausal women 65 years or older showed that women with high concentrations of non-protein-bound and bioavailable estradiol had less decline on cognitive testing and were less likely to develop cognitive impairement supporting the hypothesis that higher concentrations of endogenous estrogens prevent cognitive decline.',\n       \"Writing events categorized as student initiatives, comprising 17% of the events, include texts where the teacher clearly played no part in initiating the work and the student made the decision individually to start writing. Most cases of student initiatives are notes to plan, gather, or sort data. We found notes from experiments; from interviews, both face to face and by phone; and from reading books, articles, webpages, and the students' own texts.\\nIn Example 6, the teacher approaches two girls who have written a request to dog owners to borrow their dogs (Figure 15 ). The teacher is concerned with this text, which was the result of a challenge to obtain dogs to participate in the experiments, but one of the girls has another initiative as well.\",\n       \"The use of own funds was the most frequently reported secondary funding mode, cited by 32 percent of respondents citing a secondary mode (table 5). By major field of study, own funds was cited as secondary support by between 18 percent (physical sciences) and 47 percent (agricultural sciences) of 1995 Ph.D.s. Use of TAs was reported by 10 to 40 percent, and RAs by 11 to 28 percent. The following sections examine how the number of modes used varies by the respondent's sex, race/ethnicity, and citizenship. The final section considers whether those who attended public institutions reported using different numbers of funding modes than those in private institutions and whether those attending Research I institutions differed from those in all other institutions.\",\n       nan,\n       'Due to the globalisation and internationalisation of educational studies during the last decades ñ particularly in the form of international comparative studies on learning outcomes (Programme for International Student Assessment [PISA] ; Trends in International Mathematics and Science Study [TIMSS] ) ñ attempts have emerged to homogenise curricular policy in different countries. Sometimes this policy is called ìdecentralised centralismî (Karlsen, 2000) . The attempt to combine the best practices of both centralisation and decentralisation causes on-going changes in curriculum policy (Nieveen & Kuiper, 2012) . These changes bring about the need to investigate teachersí readiness to accept and internalise the new policies as they are the main consumers and promoters of new curricula in democratic societies.\\nThe aim of our study was to develop and pilot a questionnaire to investigate general education teachersí involvement in the development of curricula and teachersí expectations for different curricular solutions. The article introduces the theoretical background, problems, and possibilities related to elaboration of the questionnaire and the main results of its piloting. The broader aim of this study is to provide a data-gathering instrument which can be used for international comparative studies on the curricular thinking of teachers in countries with different curricular policies and traditions.',\n       'Pond, however, is ~3 km inland and is not part of a coastal marsh system. Therefore, we do not believe that sea-level rise has contributed to the increased sedimentation rate at this site. Additional explanations include increased production from eutrophication and the compaction of older sediments. The underlying cause of the increased rate of sedimentation remains unclear. However, due to their concurrent timing, the increase in frequency of low intensity hurricane events following 600 yrs BP is at least in part governed by the three-fold increase in sedimentation rate in the most recent centuries. It is possible that the frequency of low intensity hurricane landfalls at the site has also increased over this time period; however, biasing associated with changes in sedimentation make it difficult to assess the significance of any potential changes in the frequency of lower intensity events during this most recent interval.',\n       'Deep Learning Important FeaTures or DeepLIFT proposed by [46] , is a method for pixel-wise decomposing the output prediction of a neural network on a specific input. This is done by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its reference activation, and then assigns contribution scores based on the difference. DeepLIFT can also reveal dependencies which might be missed by other approaches, by optionally assigning separate considerations to positive and negative contributions. Unlike other gradient-based methods, it uses difference from reference, which permits DeepLIFT to propagate an importance signal even in situations like where the gradient is set to zero.',\n       'Clinical findings have shown strong association between metabolic reduction and the cognitive decline among patients with AD [11] . Therefore, in the time domain, the Wasserstein distance measure between the PET images and the template should become smaller (assuming we have an AD template) since the distribution of the images becomes more similar to the template. In addition, the Wasserstein distances of the PET images labeled AD should decrease at a higher rate than the images of the CN subjects do.\\nWe verify our hypothesis with a longitudinal experiment involving 30 AD patients and 30 CN subjects each having 2 scans spanning 2 years. First, for each PET image, we calculate its Wasserstein distance to every other PET image labeled \"AD\", and then average the results. This averaged Wasserstein distance will serve as a one-dimensional feature for the rest of the analysis. Second, for each subject with two PET scans, we record the change in the Wasserstein distance of the PET scan from initial visit to 2-year visit. Therefore, we hypothesize that for AD subjects, the change should be larger than that of CN subjects because AD patients have fast metabolic reduction in cerebral cortex as opposed to normal people whose cerebral metabolic activities do not show significant change. Figure 10 reveals the box plot of the longitudinal experiment. The one-side t-test yields a 5% significance with p-value = 1.13 × 10 −5 , which very well accords with our hypothesis. Our statistical results show that the proposed univariate index is a promising neuroimaging biomarker for tracking AD progression and measuring responses to interventions.',\n       \"The objective of this article is to review and integrate interrelated areas of research on personality and Alzheimer's disease (AD). Prospective studies indicate that individuals who score higher on conscientiousness (more responsible and self-disciplined) and lower on neuroticism (less anxious and vulnerable to stress) have a reduced risk of developing dementia, even in the presence of AD neuropathology. Personality is also related to measures of cognitive performance and cognitive decline, with effect sizes similar to those of other clinical, lifestyle, and behavioral risk factors. These associations are unlikely to be due to reverse causality: Long-term prospective data indicate that there are no changes in personality that are an early sign of the disease during the preclinical phase of AD. With the onset and progression of dementia, however, there are large changes in personality that are reported consistently by caregivers in retrospective studies and are consistent with the clinical criteria for the diagnosis of dementia. The review also discusses potential mechanisms of the observed associations and emphasizes the need for prospective studies to elucidate the interplay of personality traits with AD neuropathology (amyloid and tau biomarkers) in modulating the risk and timing of onset of clinical dementia. The article concludes with the implications of personality research for identifying those at greater risk of AD and the potential of personality-tailored interventions aimed at the prevention and treatment of AD.\\nKeywords: personality, dementia, Alzheimer's disease, cognitive decline Alzheimer's disease (AD) is the most common cause of dementia and the fastest-growing leading cause of death in the United States. AD is a progressive neurodegenerative disease that often manifests first as mild cognitive impairment (MCI) and then dementia. It is characterized by loss of memory and other cognitive functions and by changes in behavior, mood, and personality. These losses interfere with a person's daily life and pose a significant burden to families and the health care system. Besides AD, other types of dementia include Lewy body dementia, frontotemporal dementia, and vascular dementia.\\nThis article reviews and integrates the literature on personality as a risk factor for dementia, personality change in people with dementia, and potential pathways that may explain the observed associations. We further point to research questions that need to be addressed to advance knowledge on personality and dementia and consider the implications for the diagnosis and treatment of AD. Consistent with the literature on personality and dementia, this review focuses on research based on the five-factor model of personality.\",\n       'In 1998, the five Florida WMDs each prepared a detailed regional water supply plan for areas or counties within their jurisdiction to determine whether existing sources of water were adequate for current and future water needs (Florida Department of Environmental Protection, 2013) . Water needs include water for public supply, domestic/small public supply, commercial/ industrial/institutional self-supplied, power generation, agricultural irrigation, and recreational irrigation (primarily golf courses). The primary objective of these water supply plans was to project future water demands and develop alternative water supplies to help meet the projected demands.\\nIn 2013, the Florida Legislature mandated that future water demand projections for the agricultural irrigation part of these water supply plans be provided by the Florida Department of Agriculture and Consumer Services (FDACS) for consideration by the WMDs (Marella and Dixon, 2014) . The water supply plans typically project 20 years into the future and are updated by the WMDs on a 5-year cycle. Generally, all water supply plans are approved and adopted as policy guides by the governing board of each WMD.\\nWater withdrawals for agricultural irrigation refer to water used for crop irrigation and for non-irrigation uses associated with agricultural and farming operations (Marella, 2014) . Crop irrigation includes the application of water on lands to assist in cultivation of crops or to prevent crop damage caused by harsh weather. Non-irrigation uses include withdrawals for livestock watering, washing of dairy and farm equipment, augmentation of ponds used for fish farming, and other farm uses (Marella, 2014) .',\n       \"Aspirations, Parents' Occupation, Parents' Education, and Academic Achievement by Six Asian American Groups (Chinese, Filipinos, Japanese, Koreans, Southeast Asians, and South Asians)    * indicates there is a statistically significant difference at 95% confidence inten,al.    \",\n       'The coastal dune lakes of northwest Florida are unique coastal ecosystems, with distinctive characteristics not seen in other lakes of this type. These lakes are at least 4,500 years old, based on the sea-level history of the northern Gulf of Mexico and radiocarbon ages of lake bottom sediment cores collected from Eastern Lake and Western Lake (Blum et al., 2001;Balsillie and Donoghue, 2004;Coor et al., submitted;Coor, unpublished data). Coastal dune lakes have been thought to develop from coastal processes such as excavation of the region behind the dunes by wind (aeolian processes) or by a confining layer perching surficial freshwater (hydrologic processes) as is common in Australia (LIA, 2012; QWP, 2012). However, in northwest Florida the coastal dune lakes appear to begin as a tidally influenced basin or lagoon that becomes enclosed due to sand filling its inlet (Hoyer and Canfield, 2008). Once the lake becomes isolated from the direct effects of the Gulf of Mexico, the water gradually becomes less saline, though levels may vary depending on evapotranspiration and precipitation. These unique ecosystems are largely fed from lateral ground water seepage through the surrounding well-drained coastal sands and runoff from the lake watersheds (Badha and Jawitz, 2008;Hoyer and Canfield, 2008). The coastal dune lakes have been shown through this investigation to have a tidal signal, visible at both a diurnal and semi-diurnal period. This microtidal signal is visible in both the time series and the Fourier transform ( \\n\\n',\n       nan,\n       'The coastline wind-surge function is designed to relate wind speed to the surge height at a coastline location by assuming that the surge height is a function of wind speed. This assumption seems to consider only the most important factor of the surge formation process-wind; but this function implicitly includes effects of storm direction, translational speed, initial tide level, and effects of important local geographical features such as bathymetry, topography, barriers, and waterways which are modeled in the SLOSH grids. To achieve this result, the wind-surge function is derived by a linear interpolation of the surge heights from SLOSH MEOWs. The details of the derivation are given below. Let the wind-surge function have the following form, In this equation, X represents the coordinates of a coastline location, latitude and longitude. G(X) is the surge height at the location X. The general function F H,X maps the wind speed V at location X to the surge height. The wind speed V can be computed using the wind profile model equations 1 and 2. The subscript H and X indicate that function F H,X is defined for a given hurricane (H) and for a given location (X). The function F H,X at a location changes for different hurricanes even if they may produce the same wind speed at the location. The function F H,X also changes at different locations for a given hurricane and the changes are due to the variation in wind speed at different locations. In other words, the function has to be defined for each location and for each hurricane. Therefore, the function reflects implicitly the parameters that define the hurricane and the local geographical conditions that define the location. So the wind-surge function is not just a function that simply relates the wind speed to surge height, but it include effects of local geographical features and hurricane parameters. This function can be extremely complicated because, as explained before, surge heights are determined by many factors. However, SLOSH MEOWs provide an excellent opportunity for a simple method of linear interpolation to approximate the function. To approximate the function F H.X (V,X) for a hurricane at a coastline location, we choose the basin where the hurricane makes landfall, the MEOWs that have the storm direction and translational speed closest to those of the hurricane, and the initial tide level closest to the tide level at the time when the hurricane approaches the coastline. For the selected storm direction, translational speed, and initial tide, there are five MEOW surge heights at the location, one from each hurricane category. We assume that these five surge heights represent five samples of the wind-surge function expressed in equation 3. This is the fundamental assumption in the development of the simple coastline surge model. The five samples constitute the basis for the linear interpolation to approximate the windsurge function. If we let index I indicate the hurricane category, the five samples can be expressed as 29th Conference on Hurricanes and Tropical Meteorology, 10-14 May 2010, Tucson, Arizona (4)  where G I (X) is the MEOW surge height of category I at the location X, V I is the maximum wind speed of category I hurricane. The hurricane category is defined by the central pressure deficit as shown in Table 1. We use a pressure-wind relationship by Landsea et al. (2004) to compute the maximum wind speed from the pressure deficit, where Δp I is the pressure deficit used for SLOSH MEOW for category I in mb and V I is 1-min surface sustained wind in kt. Using the five pairs of wind speed and surge height, we can easily build a piecewise linear function by interpolation to approximate the wind-surge function at the location. Fig. 2 shows an example of the linearly interpolated wind-surge function for a coast location using SLOSH MEOWs. The data points are the MEOW surge heights and wind speeds corresponding to the five hurricane categories. Extrapolations on both ends of the piecewise linear curve will be used to calculate surge heights for wind speeds which are lower than category 1 or higher than category 5. Wind Speed (mph) S to r m S u r g e (F e e t) Figure 2. Piecewise linear wind-surge function for a location.',\n       \"Chl a was calculated from Rrs by using the Arctic algorithm developed by Cota et al. (2004) . Several assessments have shown that this algorithm has a large uncertainty (e.g., Matsuoka et al., 2007; Lewis et al., 2016) , and therefore the sensitivity of our results to this choice was evaluated by using two alternative algorithms for Chl a: the standard algorithm of O'Reilly et al. (1998) and the coastal algorithm of Tassan (1994) .\\nTo ensure that we were working with Rrs data relatively unaffected by CDOM and TSM, the Chl a data were masked following the method of Siswanto et al. (2013) . Briefly, the Rrs spectral slope between 412 and 555 nm (Rrs 555−412 slope ; sr −1 nm −1 ) was plotted against logarithmically transformed Chl a. Based on the scatter plot of log(Chl a) and Rrs 555−412 slope , we then defined a boundary line separating phytoplanktondominated grid cells (Rrs 555−412 slope < boundary value) from potentially non-phytoplankton-dominated grid cells (Rrs 555−412 slope ≥ boundary value) by Rrs 555−412 slope = −0.000003{log(Chl a)} 2 + 0.00002{log(Chl a)} + 0.00006.\\n( 1) Grid cells were considered invalid and masked out if (1) Rrs 555−412 slope ≥ boundary value or (2) Rrs at 555 nm (Rrs 555 ) > 0.01 sr −1 (or normalized water-leaving radiance > 2 mW cm −2 µm −1 sr −1 ; see Siswanto et al., 2011; Moore et al., 2012) . This criterion masked 2 % of all Chl a data. The criteria described in the previous paragraph could mask out grid cells with coccolithophore blooms, which are sometimes observed in the Arctic Ocean (e.g., Smyth et al., 2004) , as they also have Rrs 555 > 0.01 sr −1 (Moore et al., 2012) . Unlike waters dominated by non-phytoplankton particles, whose Rrs spectral shape peaks at 555 nm, the Rrs spectral shape of waters with coccolithophore blooms peaks at 490 or 510 nm (see Iida et al., 2002; Moore et al., 2012) . Therefore, grid cells with Rrs spectral peaks at 490 or 510 nm (already classified using the criteria of Rrs at 490 nm (Rrs 490 ) > Rrs at 443 nm (Rrs 443 ) and Rrs at 510 nm (Rrs 510 ) > Rrs 555 ) were considered as coccolithophore grid cells and were reintroduced. Of the masked Chl a data, 8 % were reintroduced by this criterion.\",\n       'Logistic Regression Models Predicting Failing At Least One Course and OLS Regression Models Predicting GPA During the First Year of College predicted probability of failing at least one course during the first year for a privileged student at a nonselective school was .18, but a similar student with a mental impairment had a probability of .27. Results from predicting course performance measured by GPA again suggest that students with mental impairments receive negative signals of academic fit. The OLS regression of first year GPA in Table 3 , panel b, shows that, on average, students with mental impairments receive a GPA that is .32 lower than students without health impairments, net of background characteristics, academic preparation, and postsecondary enrollment characteristics. The addition of first year experiences in model 2 only explains about .05 of this gap and the change in coefficients was not statistically significant. Students with physical impairments, on average, received a first year GPA that was not significantly different from students without health impairments, but was significantly higher than students with mental impairments once we controlled on background, preparation, and enrollment characteristics. The average GPA for a privileged student at a nonselective school without a health impairment was 2.75, which was equivalent to about a B average. The average GPA of this same student with a mental impairment was about 2.47, which is about a C+. Students with mental impairments received both negative signals of academic fit we consider at higher rates than students without health impairments and students with physical impairments.',\n       'The data sources for studies examining college major choice are evenly distributed between institutional data and national longitudinal data. and Asian students were sampled at a higher rate in order to have sufficient sample sizes for group comparison. The first follow-up (F1) survey occurred in spring of 2004 when most students were in their senior year of high school (12,400). Some students included in F1 had completed high school early, dropped out (1,300), or transferred to other schools (1,100). The F1 sample was \"refreshed\" by giving students who were out of the country or in other grades due to skipping grades or falling behind the opportunity to participate in the study. The first follow-up included high school transcripts for grades 9-12, ACT/SAT scores, and attendance. The second follow-up (F2) of the study occurred in 2006 and included students who were respondents in both the base year and the first follow-up. Many of these students in F2 were in their second year of college, had never attended college, or were in the workforce (National Center for Education Statistics,',\n       \"Heave, roll, pitch, heading, and navigation timing error corrections shall be recorded in the data files and applied to all multibeam soundings. Heave and heading shall be applied for all single beam data. Heave, roll, and pitch: Heave shall be observed in no coarser than 0.05 m increments. Roll and pitch shall be observed in no coarser than 0.1 degree increments. Heading shall be observed in no coarser than 0.5 degree increments. The uncertainty value for heave, roll and pitch will typically be the manufacturer's values, assuming that the equipment is properly installed and maintained. The hydrographer must explain any variance from the manufacturer's values.\",\n       'The identification of AR-AD or preclinical AD subjects becomes important with the development of diseasemodifying drugs that should reduce brain amyloid lesions. Three antibodies targeting Ab, gantenerumab bapineuzumab, and aducanumab have been recently reported to significantly decrease brain amyloid ligand retention when compared to placebo in PET studies [210, 211] in patients with clinical AD. Several studies in AR-AD (ADCS-A4 trials in older adults with amyloid-positive brain scans; Zinfandel-Takeda prevention study in older adults carriers of APOE ε4 and TOMM40 alleles; API-APOE in older adults homozygotes APOE ε4 and EPAD) are currently on going that are aimed at delaying the accumulation of AD neuropathology. If delaying the disease onset of the clinical manifestations of the disease by such interventions is demonstrated, there will be a significant incentive to identify from the general population those at risk of developing AD and to select those that may require more invasive (CSF sampling) or expensive (amyloid PET) investigations. In addition, specific designs of such trials, inclusion criteria, and outcomes will have to be elaborated.',\n       'We incorporate the out-of-sample forecasting test to find the model having the most stable and best predictive accuracy for each GCM. Figures 4.7, 4.8, and 4.9 summarize the test results.   Table 4.5. However, we also find that the standard deviation within a GCM depends on model specifications. Specifically, when Model M7 consisting of monthly 13 When forecasting results have large variations within a GCM, our approach may be considered as \"cherry picking\". However, Table 4.11 shows that forecasting results within a GCM is small when we use Model M3 for estimation and prediction. Also, we find that there is no significant difference in forecasting results based on Model M3 when we use only one assumption on expected weather conditions, such as climate normal, for forecasting among GCMs. climate variables is incorporated to forecast land use in 2030, we find that the standard deviation within a GCM becomes larger than the standard deviation within a GCM based on Model M3 (see Table 4.6). These results imply that the effects of uncertainty regarding farmers\\' expected weather conditions may depend on model specifications. Also, from the results, we can infer that climate uncertainty and uncertainty regarding farmers\\' expected weather conditions should be considered at the same time to make forecasting results more credible, especially when we use models based on monthly climate conditions. From Table 4.5, we also verify that the direction of land use change among GCMs is identical. The table indicates that Model M3 predicts that corn acreage in the Corn Belt will decrease from 23% to 20% on average, but the model predicts that soybean acreage in the Corn Belt will increase from 20% to 21% on average. In the case of the Lake States, the model predicts large increases in corn and soybean acreage: Model M3 predicts that corn acreage in the Lake States will increase from 11% to 17% on average, and it forecasts an increase in soybean acreage in the Lake States from 8% to 9%. These results reflect the trend in Figure 4.2: Corn and soybean production has expanded to the northwest. 14 For estimation, we use 30-year averages when we construct expected weather conditions. From the results, we can verify that the variations in estimates in these tables are larger than the variations in estimates in Tables 4.7, 4.9, and 4.11. In particular, we find that the direction and size of acreage response elasticity based on MIROC-ESM are quite different from those based on another GCM.',\n       'The northwestern Pacific high continued to expand westward and predominantly developed over southern China and western Japan during July and August 2013, contributing to severe heat and dry conditions. Enhanced warm moist air flowed over northeast China and the Sea of Japan coastal areas of Japan along the western and northern periphery of the Pacific high (Fig. SB7.7), contributing to heavy rain in these countries. In addition, upper cold air occasionally flowed over the areas in association with the southward meandering of westerly winds, contributing to heavy rain brought by unstable atmospheric conditions. Convective activity was significantly enhanced over large parts of the Asian summer monsoon region (south and Southeast Asia) in association with sea surface temperature anomaly patterns in the Pacific (above normal around Indonesia and the Philippines, and below normal in the equatorial central eastern Pacific). This heightened convective activity contributed to the enhancement of the northwestern Pacific high. (i) Temperature South Asia in general experienced moderately warm temperatures in 2013. The annual mean temperature for India in 2013 was +0.35°C above the 1961-90 average, making it the 13th warmest year on record since nation-wide records commenced in 1901 ( Fig. 7.37). Warmer temperatures during the winter season (January-February, +0.65°C) and the pre-monsoon season (March-May, +0.57°C) mainly contributed to the warmer annual temperatures. The pre-monsoon season this year, with daytime (maximum) temperature anomaly of +0.68°C, was the third warmest on record since 1901 after 2010 (+1.4°C) and 2002 (+0.71°C).',\n       nan,\n       \"To answer Research Question 1, we found the variabilities in students' attitudes towards science and science performance may be influenced by unique educational systems in the three countries. South Korean students outperformed peers in Turkey and the United States in TIMSS 2015, but their attitudes towards science were very low. This unique pattern may be associated with the country's high standards of academic excellence. Therefore, South Korean students are exposed to the rigorous education system and have too much stress related to high stakes testing resulting in low confidence and motivation in learning science. Yoon et al. (2014) examined South Korean students' interest and confidence in learning science in third, seventh, and tenth grades. The researchers found South Korean students have difficulties understanding the nature of science and see schools as a place to listen to lectures and do scientific practices in science. The researchers suggested that students' negative attitudes towards science might be related to students' learning experiences in classrooms such as having limited engagement and hands-on activities in science courses (Yoon et al. 2014) . Topçu et al. (2016) reported that Turkish students' enjoyment of science had a predictive effect on their science achievement as opposed to South Korean students. Students' level of anxiety, however, affected their achievement scores negatively in both countries.\",\n       '• Both HIT and MICT regimens can be effective for increasing CRF in healthy adults and patients with CVD. When total work performed during training is held constant, HIT is likely to elicit greater increases in CRF than MICT. Results across studies are inconsistent in comparisons of the effects of HIT and MICT on increasing CRF. Reasons for these differences may include population-specific response differences, training protocol variations (intensity, session duration, training duration), and differences in testing protocols.\\n• The role of HIT regimens in the reduction of cardiovascular clinical events remains unclear, and the added risk of musculoskeletal and cardiac complications in selected patients needs additional evaluation. Most studies on the clinical benefits of HIT in cardiac rehabilitation have used MICT for comparative purposes, and long-term validation in patient populations is needed.\\n• Although HIT may be as safe as MICT for patients with CVD, more data are needed. In summary, there is an age and sex effect on the distribution of CRF in the general adult population, with women and older people having lower values. Also, inactive men and women vary in their CRF, in part because of genetic differences and other factors, and there are genetic-based interindividual differences in their response to a standardized physical activity regimen. 10, 288 However, CRF responses to a standardized physical activity regimen (similar type, amount, and intensity as percentage of capacity) are not significantly influenced by age or sex. 278, 289 Thus, a standardized approach to recommending dose parameters can be used in adult populations, taking into consideration individual levels of CRF, exercise preferences, and opportunities for increasing physical activity over the long term. Most of the lower mortality risk associated with a higher CRF occurs by the time a CRF of 10 to 12 METs is achieved. CRF values >12 METs are associated with a relatively lower impact on risk of all-cause and CVD mortality. Below 10 METs, as CRF decreases, risk progressively becomes higher at an accelerated rate. 290 Thus, to lower CVD risk by increasing CRF, the gains appear in men and women with baseline CRF ≤10 METs. Results from various studies evaluating CRF and CVD risk indicate that an increase in CRF of even 1 MET is associated with a 10% to 20% decrease in mortality rates. 2, 16, 18, 97 In addition, a review of varied physical activity regimens (Table 7) indicates that exercise increases CRF by at least 10% (a 1-MET increase for individuals with a capacity of 10 METs). Thus, to decrease CVD risk, physical activity regimens should be implemented with an initial target of increasing CRF ≥10%. Further increases in CRF may require additional increases in physical activity intensity or amount. Recommendations listed in Table 8 provide information on each of the physical activity components that should be considered in the implementation of a physical activity program.',\n       nan,\n       \"In the AS-TI-CU-SA model which implanted school achievement (SA) variable in the precedent AS-TI-CU model. The model fitting was calculated as being acceptable, as shown in Table 5 . The Chi-Square divided by the degree of freedom is 1.44, which is reasonable with the probability level of 0.07. Likewise, the other indices such as TLI, CFI, and RMSEA consistently fell within each criterion. Because the Korean sample (N = 219) was divided into 10 th -graders (n = 109) and 11 thgraders (n = 110) by different examinations of school achievement (SA), each model was depicted respectively (see Figure 4 and Figure 5 ). In AMOS7 output format, every estimate which has their critical ratio in the condition of |CR| > 2.0 are statistically significant in P = 0.05 level (Han, Omta, & Trienekens, 2007) .\\nRegardless of the additional latent variable, school achievement (SA), Figure 4 indicates that the AS-TI-CU-SA model fits with Korean 10 th -graders and confirms the consistent structure with parameter significance to Kim and Song's (2009) AS-TI-CU model fitted with Japanese 10 th -graders. These consistent findings include (1) that Intrinsic AS which concerns general science (favourableness towards careers in S&T)\\nestimates TI (topic interest in how rainbows occur) and CU (understanding on magnetic field around wire) in physics; (2) Extrinsic AS (agreement on importance of science) does not contribute to stimulating TI or CU; (3) Even though both variables concern the contents of physics, TI and CU have little causal relationship.\\nWith regard to stimuli of school achievement (SA), CU was examined to exclusively estimate SA most robustly by 0.48. Any other latent variables do not possess causal relationships with SA with parameter significance (P > 0.05). That is, if students have improved their conceptual understanding in physics from 1 to 100 scores, they would achieve school achievement from 1 to 48. The AS-TI-CU-SA could explain 33% of the variance in SA, and due to that there exists other variables beyond the scope of this study. What is also notable is that Intrinsic AS estimates little of SA. Due to their sequent directions of causal influence (Intrinsic AS → CU → SA), indirect effects of Intrinsic AS on SA is seemingly predicted, and discussed later in this section.\\nAmong both 10 th -and 11 th -graders, multiple group analysis examined the group difference by comparing critical ratios between each pair of parameters (group significance). As shown in Figure 5 , the three identical estimates with parameter significance, but without group significance (i.e., none group difference), were confirmed. Furthermore, the relationship from TI to SA was identified as being negative (-0.31) with both parameter significance (CR = 4.57; P < 0.001) and group significance (CR = -2.42; P = 0.05). That is, higher topic interest (TI) decreased school achievement (SA) only among the 11 th -graders.\\nAs assumed in Figure 4 , school achievement (SA) was examined to determine whether it has an indirect effect. The first column in Table 6 presents direct effects on SA, which are identical on the estimates depicted on each 10 th -and 11 th -model. In the second column, the indirect effects were also examined from Intrinsic AS (0.37 in the 10 th ; 0.20 in the 11 th ). That is, Intrinsic AS, which was found to be a major predictor of CU and TI, was not measured to influence school achievement (SA) by direct effects (-0.01 in the 10 th ; insignificant in the 11 th ). Rather, the indirect effect of Intrinsic AS was estimated to be robust among the final model both in 10 th -and 11 th -graders. This causal relationship confirms that Intrinsic AS first stimulates CU extending its influence to SA (Intrinsic AS → CU → SA).\\nIn summary, the AS-TI-CU-SA model concludes the three empirical findings. First, the SEM analysis of the Korean students (N = 219) attested another piece of evidence to the effects of attitudes towards science (AS) on topic interest (TI) and conceptual understanding (CU). The dichotomous AS-students perceive science to be important but not for their career or school Table 3 ), which is consistent with the precedent reports among Japanese and European students (Jenkins & R. G. Pell, 2006; Kim & Song, 2009; Matthews, 2007; Osborne & Collins, 2001; Stefánsson, 2006) . It reassures that student's preference on careers in S&T and school science (Intrinsic AS) is a strong determinant of how much students understand physics concepts among the Korean samples (see Figure 4 and Figure 5 ). Second, this reassurance is followed by the discussion on stimuli of school achievement (SA). Intrinsic AS was examined to extend its influence into SA mostly by indirect effects passing through CU (see Table 6 ). Third, the multiple-group analysis of structural equation modelling identifies the difference between the 10 th -and 11 th -graders. Among the 11 th -graders who generally concentrate more on advancing to higher education than the 10 th -graders do, the interest in the 10 physics topics in school science moderately hinders them in obtaining higher school achievement in physics.\",\n       'Cerebral perfusion cannot be predicted from BP alone on account of the non-linear pressure-flow relationship due to autoregulation; this is in contrast to the linear or curvilinear relationship between pressure and flow in nonautoregulatory tissues. \"Autoregulation\" most commonly refers to CBF adaptation to acute and chronic changes in arterial BP and perfusion pressure. Pressure autoregulation maintains a fairly stable perfusion over the range of mean systemic pressures 60-150 mmHg. Static autoregulation refers to long-term \"steady-state\" control, whereas dynamic autoregulation refers to the adaptation of perfusion to beatto-beat variations in intracranial pressure and BP [25] . Many medical conditions are associated with autoregulatory impairment, such as hypertension, hypotension [26] , diabetes mellitus (DM) [27] , vascular disease, smoking, and stroke [28, 29] . Even in healthy people, autoregulation may fail if BP falls below the lower limits during acute conditions such as syncope. With impaired autoregulation, the sigmoid autoregulation curve, which expresses the relationship between CBF and mean BP, becomes more linear, and perfusion becomes pressure-dependent. Figure 1 illustrates this concept. Chronic hypertension and hypotension alter CBF pressureflow relationship and may affect the autoregulatory range. Orthostatic and postprandial hypotension are defined as a ≥20 mmHg decline in systolic BP or as a ≥10 mmHg decline in diastolic BP when in an upright position, or within 1 h after a meal [30] . DM is the most common cause of autonomic failure, where altered BP regulation is characterized by supine hypertension and orthostatic and/or postpradial hypotension. Therefore, the mean BP range may vary greatly from >180 mm Hg in supine position to ≤80 mmHg in an upright position.\\nTCD is commonly used as a noninvasive assessment of static and dynamic autoregulation using beat-to-beat recordings of BP and blood flow velocity (BFV) [31] at baseline and during interventions such as standing up or head-up tilt [19, 32] . Three patterns of autoregulatory responses were identified using a regression analysis approach in patients with autonomic failure and orthostatic hypotension:, narrowed autoregulation range, normal autoregulation, and expanded autoregulation over a wide range of BP (mean BP 110-180 mmHg) [26] . With normal autoregulation, regression analyses have shown no correlation between BP and BFV, while with expanded autoregulation, BP and BFV remained highly correlated but the slope of regression line was flat. Therefore, BFV declined only mildly and was usually maintained in an upright position. The autoregulation curve in hypertension was shifted to the right toward higher BP values, and therefore lower limits of autoregulation occurred at lower BP levels. With this shift, the autoregulation window narrows and the slope of the CBF-BP curve becomes steeper. In this setting, vasodilatation responses to low BP may be reduced, especially with atherosclerosis, and vasoconstriction responses to high BP may be increased, due to endothelial activation and higher sympathetic tone. In autoregulatory failure, BP and BFV are strongly correlated, marked by a steep regression slope, which results in a rapid decline in BFV even with small reductions in BP. Therefore, in an upright position, BP may fall below the range of effective regulation, and perfusion decreases. As suggested by the above autoregulatory patterns, daily activities such as sitting, standing up, or eating a meal may reduce BFV and potentially induce hypoperfusion [33] , which could lead to syncope [34] , falls, or ischemia and cognitive changes [35] .\\nDynamic autoregulation is often assessed from spontaneous fluctuations in BP and BFV [31] at baseline and during interventions such as the Valsalva maneuver, standing up or head-up tilt [19, 32] CBF fluctuations at 0.01-0.03 Hz were linked with intracranial pressure [36] , central sympathetic activity [37] , microcirculation, and cerebral oxygenation [38] . Autoregulation is quantified using mathematical modeling, Fourier transform analysis, coherence function, and more recently developed nonlinear methods [22, 27, 39, 40] .\\nA substantial phase lead of CBF velocities with respect to the peripheral BP indicates intact autoregulation [31, 41, 42] . Nonlinear approaches, such as multiple coherence [19] and multimodal pressure-flow (MMPF), enable assessment of autoregulation at multiple time scales and have greater sensitivity and specificity to defects in autoregulation than do linear methods [27, 40] . An application of the MMPF method demonstrates, for example, that hypertension and DM significantly impair autoregulation to an observable degree after an ischemic stroke [40] . Mathematical modeling has been increasingly used to assess timedependent relationships between BP, BFV, and intracranial pressure and volume to predict cerebrovascular capacity and flow reserve controlled by autoregulation, perfusion pressure, and cerebrospinal fluid production and reabsorption [43] .',\n       \"The logistic regression analysis examined the influence of demographic (Sex, Race, Age), pre-college (ACT, High school GPA), and college variables (institution, type of students, academic remediation, college GPA, and credit hours earned) on whether a student declared a STEM major or a non-STEM major. Table 2 displays the parameter estimates, significance values, and fit statistics for the three regression models. The beta coefficient (β) value in the logistic regression tables represents the change in log odds of the dependent variable occurring related to one unit change in the predictor or independent variable, with other variables held constant. The greater the β value, the more the predictor is weighted in the model. Model 1 examined the demographic variables, sex, race, and age. Results showed that race significantly predicted persistence in STEM degree completion. White students were more likely than other racial groups to persist in STEM completion. Model 2 added pre-college variables to the baseline model which include high school GPA and ACT score. Race, ACT, and high school GPA significantly predicted STEM persistence when other variables were controlled.\\nModel 3 includes all three clusters of variables, demographic, re-college, and college variables, in the multiple regression analyses using the Enter method. A review of the parameter estimates and associated probabilities identified that the likelihood of declaring a STEM major was uniquely influenced by students' sex, race, ACT, high school GPA, institutes, type of students, remediation courses, first year GPA as well as cumulative number of credit hours earned.\\nAn examination of the odds ratios showed that holding other thing equal, minority students were less likely than White American students to persist in a STEM major. Other things being equal, younger students were more likely to persist than older students and students who had earned more course credits were more likely to persist than those who earned fewer course credits. Of all variables selected in the model, high school GPA appears to be the strongest predictor of persistence in completion of a STEM major. \",\n       nan,\n       'Common reed is found worldwide. It tolerates a range of abiotic conditions and is found in both freshwater and coastal habitats, although its establishment and growth is limited by fl ooding duration and high salinity and sulfi de levels (Chambers 1997). Reeds have been shown to form extensive stands in tidal marshes with salinities <15 ppt. Small, more recently established plants grow well at salinities from 0-5 ppt, exhibit some reduction in growth up to 35 ppt, and have diffi culty persisting when salinities exceed 35 ppt (Chambers et al. 1999). In North America, the range of reeds has expanded dramatically since the late 19th century, and in',\n       'The results of student fixed-effects models based on equation 1 estimated on the state test-audit test gaps in each subject are shown in models 3 and 6 of table 4. Also shown in the table are models with the state and audit test scores as dependent variables. The first row displays estimates from all students in our analytic sample. The next three provide separate estimates for black, Hispanic, and economically disadvantaged students, respectively; we do not separately estimate regressions for white and Asian students because only a small fraction of these students were in schools facing accountability pressure. The all-student coefficient on the Failed AYP variable from model 1, 0.0374, indicates that students in schools in the year immediately following an accountability threat from NCLB have state math test scores that are about 4 percent of a standard deviation higher than students in schools that face no accountability threat from NCLB. The coefficient from model 2, −0.0232, from a regression with the audit math test as the outcome, indicates a significant negative test score difference between students in schools facing accountability threats relative to those in schools not facing such threats. The math gap, shown in model 3, is essentially the difference between columns 1 and 2. The coefficient, 0.0607, is positive and statistically distinguishable from zero, which suggests that the NCLB accountability threat has a larger effect on math state test scores than on audit math scores. Increases in the state test-audit test gap in math suggest that schools are responding to the incentives in NCLB to raise test scores on the assessment linked to the state standards and AYP calculations. In this case, these effects do not generalize to performance on the audit test; in fact, they produce a small decline in these scores.\\nIn reading, the small and negative effects we find on both reading scores produce a null reading gap. This could have occurred for at least two reasons. First, many of the studies cited earlier have found larger effects of accountability pressure on math compared to reading. Second, the fraction of students in Houston failing mathematics tests was significantly higher than for reading, such that schools facing accountability pressure were more likely to have missed AYP targets because of their math scores and thus to have had an incentive to focus more heavily on math.\\nThe conclusion that schools facing accountability threats tend to produce larger state test-audit test math gaps holds across black, Hispanic, and economically disadvantaged subgroups, which all have positive math gap effects. That is, we find that accountability pressure increases the gap in performance on the two tests. The point estimate for blacks is somewhat larger than for other groups, and the patterns across the state and audit tests + p < .10; *p < .05; **p < .01; ***p < .001\\n: a c c o u n t a b i l i t y, i n e q u a l i t y, a n d a c h i e v e m e n t 2 3 3 differ. While the Hispanic gap between the two tests emerges because of gains on the state test and small losses on the audit test, black students experience no gains on the state test and a loss of 0.06 standard deviations on the audit test. We see this pattern emerge again for reading tests, where the effects on reading gaps between the state test and audit test are small, positive, and statistically significant for black students. This gap is produced by black students making no gains on the state test and experiencing losses on the audit test. These effects may be conservative because they do not distinguish among the types of schools most at risk under NCLB. As noted earlier, we have defined \"risk sets\" of schools based on their probability of failing to meet AYP targets. Incentives-based perspectives predict that the effects of incentives to increase state test scores rather than audit test scores will be the strongest for schools at the margin of failing to meet AYP targets. This hypothesis predicts that (1) schools at very low risk of failing to meet AYP targets will have null or negative accountability-induced gaps (that is, their gains on the audit test will be larger than those on the state test) as these schools focus more on skills that are not test-specific, and (2) schools at the margin of failing to meet AYP targets will have large accountability-induced gaps and schools virtually certain of failing to meet AYP targets will have somewhat smaller accountability-induced gaps than schools at the margin of failing. On the other hand, schools well below the AYP threshold face the most severe sanctions in the medium to long term. This perspective predicts that schools virtually certain to fail to meet AYP targets will have the largest accountability-induced gaps, schools at the margin will have somewhat smaller gaps, and schools at low risk of failure will have no gap or negative gaps overall. Table 5 presents the effects of accountability pressure defined as high and low risk of failing to meet AYP targets for all students and separate estimates for black, Hispanic, and economically disadvantaged students. If schools are only focused on short-term incentives, we would expect to see the greatest response by schools at medium risk of failing to meet AYP targets. The first coefficient in column 1 of table 5, 0.0418, indicates that for all students the high-risk-medium-risk difference in math state test scores is about 4 percent of a standard deviation. In other words, students in schools at high risk of failing to meet AYP targets have higher math state test scores in the subsequent year than students in schools at the margin of passing AYP targets. By contrast, students in schools at high risk have lower math audit test scores (−0.0585) in the subsequent year than students in schools at the margin. The accountability-induced state test-audit test gap is therefore 0.100 of a standard deviation, which indicates that relative to students in schools at the margin of passing AYP targets, students in schools at high risk of doing so have larger gaps. The high-risk-mediumrisk differential in the reading gap is also positive, but smaller, at 0.0413. Turning to the low-risk-medium-risk differential, we find negative gap scores in math and no gap in reading. Students in low-risk schools gained on audit tests even as their state tests declined. We note that these results are not due to ceiling effects on the state tests.\\nOverall, the pattern of coefficients in table 5 suggests that when schools face additional pressure, they either become more aligned to state standards or \"teach to the test.\" We make this inference because high-risk schools see increases in state test scores and decreases in audit test scores in both subjects, while lowrisk schools are more likely to make progress on the audit test. Our results alone cannot differentiate between these two mechanisms, but we believe that it is important to note that greater accountability pressure appears to produce specific versus general gains. We return to the normative questions raised by this finding in the discussion.\\nMoving to the subgroup results, the bottom panels of table 5 show that black, Hispanic, and economically disadvantaged students experience approximately the same accountabilityinduced state test-audit test gap in high-and low-risk schools in both subjects. However, the sources of the gap vary across subgroups, for the math test in particular. Based on our point estimates, black students in high-risk schools experience audit test losses approximately twice as large as those experienced by Hispanic : Source: Authors\\' calculations from Houston Independent School District data. Notes: Although our data cannot explain why black students lose more than Hispanic students on the audit math tests, we note that the pattern of Hispanic students benefiting more from accountability pressure has been documented in other studies (Hanushek and Raymond 2004; Lauen and Gaddis 2012) .',\n       \"This descriptive analysis has demonstrated racial/ethnic and gender differences among the 1988 eighth-grader cohorts for understanding uneven postsecondary S&E enrollment across groups. The examination of school variables has identified different learning opportunities available to the groups and the examination of student behavior variables has identified distinct psychological attributes. The data suggest two types of inequality: underrepresented minority students were disadvantaged by both inferior learning opportunities and eroded psychological attributes; whereas girls suffered essentially from a psychological disadvantage. Racial/ethnic differences were observed in a number of factors relevant to preparation for postsecondary S&E education. Relative to white and Asian students, underrepresented minorities were less likely to expect to complete college (i.e., earn a college degree), though they were no less likely to want to attend college. Fewer underrepresented minority students than other students expressed strong personal interest and self-confidence in studying math and science. Moreover, underrepresented minority students on average had a smaller chance of participating in favorable academic programs, such as high ability programs, college preparation programs, and advanced curricula in math and science. Further, underrepresented minority students' schools also seem more restrained than other schools in providing curricular and extracurricular programs, though no instructional difference was revealed across racial/ethnic groups. Facing such difficulties, underrepresented minority students performed poorly in math and science compared to white and Asian students. Gender differences were found largely in the psychological respect only. Relative to boys, girls showed high aspiration for education attainment in general, and girls did not lag behind boys in terms of effort in math and science learning. However, proportionally fewer girls than boys expressed motivation or personal interest in learning subjects related to S&E. In contrast to underrepresented minority students who were disadvantaged by limited resources and opportunity, girls did not seem to suffer tremendously from limited learning opportunities. No gender difference was found regarding math and science curricula, and girls were more likely than boys to be placed in gifted/talented programs. The rates for participating in science-related enrichment activities were lower for girls than boys, but the difference was hardly attributable to limited opportunities. No substantial gender difference was found in school program offering and classroom instruction. Unremarkably, however, given the identified psychological disadvantage, girls' achievement levels in math and science were lower than boys'. The descriptive analysis revealed racial/ethnic and gender differences relevant to preparation for postsecondary S&E education. These predictor variables were then used in a logistic regression analysis to further examine the relationships between S&E program entry and these variables. The specific goal of the regression analysis was to understand which factors are predictive of the group difference in S&E program entry. The outcome measure in this analysis was enrollment in postsecondary S&E among high school students surveyed in the NELS:88 base year through third follow-up surveys. College major selection normally takes place in the early years of postsecondary schooling, though it varies in timing for individuals and institutions. Some students may make the choice as early as the first years of college, many do in their junior year. Institutions may have different requirements regarding selecting a major. In community colleges, the decision is normally made during an early stage of entrance, while in liberal arts colleges the choice may be made much later. NELS:88 data on major selection were gathered in the second year in college. Arguably, the data should represent a substantial portion of the cohort members' initial S&E program entry, though by no means cover the full range of program enrollment, which may include delayed major selection or even program change and program termination. As discussed earlier, the postsecondary academic fields used to define S&E programs are as follows: engineering; earth, atmosphere, and ocean sciences; mathematical or computer sciences; physical sciences; and biological and agricultural sciences. A related issue in defining S&E majors with NELS:88 data was whether to include into the analysis respondents who did not enter postsecondary education after high school. A number of prior studies on S&E entrance have compared college students who majored in S&E to their peers who majored in other fields, whereas cohort members who did not enter college were excluded from analyses (e.g., Ware and Lee 1988). Making such retrospective contrasts between S&E majors and other majors is analytically convenient because it avoids the confounding issues of college entrance. It seems an easier approach to isolating factors that are responsible for academic field selection because those who did not enter college were not considered in the analysis. Using this approach on a broad, national scale, however, does not allow one to address the racial/ethnic and gender gaps existing in the overall population. It leaves unanswered questions as to how certain groups in the total population are underrepresented in getting into the S&E pipeline as a result of their lesser likelihood of entering college or even graduating from high school. Therefore, as explained in the Sample Design section, this 7 1 _a_ analysis included the respondents who did not go on to college after high school, including the high school dropouts.\\nFindings from BPS data on S&E enrollment show the following racial/ethnic and gender gaps in postsecondary S&E pipeline entry. Among African-American students, 12 percent were enrolled in S&E programs in their first year of college, not substantially lower than whites' 13 percent; Hispanics had a higher rate of S&E enrollment than whites; and Asian-Americans had the highest rate, 21 percent (see figure 4). While the rates fluctuated over the years, by the end of the last year, the rates for African-Americans, Hispanics, and whites all came close to 17 percent. Only the rate for Asian-Americans remained higher (21 percent).  The racial/ethnic differences in S&E enrollment among students attending 4year colleges are similar to those found in the total sample (figure 5). There is no large minority-white gap, except that Asian-Americans' enrollment was much higher than the rest over the years. Relative to whites, Hispanics rated slightly higher at the beginning year but slightly lower in the last year. The rate of African-Americans in S&E enrollment remained relatively low, though the gap is less than dramatic. BEST COPY AVAILABLE Figure 5. ' acialllethnic differences in S&E enrollment by academic year: Students attending 4-year colleges Women consistently enrolled in S&E programs at lower rates than men in the total BPS sample (figure 6). Compared with racial/ethnic gaps, the gender difference was widerwomen's enrollment rate was less than a half of men's (7.6 percent and 20.4 percent, respectively) at the beginning year of the survey and the difference persisted at the end of the survey (10.4 percent versus 24.2 percent). The pattern was largely the same as found with data for respondents who enrolled in 4-year institutions (see figure 7). The differences of enrollment rates for race/ethnicity and gender were fairly stable across the 5 yearsa typical time span for college years. Clearly, BPS data indicate that racial/ethnic difference in S&E entry is not very substantial. Asian-Americans continued to have high enrollment rates; the rest were fairly close to each other. However, the gender gap is salient, with women severely underrepresented in S&E entry. This result is consistent with the pattern found with NELS:88 data. This overview of S&E enrollment differences is meant merely to introduce readers to the large pattern of the S&E pipeline demographics. With vast heterogeneous postsecondary institutional and programmatic alternatives as well as individuals' choices of how to go through the system, readers are cautioned that this picture may be overly simplistic and requires detailed analysis. BEST COPY AVAILABLE \",\n       'Our goal is to segment a dataset of medical images, and we focus on brain MRI in our experiments. Let {I i , S i } N i=1 represent a small dataset of labeled atlases, each consisting of the grayscale image I and discrete segmentation map S, such that each voxel of S corresponds to one of L anatomical labels.',\n       \"The meeting began with an overview of the problem, which was characterized as ''the tsunami of healthcare, worse even than the economic crisis we are now seeing.'' The incidence of dementia is rising astronomically because of our aging population. In 2002, the United Nations Population Division reported that the number of people aged 60 years or older tripled over the previous 50 years, and will triple again over the next 50 years. Couple that with the association between increased age and the incidence of dementia: the incidence of dementia doubles approximately every 5 to 6 years after age 65 [2] , meaning that among individuals 80 to 85 years old, as many as 40% to 45% have some form of dementia. More importantly, research over the past 30 years shows that by the time a person is diagnosed with dementia, the disease has already caused neurodegeneration of vast areas of the brain. Hippocampal volume loss, one of the earliest measurable signs of AD, quantifies damage that has already occurred. Yet we do not have tools to measure important antecedent conditions such as dendritic and synaptic loss. Such measures may one day be used as biomarkers of AD to identify neurodegeneration earlier and diagnose the disease in its early stages, but equally important, without these tools, we cannot even begin to understand the fundamental mechanisms that underlie dementia: why synapses are being lost, and why dendrites are pruned in specific ways. Yet such an understanding is critical if we are to develop effective preventive strategies.\\nAmong the universe of people who will develop dementia, there is a wide range of patterns and trajectories in which the condition will progress. After the disease process begins, cognitive function remains at a plateau for a variable amount of time, and then begins to decline at variable rates. The goal of preventive therapy is thus to increase the length of the plateau, and to slow the slope of decline. From a diagnostic and treatment perspective, the challenge is to predict who among normal individuals will likely develop incident dementia at younger ages, i.e., less than 65 years, and at older ages, R85 years. Such information can lead to strategies for preventing the development of key risk factors (i.e., primordial prevention), and therapies to slow the progression to clinical dementia. This will require the application of existing technologies (e.g., imaging, biomarkers, and genetic risk factors) to younger people, and the development of new technologies, new genetic markers, and new biomarkers. The overall goal is to extract as much information as possible from very early indicators, to predict an individual's disease trajectory and how a putative treatment might alter that trajectory. The multifactorial nature of the disease will undoubtedly require a pattern analysis of various indicators, as has been the case in genomics and proteomics studies. Eventually, we hope to identify the relevant set of indices so that when an intervention becomes available, appropriate individuals can be selected to test the effectiveness of that intervention.\\nWith existing technologies and the lack of a validated surrogate marker for AD, prevention trials take 15 to 20 years before results can be available. Realistically, this must be shortened to about 3 to 5 years, which will require new technologies and surrogate markers. To convince pharmaceutical regulatory bodies such as the Food and Drug Administration and European Medicines Agency (EMEA) to accept new technologies and surrogate markers, they will have to be validated on large, diverse populations. Thus the need for a diverse population cohort is twofold: first, to validate new technologies and biological markers that can serve as proxies (i.e., surrogate markers) for the biological process that leads to dysfunction; and second, to identify individuals in the earliest stages of the disease, for enrollment in short-term clinical trials of preventive strategies.\",\n       'In the MCI group (Supplementary Table 3 ), functional competence on the UPSA was significantly correlated with working memory (r 5 0.38) and cognitive control (r 5 0.47) factors. In the EHC group executive function was a significant correlate of functional competence (r 5 0.33). In AD relationships between WM/EF and functioning were more widespread.\\nWith respect to episodic memory correlations with WM/ EF in the EHC and MCI groups, these were consistently small and not significant, suggesting that WM/EF is not driven by memory failures. In AD, WM/EF impairments were correlated with memory suggestive of a \"disease blurring\" factor due to more global impairments.\\nWe next sought to determine the amount of variance in the UPSA that could be predicted by all three WM/EF factors in a multivariate regression model in the whole group ',\n       'Composite variables were constructed for students; the composites included on the parent tape are a subset of those on the student file. The conventions used to assign SAS and SPSS variable names are as consistent as possible with HS&B and NLS-72. In those two surveys, variable names were assigned according to the survey wave and the question number. A similar system was developed for NELS:88. For example, BYP85G, is from the base year parent survey, question 85, part G. Most composite variables were constructed using responses from two or more questionnaire items. In some cases, composites were constructed from variables from different databases. Others were constructed by recoding a variable and a very few were simply copied from a different data source to this file for the user\\'s convenience. Composite variables may be valid throughout the survey (e.g. SEX) or they may be specific to this particular survey wave. The names of the latter begin with BY for base year. Hence, BYFAMSIZ categorizes the base year family size. Weights are similarly labeled: BYQWT for the selection weight for student questionnaire completion adjusted for nonresponse during the base year, and so on. Composite variables, such as SEX, RACE, or G8ENROL, which will remain valid throughout the survey waves, have names that will remain unchanged. The only reserve code used for composite variables is that of missing data. For one-column variables that is an 8, for variables greater than one column, the leftmost columns are filled with \"9\"s (9....8). This reserve code is used when the sources for data are either item nonresponse or nonparticipation in all or part of the components of the study. Appendix D contains explanations of the conditions under which specific composite variables were assigned a missing code. ',\n       'The autocontext strategy is used: the cropped segmentation derived from MRI U-Net is given as second input channel to the network. This technique is one of the most well-known types of context information (Tu and Bai, 2010; Chen et al., 2016; Mirikharaji et al., 2018) and we aimed at investigating its effect also on our application of interest.',\n       \"Alzheimer's Disease (AD) is a neurodegenerative disorder that results in the increased production of amyloid-B peptide and Tau protein hyperphosphorylation, as well as the degeneration and death of neurons. Although the causes of late-onset AD pathology are unknown, family studies have demonstrated that complex genetic and environmental mechanisms contribute to disease risk [1, 2] . Since the elucidation of the APOE locus in 1993 [3] , over 660 candidate genes for AD risk have been identified, however results are inconsistent between studies [4] . To further characterize complex genes associated with AD, a growing number of studies are using an intermediate phenotype approach, which utilizes biomarkers, such as structural brain imaging of hippocampal atrophy, as endpoints in genetic analyses of risk.\\nThe brain-derived neurotrophic factor (BDNF) gene has been a candidate risk gene for diseases involving memory loss due to its facilitation of long-term plasticity in the hippocampus, a function that breaks down during the onset of AD. Moreoever, accumulating evidence points to a protective role for BDNF in neurons through increased neuroprotection [5, 6] , and reduction of Aβ peptide [7] . Post-mortem studies show that BDNF expression is severely decreased in the hippocampus, temporal, and frontal cortex in AD [8, 9] . Thus, decreased BDNF in the brain might contribute to advanced aging as well as AD [10] . There is a well-known functional single nucleotide polymorphism (SNP) in the 5' proregion of the human BDNF gene at nucleotide 196. The SNP results in a Valine (Val) to Methionine (Met) amino acid substitution at codon 66 (Val66Met, rs6265, G>A). When Val-BDNF and Met-BDNF are produced together in neuronal cells they form heterodimers, which alter BDNF trafficking and decrease secretion of BDNF [11] . Imaging genetics studies, which may be more sensitive then traditional gene-association studies, have recently identified a role for the BDNF Val66Met SNP in hippocampal volume loss [12] , memory impairments [13] , reduced medial temporal lobe activity [14] and modified experience-dependent plasticity in the motor cortex [15] in healthy humans. Increasing age may also mediate the effects of the Val66Met SNP [16] . Some studies have also shown that variation in this Val66Met polymorphism may increase risk for Alzheimer's Disease and impact cognitive performance [17, 18] . However, there is still conflicting evidence of the relationship between BDNF genetic variation and AD [19] [20] [21] [22] , with several studies showing no relationship. Finally, other functional SNPs in BDNF have been identified that may impact human brain function [23] , demonstrating the importance of investigating multiple BDNF SNPs using an AD phenotype approach to clarify BDNF's role in brain neurodegeneration.\\nThus, our goal was to use neuroimaging and cognitive phenotypes that have been associated with AD, and test whether genetic variation in BDNF impacts these phenotypes in a large sample from the Alzheimer's Disease Neuroimaging Initiative (ADNI). ADNI is an NIH-sponsored, multi-site study assessing MRI, biological, clinical and neuropsychological traits to measure the progression of mild cognitive impairment (MCI) and early AD. This large dataset includes approximately 800 participants with imaging data, cognitive, and genetic data at several time points. There has been one analysis of BDNF Val66Met and brain metabolism in the ADNI sample [24] , however no studies have investigated the relationship of several BDNF SNPs to AD endophenotypes in this dataset to date.\",\n       'As in Study I, we used interjudge agreement correlations as the data. Again the unit of analysis was the trait, with the 80 bipolar trait scales serving as observations. Table 1 presents the correlations of the agreement indices with observability, evaluativeness, and each of the Big Five domains, computed across the bipolar trait scales. The correlations on the right-hand side of Table 1 show that both the observability and the evaluativeness effects on peer-peer and self-peer agreement were replicated.\" Moreover, the evaluativeness effect was again stronger for self-peer (r = -.53) than for peer-peer agreement (r = -.29), as shown by a paired-samples test for the difference between correlations, r(37) = 3.5, p< .01. With respect to the Big Five domains, we also found a similar pattern of correlations across the two studies; traits from the Agreeableness domain again elicited significantly less interjudge agreement than traits from the other domains, and traits from the Extraversion domain elicited somewhat higher levels of agreement. Overall, the multiple correlation of the Big Five domains was .44 with self-peer agreement and .29 with peer-peer agreement. This close replication of our Big Five effects with a set of traits representing McCrae and Costa\\'s (1987) variant of the Big Five domains shows that our findings are not specific to a particular selection of traits. For example, the fifth factor was not related to agreement, whether it was measured by traits related to Intellect or to Openness. We also replicated the difference between the two types of agreement by comparing peer-peer and self-peer agreement for the 80 bipolar trait scales.\\'\\' Averaged across all 80 scales, peer-peer agreement was 8. In Study 2, all p values for replicated effects were based on one-tailed significance tests. 9. The difference between peer-peer and self-peer agreement was somewhat smaller in McCrae and Costa\\'s (1987) data than in our own data (Study 1), probably because of the much greater length of acquaintanceship among their subjects. Because the peers were long-time friends of the subjects, they probably shared more information with them (cf. Kenny, 1991) and were also more emotionally involved, making their judgments more similar to self-judgments. Both factors would tend to reduce the difference between self-peer and peer-peer agreement. Importantly, however, this difference remained significant and noneof our other findings changed appreciably (see Figure 6). .22 (SD = .08), whereas self-peer agreement averaged .20 {SD = .08). Although this difference is small in magnitude (one-quarter of a standard deviation), it was statistically significant as shown by a pairedsamples / test across the 80 trait scales, r(79) = 2.7, p< .01. Moreover, this difference held for both the original 40 Goldberg scales, r(39) = 2.1, p < .05, and for the 40 scales McCrae and Costa added, t(39) = 1.8,p< .05. Finally, we tested our integrative model of the determinants of interjudge agreement. As in Study 1, we conducted a regression analysis with all five predictors entered simultaneously, using the Friedrich solution described in Aiken and West (1991). The regression weights from this analysis are given in parentheses in Figure 6. Again, all five determinants contributed independently to the prediction of agreement, and the multiple correlation was .67, similar to the .69 value obtained in Study 1. Moreover, both the direction and the magnitude of the regression weights were replicated. To test the generalizability of the model more formally, we conducted a double cross-validation analysis. The multiple correlation was .65 when we applied the regression equation obtained in McCrae and Costa\\'s sample to our sample in Study 1; conversely, when we applied the equation from our sample to theirs, the multiple correlation was .62. These results reveal relatively little shrinkage in the crossvalidation samples and thus provide impressive evidence for the generalizability of the model presented in Figure 6.',\n       \"To determine whether hypothalamus-pituitary-adrenal axis (HPAA) dysfunction is prospectively associated with global cognitive impairment in later life. ; and for NSHD between 2006-2010 and in 2015. Serial salivary cortisol samples were collected multiple times within a 24-hour period at mean ages 61.2 and 65.9 years in Whitehall II and at age 60-64 years from NSHD participants. Cortisol profile is defined using cortisol awakening response and AM:PM ratio. Cognitive function was measured using the Mini-Mental State Examination in Whitehall II and Addenbrooke's Cognitive Examination, third version, in NSHD, harmonized into a 30-point score. Models were adjusted for age, sex, diagnoses of hypertension and diabetes, body mass index (BMI), educational attainment, and interval between HPAA and cognitive assessments.\\nIn fully adjusted models, increased AM:PM cortisol ratio was prospectively associated with better later-life cognitive function years later (0.02 fewer errors per SD increase in AM:PM cortisol ratio, p < 0.01) and verbal fluency (0.03 SD increase in verbal fluency per SD increase in AM:PM ratio, p < 0.01). Increasing age, lower educational attainment, diagnosis of hypertension, diagnosis of diabetes, and increased BMI were associated with worse cognitive function and poorer verbal fluency. There were no associations between depression and later-life cognition or reverse associations between cognition and later-life cortisol profiles.\\nLoss of diurnal HPAA variation is evident in individuals subsequently experiencing more cognitive impairment. It may serve as an early preclinical marker of cognitive decline.\",\n       \"Total RNA was extracted from clarified cell culture supernatants (16,000 g x 10 min) using RNeasy ® Mini kit (Qiagen) following manufacturer's guidelines. RNA has been eluted in 30 μl and stored at -80 °C until use.\\nRandomly amplified cDNA was generated using Sequence-independent Single-Primer Amplification (SISPA) Round A/B technique as described [14] with minor modifications. Briefly, in Round A, RNA has been retrotranscribed with SuperScript III Reverse Transcriptase (Thermo Fisher Scientific), using 40 pmol of Sol-PrimerA (5′-GTT TCC CAC TGG AGG ATA -N9-3′). Second-strand DNA synthesis was obtained using Sequenase DNA polymerase (Thermo Fisher Scientific) by incubating first strand cDNA at 37 Purified products were quantified using the Qubit ® DNA HS Assay Kit (Thermo Fisher Scientific), then genomic libraries were prepared using Nextera DNA Flex kit (Illumina, San Diego, CA). Sequencing was performed using an Illumina MiniSeq ® platform (Illumina) generating 2x150 bp paired-end reads. Raw data were checked for quality using FastQC (https ://www. bioin forma tics.babra ham.ac.uk/proje cts/fastq c/) and for bacterial, archaeal, and viral genomes correspondence using Kraken2 with MiniKraken2 Database [15] , then were trimmed with Trimmomatic ver. 0.38 for quality (Q score > 25) and length (> 36 bp) by (i) removal of any adaptor sequences; (ii) removal of leading bases with PHRED < 25 and of trailing bases with PHRED < 25; (iii) clipping of the remainder of the read when a sliding window of 20 bases has average PHRED < 25; (iv) removal of reads with length < 36 bases [16] . Paired-end trimmed reads were analyzed with Geneious ® software (version 11.1.5) (Biomatters Ltd, New Zealand). Consensus sequence was reconstructed and mapped to the SARS-CoV-2 reference sequence NC_045512.2 using Bowtie2 in sensitive-local mode with consensus threshold at 65% [17] .\\nThe variant calling was carried out by the Variant Finder Tool (Geneious) filtering out variants with a p value greater than 0, using a minimum variant frequency of 0 and default parameters for Maximum Variant p-value (10 −6 ). The minimum sequencing coverage for each variant position was 10 reads. Variant's frequencies were evaluated as a sum of variant frequencies at that position. Each sample was processed and analyzed in two independent experiments.\",\n       nan,\n       \"It is worth emphasizing that while accurate decoding in a region points to underlying differences in the neural representations associated with different experimental conditions (e.g., for reviews see Haynes and Rees, 2006; Kriegeskorte, 2011; Naselaris et al., 2011; Norman et al., 2006) , a lack of decoding or 'null effect' (i.e., 50% chance classification) can either reflect that the region 1) is not recruited for the conditions being compared, 2) contains neural/pattern differences between the conditions but which cannot be discriminated by the pattern classification algorithm employed (i.e., a limit of methodology, see Pereira et al., 2009; Pereira and Botvinick, 2011) , or 3) is similarly (but non-discriminately) engaged in those conditions. With respect to the first possibility, given that we selected frontoparietal cortex ROIs based on their involvement in the motor task at the single-subject level (using the contrast of [Plan & Execute > Preview] across all conditions), it is reasonable to assume that all the localized areas are in some way engaged in movement generation. (Note that this general assumption is confirmed by the higher-than-baseline levels of activity observed in the signal amplitude responses during the Plan-and Execute-phases of the trial in areas of frontoparietal cortex [Figures 2 and 5] and that this even appears to be the case in the independently localizer-defined lateral occipitotemporal areas, EBA and pMTG [ Figure 6] ). Although it is understandably difficult to rule out the second possibility (i.e., that voxel pattern differences exist but are not detected with the SVM classifiers), it is worth noting that we do in fact observe null-effects with the classifiers in several regions where they are to be expected. For instance, SS-cortex is widely considered to be a lower-level sensory structure and thus anticipated to only show discrimination related to the motor task once the hand's mechanoreceptors have been stimulated at object contact (either through the hand directly or through the tool, indirectly). Accordingly, here we find that SS-cortex activity only discriminates between grasp vs reach movements following movement onset (i.e., during the Execute phase of the trial). Likewise, in motor cortex we show decoding for upcoming hand-and tool-related actions but, importantly, find no resulting across-effector classification. This latter result is highly consistent with the coding of differences in the hand kinematics required to operate the tool vs hand alone and accords with the presumed role of motor cortex in generating muscle-related activity (Kalaska, 2009; Churchland et al., 2012; Lillicrap and Scott, 2013) . These findings in SS-cortex and motor cortex, when combined with the wide-range of decoding profiles found in other areas (i.e., from the hand-selective activity patterns in SPOC and EBA at one extreme, to the tool-selective activity patterns in SMG and pMTG at the other, for summary see Figure 7 ), suggest that the failure of some areas to decode information related to either hand-or tool-related trials (but not those of the other effector) is closely linked to an invariance in the representations of those particular conditions. (To the extent that in cases where the activity of an area fails to discriminate between experimental conditions it can be said that the area is therefore not involved in coding [or invariant to] those particular conditions, we further expand upon interpretations related to these types of null effects in the 'Discussion' section.)\",\n       \"In this pooled analysis of data from the phase II ABBY and BLAZE studies and the phase Ib GN29632 study, we showed that the PK of crenezumab in patients with mild-to-moderate AD was dose proportional across the dose range tested (15-120 mg/kg q4w IV and 300 mg q2w SC) and was characterized by clearance and t 1/2 values expected of IgG mAbs (0.159 L/day and t 1/2~2 0 days, respectively) ( Table 2 ) [22] . Body weight was shown to influence the elimination clearance of crenezumab (CL el ), intrinsic clearance of crenezumab-Aβ complex (CL int ), and central distribution volume (V cent ), all of which increased with increasing body weight (Fig. 7) . These findings are consistent with the results observed for other therapeutic IgG molecules that exhibit linear kinetics in humans [22] . Total plasma Aβ(1-40) and Aβ(1-42) levels increased significantly following each administration of crenezumab demonstrating peripheral target engagement of monomeric Aβ(1-40) and Aβ . The increase in total Aβ(1-40) and Aβ(1-42) levels can be attributed to slower elimination of crenezumab-Aβ complex than free Aβ(1-40) and Aβ(1-42): 0.36 vs 114 [Aβ(1-40)] or 287 [Aβ(1-42)] on day 1. The observed increase in total plasma Aβ levels was dose dependent but not dose proportional and reached a plateau with the 120 mg/kg q4w IV dose. A TMDD model developed to characterize the observed serum crenezumab concentrations, plasma Aβ(1-40) and Aβ(1-42) levels, and the PK/PD relationship between them, successfully described the observed data, including the non-dose-proportional increase in plasma Aβ levels. This model suggested that reductions in free Aβ levels associated with crenezumab dosing are better maintained at higher doses, even after the total plasma Aβ levels had plateaued (Fig. 8 ).\\nOur analysis suggested that age and GFR explain some of the BSV in baseline Aβ levels. This is consistent with a previous study by Toledo and colleagues [18] who analyzed baseline Aβ measures in 715 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://www.adni-info.org/index) and reported that age, platelet count, total protein, and creatinine concentration were independent predictors of baseline Aβ(1-40) and Aβ(1-42) levels, explaining 12.1% and 12.9% of the observed variability in the respective parameters [18] . The model estimated K d of crenezumab against Aβ(1-40) and Aβ were similar to each other. This is consistent with in vitro observation that crenezumab has similar binding affinity to these two Aβ species (~10 nM) [F. Hoffmann-La Roche Ltd.; data on file]. Patient factors, such as age or sex, were not identified as significant covariates for K d , suggesting that the binding of crenezumab to Aβ is independent of currently evaluated patient factors.\\nThe PK/PD data from this study provide evidence of peripheral target engagement by crenezumab at evaluated dose levels. This complements the target Fig. 7 Predicted impact of patient baseline characteristics on PK profile of crenezumab and plasma Aβ levels. White vertical line refers to the predicted parameters for a 72-year-old male, weighing 72.3 kg, with a GFR of 72.5 mL/min/1.73 m 2 after a crenezumab 60 mg/kg IV dose. Red bar depicts the 5th-95th percentile crenezumab exposure (left column) or Aβ levels (right column) range across the entire population. Green bar represents the influence of a single covariate on the predicted parameters. The upper and lower values for each covariate represent 90% of the observed covariate range in the population. Abbreviations: Aβ beta-amyloid, AUC area under the curve, BWT body weight, C max peak concentration, GFR glomerular filtration rate, IV intravenous, PK pharmacokinetics engagement in the central nervous system previously suggested in the phase II ABBY and BLAZE studies through the increase of total monomeric Aβ(1-42) in CSF of patients with mild-to-moderate AD treated with crenezumab [8] . The increase in total monomeric Aβ(1-42) is likely due to slower elimination of crenezumab-Aβ complex. We further evaluated relationships between crenezumab concentration and Aβ(1-42) increase in CSF, but no clear correlation was observed ( Fig. 5) , which could be due to limited ranges of evaluable doses and high variability between patients for demonstrating exposure-response relationships in CSF. The concentration of crenezumab in CSF was much higher than the concentration of Aβ, yet still much lower than in serum (approximately 0.3% of serum). This suggests that a wider dose range is needed to be able to quantify the exposure-response relationship in the central nervous system.\\nThe PK/PD simulation provided several important insights. Firstly, the simulation showed that circulating crenezumab is predominantly unbound. This indicates that the transfer of crenezumab to peripheral organs and Fig. 8 Simulations illustrating effects of varying crenezumab doses (mg/kg q4w) on plasma Aβ kinetics based on the developed PK/PD model. Note that total and free crenezumab concentrations overlap with each other. Abbreviations: Aβ beta-amyloid, PD pharmacodynamics, PK pharmacokinetics, q4w every 4 weeks the central nervous system is likely not influenced by its binding to Aβ. The low concentration of crenezumab-Aβ complex, compared with total crenezumab, also suggests that the transfer of the binding complex is unlikely to serve as a new source of Aβ for peripheral organs including the central nervous system.\\nAnother interesting observation is that predicted free Aβ levels continued to decrease with increasing dose, even after the accumulation of total Aβ reached a plateau ( Fig. 8 ), suggesting that total Aβ change may not be fully reflective of actual drug effect. The reason for this apparent discrepancy between total and free Aβ profiles is that accumulation of total Aβ is due to slower elimination of crenezumab-Aβ complex than free Aβ. Therefore, once the crenezumab-Aβ complex becomes the predominant species of total Aβ, there will be no further increase in total Aβ with increased crenezumab dose. In contrast, binding of crenezumab to residual free Aβ can still occur with a higher free crenezumab concentration. Additionally, since analytical quantification of free Aβ is technically challenging due to changing levels of bound vs free Aβ in vitro after CSF collection that might differ from the levels in vivo, the model proposed in the current study could be a promising tool to integrate available information, such as total Aβ levels, and provide further insights on the kinetics of unmeasured species such as free Aβ.\",\n       nan,\n       'From the CES Annual Report 1995Report -96 (1996: Half a century ago visionaries representing both the Census Bureau and the external research community laid the foundation for CES and the RDC system. They saw a clear need for a system meeting the inextricably related requirements of providing more and better information from existing Census Bureau data collections while preserving respondent confidentiality and privacy. The CES and the RDC system meet those requirements. They meet the commitments of the Census Bureau (and, recently, of other agencies) to preserving confidentiality while contributing paradigm-shifting fundamental research in a range of disciplines and up-to-the-minute critical tools for decision-makers. Our increasingly complex and interconnected economy and society require more information, on evolving topics, delivered in rapidly changing forms. Information technology changes at least as rapidly, constantly providing both new ways to collect and present information, and new threats, real and perceived, to the security of that information. The CES and RDC system of the future must continue to find new ways of meeting these fundamental responsibilities.',\n       'beliefs. Conditions such as perceived social support (from parent, teachers and peers) and motivational beliefs have been known and discussed extensively and proven empirically (Ahmed, Minnaert, van der Werf, & Kuyper, 2010; Eccles[Parsons] et al., 1983; Mutodi & Ngirande, 2014) .\\nUnderstanding the role of motivation and social support on achievement in mathematics has attracted serious attention in recent years. However, extant literature suggest that the presence of social support (or lack thereof) may lead to positive or negative affective disposition, which in turn influences achievement (Ahmed et al., 2010; Bofah & Hannula, 2015; Eccles[Parsons] et al., 1983; Eccles, 2007; Roeser, Eccles, & Sameroff, 2000; Rosenfeld, Richman, & Bowen, 2000) .\\nThe concept of motivation stands at the center of achievement. Theories of motivations such as the Expectancy-value theory (EVT) have treated motivation as a concept that varies in amount whereas others theories such as the self-determination theory place emphasis on the type of motivation rather than the amount (Ryan & Deci, 2000) . Ryan and Deci (2000) have theorized motivation to include extrinsic and intrinsic motivation and indicated that motivation is influenced by the \"response to what the social environment affords\" (Ryan & Deci, 2009, p. 175) .\\nIn the EVT model (Eccles[Parsons] et al., 1983) , motivational beliefs such as intrinsic value and utility value or value beliefs have been discussed as predictors of performance and choice.\\nIntrinsic value is the enjoyment someone achieve from doing a given task; the task is an end to itself-the task must be intrinsically rewarding, and this notion have been discussed to be similar to the concept of intrinsic motivation by Ryan and Deci (2009) . Moreover, Intrinsic value has been linked to the construct of individual interest, enjoyment and liking (Gaspard et al., 2015, p. 664 ), hence the term like to be synonymous to intrinsic motivation in the present study.\\nValue beliefs captures more \"extrinsic\" or the usefulness of engaging in a certain task for short-and long-term goals (Gaspard et al., 2015, p. 664) . As Gaspard et al. (2015) indicate: \"the task is a means to an end rather than an end in itself\" (p. 664). In other words, value beliefs or usefulness refers to how a task fits into an individual\\'s future plans, for instance, taking a mathematics class to fulfill a requirement for a science degree (Wigfield & Cambria, 2010) . Value beliefs, is thus instrumental in nature and tied to extrinsic motivation (see Ryan & Deci, 2000 for further discussion).\\nSeveral arguments have been discussed in the literature concerning motivational belief and achievement. One argument is that the complex relationship between motivational belief and achievement could be best understood under wider social and psychological settings in which it operates (Bronfenbrenner, 1979; Eccles[Parsons] et al., 1983) . Perception of social support that adolescents receive from significant others have been known to facilitate adaptive behaviors in most academic domains (Cutrona, Cole, Colangelo, Assouline, & Russell 1994) . Parents, teachers, and peers are major sources of support during adolescence. Most often, studies assess perceived social support from these social network agents using a single construct (cf. Ahmed et al., 2010; Wentzel, 1998) without distinguishing each support unit. However, each support source may influence motivation and achievement differently depending on the measurement model. Moreover, the psychological mechanism through which social support (from teachers, parents and peers) influence academic achievement may be different for each different source. For instance, Wentzel (1998) showed that academic motivation such as interest and achievement goal mediates the relationship between social support and academic performance (composite final grades in English, science, social studies, and mathematics). Ahmed et al. (2010) found that motivational measures like competence, importance, enjoyment (including interest), and anxiety significantly mediate the associations between social support (from teachers, parents, and peer) and achievement.\\nHowever, most of these studies primarily was conducted with Western educational sample and settings. Moreover, the relationship between motivation and achievement might be moderated by culture values (Meece, Glienke, & Burg, 2006; Rosenfeld et al., 2000) . Evidence indicates that students with higher extrinsic motivation show lower academic achievement in some countries (Marsh et al., 2013) , but not in others (Bofah & Hannula, 2015; Chiu & Chow, 2010) . Our present sample, which is a national representation of five African countries, will help incorporate cross-cultural perspectives into the previous research, thus will help test the generalizability of the previous findings. This can help challenge the foundations of current theories and provide ways to improve upon it.\\nGender differences in motivational beliefs and mathematics achievement have received a lot of attention in the literature on mathematics education. Many social and cultural barriers influence gendered motivational beliefs and achievement. These barriers vary across and within countries, but generally the influences on girls are significant (Chaman, Beswick, & Callingham, 2014) . Studies have argued that gender differences in motivational beliefs and mathematics achievement are culture-specific (Forgasz, Leder, Mittelberg, Tan, & Murimo, 2015; Hyde & Mertz, 2009 ). Other studies have indicated that, mathematics-achievement gap \"is due, in a large part, to sociocultural and other environmental factors\" (Hyde & Mertz, 2009) , and \"… the fact that girls\\' progress in mathematics has been improving over time, even though boys still perform better, suggests that mathematics ability is not innate but susceptible to social influences and instruction\" (Stromquist, 2007, p. 37) . Studies have shown that in some countries (e.g., gender-equal cultures) girls have reached parity with boys in mathematics performance (Hyde, Fennema, & Lamon, 1990; Hyde, Lindberg, Linn, Ellis, & Williams, 2008; Hyde & Mertz, 2009 ), a pattern that is not found in some other nations (Bofah & Hannula, 2015) .\\nResearch on gender differences in motivational beliefs (like and values of mathematics) have yielded inconsistent outcomes. Some reported higher values for boys in Germany (Gaspard et al., 2015; Marsh, Trautwein, Lüdtke, Köller, & Baumert, 2005) , whereas other studies reported no differences in the United States (Eccles, Wigfield, Harold, & Blumenfeld, 1993; Jacobs et al., 2002; Wigfield et al., 1997) . In general, \"… girls perceive mathematics as a subject of importance in the school context but also perceive it as personally unimportant and unrelated to their future plans\" (Gaspard et al., 2015, p. 672) . With like mathematics, males have been found to report higher liking for mathematics in Germany as well as Australia (Frenzel, Pekrun, & Goetz, 2007; Gaspard et al., 2015; Watt, 2004; Watt et al., 2012) . In Australia, Thomson, Hillman, and Wernert (2012) reported evidence from the 2011 Trends in International Mathematics and Science Study (TIMSS 2011) that boys were more likely than their female peers to like mathematics, and value it. They further noted a positive relationship between these measures and mathematics performance on the TIMSS assessment.\\nModern expectancy-value theory (EVT; Eccles[Parsons] et al., 1983 ) is a useful framework for conceptualizing the relationship between achievement, motivational beliefs and gender. The theory proved to be highly effective in explaining gender differences in achievement. EVT indicates that motivational beliefs are important factors in explaining gender differences in academic choices (Eccles, 2009) .\\nThe present study expands the present literature and examines the unconditional or mediational role of students\\' motivational belief measures such as the \"like mathematics\", and the value of mathematics on the relationship between perceived social support network (PSS: from parents and teachers) and achievement in an African context. Specifically, we tested the cultural specificity associated with the mechanism through which perceived social support from parents and teachers influence achievement jointly by students\\' motivational belief measures (e.g., Like and value mathematics). The study also examines whether students\\' gender moderates the indirect effect of PSS on achievement through their motivational beliefs. To date, no study has examine the moderated mediation of gender in the relationship between PSS and achievement via motivational belief using TIMSS in the African context. The hypothesized multiple mediation or unconditional model (panel A; statistical form in panel B) and the moderated mediation or conditional model (panel C; statistical form in panel D) is presented in Fig. 6 .1. The study used multiple mediators because a single motivational measure cannot explain the complex interplay between social support network and achievement (Preacher & Hayes, 2008 ).',\n       nan,\n       'Shortages of teachers, most simply put, occur where demand, or the number of teaching positions funded, outstrips supply, or the number of teachers available. Analyses of shortages then must begin by assessing demand and supply. What has happened to the quantity of demand for new teachers? Demand for teachers appears to be on the rise. Since the mid 1980s, after a decade and a half of decline, school enrollments have steadily increased and are projected to continue to do so (CCD). Total public school enrollment, for example, rose about 5 percent from 1984 to 1990. As a result, schools are hiring teachers. At the beginning of both the 1987-88 and 1990-91 school years, an overwhelming majority of schools had job openings for teachers. Moreover, this hiring was not simply done to replace teachers who moved or retired. The number of employed elementary and secondary teachers has steadily increased since the mid 1980s (CCD). For example, from 1987-88 to 1990-91, the total population of elementary and secondary teachers jumped from 2,630,000 to 2,915,000. Has the quantity of teacher supply been adequate? Unlike demand trends, changes in the adequacy of teacher supply are far more difficult to assess. As a result, they have proven to be the focus of the bulk of research on teacher shortages and, hence, will be the focus of this paper. Much of the research on teacher supply has focused on the teacher reserve pool -the quantity of potential teachers. But, the .reserve pool of potential teachers is large, diverse and probably, unknowable. Newly qualified teachers who have recently graduated from stateapproved teacher training programs at colleges and universities are perhaps the most obvious and quantifiable source of supply. But, newly qualified teachers comprised only about 20 percent of those hired in 1987-88 and 1990-',\n       'All experiments were conducted using python version 2.7.12. The neural network was built with the Keras deep learning library using TensorFlow as backend. TensorFlow, which is developed and supported by Google, is an open-source package for numerical computation with high popularity in the deep learning community. The library allows for easy deployment on multiple graphic processing units (GPUs) (CPU-based experimentation would be prohibitive because of time constraints). The Keras wrapper provides an application programming interface (API) for quicker development and has all functionalities needed to implement the network with the exception of 3D separable convolutions, which we built as a custom layer in TensorFlow. In this paper we employed a Linux machine and two Nvidia Pascal TITAN X graphics cards with 12GB RAM each. The model was parallelized across GPUs such that the feature extractor network works on the AD vs HC and MCI-to-AD conversion problems simultaneously to speed up training. Iterating over the whole training set once, i.e. a single epoch, takes about 30 sec and prediction for a single MCI patient requires milliseconds. Since prediction would not require model parallelization or a lengthy training process, a pre-trained network is practical to be applied on a lower-end GPU (or possibly a CPU) relatively cheaply in a realistic scenario. Across all experiments certain network settings remain unchanged. These include the dropout rate -set at 0.1 for all layers and blocks; the L2 regularization penalty coefficient set at 5*10-5 for all parameters in convolutional and fully connected layers; and the convolutional kernel weight initialization which follows the procedure described by He et al. 2015 . The objective function loss is minimized using the Adam optimizer by Kingma and Ba, 2014 with an exponentially decaying learning rate:\\nAll other parameters are kept at their default value provided in the original Adam paper (Kingma and Ba, 2014) . The network hyperparameters were picked because they resulted in sufficiently good performance on the validation set. A training batch size of 6 samples for both the AD and MCI conversion problems is randomly sampled from the dataset when training the network until the dataset is exhausted.',\n       'An idealized configuration of the MITgcm primitive equation model (Marshall et al. 1997) is used to help interpret the shipboard observations and assess the sensitivity of the water mass distributions to the wind forcing. The model is configured in a 1000 km by 1200 km domain with uniform horizontal grid spacing of 2.5 km (Fig. 3). The model has 12 levels in the vertical with uniform grid spacing of 5 m. There is a large island that represents Alaska and a smaller peninsula that extends from the western boundary representing the west side of the Bering Strait. The model Bering Strait is 100 km wide and lies between the island and the western peninsula. The bottom topography is flat (40 m depth) over most of the domain, with a slope around the island that shoals to 10m over a horizontal scale of 30 km. Herald Canyon is represented by a narrow region of deeper bathymetry that extends north of the strait and deepens from 40 m to 60 m depth (Fig. 3). There is also a region along the northern boundary where the topography descends from 40 m to 60 m, meant to represent the shelfbreak. The Coriolis parameter is 1.2x10 -4 s -1 and taken to be constant. Calculations have also been carried out with a deep basin to the north of y=1200 km, and the resulting circulation and water mass distributions are essentially the same as reported here. Horizontal viscosity is parameterized using a Smagorinsky scheme with a nondimensional coefficient of 2.5 (Smagorinsky, 1963). Vertical viscosity is 10 -4 m 2 s -1 . The lateral boundary conditions are no-slip, and a quadratic bottom drag of 10 -3 is applied. Statically unstable profiles are vertically mixed with an enhanced vertical diffusion coefficient of 1000 m 2 s -1 . A linear equation of state is used with a thermal expansion coefficient of 2x10 -4 o C -1 .',\n       'In the current study, a scale for determining students\\' attitudes towards science, (science is a broader area than science lessons) was developed. The attitude towards the science scale developed as a result of this study consisted of 36 items (excluding the check item) and was organized as a five-point Likert scale (see Appendix). To establish construct validity of the scale, factor analysis was conducted. EFA showed that scale was constructed of four factors, the relevance values between items and factors were between .502 and .776, item-test correlation values were between .411 and .766, and the factors explained 59.559% of the total variance. To provide evidence to four-factor construct that was obtained from EFA, CFA was carried out on a different sample. CFA also confirmed that four-factor construct (enjoyment, confidence, usefulness, interest). It was seen that Cronbach Alpha coefficients found from each factor making up the scale and from the entire scale were higher than 0.70. This provided evidence for the reliability of the scale. This scale development study, with established validity and reliability, consists of attitude components completely reflecting the science field.\\nStudies conducted on attitude towards science have received great attention from the past to the present. Attitude studies have been conducted at various grade levels, on various main themes and in various cultures in the national and international literature (Fraser, 1982; Schreiner & Sjoberg, 2004) . The number of studies conducted on science education has increased in Turkey in recent years because of the failures of students seen in the science field in various tests, including OSYM (Student Sel ection and Placement Centre), PISA (Programme for International Student Assessment), TIMSS (Trends in International Mathematics and Science Study), and the regression in the levels of interest on this field (Aydeniz, 2017). Establishing students\\' attitudes towards science and identifying the source of their negative attitudes before the educational process is a crucial issue for providing quality science education (Gomleksiz & Yuksel, 2003) .\\nWhen relevant research studies were reviewed, it was found that there were both similarities and differences between the findings of the current study and that of prior research studies. Investigating students\\' attitudes towards the science field, Kenndy, Quinn, and Taylor (2016) purported that the attitudes towards science had six subdimensions, enjoyableness, self-efficacy, difficulty, usefulness for career, relevance for everyday life, and intention to enroll. Yasar and Anagun (2009) established that attitudes had three sub-dimensions, dependent on proofs, curiosity and persistence, and Wang and Berlin (2010) identified that attitudes towards science class had a single dimension. Sener and Tas (2016) found out that attitude towards sciences had five subdimensions as daily life and learning new knowledge, difficulty in application, problem-solving, motivation, and anxiety. As a result of this study, on the other hand, it was deduced that attitudes towards science had four sub-dimensions, namely enjoyment, confidence usefulness and interest. The enjoyableness and usefulness for the career sub-dimension of Kenndy, Quinn, and Taylor (2016) are similar to the enjoyment and usefulness sub-dimension of the current study and both subdimensions point to students\\' perceived competence in the areas of science. There are some studies that found different sub-dimensions when compared to the current study (Yasar and Anagun, 2009; Kenndy, Quinn, and Taylor, 2016; Sener and Tas, 2016) . It is thought that these factors making up the basis students\\' attitudes towards science are the students\\' belief in the facilitation of their daily life using their science knowledge, their levels of self-confidence to succeed in the science field, and the levels of their knowledge in these fields.\\nConsidering the mean total scores received from the attitude towards science scale, it was seen that the 5th, 6th and 7th graders received higher scores in comparison to the 8th graders. However, it was seen that the difference between the mean scores was not significant. The findings showed that positive attitudes developed towards science diminished in time in contrast to what was anticipated with the improvement of cognitive development depending on the rise of grade level. Pell and Jarvis (2010) ascertained that students\\' scientific attitudes regressed during the process from the age of five years till the age of 11 years as their age increased and this regression was more conspicuous in female students in comparison to the male students. In a similar study, they saw that the attitudes of the students at the range of 11 years old and 14 years old towards science showed regression as students\\' education levels increased. It was also put forward that thi s regression was more distinct in female students (Kind, Jones, & Barmby, 2007) . The regression occurred in students\\' attitudes depending on the increase of their grade levels as a result of these studies was in conformity with the data collected as a result of the research. In a study investigating attitudes towards science class in Taiwan, it was concluded that there was no significant difference in science attitude based on grade level and gender in similar to the result of this study (Wang & Berlin, 2010) .\\nIn a cross-sectional study conducted on the change of attitudes of students enrolled in the 3rd grade up to the 12th grade towards science, results were similar. In contrast to this study, former study showed a regression in the levels of student atti tudes as their grade level increased (Said, Summers, Abd-El-Khalick & Wang, 2016) . It was determined that the regression in these attitude changes occurred because the students thought that their skills in science education fields worsened and due to the l oss of their faith in the benefits and necessity of science education.\\nThe efficiency in teaching and learning science can be improved using an attitude towards the science scale. Thus, teachers may use the attitude scale developed in this study to establish students\\' attitudes towards science both before and after education. Moreover, this scale may also be used to determine the degree of their attitude gains in the affective dimension of science curriculum the following education. Thus, organization and development of activities included in the curriculum concerned with attitude gain may be managed. Cross-sectional studies with various scales may be conducted for different grade levels with different disciplines in future studies. Moreover, various scale studies may be carried out to find out the relationships between different age groups and attitude scores towards science. benimsenmeye başlaması nedeniyle fene yönelik ilginin arttırılmasının amaçlanması ve gelecekte ülkelerin ihtiyaç duyacağı mesleklerin fen ile ilişkili olması gibi pek çok fakör fene yönelik tutumların ölçülmesini gerekli kılmaktadır.\\nAraştırmanın Amacı: Bu çalışmada fene yönelik tutum ölçeği (FYTÖ) geliştirilmesi, 5, 6, 7 ve 8. sınıf düzeyinde öğrenim görmekte olan öğrencilerin fene yönelik tutumlarının ölçülmesi amaçlanmıştır. Ayrıca çalışma farklı düzeyde yapılarak ise sınıf düzeyi ile fene yönelik tutum arasındaki ilişkinin araştırılması amaçlanmaktadır. Çalışmanın sonuçları, fene yönelik tutumların belirlenmesi ve tutumların istenilen düzeylere ulaşabilmesi için araştırmacılara, program hazırlayıcılara ve öğretmenlere yol gösterici olacağı düşünülmektedir.\\nAraştırmanın Yöntemi: Bu ölçek geliştirme çalışmasında nicel araştırma yönteminin temel alındığı tarama modeli kullanılmıştır. Çalışmanın evrenini 2017-2018 eğitimöğretim yılı bahar döneminde bir il merkezinde bulunan üç farklı ortaokulda öğrenim gören öğrenciler oluşturmaktadır. Bu çalışmanın örneklemi 316\\'sı kız 375\\'i erkek olmak üzere toplam n= 691 öğrenciden oluşmaktadır.\\nÖlçek geliştirme sürecinin ilk aşamasında ilgili ulusal ve uluslararası alan yazın incelenerek fene yönelik tutumların teorik altyapısı oluşturulmuştur. Aynı zamanda fen alanında daha önce yapılmış ölçek geliştirme çalışmaları ile MEB tarafından Öğretim Programı\\'nda tutumla ilişkili olarak yer verilen kazanımlar gözden geçirilmiştir. Uygulanma ve hazırlanma kolaylığı nedeniyle bu ölçek geliştirme çalışmasında Likert tipi ölçek kullanılmasına karar verilmiştir. Likert ölçeği, ölçeğin uygulanacağı grubun algılama ve ayırt edebilme düzeyine en uygun olacağı düşünülen beş puanlı likert biçiminde düzenlenmiştir. Ölçekte yer alan maddeler, Fen Bilgisi Eğitimi alanında uzman iki kişi, Türk Dili ve Edebiyatı alanında uzman bir ki şi ve iki Fen Bilimleri öğretmeni tarafından incelerek görüşleri alınmıştır. Uzmanlardan alınan dönütler doğrultusunda, doğrudan tutumları ifade etmediği belirtilen 8 madde ölçekten çıkarılmış ve diğer maddeler üzerinde önerilen düzenlemeler yapılmıştır. Revize edilen tutum maddeleri, ölçeğin uygulanacağı evreni temsil eden 60 kişilik bir öğrenci grubu tarafından olumlu-olumsuz-nötr şeklinde değerlendirilmiştir. Öğrenci grubu ile yapılan ön deneme sonucu maddelerde herhangi bir anlaşılmayan ifade olmadığı görülmüştür. Bu çalışma sonucu ölçekte 20\\'si olumlu, 20\\'si olumsuz toplam 40 maddenin kullanılmasına karar verilmiştir.\\nÖlçekte yer alan 40 maddeye bir adet kontrol maddesi ilave edilerek pilot ölçekte toplamda 41 madde olması sağlanmıştır. Kontrol maddesi kullanılarak ölçekte yer alan maddelere rastgele cevap verenlerin ayırt edilmesi amaçlanmıştır. Kontrol maddesi \"Bu madde ölçeği okuyarak cevaplayıp cevaplamadığınızı kontrol etmek için yazılmıştır. Eğer bu maddeyi okuyorsanız \"4 no.lu kutucuğu işaretleyiniz.\" şeklinde ölçeğin 20. madde olarak yer almıştır. Ölçek maddeleri üzerinde gerekli olan yazım, imla ve biçimsel düzenlemeler yapılarak ölçek pilot uygulama aşamasına hazır hale getirilmiştir.\\nÖlçeğin pilot uygulaması 20\\'u 5. sınıf, 20\\'u 6. sınıf, 30\\'u 7. sınıf ve 20\\'si 8. sınıf olmak üzere toplam 95 kişilik bir örneklem ile gerçekleştirilmiştir. Pilot uygulama neticesinde kontrol maddesine beklenen seçeneğin işaretlenmediği ölçekler değerlendirmeye alınmamıştır. Pilot uygulama sonucunda elde edilen veril er SPSS 22.0 paket programına aktarılarak analiz edilmiştir. Ön deneme ve pilot uygulama aşamalarından elde edilen veriler sonucu nihai ölçekte 36 maddeye yer verilmiştir. Ölçeğe 20. madde olarak ölçeğin rastgele cevaplandırılıp cevaplandırılmadığını belirlemek adına kontrol maddesi ilave edilmiştir. Gerekli yazım, imla ve biçimsel düzenlemeler yapılarak ölçeğin nihai formu oluşturulmuştur. Oluşturulan nihai formun uygulanması neticesinde elde edilen veri setinde öncelikle kontrol maddesine \"katılıyorum\" cevabının verilmediği 50 ölçek örneklemden çıkarılmıştır. Elde edilen 691 ölçeğin verileri (363\\'ü AFA ve 328\\'i DFA çalışmalarında kullanılmak üzere ayrı ayrı aktarılmıştır) SPSS 22.0 ve LISREL 8.80 paket programlarına aktarılmıştır.\\nVeriler, öncelikle faktör yapısının saptanması amacıyla Açımlayıcı Faktör Analizi\\'ne (AFA) tabi tutulmuştur. Ardından elde edilen yapının kabul edilebilir olup olmadığına ilişkin kanıt oluşturmak için Doğrulayıcı Faktör Analizi\\'ne (DFA) tabi tutulmuştur. Fene yönelik tutum ölçeğinden elde edilen puanların sınıf düzyine göre farklılaşıp farklılaşmadığını tespit etmek amacıyla ise veriler tek yönlü varyans analizine (Anova) tabi tutulmuştur.\\nAraştırmanın Bulguları: Açımlayıcı Faktör Analizi sonucunda bir maddenin ölçekten çıkarılmasına karar verilmiştir. Temel bileşenler analizi ve Varimax döndürme tekniği sonucunda Fene yönelik tutum ölçeğinin dört faktörlü bir yapıya sahip olduğu görülmüştür. AFA sonucu elde edilen dört faktörlü yapının değerlendirilip doğrulanması amacıyla LISREL 8.80 paket programı kullanılarak DFA yapılmıştır. DFA sonucu ki-kare iyilik uyumunun serbestlik derecesine bölümü 2, RMSEA değeri 0.051, SRMR değeri 0.05, NFI değeri 0.93 ve NNFI, CFI ve IFI değerleri 0.97 olarak bulunmuş olup ölçeğin mükemmel uyuma sahip olduğunu göstermektedir. DFA sonucunda maddelerin sahip oldukları faktör ağırlıklarının .42 ile .88 arasında değişmekte olup anlamlı bulunmuştur.\\nFYTÖ\\'de yer alan 36 maddenin güvenirliğine ilişkin kanıt oluşturabilmek amacıyla Cronbach Alpaha katsayısı hesaplanmıştır. Ölçeğin tamamından elde edilen Cronbach Alpha katsayısı .93 olarak bulunmuştur. Hoşlanma faktörü için hesaplanan iç tutarlık katsayısı .91, Güven faktörü için hesaplanan iç tutarlık katsayısı .74, Fayda faktörü için hesaplanan iç tutarlık katsayısı .76 ve İlgi faktörü için hesaplanan iç tutarlık katsayısı .72 olarak bulunmuştur.\\nSonuç ve Öneriler: Bu çalışma sonucunda geliştirilen Fene Yönelik Tutum Ölçeği, 36 maddeye (kontrol maddesi hariç) sahip ve 5\\'li likert biçiminde düzenlenmiştir. Örneklemden elde edilen veriler ölçeğin dört faktörlü bir yapıya sahip olduğunu göstermiştir. Ölçeği oluşturan faktörlerin her birinin ve ölçeğin tamamının güvenilirliği yüksektir. Veriler üzerinde sırasıyla gerçekleştirilen AFA ve DFA sonuçları oluşturulan yapının kabul edilebilir olduğu kanıtlamaktadır. Geçerliği ve güvenirliği kanıtlanmış olan bu ölçek geliştirme çalışması fen alanını kapsayan tutum öğelerinin tamamını kapsamaktadır. Fene yönelik tutum ölçeğinden alınan toplam puanların tek yönlü varyans analizi sonucu 5, 6, 7 ve 8. sınıf öğrencilerinin puanları arasında anlamlı bir farklılığın bulunmadığı görülmüştür.',\n       'Coast Survey would install GPS tide buoys ( Figure 14) during some surveys, as this technology moves from a testing to operational phase, to measure both horizontal and vertical position using GPS technology. Tide buoys provide a new opportunity to acquire water level data in remote locations that do not have infrastructure to mount a tide gauge. The use of GPS-tracked buoys can increase the geographic density of water level observations away from shore, with an explicit reference to a geodetic datum (ellipsoid) in support of ellipsoidally referenced surveys. If the tidal datum-to-ellipsoid relationship is determined beforehand, NOAA can significantly reduce the need for dedicated shore-based tide gauge resources (time, money, logistics, and personnel) in support of hydrographic surveying. GPS tide buoys remain operational for the same duration of time as traditional, short-term tide gauges (typically for the length of the survey or 30 days, whichever is greater) in order to establish the required ellipsoid and tidal datum relationship for ellipsoidally referenced surveys. Buoy deployment requires a limited area of flat and preferably sandy seafloor, similar to the type of bottom characteristics sought for small boat anchoring. Since the survey has not yet taken grab samples of the sea bottom, however, the team chooses the location based on the information on a nautical chart that may be outdated. During installation, GPS tide buoys are tethered to the anchoring hardware with a 15-meter, 1\" diameter rubber cord, followed by a section of 3/16\" Amsteel rope. The rubber cord attaches to the bottom of the buoy, and the rope attaches the rubber cord to the anchor. The combined length of the rubber cord and the rope exceeds the nominal water depth by a factor of approximately two (i.e., \"mooring scope\" = 2). The GPS buoy is deployed by floating the buoy away from the vessel to the extent of the rubber cord and rope. The anchor is then lowered slowly to the point where the rope attaches to the rubber cord, at which point the anchor is released. During recovery, the GPS buoy (float) is brought aboard the vessel along with the length of the rubber cord. The total anchoring hardware is then hauled in by rope. There is very little maintenance of the buoys required during a survey deployment. Occasionally the batteries must be replaced or recharged, and field units must retrieve the buoy with a small boat and bring it back to the ship or shore. When they bring the buoy on board, the team attaches a temporary float to the end of the mooring so that it can be re-used after the buoy batteries have been refreshed. At the end of the survey, the field unit recovers all components of the buoy. The Hydrolevel™ buoys are programmed to send out a \"health message\" email to a predetermined distribution list at regular intervals using the Iridium satellite service. Currently, Coast Survey buoys are configured to send a message once every hour. If the buoy reports its position outside of a certain radius (\"watch circle\"), it issues a separate alert. Field personnel respond to emergencies where the buoy breaks its mooring or stops sending messages.',\n       'ICF quantified the emission reductions of a farm producing corn for ethanol in 2022 from implementing CPS 340, CPS 345, and CPS 590 in the COMET Planner individually and all three combined. Figure 4-1 shows the range of emissions from the ICF: 2014 Current Conditions on the far left to the ICF: 2022 Building-Blocks Scenario on the far right. The current conditions LCA represents current emissions, which are estimated at 21,814 g CO 2 e/MMBtu of ethanol. The 2022 BAU Scenario incorporates projected changes in corn yields between 2016 and 2022 from the 2016 USDA Baseline. These are estimated at 20,259 g CO 2 e/MMBtu. The ICF: 2022 Build-Blocks Scenario, estimated at 16,734 g CO 2 e/MMBtu, further accounts for the adoption by corn farmers of all three CPSs in 2022. The central three bars represent the three CPSs isolated from each other. The values presented in Figure 4-1 do not include the ethanol co-product credit from DGS displacing corn, soybean meal, and urea. To be consistent with the analysis in Chapter 3, ICF modified the GREET model inputs including corn yields, fertilizer application and nitrogen emission rates, and ethanol production technology (e.g., dry mill refining with corn oil extraction) to develop the unique co-product credit for each scenario. Both the BAU and the Building-Blocks scenarios were modified to incorporate corn farming farm inputs and fertilizer N 2 O. In the Building-Blocks scenario, the ethanol yield from corn for Dry Mill ethanol refineries with corn oil extraction was increased from 2.8 gallon/bushel to 2.95 gallon/bushel. Utilizing the AR4 GWPs for CH 4 and N 2 O, Table 4-3 shows the resulting DGS credit per MMBtu and the resulting total emissions impacts for the Domestic Farm Inputs and Fertilizer N 2 O emission category. ',\n       'This study involved full-day kindergarten teachers who had participated in the ELM whole-classroom study (Clarke et al., 2011) . Classrooms were assigned randomly to condition, and then within classrooms teachers selected students whom teachers expected would most benefit from smallgroup instruction. Specifically, classrooms were randomly assigned to treatment or control conditions, blocking on teachers\\' prior experience with ELM. That is, we randomly assigned teachers with 1 year of ELM experience to ROOTS or control and then randomly assigned teachers new to ELM implementation to ROOTS or control. In schools with multiple classrooms, we also assigned classrooms to condition within school. Blocking, also called stratification, on ELM experience and school experimentally controls for biases that might stem from systematic differences between conditions (e.g., more ROOTS teachers with no prior ELM experience). A total of 29 classrooms were included: 14 in the treatment condition (ELM + ROOTS) and 15 in the control condition (ELM only). Teachers were asked to nominate the five lowest performing students or those who would most benefit from a small-group math intervention. Teachers nominated 140 students as eligible for small-group instruction, with 67 students in intervention classrooms and 73 students in control classrooms. Twenty-three teachers identified five students, one teacher identified three, four teachers identified four students, and one identified six, with the deviations from five students split similarly across conditions.\\nClassroom teachers in both conditions provided whole class ELM instruction throughout the year for all students, and treatment and control classrooms provided the same amount of daily mathematics instruction. In intervention classrooms (ELM + ROOTS), the \"ROOTS students\" received all of the whole-class ELM instruction. On 3 days per week, however, instead of practicing that day\\'s ELM topics independently at the end of the lesson (i.e., math practice worksheets), they received ROOTS instruction. Given that ROOTS was not offered in control classrooms, nominated control students participated in whole class ELM instruction, 5 days per week, including all of the individualized math practice. We controlled for time by delivering the ROOTS instruction during the individual, worksheet-based math practice portion of ELM in treatment classrooms. ROOTS instruction began in January and continued until the end of May. Trained instructional assistants provided ROOTS instruction.',\n       'As yet, not enough SARS-CoV-2 RNA sequences have been deposited to allow a meaningful analysis. Table 1 juxtaposes all hotspot mutations in isolates from different geographic regions. The majority of de novo mutation hotspots arose after SARS-CoV-2 had been transmitted to regions outside China and been allowed active replication in different environments (Tables S1 to S6). The mutations in positions 8782 and 28144 with frequencies 29/99 in sequences from China (Table S1) were found outside China only in the US I and II samples, though with moderate frequencies (15/97 or 15/112) and at even lower frequencies in the Indian samples (7/99) ( Table  1) . Hence most of the world-wide mutation hotspots described here (Tables S1 to S6) must have originated and been selected in the course of massive replication of SARS-CoV-2 in its world-wide expansion.',\n       'The output of MIDAS is a spatial map reflecting significant group effects or correlations with non-imaging variables. As such, this map can be used to perform feature selection for a subsequent classification, or regression task using a properly nested cross-validation scheme. In that sense, MIDAS bears similarities to multivariate feature selection methods Guyon et al. (2002) . Significant regions detected after FDR correction ðq < 0:05Þ by all methods using the structural MRI mild cognitive impairment dataset. The color intensity indicates -log p value. Warmer colors indicate increased tissue density correlated with Adas-cog-13 score, while colder colors indicate decreased tissue density correlated with Adas-cog-13 score. The color scale is matched for all methods to facilitate comparisons. Fig. 17 . Significant regions uncorrected for multiple comparisons ðp < 0:05Þ by all methods using the structural MRI mild cognitive impairment dataset. The color intensity indicates -log p value. Warmer colors indicate increased tissue density correlated with Adas-cog-13 score, while colder colors indicate decreased tissue density correlated with Adas-cog-13 score. The color scale is matched for all methods to facilitate comparisons. et al. (2015); Rondina et al. (2014) . These methods are designed to identify a set of appropriate features for making predictions on unseen data. Towards this end, they are often based on elaborate measures whose null distribution is difficult to estimate. Importantly, as these methods are particularly concerned with maximizing the accuracy of the predictions, they may be influenced by confounding variations in the data, rendering the features uninterpretable with respect to the processes under study Haufe et al. (2014) . Lastly, they often choose a small set of features, which may not fully reflect the true underlying variability despite their superior prediction performance. On the other hand, MIDAS may not result in improved predictive accuracy, but yields tractable, analytically solvable statistics for interpretable inferences.',\n       \"The rapid global transmission of coronavirus disease 2019 (COVID-19) has offered some of the most heterogeneous, diverse, and challenging mutagenic environments to stimulate dramatic genetic evolution and response from severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This work provides the most comprehensive genotyping of SARS-CoV-2 transmission and evolution up to date based on 15 140 genome samples and reveals six clusters of the COVID-19 genomes and associated mutations on eight different SARS-CoV-2 proteins. We introduce mutation h-index and mutation ratio to qualify individual protein's degree of nonconservativeness. We unveil that SARS-CoV-2 envelope protein, main protease, and endoribonuclease protein are relatively the most conservative, whereas SARS-CoV-2 nucleocapsid protein, spike protein, and papain-like protease are relatively the most nonconservative. We report that all of the SARS-CoV-2 proteins have undergone intensive mutations since January 5, 2020, and some of these mutations might seriously undermine ongoing efforts on COVID-19 diagnostic testing, vaccine development, antibody therapeutics, and small-molecular drug discovery.\",\n       \"Unobserved individual abilities may also affect the likelihood of pursuing an advanced degree. To test that hypothesis, we follow Rosenweig and Schultz (1983) by collecting the residuals from the earnings equation. These residuals represent individual ability uncorrelated with education level, major level skills, parents' education level, or demographic variables included in the model. They will also include random noise in the earnings function, so they will measure the unobserved ability with error. An auxiliary multinomial logit estimation of education choices on the earnings residuals will illustrate the direction of the effect of unobserved ability to earn income on the probability of seeking graduate or professional education. Note that the measurement error inherent in this method will tend to bias the coefficients toward zero. Table 5 reports the estimated marginal effect of the earnings residual on the probability of pursuing each degree. Those with higher unobserved ability to earn income were less likely to stop at the bachelor's degree level and were more likely to pursue advanced degrees of all types. Consequently, sorting on unobserved ability works in the opposite direction as sorting on observed quantitative skills.\",\n       'Irrigation modifies land-surface water and energy budgets, and also influences weather and climate. However, current earth-system models, used for weather prediction and climate projection, are still in their infancy stage to consider irrigation effects. This study used long-term data collected from two contrasting (irrigated and rainfed) nearby maize-soybean rotation fields, to study the effects of irrigation memory on local hydroclimate. For a 12 year average, irrigation decreases summer surface-air temperature by less than 1\\n• C and increases surface humidity by 0.52 g kg −1 . The irrigation cooling effect is more pronounced and longer lasting for maize than for soybean. Irrigation reduces maximum, minimum, and averaged temperature over maize by more than 0.5\\n• C for the first six days after irrigation, but its temperature effect over soybean is mixed and negligible two or three days after irrigation. Irrigation increases near-surface humidity over maize by about 1 g kg −1 up to ten days and increases surface humidity over soybean (∼ 0.8 g kg −1 ) with a similar memory. These differing effects of irrigation memory on temperature and humidity are associated with respective changes in the surface sensible and latent heat fluxes for maize and soybean. These findings highlight great need and challenges for earth-system models to realistically simulate how irrigation effects vary with crop species and with crop growth stages, and to capture complex interactions between agricultural management and water-system components (crop transpiration, precipitation, river, reservoirs, lakes, groundwater, etc.) at various spatial and temporal scales.',\n       'The scale size h plays a critical role in the amount of features incorporated from neighboring voxels in B(j, h) for each j. A simple approach is to fix h according to some prior or empirical information.\\nHowever, a small h may miss important spatial information, whereas a large h may smooth out some local details and dramatically increase the computational burden. Alternatively, we may consider a sequence of nested neighborhoods corresponding to multiple scales at each location. Specifically, let h = {h 0 < h 1 < ... < h S } be a sequence of scales with h 0 = 0 and h S being the maximum scale.\\nThe scales can be chosen based on previous studies or empirical experiences. For example, scales can be defined as the radius of spherical neighborhood in a form of {h s = c s } with constant c > 1.\\nIn our numerical examples, we used c = 1.2 which balanced the computation intensity without losing important spatial information. One may choose an optimal h based on a specific criterion, such as WRE, and then use the PCs extracted based on the optimal h for imaging classification.\\nAlternatively, one may integrate the PCs extracted from all scales h for imaging classification.\\nFor imaging classification, we propose a multi-scale procedure to determine W G and W L across multiple scales. Without loss of generality, we consider the cross-sectional studies so that (x i , y i ) are independent across subjects. Specifically, at a given scale h, we consider a weighted likelihood function given by\\nwhere X and Y , respectively, denote the imaging and class information and p(x id |y i , θ j ) is the likelihood function of x id given y i . Moreover, as discussed in the voxel-wise linear regression model in Section 3.1, θ j may contain the discriminative information of features at j. Based on the weighted likelihood function (12) The stopping criterion in Algorithm 3 can be either global or local criteria. For instance, for the global criteria, we may stop the algorithm if WRE cannot be further decreased. For the local criteria at each location j, we may check the improvement of θ j .',\n       'The proposed method involves two computationally expensive steps: (1) the pairwise non-rigid registration between the atlases and each target image (approx. 2-10 min per registration step), and (2) the MRF energy minimisation step (approx. 5-10 min per segmentation task). To increase computational efficiency, an extension to the proposed framework could move from a voxel-wise representation of the images to a supervoxel representation. This change in the graphical representation could enhance the scalability of the proposed method to larger databases.\\nThe formulation proposed in this paper assumes that the atlases are a good representation of the anatomy of the target image. This was achieved by atlas selection based on global image similarity as commonly used in multi-atlas segmentation [8] . To account for remaining anatomical variability in the selected atlases, we used local similarity measures for label fusion. However, when scaling the proposed method to larger databases of dissimilar images, the aforementioned assumption may no longer hold, and sparse connections between similar images only could ensure accurate label propagation, as well as alleviate computational burden due to registration.\\nIn the scope of this paper, the data term was used exclusively to encode manual annotations. However, as briefly described in Section 2.3, the data term could also incorporate conditional label probabilities based on the observed intensities. These intensity models could be learned from the annotated data similar to [26] and applied to unlabelled regions in all images. This could make it feasible to further reduce the annotation rate while maintaining robust segmentation results. Furthermore, it would be of great interest to extend the data term to incorporate weak annotations such as bounding boxes or image tags.',\n       'At this point I\\'ll simply introduce this topic, so we all can think about as we examine the process in more detail in the remainder of this paper. -National level The process of priority setting and establishing categories for funding tends to be more long-term considering impact and appropriation of funds. The SAES community expends considerable time and effort by in \"tending\" the system. There have been some big payoffs for the efforts. For example, the start of the competitive grants program in 1977 and the influx of additional funds for biotechnology in 1985. This program was grown further with the initiation of the National Research Initiative (NRI) in 1991. The Water Quality Special grant is another example of the success of initiatives by the Land Grant Agriculture community. While there has been debate about whether emphasis should be placed on \"formula funds\" or \"competitive grants\", it is interesting to note that in recent years the only times there have been increases in formula funding was in those years where a major competitive grant program was started or \"grown\". Although arguments showing the importance of, and impressive returns from formula funds, Congress has favored research funding options where they have more control over the agenda. -State and Local Some states realized significant growth in state funding for AES during late 70s and early 80s Unfortunately not all of it has \"stuck\" as states got into fiscal trouble. However, it a very critical point today, whether to maintain what we have or in some cases to actually capture some growth. -A simple outline of various kinds of funds used in AES programs is shown in Table 1: \"Funding Sources and Uses\"',\n       'PBL is an instructional approach providing \"the contextualized, authentic experiences necessary for students to scaffold learning and build meaningfully powerful science, technology, engineering, and mathematics concepts\" (Capraro, Capraro, & Morgan, 2013, p. 2) . PBL has been discussed alongside and integrated into STEM education because it can bridge secondary STEM courses with postsecondary specializations and STEM professions (Capraro et al., 2013) . In the real world, STEM professionals are expected to solve diverse, ill-defined, and non-routine problems that have multiple possible solutions. Similarly, PBL provides students with ill-defined and authentic tasks and encourages them to experience problematic situations in a school-based learning environment. Ultimately, PBL improves the quality of education and enables students to become ideal STEM professionals with 21 st century skills after graduating from secondary and post-secondary institutions (Bell, 2010) . For this reason, PBL has been introduced to teachers as an appropriate instructional approach and has been implemented into STEM classrooms.\\nThe impact of PBL implementation in STEM classrooms has been investigated. The effects of PBL on students have been discussed within three domains: the affective, behavioral, and cognitive domains. Regarding the affective domain, students\\' interest, self-confidence, and self-efficacy have been examined. STEM classrooms integrating PBL were found to exert a positive influence on students\\' interest in learning and their conviction in the utility of STEM subjects (Baran & Maskan, 2010) . Furthermore, PBL activities increased student self-confidence and self-efficacy (Baran & Maskan, 2010) . PBL is usually designed and implemented through collaborative group work; hence, PBL has also been found to exert a positive impact on students\\' communication and collaboration skills (Domínguez & Jaime, 2010; Kaldi et al., 2011; van Rooij, 2009 ). Finally, the positive effects of PBL on students in the affective and behavioral domains influenced their academic achievement (Han, Capraro, & Capraro, 2015) . As students\\' interest in and positive attitudes toward learning STEM disciplines increased, their test scores and overall academic achievement improved.\\nStudies (i.e., Capraro et al., 2013; Han & Carpenter, 2014) have investigated the reasons behind the positive impact of PBL on students in the affective, behavioral, and cognitive domains. PBL exerts a positive influence because of its unique constructs and components of PBL. With Korean middle-school students as participants, Han and Carpenter (2014) validated five constructs of STEM PBL: self-regulated learning, collaborative learning environment, interdisciplinary learning environment, technology-based learning, and hands-on activities. Capraro et al. (2013) theoretically investigated components of PBL. It remains necessary to investigate how the constructs and components of PBL account for its effect on students\\' decisions regarding STEM majors. In this sense, the current study analyzed data pertaining to students\\' attitudes toward the constructs of PBL and examined the relationships among the constructs and the selection of a STEM major.',\n       \"We use data from the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). The ECLS-K is a national cohort-based study of children from kindergarten entry through middle school. Information was collected from children, parents, teachers, and schools in the fall and spring of children's kindergarten year (1998) and 1st grade, as well as the spring of 3rd, 5th, and 8th grade (2007) . Schools were probabilistically sampled to be nationally representative. More than 20 students were targeted at each school for the first (kindergarten) survey round. This results in a student panel which also serves as a repeated cross section for each school. The ECLS-K assessed student skills that are typically taught and developmentally important, such as math and reading skills. We focus on 5th grade reading classes.\",\n       nan,\n       'On the number line 63 lies between 60 and 70. It lies closer to 60 though so we round it off to 60, as the nearest ten. Remember also that in this case, if the units digit is less than the number 5, we round down to the nearest ten.\\nExample 2: Round 2 499 off to: a) the nearest ten b) the nearest hundred c) the nearest thousand a) Let us start by looking at the units digit. It is more than 5 so we round up to the nearest ten. The answer is therefore: 2 500 b) Now we need to look at the tens digit because we are rounding off to the nearest hundred. It is also more than 5 so we round up to the next hundred. Answer: 2 500 c) For rounding off to the nearest thousand we look at the hundreds digit. It is less than 5 so we round down to the nearest thousand. Answer: 2 000\\nNow let us do one with decimals. Please solve the following:\\n1. 17 ÷ 4 = ? Round your answer off to the nearest whole number.\\n2. Mr Farmer decides to share his 17 cows evenly between his 4 children. To avoid conflict, each child must receive the same number of cows. How many cows will each child get?\\nIf you have the class work correct, please continue with the homework which is page 23 of your textbook, numbers 1 -8.\\nIn the approach applied in the lesson vignette above, mathematics is viewed as a ready-made system with general applicability. Consequently, mathematics instruction is seen as a process of breaking up formal mathematical knowledge into learning procedures and then learning to use them accordingly.\\nLet us examine another similar problem to Example 3 above: This example illustrates where conventional mathematics as we perform it outside of any prescribed context, can actually support or conflict with the answer depending on the context of the problem. In solving the problem above, it is the context that must take preference over the mathematical convention of rounding off \"down\" to the nearest whole number when our indicator is below 5. But when we teach mathematics predominantly formally and outside of context, do our students learn to know the difference between conventional mathematics and mathematics as a tool operating within a social context?',\n       'Machine learning has been successfully applied to many areas of science and engineering [1] . Some examples include time series prediction [2] , optical character recognition [3] , signal and image classification in biomedical applications for diagnosis and prognosis [4] , etc. The support vector machine (SVM) is a recently developed paradigm in machine learning [5] with applications to brain image processing and classification [6] - [11] . In this scenario, the purpose of these techniques is to provide objective clinical decisions and an early detection of abnormal perfussion/metabolic patterns [11] .\\nThe performance control of a SVM is a major requirement in any classification problem [12] , i.e. the development of computer-aided diagnosis (CAD) systems [13] , [14] . Several sophisticated CAD systems have been recently proposed for the diagnosis of AD [15] - [18] . As an example, in [18] a viewaligned hypergraph learning method based on the sparsity representation is proposed. Although, these systems achieve a good performance in terms of accuracy and a reasonable computational cost they employ all original features for model construction, while there may exist noisy or redundant information in original features [18] . It is interesting to select those most informative features in terms of class-separability for subsequent model construction but, in the neuroimaging field with an uncertain labeling process (ground truth), the learning ability of such methods could be significantly affected. Nevertheless, this is the main goal of the proposed methodology, to use the class-information at the validation stage to propose more accurate models.\\nTypically, the performance control is specified in terms of minimum error rate or overall accuracy, although many factors including noise, the inherent complexity of the classification task, computational constraints, etc., may inhibit the system from achieving the performance requirements for an specific application [19] . Fortunately, other solutions based on the optimal classification theory proposed in [12] , i.e. the ones based on controlled error rates [20] , have been analyzed and demonstrated demonstrated their reliability and efficiency as methodologies for the classifier design. As an example, this methodology was firstly presented in the neuroimaging field in [13] , where the development of the CAD systems using functional image modalities, such as positron emission tomography (PET) or single-photon emission tomography (SPECT), established a confidence level in diagnostics.\\nOn the other hand, decision theory [21] , that is, the application of statistical hypothesis testing to the detection problem, is a well-known statistical technique that allows model/feature selection in the cross-validation (CV) loop [10] , [22] . The so-called case-based learning (CSL) employs a model selection algorithm in order to select the optimal classifier that minimizes the CV error (see figure 1 ) in a semi-supervised fashion. In a nutshell, this method consist in performing hypothesis testing [21] on the set of unlabeled validation responses or outcomes by the extraction of extended datasets under null&alternative hypotheses. Other approaches for model/feature selection are based on Information Theory, filter methods, embedded and wrapper methods, etc. [23] , [24] . Unlike the latter methods CSL evaluates a likelihood ratio test on the class-dependent features and selects the most probable model among them. In particular, supervised feature extraction (SFE) allows to obtain different datasets of features by hypothesizing on the unknown outcomes or responses of the validation pattern. As an example, in the binary classification problem, with classes ω 0 and ω 1 , two different datasets can be derived with two prediction models for each validation pattern, corresponding to the null H 0 : ω 0 & alternative H 1 : ω 1 hypotheses. The difference between them can be assessed in terms of probability by using either a model-based hypothesis testing framework, as preliminary proposed in [10] , or the classifier configuration derived from the novel datasets, i.e. in the output-score space of the support vectors, as shown in this paper. The influence of the validation pattern on these prediction models, i.e. the trained SVM, will depend on the relevance of the features that represent the samples in feature space and on the inherent complexity of the classification task beforehand. Here, it is measured in terms of the output scores of a confidence-based support vector machine classifier whilst in [10] this issue was not managed. In addition, this paper effectively demonstrates the benefit of the proposed approach by theoretically simulating the histogram of two classes under the class-hypotheses, showing the reduced overlap between distributions when the real hypothesis is considered.\\nThis paper is organized as follows. In section II, a background to the Bayes theory for solving classification problems is provided. A connection of this theory to the CSL methodology is derived in section III providing a novel likelihood ratio based on the error-rate margin under the classhypotheses. In the following subsection III-B two classical feature extraction methods are proposed for construction the extended datasets, such as Least Squares (LS) and Partial LS. In addition an implementation using the SVM classifier is shown in subsection III-C where the two-class classification problem is assumed, although it can also be extended to a multi-class case. Finally, section IV, presents experimental results to demonstrate the efficiency of the proposed method using synthetic and medical image databases. A full experimental framework is provided to demonstrate the benefits of the CSL acting on baseline approaches, i.e. using LS and PLS FE methods and SVM classifiers for leave one out-CV error minimization. In section V, conclusions are drawn.',\n       \"The coronaviruses (CoVs) are a family of enveloped positive-stranded RNA viruses broadly distributed among mammals and birds that cause respiratory and intestinal infections in animals and humans, and in some cases neurologic illness or hepatitis (1, 2) . The CoVs have been the causative agent of two large-scale pandemics in the past two decades: 1) Severe acute respiratory syndrome (SARS) in 2002 and 2003 in Guangdong province, China (3, 4) ; and 2) Middle East respiratory syndrome (MERS) in 2012 in Middle Eastern countries (5, 6) . After these two pandemic events, several pieces of evidence suggested possible future disease outbreaks: 1) CoVs undergo genetic recombination (7) , which may lead to new evolving genotypes; 2) the presence of a large reservoir of SARS-related coronaviruses (SARSr-CoVs) in horseshoe bats in China (8, 9) ; and 3) previous studies determined that some bat SARSr-CoVs have the potential to infect humans (2, (10) (11) (12) (13) .\\nThe SARS-CoV-2, previously known as the 2019-novel coronavirus 2019-nCoV (14) (Figure-1), is a newly identified β-coronavirus that caused an epidemic of acute respiratory syndrome in humans, which started in December 2019 in the context of a seafood market in Wuhan, China (14) . Later, in February 2020, The World Health Organization (WHO) named the disease as coronavirus disease 2019 . The COVID-19 has now progressed to be transmitted by human-to--human 'contact' and spread within few months not only throughout China but also worldwide, affecting over 4 million people and killing more than 279.000 of them in 187 countries as of May 10th 2020 (15) . Typical clinical symptoms of CO-VID-19 patients are fever, dry cough, breathing difficulties, headache and pneumonia and in some cases gastrointestinal infection symptoms.\",\n       'According to all approaches discussed above the size of networks is expected to enhance research performance (H1). Disciplinary heterogeneity of research groups (H2), an applied research orientation (H3), and industry collaboration (H4) increase research performance in the perspective of the mode-2 theory. The structural holes approach in social network analysis posits that heterogeneity of networks and low social control/ constraints in networks have a positive impact on performance. Thus it is expected that a large amount of industry ties (H4) as well as of international ties (H5) enhance performance. Low network density in ego-networks (H6) is an indicator of efficient networks and low control/ constraints. Sparse networks therefore could support high performance. On the other hand, the concurring view on the most important asset from networks holds that dense networks will support fruitful scientific exchange and knowledge transfer (H6 reversed). From an actorcentered learning perspective on networks, the strategic relevance of networks, an open choice of partners and the intake of new ties into networks are seen as indicators for a governance mechanism that might avoid the pitfalls of closed ossified networks. Open networks are expected to have a positive impact on performance (H7). Next, arguments from organizational ecology and management posit that specialization and differentiation of research profiles lead to higher performance (H8) as well as to larger networks (H9). Organizational ecology draws attention to a possible negative effect of a coarse grained environment -here a concentration of funding in focused programs -on specialization and performance (H10). Finally, from a resource based view on organizations we expect that groups from large established research institutions such as the Max Planck Society or the National Research Centers are in a better position to attract large networks (H11) and to invest in specialization (H12). This in turn yields a higher performance (H13). 3 Networks and scientific performance -preliminary evidence',\n       \"Coastal zones in the U.S. and throughout the world are subject to increasing hazards including storms and storm surge, sea level rise, and harmful algal bloom. Tropical cyclones and associated surge and inundation along the southeastern US coastline area major concern for coastal communities and their economies. Coastal waters in the southeastern US support ecologically and economically significant ecosystems, providing tourism, boating, fishing, and other recreational opportunities with an annual economic benefit of $675+ billion. With 73.5% of the population living in the coastal zone and 77.1% of GDP coming from shore-adjacent counties, this concern about tropical cyclones is particularly important to the State of Florida as it ranks in the top five of US states in the total ocean economy for its reliance on coastal tourism, recreation, and transportation sectors for employment [1, 2] . Florida's battle with tropical cyclones is notorious as it has been affected by more hurricanes than any other state. For example, between 1900 and 2010, Monroe County, located along the southwest Florida coast was affected by 32 hurricanes, which is more than any other county in the United States [3] .\\nManagement of the Floridian coastal environment is a challenging task for several state and local agencies including Florida Department of Environment Protection, Florida Division of Emergency Management, Water Management Districts, and coastal counties as well as local governments. The work of these agencies is heavily dependent on information made available by such federal agencies as National Oceanic and Atmospheric Administration (NOAA), Federal Emergency Management Agency (FEMA), U. S. Geological Survey (USGS), etc. Within NOAA (the primary agency associated with surge and inundation hazards), the National Weather Service (NWS), National Ocean Service (NOS), in the horizontal direction and a terrain-following sigma grid in the vertical direction. As such, the model can accurately represent complex shoreline and geometries in coastal regions. It uses a robust turbulence closure model to represent vertical turbulent mixing [15] and a Smagorinsky type model for horizontal turbulent mixing. The model uses bathymetry and topography which are referenced to the NAVD88 vertical datum for all domains to accurately simulate the coastal inundation. CH3D has been applied to such water bodies as Charlotte Harbor, Biscayne Bay, Apalachicola Bay, Florida Bay, Indian River Lagoon, Lake Okeechobee, Lake Apopka, Sarasota Bay, St. Johns River, Tampa Bay, Naples Bay, and Rookery Bay in Florida, as well as Chesapeake Bay, New York Bight, Long Island Sound, and the Gulf of Mexico.\",\n       \"This research makes an important contribution to the study of black agrarianism in that it is among the first to examine a young black farmer population. As a qualitative case study, these findings are limited to this specific bounded case and the participants therein. However, transferable lessons can be drawn that apply to broader populations of young farmers of color. Many of the findings were consistent with those found in previous research about black farmers. Much like the older generations, these young black farmers tended to farm on smaller parcels of land and engage in more diversified production compared to their white peers (Brown & Larson, 1979; Pennick et al., 2007; USDA NASS, 2014b) . Landownership as a source of power and self-sufficiency emerged as a theme from this study, which is consistent with previous literature (Dyer et al., 2009; Hinson & Robinson, 2008; Quisumbing King et al., 2018) . Notions of connection to land, independence, and selfsufficiency as food-producing citizens track with ideals held by black agrarians. As black agrarianism suggests, these participants farmed in order to care for the land, provide knowledge and resources to their communities, and maintain resilient links between their people, their history, and place (Quisumbing King et al., 2018; Smith, 2004) . Interestingly, this theme emerged even in conversations with participants who did not yet own their own land but hoped to soon, indicating the pervasiveness of the notion that land equals power in the black community. The paradox between autonomy and community support is telling of a population of farmers who both value their autonomy-as a way to protect themselves from the dependence on an American society that once deemed them subhuman-and a desire for social cohesion through a supportive and informative community. Despite the general consistency between this research and prior black agrarianism research, the findings from this study diverged from black agrarianism's political and activist emphasis. Participants were motivated to take care of themselves, their families, and their communities, but they did not necessarily see their involvement in agriculture as an explicitly political act.\\nPerhaps the most marked diversion from previous scholarship on black farmer populations was the lack of explicit discrimination, racism, or oppression experienced by participants (Balvanz et al., 2011; Daniel, 2013; Gilbert et al., 2002) . Though we certainly cannot extrapolate to say that racism and discrimination against African Americans is no longer a problem in agriculture, it is noteworthy that these concerns were not at the forefront of the minds of these young black farmers. It is easy to observe the racism in agriculture through instances like the Pigford case, but that search for explicit racism obscures the underlying concerns of these young black farmers. Just because these farmers felt that they had not faced explicit discrimination does not mean that they did not face other barriers or feel uncomfortable in agricultural spaces. While their ancestors may have been denied loans from the USDA or faced other overt forms of oppression, these participants were living in the era of micro-aggressions, in which their race-based concerns centered on more subtle issues such as a lack of representation in the agriculture industry or encounters with implicit bias. In terms of barriers to success in agriculture, participants' worries were not racially focused, but reminiscent of the concerns of young farmers across the country (Ackoff et al., 2017; Benson et al., 2014) . Participants worried about debt, about being able to afford and acquire land, and about gaining access to the training and resources they needed to be successful. They did not believe their race was a barrier, beyond simply making them a bit of an anomaly in their agricultural communities, but they did believe that factors such as being young or not being born into farm families were hindrances to their ability to be successful in agriculture.\",\n       'Complementary foods should be gradually introduced to infants after 6 months of life.',\n       nan, nan,\n       \"The data that support the findings of this study were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and are available from the ADNI database (adni.loni.usc.edu) upon registration and compliance with the data use agreement. A list including the anonymized participant identifiers of the currently used sample and the source file can be downloaded from the ADNI database (http://adni.loni.usc.edu/). The R-script used for the current study can be obtained from the first author upon request.\\nReceived: 12 November 2018 Accepted: 14 March 2019\",\n       'HC - Baseline 27, 51, 59, 69, 112, 132, 137, 165, 166, 168, 175, 195, 207, 214 HC -One- Year 9, 17, 18, 21, 30, 50, 59, 64, 81, 110, 132, 137, 143, 149, 164, 166, 169, 177, 180, 182, 183, 184, 195, 199, 207, 208, 211, 214, 218, 235, 242, 244, 249, 252, 259 AD - Baseline 9, 20, 51, 137, 149, 164, 165, 175, 188, 195, 199, 207, 214, 221, 228, 235, 246, 252, 259 AD -One- Year 9, 124, 132, 195, 245, 246, 252, 257, 259 Table 9 . Hubs in each cohort and visit identified using a normalized betweenness score 1.5 standard deviations above the mean based on the graphical lasso.',\n       'The approval sheet with a digital signature shall be included in the PDF file. It is important to note that there is a distinct difference between a true digital signature and a digitized signature. The latter is simply an image or other capture of a person\\'s pen-and ink signature. By using a document scanner or an electronic pen capture device, a person\\'s signature may be digitized. However, simply attaching this type of signature to an electronic document is not the same as attaching a digital signature. A digital signature, by contrast, appends a cryptographic \"key\" to the document that can be used to verify the identity of the signer (authentication), ensure that no changes have been made to the document since signing (integrity), and ensuring that the signer cannot deny having signed the document (non-repudiation). Until such time as an organization-wide digital signature solution is implemented, the nature of self-signed digital signatures will limit authentication and non-repudiation capabilities of the system. The mechanism of applying the digital signature may include a digitized version of a person\\'s signature, or it may not. The approval sheet shall contain the following statements: • Approval of the deliverable files, Descriptive Report, digital data, and all accompanying records. This approval constitutes the assumption of responsibility for the stated accuracy and completeness of the hydrographic survey. • Indication of the completeness of the survey and adequacy for its intended purpose. Recommendation of additional work is required. • The amount and degree of personal supervision of the work. • Additional information or references helpful for verifying and evaluating the survey. List all reports and data not included with the survey records or Descriptive Report that have been submitted to the Hydrographic Branch or to another office (e.g., Data Acquisition and Processing Report, Vertical and Horizontal Report, Tides and Water Levels Package, Coast Pilot Report). Include date of the report or date of submission. If appropriate, other personnel responsible for overseeing or directing operations on this survey sheet may also sign the Approval Sheet.',\n       nan,\n       \"The implications of such thinking were frequently expressed as concerns about 'teaching to the test', which was sometimes recognised by primary schools:\",\n       'Model output was saved at a subset of model nodes (stations) in order to keep output files manageable in size. Output stations were spaced evenly across the bay in order to capture spatial variability (Fig. 3) . While ADCSWAN has the ability to write out numerous variables (including wave heights, periods, etc.), the focus of this study is on WLs so discussion here will be limited to that variable. The model was run in 3-month-long segments with a 2-week overlap to avoid discontinuities in dynamic processes. Smaller segments were necessary for integrating SLR as well as for model stability reasons. Output data from these segments were then recombined into continuous time series at each station.\\nAnalysis was performed for both the historic and future periods. With an identical configuration (for all modeling components) to the historic period simulation, the only free variable is the AOGCM forcing under climate change. Therefore, a comparison shows how extreme events can be expected to change (in a relative manner) over time. ',\n       'The concept of multifunctionality in agriculture has become important in characterizing agricultural systems. Multifunctionality is the production of multiple outputs of agricultural activity in addition to producing food and fiber, such as maintaining the viability of rural communities and environmental sustainability (Wilson, 2008). The adoption of policies to support multifunctionality in farming has been especially important and dominant in Europe and Asian countries, as an effort to maintain flexibility in their farm and rural policies and to justify decoupled farm sector support. In this study we adopt the OECD definition of multifunctional agriculture which recognizes that \"Beyond its primary function of producing food and fibre, agricultural activity can also shape the landscape, provide environmental benefits such as land conservation, the sustainable management of renewable resources and the preservation of biodiversity, and contribute to the socio-economic viability of many rural areas. Agriculture is multifunctional when it has one of several functions in addition to its primary role of producing food and fibre\" (Maier and Shobayashi, 2001). There is a lack of empirical analysis in the U.S. about what factors motivate multifunctional agriculture activities. Although, \"multifunctionality\" per se is not a widely used or accepted term in U.S. agricultural policy (Bohman et al., 1999;Freshwater, 2002;and Blandford et al., 2002), its principles are particularly fundamental to some working lands conservation policies and programs, such as the Environmental Quality Incentive Program (EQIP), the Conservation Security Program (CSP), and the Wildlife Habitat Incentive Program (WHIP). Farmers are also engaged in providing recreational and agri-tourism services such as hunting, fishing, bird-watching, farm tours, petting zoos and hospitality services. Rice farmers in the U.S. have enjoyed in recent years very profitable conditions from the cultivation of rice (Baldwin et al. 2011). Despite favorable conditions, the farmer\\'s scope of activities has expanded by adapting more efficient practices and engaging in other activities that provide farm income diversification. Participation in conservation programs, which introduce and encourage environmental considerations into agricultural operations, can be used as an indicator of a farm being multifunctional. Additional indicators of multifunctionality are the engagement in on-farm income diversification through the provision of recreational activities and agritourism services such as duck hunting. Selecting rice as a case study to analyze multifunctionality in U.S. agriculture is useful for two reasons. First, because rice is a staple crop with a very wide global distribution, it has always been the recipient of high domestic support in national policies, even in the case of the U.S. where consumption levels are relatively low compared with other countries. Second, rice production has received considerable attention as a multifunctional crop in different regions, as for example in the European Common Agricultural Policy or in Japan and other Asian countries (Cooper et al, 2009;Matsuno et al., 2006). The objective of this paper is to identify factors that affect U.S. farmer participation in initiatives considered multifunctional in rice production. A binary logit model is estimated for the empirical analysis. This modeling framework is selected because of its wide application to many empirical studies and the fact that it is appropriate for a binary dependent variable. However, as we subsequently discuss, a multinomial logit model could have application.',\n       'Data collection and sharing for this project were funded by the ADNI (NIH grant U01 AG024904) and Department of Defense ADNI (award no. W81XWH-12-2-0012). The ADNI was launched in 2003 by the Figure 3 Voxel-wise analysis of reduced amyloid load in older APOE2 carriers without dementia\\nFigure shows results of a voxel-wise 2-sample t test assessing reduced AV45 standard uptake value ratios (SUVRs) in APOE2 carriers compared to the APOE3 control group across the entire brain while controlling for age, sex, education, and clinical diagnosis (cognitively normal or mild cognitive impairment). Statistical map was thresholded at p , 0.05, corrected for multiple voxel-wise comparisons with the false discovery rate. Analogous analyses for fluorodeoxyglucose SUVR or gray matter volume did not reveal any significant effects.',\n       'In its simplest form, GCTA allows to estimate the amount of variance captured by the matrix of genetic relationships, assuming that each SNP captures the same amount of variance. Through genomic partitions we can create different genetic relationship matrices based on non-overlapping regions of the genome. The SNPs on each of these partitions can capture then a different amount of variance (although, as before, SNPs within a given partition are supposed to capture all the same amount of variance). We grouped SNPs in the following partitions:\\n1. Partition based on genic status. Using 66,632 gene boundaries from the UCSC Genome Browser hg19 assembly, we made a first set with all SNPs within these boundaries, two further sets that included also SNPs 0 to 20 kbp and 20kbp to 50 kbp upstream and downstream of each gene, and a last set including the SNPs not located in regions less than 50 kbp upstream or downstream of genes. Both exonic and intronic regions were included in the genic regions. These partitions do not correspond exactly to those used by Toro et al. (2015) which were: one with strict genic/non-genic boundaries (0 kbp), another with genic ± 20 kbp versus the rest, and finally genic ± 50 kbp versus the rest.\\n2. Partition based on preferential central nervous system (CNS) expression (Raychaudhuri et al. 2010 , Lee, DeCandia, et al. 2012 ) using ± 50 kbp as gene boundaries, and based on markers of brain cell types as defined by two recent scRNA-Seq studies (Skene et al. 2018 ) using ± 20 kbp as gene boundaries.\\n3. Partition based on allele frequency. A partition based on MAF with 4 groups: from 0.1 to 5%, from 5 to 20%, from 20 to 35% and from 35 to 50%. In Toro et al (2015) , only the last 3 partitions were included, covering the range from 5 to 50% of MAF.',\n       \"Both the Empress Hotel, built on Punta El Medio, and the infrastructure in front of the Playamar Condominium were heavily impacted by Hugo ( Figure 10). Although the beach on the western flank of the point changed little (Playamar profile, Figure LlA), the sidewalk was undermined and the adjacent streets flooded. East of Punta Las Marias, the shoreline has retreated more than 50 meters since the 1930's. Hugo damaged the seawalls and gabions but did not cause extensive flooding or alter beach width or the profile significantly. Farther east, Hugo cut heavily into the berm behind Waldorf Towers (Figure lIB) destroying the sidewalk (Figure 12). In contrast, between the Playamar profile and the Waldorf Tower sites, a broad depositional beach withstood the storm waves with minimal amount of back beach flooding and no noticeable change in beach width and profile.\",\n       'We use an existing relative sea-level reconstruction from southern New Jersey  to describe the long-term baseline on which the simulated storm surges occur. The reconstruction was produced using foraminifera and bulk-sediment δ 13 C values measured in cores of dated salt-marsh sediment from two sites located ~58 km apart (Cape May Courthouse and Leeds Point; Figure 3.2). The Cape May Courthouse record spans the period since ~AD 700 and the Leeds Point record spans the period from ~500 BC to ~AD 1600. Each reconstruction has a 2σ age uncertainty and a 1σ vertical uncertainty. The two reconstructions were combined to produce a single, regional relative sea-level record, which is what an observer at the coast would have experienced, and is the net outcome of multiple and simultaneous processes including GIA. A caveat of using this data set is that relative sea level differences between southern New Jersey and NYC may arise over decades to centuries because of spatial differences in the rate of GIA (Engelhart et al., 2011), the fingerprint of ice sheet melt (Mitrovica et al., 2011), and role of ocean currents including the strength and position of the Gulf Stream (Davis and Mitrovica, 1996;Yin and Goddard, 2013). Relative sea-level reconstructions produced from salt-marsh sediment do not preserve the seasonal to inter-annual variability that is evident in tide-gauge time series because biological sea-level indicators such as foraminifera and plants respond to longer lived trends and because the slices of sediment used in the reconstruction have a thickness (typically 1 cm) making them time averaged. Therefore, the proxy reconstruction records multi-decadal to centennial-scale relative sea-level trends. Seasonal and inter-annual sea-level fluctuations caused by temporary weather patterns (winds and pressure), coastal sea-surface temperatures and salinities, and ocean currents can produce fluctuations of up to ~0.3 m in addition to this trend, as evidenced by the size of annual-average relative sea level departures from the overall trend measured at The Battery tide gauge in NYC.',\n       'The symposium \"Putting the Whole Grain Puzzle Together: Health Benefits Associated with Whole Grains\" sponsored by the ASN brought together researchers to review the evidence regarding the health benefits associated with whole grains. Current scientific evidence indicates that whole grains play an important role in lowering the risk of chronic diseases, such as coronary heart disease, diabetes, and cancer, and also contribute to body weight management and gastrointestinal health. The essential macroand micronutrients, along with the phytonutrients present in whole grains, synergistically contribute to their beneficial effects.\\nCurrent evidence lends credence to the recommendations to incorporate whole grain foods into a healthy diet and lifestyle program. The symposium also highlighted the need for further research to examine the role of whole grain foods in disease prevention and management to gain a better understanding of their mechanisms of action.',\n       \"In this section, the descriptive analysis results are given. Prospective teachers produced 135 valid metaphors. Most frequently repeated metaphors were chameleon (f=10), jigsaw puzzle (f=9), play dough (f=5), exchange rate (f=3), seasons (f=2), Black Sea's weather (f=2), domino stones (f=2), human life (f=2), Indian silk (f=2), swamp (f=2), traditional agriculture (f=2), tuning of the oriental music instrument (f=2), vegetable soup (f=2), and weather forecast (f=2).\\nThe metaphors and main categories are presented in Table 1 . In Table 1 , it is appear that the inevitableness main category includes greatest number of metaphors (n=19). The uncertainty main category includes 16 metaphors. In addition, the most frequent metaphor is jigsaw puzzle (f=7) in this category. The changeability category includes ten metaphors. In this category, the most frequent metaphor is chameleon (f=10). Some metaphors such as exchange rate, domino stones, jigsaw puzzle, and play dough are seen in two different categories. This is because during the category creating process the reasons explaining the metaphor were considered.\\nThe frequency of the main categories are presented in Diagram 1.\",\n       \"PPMI mimics the landmark Alzheimer's Disease Neuroimaging Initiative (ADNI) in terms of a focus on standardizing protocols and providing the research community with open access to data and biosamples. At the time of the London meeting, over 180,000 data downloads and over 40 biological specimens had been requested through the ADNI. Data collected through the PPMI include clinical (motor and non-motor, neurobehavioral/cognitive, autonomic, olfaction, sleep), imaging (DaTscan TM , AV133, amyloid, DTI/RS MRI), and corresponding biological samples (DNA, blood, CSF). The PPMI study population originally included 400 newly diagnosed and unmedicated PD subjects as well as 200 age-and gender-matched healthy controls, and 70 subjects with a clinical diagnosis of PD but without evidence of dopaminergic deficiency (SWEDD) by dopamine transporter SPECT imaging [27] . Subsequently, three other subgroups were added: 100 pre-motor, 500 subjects with LRRK2 mutations, and 100 with ␣-synuclein mutations (50 with PD and 50 unaffected family members). There are also future plans to incorporate novel data sources that include wearable sensors in PPMI.\",\n       \"It is another serological method and called enzyme immunoassay (EIA). ELISA is a plate-based method that has been used for detecting and quantifying soluble substances such as proteins and antibodies in clinic and research laboratories. It includes direct and indirect formats (Zhang et al., 2014) . The indirect ELISA, the most popular and more sensitive than the direct ELISA, an antigen (e.g., a recombinant protein (N protein) of SARS-CoV-2 virus) is coated onto the inner surface of 96-well or 384-well polystyrene plates (Gao et al., 2015) . A diluted patient's plasma which may have anti-SARS-CoV-2 IgG/IgM is added to the wells. The plate is incubated for one hour to allow the antibodies to interact with coated antigens. After washing the plate to eliminate unspecific interactions, a conjugated antibody with a reported enzyme such as horseradish peroxidase (HRP) or alkaline phosphatase (AP) is added to form sandwich complexes (Li et al., 2013; Zhang et al., 2015) . These complexes are detected and quantified by adding a substrate (e.g., 3,3′,5,5′-tetramethylbenzidine) that is utilized by the report enzyme and leads to change in the reaction color (Madersbacher & Berger, 1991; Lee, Harrison & Lewis, 1990) . The color is detected and measured by a plate reader (Fig. 4) . ELISA is relatively fast (2-5 h) and cheap compared to rRT-PCR, and it is similar to FLA regard to accuracy. It has been reported that ELISA results were 50% (IgG) and 81% (IgM) for patients on day zero and became 81% (IgG) and 100%(IgM) on day five of SARS-CoV-2 infection . Another study accomplished by showed that using ELISA to detect IgM and IgG on day four of symptom onsite revealed a sensitivity of 77.3% and specificity of 100% for IgM while those were 83.3% and 95% respectively for IgG. Worth mention that there are other serological methods that are less common than FLA and ELISA (La Marca et al., 2020) . A colloidal gold immunochromatography assay (GICA), and Chemiluminescent immunoassay (CLIA) were developed to diagnose COVID-19; however, they have low sensitivity at the beginning of the infection (Infantino et al., 2020; . Pan et al. (2020) reported that the sensitivity of GICA were 11.1% on the first week and 92.9% on the second weeks after the onset of symptoms. Neutralization assays, on the other hands, are standard methods for determining antibody efficacy (e.g., serum virus neutralization (SVN) assay). They are used to check whether a patient has active antibodies that can neutralize the SARS-CoV-2 infection (Gauger & Vincent, 2020; Muruato et al., 2020) . These assays play a key role in determining if an individual is eligible to donate his/her convalescent plasma as a treatment for seriously ill people although such treatment has not been fully validated . Both molecular and serological methods are not perfect in terms of detecting COVID-19 virus and each method has its own limitations (Bisoffi et al., 2020) . Though molecular methods are more reliable than serological methods, both methods could give false results due to various reasons. For instance, incorrect sampling, inadequate viral material in the specimen, improper RNA extraction, cross-reactions with other viral species, contamination and technical issues could lead to positive and negative false results. To overcome such issues and increase the certainty of given results, these methods can be followed by secondary diagnostic methods such as a chest CT scan and x-ray imaging Wong et al., 2020) .\",\n       \"The major fiber pathways in the brain are already present and identifiable at birth, but very rapid changes in DTI indices of WM microstructure are seen across infancy (for reviews see (Dubois et al., 2014; Qiu et al., 2015) ). For instance, a large study including 211 young children and 295 scans indicated that in the first two years of life, FA in ten major WM tracts increases by 16-55%, RD decreases by 24-46%, and AD decreases by 13-28%, with faster changes in the first year than in the second for all tracts investigated (Geng et al., 2012) . Such massive changes are not surprising given the enormous behavioral and psychological development seen in this period of life. Studies focusing on early life brain development will however not be covered in the present review, as the nature and scale of these changes are very different from those seen in later childhood and adolescence. In addition to the methodological challenges for dMRI studies of children and adolescents discussed in this review, studies of infants and young children have other specific major challenges, including particular difficulties in obtaining relatively motion-free images from babies (as sedation is not used without clinical indication), image registration and alignment, use of adult-based brain atlases, and dramatically changing image intensity contrasts which may lead to misclassification of tissues, especially when using automated software, and make comparisons across age difficult (Sled and Nossin-Manor, 2013) .\\nNumerous cross-sectional studies have been carried out to investigate age-related differences in DTI parameters in children and adolescents, which, despite sample and methodology differences, consistently demonstrate FA increases and overall diffusivity decreases with increasing age in most WM regions (e.g., (Asato et al., 2010; Ashtari et al., 2007; Barnea-Goraly et al., 2005; Ben Bashat et al., 2005; Clayden et al., 2012; Eluvathingal et al., 2007; Giorgio et al., 2008; Klingberg et al., 1999; Lebel et al., 2010 Lebel et al., , 2008 Muftuler et al., 2012; Pohl et al., 2016; Qiu et al., 2008; Schmithorst et al., 2002; Snook et al., 2005; Tamnes et al., 2010; Wu et al., 2014; Yu et al., 2014) , for reviews see (Cascio et al., 2007; Schmithorst and Yuan, 2010) , for a metaanalysis see: (Peters et al., 2012) ).\\nLongitudinal DTI studies of children and adolescents are becoming more common, but are still scarce (Table 1) . In contrast to the crosssectional studies, longitudinal studies track changes over time within individuals and can directly relate these estimates to influencing factors, outcomes or concurrent developmental changes for instance in cognition, social and affective processing or symptomatology. Most of the available studies have employed accelerated longitudinal designs which allow for investigation of wide age-ranges over shorter duration data collection periods, but with the trade-off being the inherent missing data since each subject's measurement schedule covers only part of the age-range of interest (Galbraith et al., 2017) . One study employing a single cohort longitudinal design has also been performed (Brouwer et al., 2012) ; focusing on a narrow age range, but alleviating the missing data issue since each individual is followed over approximately the same period. Notably, all available longitudinal studies include only two observations for all or the majority of the participants, and this remains a major limitation of the field, as such datasets do not allow for optimal modelling of non-linear within-person change.\\nThe first published longitudinal DTI studies were of very small scale with limited power to detect true effects, and more vulnerable to effects of outliers. Giorgio et al. (2010) analyzed data from 24 adolescents in the age-range 13-22 years and used TBSS to obtain DTI measures from the WM skeleton of scans collected at two time-points on average 2.5 years apart, as well as probabilistic tractography to isolate selected tracts. Their results showed bilateral significant FA increases in widespread regions in the WM skeleton and the arcuate fasciculi, but not the corticospinal tracts. The FA increases were mainly driven by increases in AD, while RD remained relatively unchanged. The same year, Bava et al. (2010) published results based on TBSS analyses of a dataset consisting of two time-points with a shorter interval (mean 1.3 years) from 22 slightly older adolescents (age-range 16-21 years). The results showed significant FA increases over time, but only in limited regions in the right hemisphere. Additionally, and in contrast to the findings by Giorgio et al. (2010) , Bava et al. (2010) also observed decreases in both RD and AD in multiple regions.\\nA larger study published by Lebel and Beaulieu (2011) , included 103 participants in a much broader age-range, 5-32 years, and with 2-4 scans per participant (mean = 2.1) acquired over 1-6 year intervals. Deterministic tractography for the following 10 major WM tracts was performed: genu, body and splenium of the corpus callosum, corticospinal tract, superior and inferior longitudinal fasciculi, superior and inferior fronto-occipital fasciculi, uncinate fasciculus, and cingulum. All showed significant nonlinear (quadratic) developmental trajectories, with decelerating increases for FA values (Fig. 3) and decelerating decreases for MD, except the corticospinal tract which had linearly decreasing MD. These changes were due primarily to decreasing RD. Another large tractography study, but with a different type of design and sample, was published by Brouwer et al. (2012) . This was a single cohort longitudinal twin study including 203 individuals with 1-2 scans per participant (mean = 1.6), with the baseline scans acquired at age 9 and the follow-up scans at age 12. FA increased in all fiber tracts investigated, with average annual change rates of 0.4-2.3%. Most tracts showed increases in AD and decreases in RD, with change rate in RD being greater than that of AD in 9 of the 14 tracts that were studied, and in the overall WM mask. So far, only one longitudinal study with more than two time-points for many participants has been published. Here, Simmonds et al. (2014) followed 128 participants with 1-5 scans (322 scans in total, mean scans = 2.5, 60/128 participants with > 2 scans) acquired approximately annually in the age-range 8-28 years. Across the WM skeleton and across atlas-defined ROIs, increases in FA and decreases in RD with age were seen. Across the skeleton, AD also decreased with age, but AD did not significantly change with development in the majority of ROIs.\\nIn a recent unique study focusing on the preschool and early school years, specifically ages 4-11 years, Krogsrud et al. (2016) analyzed longitudinal DTI data from 159 participants scanned twice. Across the averaged WM skeleton, FA showed a significant linear increase over time, while there were linear decreases in MD and RD, and AD showed only a weak decrease. Some regional differences were observed, although the same general pattern was seen for most of the investigated atlas-defined tracts. These mostly linear WM microstructural changes during middle and late childhood complement the above described findings by Lebel and Beaulieu (2011) of nonlinear trajectories across adolescence.\\nIn summary, the few available longitudinal DTI studies generally confirm the findings from the cross-sectional studies, documenting continued WM microstructural development though childhood and adolescence with decelerating bilateral and widespread increases in FA and decreases in MD and RD, while the conclusion for AD is less clear. It is not possible, either on a group or individual level, to specify an age at which the brain becomes 'fully developed'. DTI indices of WM microstructure are never static, but rather reflect lifelong maturation, experience-dependent plasticity and degeneration. Although a certain degree of stability can be seen in adulthood relative to childhood and C.K. Tamnes, et al. Developmental Cognitive Neuroscience 33 (2018) 161-175 adolescence, there are no periods of life in which brain WM microstructure is fixed. DTI studies including participants across major parts of the human lifespan have extended the findings from the developmental studies, indicating non-monotonic age trajectories of FA, MD and RD characterized by three phases: (1) initially fast, but decelerating changes through childhood and adolescence and into early adulthood followed by (2) relative stability in mid-adulthood, with subsequent (3) accelerating changes in senescence (Lebel et al., 2012b; Sexton et al., 2014; Westlye et al., 2010) .\\nNotably, DTI can also be used to investigate developmental changes in tissue microproperties in subcortical GM structures and the cerebral cortex. Cross-sectional results from a large sample of children and adolescents by Lebel et al. (2008) for instance indicate greater magnitudes of age-related DTI changes in deep GM structures, specifically for FA increases in regions including the caudate nucleus, globus pallidus and putamen, than in WM tracts (see also (Pal et al., 2011) ). Grydeland et al. (2013) investigated intracortical MD in a large cross-sectional lifespan sample and observed linear decreases in large frontal and temporal regions in childhood and adolescence. However, DTI measurement in cortical and subcortical GM is challenging, particularly for thin cortical and small subcortical regions, due to the relatively low resolution of standard DTI sequences, which likely result in partialvolume effects (Grydeland et al., 2013; Koo et al., 2009) .\\nDevelopmental studies using more recent and advanced dMRI models such as DKI and NODDI are also becoming more common, yet only cross-sectional studies are so far available. Several DKI studies of children and adolescents have shown that mean kurtosis shows agerelated increases (Das et al., 2017; Falangola et al., 2008; Grinberg et al., 2017) . A rare developmental NODDI study by Chang et al. (2015) including 66 participants 7-63 years old indicate that the age-related Fig. 3 . Longitudinal age-related changes of fractional anisotropy (FA) in healthy individuals aged 5-32 years. Spaghetti plots with the best fitting models and bar graphs depicting the percentage of subjects whose FA increased (green), decreased (red), or did not change (blue) in six age groupings are shown for different WM fibers, derived using a deterministic tractography method. Adapted with permission from Lebel and Beaulieu (2011) . (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) C.K. Tamnes, et al. Developmental Cognitive Neuroscience 33 (2018) 161-175 FA increase during childhood and adolescence is dominated by increasing NDI, which points specifically to increases in fiber diameter and myelination, while the decrease in FA later in life may be driven by increasing ODI (Fig. 4) . Moreover, the results from this study indicated that NODDI metrics predicted chronological age better than DTI metrics, a conclusion also recently supported by Genc et al. (2017) . Further work, including longitudinal studies, is needed to explore the potential of these promising techniques for increasing our understanding of brain development.\",\n       \"Home, school and community environments and the interactions within (Reddy et al., 2016b) mould education and learning. In addition to achievement data, TIMSS collects contextual information about the participating learners, teachers and schools. This helps researchers understand what factors predict learners' academic success (Topçu, 2016) .\\nIn the sub-sample of South Africa in the TIMSS 2015, the schools were analysed according to the three categories as no-fee paying, fee-paying and independent schools with regard to schools and home contexts. The analysis was conducted on the variables that could possibly be attributed to sustainable development such as parents with tertiary education, access to running tap water, access to flush toilets, high emphasis on academic success, almost always speaks the test language and bullied on a weekly basis. These aspects are in this paper and Figure 3 was sourced from Reddy et al. (2016b) . It can be noted from Figure 3 that households with post matric education, access to running tap water, access to a flush toilet, high emphasis on academic success and almost always speaks the test language are the highest for independent schools. Bullying on a weekly basis is happening in most of the no-fee paying schools. The conclusion is that Eastern Cape's very low performance can be attributed to its contextual factors.\\nThe above situation in the Eastern Cape is further analysed by the authors in terms of percentages of the contextual factors against the South African averages. Tables 2-6 show the analysed data of these contextual factors.\",\n       'The EPA Chesapeake Bay Program has developed an awareness of the value of seagrasses in the ecology of the Bay, and in July 1989 developed an Agreement Commitment Report signed by the states ofVirginia, Maryland, and Pennsylvania, the District of Columbia and the Environmental Protection Agency entitled \"Submerged Aquatic Vegetation Policy for the Chesapeake Bay and Tidal Tributaries.\" This agreement states that the signers will work together to implement four major areas relative to seagrasses. These include assessment of the distribution and abundance of the resource, development of protection and restoration guidelines, and implement an education component to increase public awareness of the value of the resource. The educational component recognizes the need for scientific research to --Chapter 2: Planning * 65 improve our knowledge and understanding of submerged aquatic vegetation to ensure that efforts to protect and restore this resource continue to be effective. It is refreshing to see that recognized in a ecosystem or watershed management approach such as is being conducted in the Chesapeake Bay.',\n       nan,\n       'The Healthy Eating Index 2010 (HEI-2010) was designed to measure compliance with the key recommendations in the 2010 Dietary Guidelines for Americans (DGA). It covers 12 components: Total Fruit, Whole Fruits (not including juice), Total Vegetables, Greens and Beans (dark-green vegetables and beans and peas), Whole Grains, Dairy (all milk products and soy beverages), Total Protein Foods, Seafood and Plant Proteins, Fatty Acids (ratio of poly-and monounsaturated fat to saturated fat), Refined Grains, Sodium and Empty Calories (all calories from solid fats and added sugars plus calories from alcohol). Adequate consumption of all components except Refined Grains, Sodium, and Empty Calories raises scores. Consumption of the latter lowers scores. A perfect overall score for the HEI-2010 is 100. Subscores for the components can be up to 20, with the ranges for each individual component being 0 to 5, 0 to 10, or 0 to 20. The HEI-2010 is the only metric in this report that applies the 2010 DGA as a point of comparison. Details of the HEI-2010 components can be found in Appendix K, Table K-1.',\n       'For all variables, outlier scores were winsorized to three standard deviations beyond the mean. The variables were then checked for skew and kurtosis and were found to be satisfactory using the general recommendations of Kline (2005), with values between −1.2 to 1.2 across all variables. Missing variables were replaced using the Linear Interpolation function in SPSS 18.0. Participants with missing data on any variable were not more likely to be male (t=.799, p=. 424) or high-ADHD (t= −.648, p=.517). After replacing missing variables, 47 participants (4.6%) still had missing data on at least one variable. These participants were still not more likely to be male or part of the high-ADHD group. Fifteen percent (n=149) of the total sample was classified as having significant parent-and/ or teacher-rated ADHD symptoms at grade 3 (high-ADHD). This is higher than the prevalence of clinically diagnosed ADHD in the US population, and thus reflects a somewhat broader case sample, as the high-ADHD group likely includes some participants who would not meet diagnostic criteria for ADHD via a thorough clinical evaluation. In a recent community sample study, Willcutt, 2012 found that although the \"and/or\" classification rule is prone to false positive clinical ADHD diagnoses, research participants classified under this broader algorithm typically show impairment across both school and home settings (70%) and have a symptom onset prior to 7 years of age (90%). Further, research suggests that the ADHD phenotype exists on a continuum (Pennington, 2002;Arnett et al., 2012), supporting the examination of participants with high-versus lowsymptom phenotypes in research. High-and low-ADHD participants did not differ on age of first entry into formal schooling, hours that the mother worked outside the home, or likelihood of being of non-Caucasian or Hispanic origin. However, high-ADHD participants included a higher percentage of boys, had mothers with lower educational attainment, and came from families with a lower income, on average, than low-ADHD participants (Table 1).',\n       'The NIA-AA criteria to identify the symptomatic but predementia phase of AD (MCI due to AD) refine previous clinical definitions of MCI and add an entity of MCI due to AD pathology based on the combination of clinical diagnosis and the presence of AD biomarkers, resulting in four levels of diagnostic certainty [15] (Table 3 ).',\n       'STEM education was a systematic teaching and/or learning process in the STEM fields and a positive correlation existed between STEM education, and the economic prosperity and power of a nation in the globalized world. In recent years, rising concerns have emerged about American STEM education. Many stakeholders wondered that whether the nation has enough well-qualified STEM students, teachers and workforce to maintain its current competitive edge. This study sought to answer those questions, presenting a unique view about the concerns. This study, besides, summarized selected major legislation which affected STEM Education in the United States. The results of the study showed that American students in elementary and secondary schools have relatively mediocre scores compared with their international peers (especially Asians), although they performed better than earlier American cohorts in science and mathematics. The quality of STEM teachers also led to concerns. The lowest certification rate of teachers was found in science and mathematics, and approximately half of the teachers did not have a degree in the subject that they teach. Lastly, this study revealed that students should learn the requisite new patterns of language and expression only through opportunity for and engagement in STEM disciplinary practices.',\n       'A course that focuses on the application of educational and communications principles and methods to the promotion of preventive health measures and the education of targeted populations on health issues, and prepares individuals to function as public health educators and health promotion specialists. Includes instruction in human development; health issues across the life span; population-specific health issues; principles and methods of public health education; administration of health education campaigns and programs; evaluation methods; public communications; and applications to specific public health subjects and issues.',\n       nan, 'The authors have no conflicts to declare.',\n       'The IWBP contains 184 lakes, with 111 lakes yielding at least 15 valid seasonal means covering 1985 to 2011. Seasonal means are constructed from summer nighttime data averaged over the warm season (see Fig. 2.8) from the Advanced Very High Resolution Radiometers (AVHRR;1985-2009, the Along-Track Scanning Radiometers (ATSR;1992, and the Moderate Resolution Imaging Spectroradiometer (MODIS; [2010][2011] aboard the Terra satellite. For MODIS and ATSR, skin temperatures were retrieved whereas AVHRR measurements were retrieved as bulk temperatures. To merge the datasets, the skin measurements of MODIS and ATSR were adjusted to equivalent AVHRR measurements. This maximizes data coverage and extends the satellite records, which improves trend calculation (Schneider and Hook 2010). The ARCLAKE project covers 258 large lakes, with 164 yielding warm season average time series covering both hemispheres from 1992 to 2011 ( Mac-Callum and Merchant 2012). It is based only on the series of three ATSRs, reprocessed with improved ARCLAKE cloud detection and physics-based surface temperature estimation. Overlaps between sensors in the series have been used to stabilize the calibration of the full time series, minimizing the potential for instrument-related artifacts in trend calculations. Using ARCLAKE, 2011 warm season average lake temperatures were generally above average in the Northern Hemisphere, especially over Europe and northeastern North America (Plate 2.1d), agreeing with land-air and sea surface temperatures (Plate 2.1c). Cool anomalies over eastern and southeastern Asia and Alaska are also reasonably consistent. Elsewhere, the signal is more mixed and less consistent with land-air and sea temperatures, with cooling over parts of Africa and Central and South America. Global and regional average anomaly time series are presented for 84 lakes common to both datasets (Fig.  2.8). Year-to-year variability is large both within and among regions, and also between IWBP and AR-CLAKE, which use different methodologies. From 2010 to 2011 IWBP shows a small warming in the Northern Hemisphere, cooling in the tropics, and no change in the Southern Hemisphere. ARCLAKE shows 2011 as cooler than 2010 generally, except for the Southern Hemisphere. The IWBP global mean trend is +0.04 ± 0.02°C yr -1 (p < 0.001), likely dominated by the large number of Northern Hemispheric lakes. Weighting the three regions (northern, southern, and tropical) equally yields a global trend of +0.02°C yr -1 . ARCLAKE trends are less certain because of the shorter time period and apparently greater interannual variability. The year-to-year variations show similar features, and all the trends are consistent within uncertainties with those of IWBP. Cooling from the 1991 Mount Pinatubo eruption and warming from the strong 1998 El Niño event are apparent. The 164 lakes in the ARCLAKE dataset ( Fig. 2.9a) show several spatially coherent patterns in trends (1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008)(2009)(2010)(2011). Trends for the African rift valley, Caribbean, and central Asian lakes are negative (0 to -0.05°C yr -1 ). In contrast, there are generally larger warming trends for lakes in North America and  in the case of Lake Nipigon, Canada). The 111 IWBP lakes (Fig. 2.9b) show more widespread warming, strongest in Northern Europe, where Lake Vanern, Lake Vattern, Lake Ladoga, and Lake Onega have warmed between 0.05°C yr -1 and 0.10°C yr -1 . Trends are slightly smaller in southeastern Europe and the Middle East. Despite differing periods and methods, both ARCLAKE and IWBP are consistent in identifying relatively rapid warming in the lakes of both North America and Europe. org and the Norwegian repository at http://www.ngu.no/ kart/permafrost. A Northern Hemisphere Arctic permafrost thermal snapshot  shows significant regional differences ( Fig. 2.10a). Warm ocean currents influence the climate of northern Scandinavia, Svalbard, and northwest Russia, resulting in up to 0°C permafrost temperatures in the discontinuous permafrost. In high Arctic Svalbard, the -3°C permafrost temperature is the highest in the continuous permafrost zone ( Christiansen et al. 2010). In the rest of Russia and North America, permafrost temperatures vary from 0°C to -2.5°C within the discontinuous zone with colder conditions in the continuous zone; permafrost temperatures reach as low as -15°C in the high Arctic (Romanovsky et al. 2010a;Smith et al. 2010). In Antarctica, t he f irst g rou nd t her ma l snapshot (Vieira et al. 2010) has been updated ( Fig. 2.10b). In Northern Victoria Land, the active layer thickened by 1 cm year -1 from 1996 to 2009 (Guglielmin and Cannone 2011), while in McMurdo Sound no clear trend was recognized from 1999 to 2007 (Adlam et al. 2010). In Northern Victoria Land, mean annual permafrost temperature increased by 0.1°C year -1 between 0.3 m and 3.6 m depth despite stable air temperatures (Guglielmin and Cannone 2011). Conversely, a reconstructed ground surface temperature history from McMurdo Sound boreholes, suggests a slight cooling from 1998 to 2003, followed by a slight warming to 2008 (Guglielmin et al. 2011). Permafrost generally warmed across the Northern Hemisphere during the past 20 to 25 years (Romanovsky et al. 2011;Isaksen et al. 2007). Smaller warming rates are generally observed in permafrost at temperatures close to 0°C, especially if it is ice-rich, because latent heat effects dominate the ground thermal regime (see section 5g). The permafrost in the European Alps and Scandinavian mountains is discontinuous, with mean annual ground temperature generally between 0°C and -3°C (Haeberli et al. 2010). The steep topography, heterogeneous surface characteristics, and snow cover result in high spatial variability of the ground thermal conditions. Decadal records (Fig. 2.11) for European moun- tain permafrost show a general warming trend with smaller increases in temperature where permafrost temperatures are close to 0°C (Isaksen et al. 2007;Haeberli et al. 2010;PERMOS 2010). Permafrost temperature anomalies associated with extreme warm years (2003,2009) are superimposed on the warming trend (PERMOS 2010;Phillips et al. 2009; Zenklusen-Mutter et al. 2010). Scandinavian trends are more pronounced (Isaksen et al. 2011). The observed trends in permafrost temperatures are consistent with changes in air temperatures, but snow cover, soil properties (including ice and moisture content), and vegetation also determine the magnitude of the changes in ground temperature (e.g., Haeberli et al. 2010;Romanovsky et al. 2010b). In the warm permafrost of the higher altitudes of Central Asia, ground temperatures have increased by up to 0.5°C decade -1 since the early 1990s, with a general increase in active layer thickness (e.g., Zhao et al. 2010). Decadal trends in active layer thickness (ALT) vary by region ( Fig. 2.12). Permafrost has disappeared in several of the Circumpolar Active Layer Monitoring (CALM) sites. A progressive increase in ALT has been observed since the 1990s in all regions, with the exception of northern Alaska, western Canadian Arctic, and West Siberia. Smaller increases have been observed in Scandinavian sites such as Arctic Sweden and Svalbard compared to East Siberia, the Russian Far East, and Greenland. In North America, a progressive increase of ALT is observed only at sites in the Alaskan Interior, whereas ALT on the Alaskan North Slope was relatively stable over the 1995-2011 period. 2) NORTHERN HEMISPHERE CONTINENTAL SNOW COVER EXTENT-D. A. Robinson Annual mean snow cover extent (SCE) over Northern Hemisphere (NH) lands averaged 24.7 million km 2 in 2011 ( Fig. 2.13). This was 0.3 million km 2 less than the 42-year average, ranking 2011 as having the 17th least extensive cover on record (Table 2.1). This evaluation considers snow over the continents, including the Greenland ice sheet. The SCE in 2011 ranged from 48.5 million km 2 in January to 2.3 million km 2 in August. Monthly SCE is calculated at the Rutgers Global Snow Lab from daily SCE maps produced by  Years 1968Years , 1969Years , and 1971 have 1, 5, and 3 missing months respectively, thus are not included in the annual. Ranks are from most extensive (1) to least (ranges from 42 to 46 depending on the month). meteorologists at the National Ice Center (a US joint NOAA, Navy, and Coast Guard facility), who rely primarily on visible satellite imagery to construct the maps. Comprehensive summaries may be viewed at the Rutgers Global Snow Lab website (http://climate .rutgers.edu/snowcover). The first three months of 2011 had snow cover extents near or in the upper quartile over Eurasia (EU) and North America (NA), resulting in the sixth most extensive January SCE. Eurasian cover began a rapid retreat in April, leading to the fifth least extensive cover over this continent. Meanwhile, above-average April SCE was noted over NA, to end a very snowy season. By May, NA had caught up with EU losses, thus there was an early end to the melt season over both continents.',\n       'Given that the experiment was successful at randomizing students into treatment and control groups, I now turn to presenting the results of the experiment. I focus first in Section 3.1 on take-up of treated students entering their password on the gateway website, and describe their website browsing behavior in Section 3.2. I then examine impacts on postsecondary outcomes by comparing treated to control group students, focusing on postsecondary enrollment in Section 3.3, and persistence through college in Section 3.4.',\n       \"We will first briefly review the standard hyper-graph learning techniques and then introduce our dynamic hyper-graph learning framework, which integrates the classification and regression task. Following the definition of hyper-graph in [15] , a hyper-graph is denoted as G = (V , E), where V = {v} is the vertex set and E = {e} is the hyper-edge set. Given Nsubjects with morphological features X = {x 1 , . . . , x N }, we first compute the N × N affinity matrix A, where each element a i j measures the similarity between subject I i and I j (i, j = 1, . . . , N). Then, we construct the incidence matrix H ( H ∈ R |V |×|E| ) to encode the hyper-edge as follows. Each vertex v is allowed to establish multiple hyper-edges where each hyper-edge characterizes the similarity between underlying v and all other vertexes. For efficiency, the subject-to-subject relationship is binarized into a bit array ('0' stands for non-connected and '1' stands for connected in the underlying hyper-edge) and further becomes one column vector in H . Note, a threshold ξ is required in binarization. As we will explain later, we use cross validation to determine the value of ξ in our experiments. Two diagonal matrices D v ∈ R |V |×|V | and D e ∈ R |E|×|E| can be calculated to represent the vertex degrees and hyper-edge degrees from the incidence matrix H as:\\n(1)\\nAfter that, the Laplacian matrix L of the hyper-graph can be computed as: where I is the |V | × |V | identity matrix. Note, we assume the hyper-edges have equal weight, for simplicity.\\nSuppose N subjects consist of P training subjects with known clinical labels Y P = [y 1 , . . . , y P ] and Q unseen testing subjects (N = P + Q). The goal of conventional hypergraph learning is to estimate the label probabilities for testing subjects by:\\nwhere\\nis the probability vector for all N subjects with the first P elements forming the probability vector F P for P training subjects and the last Q elements forming the probability vector F Q for Q testing subjects. Eq. 3 can be solved efficiently using Augmented Lagrange Methods (ALMs) [19] . The intuition behind hyper-graph learning is illustrated in the left panel of Fig. 2 . The hyper-graph is fixed once built from the observed morphological features X, and then the latent labels on the testing subjects are determined, based on the encoded data representation in the hyper-graph, with respect to the neighboring training subjects with known labels. The prior knowledge of labels Y P is not used to guide the learning of data representation.\\n2) Extend to Multi-Modal Scenario: In order to apply hypergraph to multi-modal imaging data, Gao et al. [9] proposed to combine multiple hyper-graph models using a linear model and estimate the unknown data labels on the combined multiple hyper-graphs. Suppose there are M set of morphological features {X m |m = 1, . . . , M}, M hyper-graphs are constructed. A weighting vector α = [α m ] m=1,...,M is learned to measure the contribution of each modality. Similarly, the labels are propagated on the combined hyper-graph by solving:\",\n       \"Along the N-S transect, ΔO 2 /Ar was generally within a few percent of atmospheric equilibrium, with slightly positive values north of 55°S (<2000 km along the cruise track) and negative values in ice-covered waters of the Weddell Sea MIZ (Figures 2f and 3b) . Negative ΔO 2 /Ar values are indicative of net heterotrophic conditions (Figures 2e and 3a) . Surface water pCO 2 and ΔO 2 /Ar showed high variability in the Weddell Sea MIZ (E-W transect) and WAP region. In these areas, pCO 2 reached minimum values of~100 μatm, while ΔO 2 /Ar in excess of 50% was observed (Figures 3a and 3b) . The lowest pCO 2 and highest ΔO 2 /Ar occurred in near-shore waters of Marguerite Bay (WAP; Figures 2e and 2f) at~11,000 km along our cruise track.\\nThe pCO 2 and ΔO 2 /Ar disequilibria we observed are substantially higher than values previously reported for the offshore pelagic Southern Ocean [Cassar et al., 2011; Reuer et al., 2007; Shadwick et al., 2014] , but they are consistent with recent observations from the highly productive waters of the Ross Sea and Amundsen Sea polynyas [Smith and Gordon, 1997; Tortell et al., 2011 Tortell et al., , 2012 . In sections 3.5 and 3.6, we discuss the relative contributions of physical and biological processes to O 2 supersaturation. Here we note only that ΔO 2 /Ar was positively correlated with Chl a (r = 0.66 and 0.43 along the E-W and WAP transects, respectively) and showed enhancements in frontal zones along the N-S transect. Unlike ΔO 2 /Ar, pCO 2 is sensitive to temperature-dependent solubility changes. During the 30 days prior to our sampling, the NOAA OISST data show an average surface water warming of 1°C along our cruise track. This warming would lead to a 4% (~15 μatm) increase in pCO 2 [Takahashi et al., 2002] , which is small compared to the observed pCO 2 variability along the cruise track. This result indicates that biological uptake exhibited a first-order control on pCO 2 distributions.\\nAs expected, pCO 2 exhibited a strong negative correlation with ΔO 2 /Ar along our cruise track (Pearson's correlation coefficient, r = À0.85 and À0.91 for the E-W and WAP regions, respectively). Figure 4 shows the corresponding relationship between O 2 and total dissolved inorganic carbon (DIC) concentrations derived from pCO 2 and ΔO 2 /Ar data. For both the WAP and E-W regions, the slope of the O 2 :DIC relationship was significantly lower than the expected photosynthetic stoichiometry (photosynthetic quotient, PQ, 1.0-1.4 mol O 2 :mol DIC [Laws, 1991] ). This discrepancy can be explained by the differential rate of sea-air O 2 and CO 2 exchange. Faster air-sea equilibration of O 2 results in a shorter residence time of this gas in the mixed layer, and a more rapid ventilation of photosynthetically derived O 2 . During our cruise, the average residence time of O 2 in the mixed layer was < 1 week, given the mean wind speed (9.2 m s\",\n       'Largely through compliance mechanisms, U.S. soil conservation policy targets highly erodible land (HEL). HEL is defined as land with an erodibility index (EI) of 8 or larger. The erodibility index is, in turn, defined by the ratio of inherent erodibility to the soil loss tolerance. Inherent erodibility for a given soil is the rate of erosion (tons per acre per year) that would occur on land that was continuously clean-tilled throughout the year. The soil loss tolerance is an estimate of the rate of soil erosion that can occur on a given soil without significant long-term productivity loss. Thus, the erodibility index captures both the propensity of a soil to erode and the potential for damage from erosion. Actual soil erosion, however, reflects a complex interaction of climate, topography, soil characteristics, land use, and land management practices. Actual erosion is typically far less than a soil\\'s inherent erodibility due to ground cover (grass, trees, crops, crop residue) and conservation practices (e.g., terraces or windbreaks) installed by farmers and landowners. While soil erosion is difficult to measure under field conditions, physical process models can be used to predict both inherent erodibility and the average annual rate of soil erosion, given climate, topography, soils, land use, and land management. The Universal Soil Loss Equation (USLE; Wischmeier and Smith, 1978) and, more recently, the Revised Universal Soil Loss Equation (RUSLE; see http://www.sedlab.olemiss.edu/rusle/overview.html), and the Wind Erosion Equation (WEE; Skidmore and Woodruff, 1968) have been used widely in conservation planning and program implementation. Because average annual erosion rates can be estimated with and without various conservation practices, the models have greatly facilitated policy implementation. Farmers, working with conservation planners, can use physical process models to develop cost-effective conservation systems. These models are used to implement conservation compliance and other USDA conservation programs. Compliance mechanisms can be viewed in two ways. As a method of policy coordination, they can reduce unintended adverse environmental consequences of farm programs. As an agri-environmental policy tool, compliance can be used to further agri-environmental objectives. In this latter role, compliance mechanisms have properties that set them apart from other agrienvironmental policy instruments-especially subsidies designed to encourage good environmental performance-making them useful in situations where subsidies would be difficult or especially costly to use. Program coordination was a key motivation for adoption of compliance provisions in the 1985 Food Security Act (FSA). In the 1970s and early 1980s, evidence suggested that farm commodity programs encouraged production of relatively erosive crops on erosion-prone land, even as conservation programs attempted to mitigate these damages (Reichelderfer, 1985). High commodity prices of the mid-1970s probably spurred the conversion of highly erodible land from pasture or native grass to crop production-a process commonly referred to as sodbusting-although evidence linking this practice with farm commodity programs is limited (Watts et al., 1983;Heimlich, 1986). Likewise, evidence showing that government payments were an important incentive to swampbusting is quite limited (see Heimlich et al., 1998, for a survey), even though the purpose of most wetland drainage has, historically, been to allow or improve crop production (Dahl, 1990). Even if government payments are not a critical underlying motivation for agricultural production on HEL or wetland, linking payments with compliance requirements can encourage improved environmental performance and deter producers from expanding crop production onto environmentally sensitive land. Withholding payments on the entire farm, rather than only on acres in violation of a compliance requirement, makes the potential sanction quite serious for many farms. Compliance is a unique policy tool that is not easily placed in traditional categories of subsidy, tax, or regulation (Heimlich and Claassen, 1998a). Compliance mechanisms are similar in some ways to both environmental regulation and environmental taxes, but bear little resemblance to environmental subsidies. Like regulation, compliance mechanisms prescribe limits on producer actions and provide for penalties (loss of farm program benefits in the case of noncompliance). Like taxes and fees, however, violation does not imply illegal activity and maximum penalties are limited and known in advance. Unlike an environmental subsidy program (e.g., EQIP 1 ), producers do not receive a benefit in exchange for taking an action that enhances (or is designed to enhance) environmental performance. Instead, they are penalized, through withholding of benefits from otherwise unrelated programs, when an environmental standard is not achieved. One could argue that programs with a compliance requirement actually seek a \"bundle\" of benefits including environmental protection. However, there is no evidence to suggest that commodity program design is influenced by the potential for environmental benefits through compliance. Thus, the economic properties of compliance mechanisms are quite different from those of a classic environmental subsidy program. In general, compliance mechanisms are not subject to some of the problems that can arise with the use of environmental subsidies. For example, poorly designed subsidies for environmental improvement can encourage producers to continue or expand crop production where it would not otherwise be profitable, sometimes undercutting environmental gains (see Claassen et al., 2001, for a full discussion). Moreover, compliance mechanisms do not require subsidy payments in addition to those already in place through price and income support or other programs. 2 Note, however, that the effective level of income support provided to complying producers is reduced by the cost of complying with soil and wetland conservation requirements. These farm commodity programs provide much of the underlying incentive for producers to comply. Compliance mechanisms may be particularly well suited to deter certain environmentally damaging actions. For example, a hypothetical subsidy program designed to prevent wetland drainage would require policymakers to pay for protection of all wetlands on agricultural land-a potentially expensive proposition-or decide which wetlands are sufficiently vulnerable to agricultural conversion as to warrant protection-a potentially difficult task (Heimlich and Claassen, 1998b). In contrast, Swampbuster penalties are assessed when a violation occurs, eliminating the need for broad-based subsidies or the need to anticipate the potential for a violation to occur on any given wetland. No direct costs are imposed on producers, although there may be an opportunity cost associated with production forgone on wetlands that would otherwise have been converted to crop production. The success of compliance mechanisms depends on the commodity programs that provide most of the compliance incentive. Farm commodity programs have been in place for more than 65 years and their benefits have been largely capitalized into the value of farmland (Goodwin et al., 2003;Ryan et al., 2001;Barnard et al., 1997;Duffy et al., 1994). 3 For many producers, the ability to purchase land or pay cash rent depends significantly on government payments. In addition to introducing compliance mechanisms, the 1985 FSA shifted the emphasis of commodity programs from price support to income support. With a market price support program, farmers could benefit from farm programs without direct participation (sometimes referred to as \"free riding\"). With income support payments, producers must participate to receive benefits. Consequently, many producers may feel that they 1 Through EQIP, the Federal Government shares the cost of installing or adopting conservation practices that address key resource concerns. 2 The Government does bear some cost for existing compliance programs. USDA provides conservation planning and technical assistance to producers without charge. Effective monitoring and enforcement by USDA can also be costly. These costs, however, are not specific to compliance mechanisms. They would generally be incurred with the implementation of other types of agri-environmental programs as well. have little choice but to accept compliance requirements, even though, strictly speaking, participation in these programs is voluntary and producers could opt out to avoid compliance requirements. The 1996 Federal Agriculture Improvement and Reform (FAIR) Act ended annual acreage setaside programs, reducing the cost of program participation and increasing the compliance incentive. Although working within the context of existing programs has some advantages over a subsidy mechanism, it also limits the potential effectiveness of compliance mechanisms. Unlike an environmental subsidy program or regulation, the design of compliance mechanisms is, by definition, constrained by the existence and design of other farm programs. In other words, the scope and features of other farm programs largely determine how effective a compliance mechanism can be. In designing a compliance mechanism, policymakers can determine: • the environmental objective(s); • minimum standards of environmental performance or practice implementation; • the programs and payments that are subject to the compliance sanction. But the effectiveness of compliance mechanisms will also depend on other factors related to commodity programs and the agri-environmental problems targeted by compliance. These include: • whether targeted environmental problems occur largely on farms that participate in Federal farm programs subject to compliance; • the producer\\'s net benefit from farm program participation before the compliance requirement; • the producer\\'s cost of meeting the compliance standard or requirement. In other words, the effectiveness of compliance mechanisms-relative to other agri-environmental policy tools-depends largely on the size and spatial distribution of government payments relative to the spatial distribution of targeted agri-environmental problems and the costs involved in mitigating those problems. Given the configuration of current farm programs, compliance mechanisms have the potential to address many cropland-based conservation and environmental problems. Data from the Agricultural Resource Management Survey (ARMS) show that farms receiving some type of government payment accounted for 86 percent of U.S. cropland. Other environmental issues, such as livestock waste management and disposal problems, occur more frequently on farms that do not participate in current farm programs and, thus, are less likely candidates for compliance mechanisms.',\n       'The ADNI protocol acquires two sets of structural data at each visit. These data are rated for image quality and artifacts by ADNI investigators . To enhance standardization across sites and platforms, the best quality data set then undergoes additional preprocessing, including correction for gradient non-linearity (Jovicich et al., 2006) and correction for intensity non-uniformity (Narayana et al., 1988) . In the present study, these optimally pre-processed images were downloaded from the ADNI database and used for subsequent analysis in this study.',\n       'As farms continue to grow, capital needs increase, risk management becomes increasingly important, and technology adoption-particularly of labor-saving technologies-has a greater influence on competitive advantage. Because large farms often have multiple operators and generations, they are more likely to have individuals with the pertinent skills in key areas (for example, financial management, risk management, and technology adoption) and to assign point people to these key areas.',\n       \"Mobility is a vital element in any population and labor force. How willingly families will migrate from one location to another, and how easily workers are able to respond to market signals and make the transition from one opportunity to another or to turn a disappointment in issues are also of particular current interest to individual states across the country, which has implications for allocations for higher education and other public expenditures, as well as other legislative initiatives. Governors, state legislators and others express concern over the loss of their well-educated citizens to other states, or whether they are providing substantial subventions to out-of-state students who then return home or move elsewhere after they complete their undergraduate or graduate/professional studies. Support for higher education is occasionally couched in such terms and calculations. This special section focuses on the extent and pattern of interstate migration of doctorate recipients from United States universities from birth through initial postgraduate employment.' 3 This is arguably one of the highest skilled segments of the American labor force, and one for which personal and professional decisions to migrate is of great importance.\",\n       'Aptamer-NPs conjugates have been studied extensively in delivering an effective dosage of drugs to the targeted active sites for enhanced therapeutic effects. Biocompatible polymers with tuneable properties are often conjugated with aptamer-mediated delivery formulation as aptamer-conjugated biopolymeric micro/nanoparticles to overcome the limitations of aptamers whilst enhancing their targeting capabilities. The effectiveness of ARC1779 PEGylated DNA aptamer was reported in impeding thromboembolism by targeting and blocking the activity of von Willebrand factor in order to inhibit both platelet activation and aggregation. 49 An EpCAM aptamer-conjugated curcumin-loaded poly(lactic-co-glycolic acid)(PLGA)-lecithin-Polyethylene Glycol (PEG) NP demonstrated high efficacies in targeted delivery of encapsulated drugs to the targeted HT29 colon cancer cells for colorectal adenocarcinoma treatment. Both the encapsulation efficiency and in vivo bioavailability of encapsulated curcumin were significantly improved as compared to the unformulated free curcumin. 49 Another study also showed the efficacy of RNA aptamerconjugated PEG-PLGA polymeric NPs in the loading and delivery of doxorubicin drug molecules to the targeted MCF-7 breast tumour cells with higher drug encapsulation efficiency, cellular internalization and cytotoxicity for breast cancer treatment. 6 These aptamer-navigated NPs were shown to offer better targeting affinity, prolong circulation half-life of encapsulated drug molecules and enhance the rate of internalization and cancer-killing effects.',\n       'As mentioned previously, the chapters that follow use a variety of statistical methods, reflecting the study\\'s purposefully multi-model approach. The analytic strategies are discussed briefly here, and are addressed in more detail in the appropriate chapters. Unless mentioned specifically, all analyses employ the procedure for generating jackknifed standard errors and (when considering student outcomes) the calculation and combination of all plausible values, as outlined in the various TIMSS technical reports (see https://timssandpirls.bc.edu/isc/publications.html) and the IEA Database Analyzer (see https://www.iea.nl/data). Chapter 4 presents international averages of country-level means (including participating sub-units) for the predictors used throughout the remaining chapters, including teacher experience, teacher preparedness, teacher education (preparation to teach mathematics), teacher time spent on mathematics, and teacher curricular alignment. Variable means are calculated for each educational system participating in each cycle of TIMSS, and confidence intervals are used to identify statistically significant differences for the same country in different iterations of TIMSS. Chapter 5 presents a number of statistical methods that we used to examine the relationship between teacher characteristics and behaviors and student outcomes. We used a multi-model approach to test the degree to which relationships remained robust across countries, within countries, and across different periods, and we assessed the stability of estimates using different statistical techniques. As a first step, we compared the results for (1) single-level linear regressions (ignoring classroom-level clusters) of individual student outcomes as predicted by teacher variables; (2) two-level linear regression models that cluster students within classrooms using SAS PROC MIXED maximum likelihood statistical software (Singer 1998); and (3) a single-level linear regression of classroom means of student variables with teacher variables. The comparison of these analyses enables the frequency of statistically significant associations between purported measures of teacher effectiveness and student achievement in mathematics to be determined. Additional analyses in Chap. 5 include an examination of the stability of multilevel model regression coefficients within each country across the different cycles of TIMSS, and an exploratory fixed effects regression analysis of changes in country-level means. Whereas the models in Chap. 5 treat each of the teacher-level variables as independent variables, Chap. 6 introduces a model that uses teacher behaviors (time on mathematics and alignment with national curriculum standards) as mediating variables for instructional quality. We used a structural equation modeling (SEM) approach to analyzing the results for each country in a single cycle of TIMSS (namely 2011), permitting the inclusion of additional variables like teacher professional development to estimate a latent construct of \"instructional quality.\" A multilevel model clustering students within classrooms using jackknifed standard errors and five plausible values was applied to each educational system. Comparison of the results in Chaps. 5 and 6 demonstrates that a wide range of statistical techniques can be used to assess whether there are temporally and cross-nationally robust associations between measures of teacher effectiveness and student achievement in mathematics. Chapter 7 departs from the focus on mean student outcomes to consider the distributional effects of teacher effectiveness. First within-country equity was measured by standard deviation of pooled country-level student achievement. Country-level fixed effect analysis was used to assess the relationship between teacher effectiveness measures and student variation (measured by standard deviations). Second, the relationship between within-classroom variation in student outcomes and teacher quality was analyzed using averaged classroom-level single-level linear regressions. Finally, differences in teacher effectiveness between higher (top quartile) and low (bottom quartile) SES classrooms, as measured by using the average number of books in the home as a proxy for SES, were examined using Welch\\'s t-tests (which are not sensitive to sample size). This last analysis used an alternative to the jackknifed standard error approach (designed for the entire sample of classrooms) because it examined a sub-sample that is vulnerable to Type I error.',\n       \"ABSTRACT: This study demonstrates how sea-level rise increases the vulnerability of coastal communities to flooding associated with coastal storms. The case study applies a GIS-based methodology to assess the vulnerability of Cape May County, New Jersey, to flood hazards caused by both riverine flooding and coastal storm surges. For storm events of differing intensities, it first identifies areas that will be inundated and how they will change with projected sea-level rise. It then assesses the social vulnerability of the county, taking into account factors such as age, gender, race, income and housing conditions. Finally, it combines physical and social vulnerabilities to create a picture of the county's present overall vulnerability, as well as how this will change with projected sea-level rise. To account for uncertainties in projections, possible ranges of both population growth and sea-level rise are incorporated in low, medium and high scenarios. The results show that sea-level rise will increase the vulnerability of the county to flood hazards considerably by increasing the areas that are exposed to the highest flood risk, hence increasing the number of critical facilities, properties, and people to the risk of flooding. Comparing the upper-and lower-bound scenarios indicates that poorly managed development could increase the county's vulnerability to flooding. These results suggest that decision-makers could reduce vulnerability by making choices that steer development away from highrisk areas. \",\n       '• Split application of nitrogen fertilizer (applying at least part of the total nitrogen after planting when crop needs are highest and risk of runoff is lower) accounted for 59 percent of nitrogen applied to cotton in 2007 (64 percent of acres). In 2010, corn farmers applied 22 percent of nitrogen fertilizer (on 31 percent of acres) after planting. • Farmer-reported nitrogen rates are higher than benchmark application rates (based on estimated plant uptake and designed to minimize nitrogen losses to the environment) for 36 percent of corn acres, 19 percent of cotton acres, 22 percent of spring wheat acres, and 25 percent of winter wheat acres. • Using multiple nutrient-management practices has greater potential to reduce the loss of nitrogen than using a single practice. Only 24 percent of cotton acres and 6 percent of corn acres combined four nutrientmanagement practices: (1) no application in the fall, (2) some application after planting, (3) nitrogen application at rates below a \"benchmark,\" and (4) fertilizers incorporated or injected below the soil surface. • Cover crops were in use on less than 2 percent of total cropland (for all crops) during 2010-11 (6.8 million acres), with adoption rates higher in some regions (e.g., the Southern Seaboard and the Mississippi Portal). Although the benefits of cover crops and no-till/strip-till are enhanced when these practices are used on the same fields, the low cover crop adoption rate suggests that these benefits are realized on few acres.',\n       'Primary sources of financial support for doctorate recipients, 1998',\n       'Most research on recruiting women into STEM fields has focused on the baccalaureate level and above (Hill, Corbette, and St. Rose 2010). An exception is research summarized in the report, Recruiting Women into STEM Fields: Another Look (Cossette et al. 2010). Based on a survey, site visits, and focus groups with NSF ATE grantees, the authors identify the following best practices used by community colleges for recruiting women to STEM programs. First, successful recruitment requires building relationships with prospective female students through activities such as peer mentoring programs, workshops \"…offering information about financial aid, child care, and other supports is important for recruiting prospective female students, as is the availability of internship and apprenticeship opportunities in STEM programs.\" \"Recruitment efforts are more likely to be successful if advertising is supplemented by personalized outreach.\" \"For low-income women and student parents, locating financial support and accessible, affordable child care can make the difference between staying on track and in school-or stepping away from college to work more hours in order to make ends meet.\" led by women faculty and scientists, and visits to STEM programs at community colleges that promote active participation and discussion. Second, offering information about financial aid, child care, and other supports is important for recruiting prospective female students, as is the availability of internship and apprenticeship opportunities in STEM programs. Cossette et al. (2010) note that for women who are out of high school with jobs and family responsibilities, the availability of financial aid, child care, and other supports can be especially compelling. Third, community colleges need to reach out to women who are already enrolled and taking courses at community colleges. Career and academic counseling can play a critical role in presenting STEM opportunities to women while they are still exploring majors during their first year of college. Finally, recruitment efforts targeted to women (and other underrepresented groups) in STEM fields should combine several strategies that reinforce each other. Circulating a brochure that pictures a diverse group of women in STEM fields may not be sufficient. Recruitment efforts are more likely to be successful if advertising is supplemented by personalized outreach; information about financial aid, child care, and other supports; mentoring; interactive workshops; and intensive visits to local programs (Cosette et al. 2010). Several of the programs summarized in Appendix 1 use innovative strategies to recruit women to STEM programs. The CalWomenTech Extension Services Project created customized outreach tools including posters, brochures, flyers and a website for each of the eight participating community colleges. These materials combined a focus on role models (female graduates from the technology program of interest) with program and labor market information and links to related female professional associations. The Grace Hopper Program offers financial incentives to enter computer science and related fields that include reimbursement for courses and NSF-funded scholarships. The South Carolina Advanced Technology Education Center places advertisements in women\\'s magazines and billboards with the message \"Women-do you want to make more money?\" and features a brochure on its website entitled Choose Engineering Technology for a High-Tech, High-Wage Job with a Future. And the Regional Center for New Generation Manufacturing targets women and other underrepresented students through statewide and regional expos, scholarship opportunities, and dual enrollment programs that encourage high school students to enroll in community college and jumpstart their education in engineering and technology.',\n       'In this prospective study, men who had ever used aspirin or nonaspirin NSAIDs had a modest nonstatistically significantly lower risk of prostate cancer. The inverse association for ever use of nonaspirin NSAIDs was statistically significant in younger men. No clear differences in the patterns of association were found by serum PSA closest in time to prostate cancer diagnosis or Gleason sum. A possible modest inverse association with prostate cancer was observed for current use of acetaminophen, an analgesic which until recently was not thought to inhibit the COX-1 and COX-2 enzymes at typically used doses. Mean PSA concentration measured in archived sera collected at multiple time points from men who had never had a diagnosis of prostate cancer did not differ between current users and nonusers of aspirin or nonaspirin NSAIDs. Thus, it is unlikely that the suggestion of an inverse association of use of aspirin and nonaspirin NSAIDs with prostate cancer that we observed is due to a difference between users and nonusers in the accuracy of detection of prostate cancer by screening for elevated PSA.\\nOur nonstatistically significant finding for aspirin use and prostate cancer is consistent with the modest magnitude of association observed in three previously published prospective studies (20, 22, 31) . The RR for death from urogenital cancers, the majority of which were likely prostate cancer, in the prospective Cancer Prevention Study was 0.82 (95% CI, 0.56-1.19) among those with baseline aspirin use of >16 times per month for at least a year compared with nonuse (31) . Within a prospective study conducted among Kaiser Permanente members, the RR of prostate cancer was 0.76 (95% CI, 0.60-0.98) comparing those who used took more than six aspirin tablets almost every day with those who did not (22) . Using the casecontrol design nested within the General Practice Research Database in the United Kingdom, the odds ratio (OR) for current use of aspirin prescribed by a physician was 0.70 (95% CI, 0.61-0.79; ref. 20) . In a fourth prospective analysis conducting in the Health Professionals Follow-up Study, no association was observed for regular, consistent, or frequent aspirin use and prostate cancer, although frequent use of aspirin (z22 days per month) was associated with a nonstatistically significant 27% lower risk of metastatic or fatal prostate cancer (23) . In the BLSA, too few cases were diagnosed at advanced stage since (32) or use in the past 30 days versus none (33) . Our findings are also consistent with some of the case-control studies, which reported estimates of roughly 0.85 for aspirin and total prostate cancer (1, 26, 27) .\\nWe observed a modest and nonstatistically significant inverse association of ever, but not current use of nonaspirin NSAIDs with prostate cancer. One prospective study that investigated the relation of total NSAIDS, which included both aspirin and nonaspirin NSAIDs, with prostate cancer reported a strong inverse association (OR, 0.45; 95% CI, 0.28-0.73) for daily use (21) . A similar strong effect was observed by Nelson and Harris (24) (26) observed an OR of prostate cancer of 0.84 for use of nonaspirin NSAIDs in the past 5 years in a case-control study of men undergoing prostate biopsy. However, the only other prospective study that evaluated nonaspirin NSAIDs (prescribed by a physician; ref. 20) and a very large case-control study (27) did not observe an association between use or duration of use of nonaspirin NSAIDs and prostate cancer.\\nAspirin inhibits the COX-1 enzyme and, to a lesser extent, the COX-2 enzyme (34, 35) . Ibuprofen and related nonaspirin NSAIDs inhibit both COX-1 and COX-2, whereas selective COX-2 inhibitors have relatively little effect on COX-1. Because the inhibition of COX enzymes by aspirin and nonaspirin NSAIDs is transient, the former by irreversible covalent binding and the latter by competitive inhibition, we investigated the relative importance of ever use, current use, and duration of use. The association for ever use of aspirin and nonaspirin NSAIDs seemed to be slightly stronger than for current use. Shorter duration of use of either aspirin or nonaspirin NSAIDs was statistically significantly inversely associated with prostate cancer, whereas longer duration of use was associated with a nonstatistically significant higher risk of prostate cancer. Further study is needed to determine whether this finding was due to chance, bias (e.g., differences in the extent of unaccounted for confounding or other sources of bias in the findings for shorter versus longer duration of use; misclassification of duration of use of anti-inflammatory medications), or biology (e.g., a greater reduction in and resolution of inflammation by use of shorter-term, but higher dose anti-inflammatory agents; or whether longer-term users of anti-inflammatory drugs are those individuals who are more susceptible to inflammation in general, and thus their use appears as a marker for prostate cancer risk). The inverse association with prostate cancer for ever use of nonaspirin NSAIDs, and possibly ever and current use of aspirin, was suggestively more pronounced in younger than in older men. In contrast to our findings, Roberts et al. (21) in a prospective study observed the inverse association between total NSAIDs and prostate cancer was strengthened as age increased. More work is needed to understand possible influence of age, if any, on the association of use of aspirin and nonaspirin NSAIDs with prostate cancer.\\nTo investigate the specificity of the association for drugs taken to alleviate inflammation and pain, we also evaluated the association of acetaminophen, which until recently was not thought to inhibit the COX-1 or COX-2 enzymes at commonly used doses and therefore, was not thought to exhibit an antiinflammatory effect, with prostate cancer. We observed a modest and nonstatistically significant inverse association for current use of acetaminophen. Two other groups have investigated the association with prostate cancer for acetaminophen. A nested case-control study reported no association for current use of acetaminophen (paracetamol) prescribed by a physician, but did note an inverse association with duration of acetaminophen use (comparing >4 years to no use: OR, 0.50; 95% CI, 0.38-0.65, P trend = 0.02; ref. 20) . A nonstatistically significant inverse association for any acetaminophen use (combining <1 tablet per day and z1 tablet per day versus nonusers: OR, 0.70; 95% CI, 0.28-1.73) was reported in a U.S. case-control study (24) . Whether these findings reflect chance, bias, residual confounding by concurrent use of aspirin or nonaspirin NSAIDs or cause is unknown; more work is needed, including on the targets of acetaminophen. For example, recent studies suggest that acetaminophen at 1 gram doses may partially inhibit COX-1 (in platelets) and COX-2 (in monocytes; ref. 36) and that the action of acetaminophen on pain relief may be mediated by its effects on COX-3, a variant of COX-1 that is expressed in the brain (37) . Because of the relatively infrequent use and the low to moderate doses taken, in the present study, we were unable to address whether acetaminophen used continuously at doses of z1 g/d is associated with prostate cancer.\\nIn the PSA era, detection of prostate cancer is largely dependent on screening for elevated serum PSA. Intraprostatic inflammation seems to be common in the prostates of older men. The epithelial damage caused by inflammation coupled with the increased vascular permeability that occurs during inflammation may be one mechanism by which PSA enters the circulation, independent of the presence of cancer. Given that the majority of the prostate cancer cases included in this analysis were diagnosed in the PSA era, we were concerned that differential detection of prostate cancer might have resulted between men who used aspirin and nonaspirin NSAIDs, and who thus might have had reduced intraprostatic inflammation, and those who did not use anti-inflammatory drugs. To address this question, we used repeated measures of serum PSA concentration over time and considered aspirin and nonaspirin NSAIDs use that was concurrent with the time of PSA measurement. However, we did not observe evidence for a lower serum PSA concentration in users of aspirin and nonaspirin NSAIDs. Thus, it is unlikely that the modest inverse association between these anti-inflammatory drugs and prostate cancer is due to reduced sensitivity of prostate cancer detection by use of these drugs. We did observe a tendency for men who currently used acetaminophen to have slightly lower serum PSA concentrations than nonusers, and the possible detection bias that might result could explain, in part, the inverse association for current use acetaminophen and prostate cancer in this cohort. How acetaminophen might influence serum PSA is unknown.\\nThe BLSA is a rich resource for research on prostate cancer and other diseases and conditions because of the ongoing collection of repeated exposure, urologic, and other health measures on participants over the long term. The overall and age-specific prostate cancer incidence rates for the BLSA men included in this analysis were higher than for the United States, which is likely due to the intensive screening the men have undergone since 1991 as part of the BLSA protocol. The greatest elevation in the incidence rate above the U.S. rate was among the oldest men, who in the community setting are less likely to be screened. For this analysis, the small number of prostate cancer cases diagnosed among the BLSA men limited our ability to detect small associations as being different from the null hypothesis. The possible attenuating bias that would result from enhanced medical contact and possibly greater cancer screening in those taking aspirin or NSAIDs for diseases that require more frequent medical contact is unlikely in this study because of the BLSA screening protocol. Furthermore, we showed that among men without prostate cancer, PSA concentration did not differ between users and nonusers of aspirin and nonaspirin NSAIDs and thus, detection bias is unlikely to account for the modest inverse association of aspirin and NSAIDs use with and prostate cancer. Because of the systematic prostate cancer screening in the BLSA and because members of this cohort exhibit healthseeking behaviors, the majority of the prostate cancers with known stage and that were included in the primary analysis were early-stage disease. Thus, we cannot evaluate whether the association for aspirin and nonaspirin NSAIDs is more apparent for disease that was advanced at diagnosis. In the analysis we took into account age and calendar year as potentially confounding factors, race and use of vitamin E or calcium supplements were not confounders. Few suspected risk factors for prostate cancer have been identified and most of these are associated with advanced disease, not early disease, which comprised the majority of cases included in this study. Furthermore, we ran the analysis excluding men who never during the period of follow-up reported using any of the three types of analgesics; these men might differ in their characteristics from users based on their propensity to not use analgesics. The findings were similar to overall, with the possible exception of a slightly stronger inverse association for nonaspirin NSAIDs, suggesting that confounding by unmeasured correlates of anti-inflammatory drugs use that are also associated with prostate cancer do not explain the modest inverse associations that we observed.\\nThe current analysis was conducted in a cohort of men with a high prevalence of use of aspirin and moderate prevalence of use of NSAIDs during the period of follow-up. At visits since 1990, the time during which a more detailed assessment of medications usage was done in the BLSA, the majority of current aspirin users took one dose of a standard over-the-counter 325 mg aspirin tablet regularly and many of the current nonaspirin NSAIDs users regularly or occasionally took one or more doses, with the most common pattern of use being a standard over-the-counter 200 mg tablet of ibuprofen. A strength of this analysis was the recurrent updating of medication records on all participants over a >20-year period and thus, reducing exposure measurement error.\\nBecause information on frequency and dose of medications used was not systematically collected in the BLSA in the 1980s, as a surrogate for duration of use we calculated the number of years of use as the number of years encompassed by visits during which the men reported use. The extent of error in the measurement of duration of use is unlikely to have differed by whether a man was subsequently diagnosed with prostate cancer. Thus, measurement error would have tended to attenuate, not enhance the association between duration of use and prostate cancer.\\nIn conclusion, in this prospective study, we observed a modest, nonstatistically significant inverse association between use of aspirin and nonaspirin NSAIDs and the subsequent diagnosis of prostate cancer, which was statistically significant for ever use of nonaspirin NSAIDs in younger men. Use of acetaminophen, which is not believed to have anti-inflammatory effects, was not consistently inversely associated with prostate cancer risk. Based on our evaluation of serum PSA concentration among men without a diagnosis of prostate cancer, our study is unlikely to have been biased by differential ascertainment of prostate cancer that might have resulted if aspirin and nonaspirin NSAIDs had influenced serum PSA concentration. Our findings add to the literature suggesting a modest benefit of anti-inflammatory drugs in relation to prostate cancer.',\n       'It is also important to look at comparability with the PCE of estimates from the Interview survey and Diary survey separately. Gieseman (1987) reported separate comparisons of the Interview survey and Diary survey estimates to PCE estimates for food because these were the only estimates available from both surveys. 3 He found that Interview food at home exceeded Diary food at home by 10 to 20 percentage points, but was still below the PCE. For what was then a much smaller category, food away from home, the Diary aggregate exceeded the Interview aggregate by about 20 percentage points. Again, the CE numbers were considerably lower than the PCE ones. It is not surprising that the Interview and Diary surveys yield different estimates, given the different approaches to data collection, including a different form of interaction with the respondent household. These differences provide the likelihood of differences in estimates between the two surveys as currently configured, as discussed in more detail later in this chapter. Bee, Meyer, and Sullivan (2012) looked further at comparing the estimates from both surveys separately to the PCE. The authors examined estimates for 46 expenditure categories for the period 1986-2010 that are comparable to the PCE for one or both of the CE surveys. Table 5-1 shows the 10 largest expenditure categories for which these separate comparisons can be made, showing ratios of the CE to PCE for these categories. Among these categories, six (imputed rent on owner-occupied nonfarm housing, rent and utilities, food at home, gasoline and other energy goods, communication, and new motor vehicles) are reported on the CE Interview survey at a high rate (relative to the PCE) and have been roughly constant over time. These six are all among the eight overall largest expenditure categories. In 2010, the ratio of CE to PCE exceeded 0.94 for imputed rent, rent and utilities, and new motor vehicles. It exceeded 0.80 for food at home and communication and is just below this number for gasoline and other energy goods. In contrast, no large category of expenditures was reported at a high rate (relative to the PCE) in the Diary survey that was also higher than the equivalent rate calculated from the Interview survey. Reporting of rent and utilities is about 15 percentage points higher in the Interview survey than the Diary survey. Food at home is about 20 percentage points higher in the Interview survey. 4 Gasoline and other energy goods are about 5 percentage points higher in the Interview survey and communication is about 10 percentage points higher. The 2010 ratios for food away from home and furniture and furnishings are close to a half for both the Interview and Diary surveys. For clothing and alcohol, the Diary survey ratios are below 0.50, but the Interview survey ratios are even below those for the Diary survey. The panel next looked at smaller expenditure categories that are comparable between the PCE and the CE. Of the 36 such categories, only six in the Interview and five in the Diary have a ratio of at least 0.80 in 2010. In the Diary survey household cleaning products and cable and satellite television and radio services were reported with a high rate (comparable to the PCE). Household cleaning products had a ratio (relative to the PCE) of 1.15 in 2010 in the Diary survey; the ratio has not declined appreciably in the past 20 years. The largest of these categories reported with a high rate (comparable to the PCE) in the Interview survey were motor vehicle accessories and parts, household maintenance, and cable and satellite television and radio services. The remaining categories were reported at low 4 There is some disagreement about how to interpret the fact that food at home from the CE Interview survey compares more favorably to PCE numbers than does food at home from the CE Diary survey. Some have argued that the CE Interview survey numbers may include nonfood items purchased at a grocery store. Battistin (2003) argued that the higher reporting of food at home for the recall questions in the Interview component is due to overreporting, but Browning, Crossley, and Weber (2003)  rates (compared to the PCE) in both surveys with ratios below one-half. These include glassware, tableware, and household utensils and sporting equipment. Gambling and alcohol had especially low ratios, below 0.20 and 0.33, respectively, in both surveys in most years.',\n       'Weather conditions have been associated with outbreaks of WNV and are related to vector production. For example, flooding of the Moravia River in 1997 caused massive broods of mosquitoes to hatch in association with an outbreak of WNV in the Czech Republic (Hubalek et al. 2000). Warm dry weather after flood conditions was associated with the formation of suitable breeding habitats for mosquitoes in Bucharest (Hubalek 2000). Further, it was found that drought conditions result in a concentration of birds and mosquito vectors near limited water resources in a small area (Shaman et al. 2002, Brinton 2002. Dispersal of avian hosts and mosquito vectors occur when the drought ends, initiating the early transmission phase of the viral cycle (Shaman et al. 2002). The lack of heavy rain to flush sewer systems results in increased mosquito populations in cities (Brinton 2002). Higher temperatures decrease larval development time, leading to the production of a greater number of mosquito vectors in a shorter time period (Alto and Juliano 2001). Research on SLE found that higher temperatures and lack of rain during the summer months resulted in sufficient habitat for Culex pipiens to breed (Monath 1980b). Although it is important to recognize the connection between weather conditions and WNV, there is also a need to study and identify human interactions with the virus.',\n       'Our previously published work suggested that men in the BLSA have a higher prevalence of the MS than women (2).\\nIn this current cohort of BLSA participants, the prevalence of the MS was significantly higher in middle-aged and older men as compared with that in younger men. Whereas there have been other published studies that have examined relationships between androgen levels and the MS (prevalence and incidence) (3, 4, 14 -16) , to date, none has reported increased prevalence of the MS across the broad age spectrum as shown in our cohort. Nonetheless, given the fact that many of the determinants of the MS (adiposity, glucose intolerance, and hypertension) are associated with aging, it is not too surprising that the MS is more prevalent in middle-aged and older men.\\nThe cross-sectional association among aging, androgen levels, and the MS showed that total T and SHBG levels were significantly lower in men with the MS, compared with men without the MS. Specifically, total T levels decreased with aging, whereas SHBG levels increased with aging. In subjects with the MS, these trends were also apparent; however, the levels of total T and SHBG were lower, compared with men without the MS. However, DHEAS levels and FTI were not significantly different in men with and without the MS (Fig.  2, C and D) . Our results are consistent with those reported by others (17, 18) in that the association of DHEAS on cardiovascular risk factors is inconsistent.\\nThe limitation of the cross-sectional analysis is its inability to assign causality or detect cohort or secular trends. Does the MS decrease androgen levels or do lower androgen levels increase the prevalence of the MS? We attempted to answer this question by examining the relationships of androgen and SHBG levels to development (incidence) of the MS in men without the MS or DM at baseline as classified by ATPIII. We found the incidence of the MS to be associated with increased BMI but not with age, whereas age-adjusted total T and SHBG levels were negatively associated with incident MS. Further adjustment for BMI attenuated the protective effects of total T and SHBG, although levels remained significant. (Table 3) . Therefore, low baseline androgen levels appear to predict incident MS. Our results are consistent with those reported by Laaksonen et al. (4) , in which these investigators also used the ATPIII criteria for classification of the MS. They found that men with total T, SHBG, or calculated free T in the lowest quartile had a significantly higher risk for development of the MS, compared with the remaining men (4). They also found that further adjustment for confounders, such as smoking, alcohol intake, socioeconomic status, cardiovascular disease, and exercise, did not diminish the association between baseline androgen levels and incident MS. However, BMI did attenuate the association, a finding we also observed. In the study by Laaksonen et al. (4) , incident DM was also significantly increased in men with total T and SHBG in the lowest quartile. We did not see an association between total T levels and incident DM, but there was a significant association of SHBG with development of DM.\\nKupelian et al. (16) also recently reported the relationship of androgens to the development of the MS in an age cohort of 40-to 70-yr-old men. Again, total T and SHBG were significantly related to the MS incidence, but FTI was unrelated. This prospective study is limited by the absence of fasting blood values for glucose and triglycerides at entry; thus, some subjects undoubtedly were already positive for MS at entry.\\nThe limitations of our study warrant further discussion. Our population consists of middle-and upper-class Caucasians, who on average were close to the upper limit of desirable body weight (although, as expected, subjects with the MS were overweight and closer to being defined as obese). Androgen levels were obtained at a single visit, which likely affected risk assessments for incident MS and DM and risks for coronary heart disease and mortality. We detected a significant positive relationship between FTI and the MS, as has been reported in premenopausal women with polycystic ovarian syndrome (19) and younger postmenopausal women (20) . In all these groups of women, increased (not decreased) free T is associated with increased total and abdominal fat, insulin resistance, and the MS. Whether increased free T in older men promotes augmented fat, insulin resistance, and the MS remains to be determined.\\nIn conclusion, lower baseline androgen levels predict incident MS.',\n       'Math Class Type was taken directly from BYS67a, BYS67b and BYS67c. The following categories were used: Adv. Math/Algebra (includes those reporting taking algebra only, other enriched math, or algebra and regular math).',\n       'whether data items not provided by a respondent in a survey but available on some administrative record for that same person should be substituted directly into the statistical record and subsequently analyzed and released. I thought not, Roger thought so, and he was optimistic that it would happen. I had to be constantly vigilant to keep him from getting it sneaked in somewhere. Scores of times I have heard Roger drawl out in meetings, \"I probably don\\'t know all the details and complications here, but I don\\'t understand how that could take so long (or be so complicated). It seems like all you have to do is...\" Boy, a lot of people didn\\'t like to hear that! 2. Herriot was unwilling to choose between two good things when there appeared to be insufficient time or resources to do both, or when the two things seem technically inconsistent. Instead, he refused to consider either/or, and displayed a remarkable talent for finding a way to do the essential parts of both, or to do a third and different thing that accomplished the first two plus more. An example: Consider, for example, the continuing conflict between the expanded provision of survey microdata to usersespecially survey data linked to related administrative records--on the one hand, and the protection of respondents\\' confidentiality and privacy, on the other hand. This conflict arose 40, maybe 50 specific times in Roger\\'s career, but he refused to view it as a conflict! He proposed and implemented solution after solution in case after casesome solutions statistical, some technical, some bureaucratic, some legal2. This reached its zenith at the National Center for Education Statistics where Herriot developed protocols of informed consent, data collection, data matching, record formatting and user contracts that, in my estimation, just barely crawled through the available space between all the constraints.',\n       'Though compelling based on the scholarship cited above, emerging evidence suggests that the narrative of women being better leaders during the pandemic demands closer empirical interrogation. First, consider Economist Intelligence Unit rankings of OECD countries by the quality of their Covid-19 responses, where higher values indicate better policy responses [30] . We graph these according to the gender of the national executive (Fig 1) . The graph certainly suggests that women-led countries are over-represented among those with the highest rankings, and consistent with early research on this topic [6] . What this ranking (and research) do not capture, however, is the full global landscape of countries. Rather, they reinforce Western biases and perceptions of gender and leadership responses to the pandemic, ignoring the responses of non-OECD countries entirely. A country such as Vietnam, for example, a manled country with 97 million people and land border with China that to date has barely 1000 Covid-19 cases and fewer than 40 deaths [31] , fails to enter the picture.\\nBy focusing only on OECD countries where democracies are over-represented, we also fail to consider pandemic management across varied regime types. While democracies and highly developed countries tend to over-provide public goods, dictatorships may have an https://doi.org/10.1371/journal.pone.0244531.g001 \"authoritarian advantage\" in implementing comprehensive pandemic policies that limit the social, political, medical, and economic impacts of the virus. Combined with lower levels of economic development, the authoritarian advantage becomes particularly beneficial pandemic-wise in agrarian societies where people already live farther apart than they would in an urban/industrial context [32, 33] .\\nKey to our argument, the type of regime and governance system may influence the leader\\'s and country\\'s overall response. Freedom House classifies countries along three categories: Free; Partly Free; and Not Free [34] . Freedom House provides annual country-level ratings related to political rights and civil liberties such as respect for the rule of law, political pluralism, the functioning of government, and electoral processes in countries and territories throughout the world. Democracies-free countries-have an incentive to provide public goods that buffer against pandemics and other natural disasters, given that leaders whose policies fail their constituents during times of crisis face almost certain removal come re-election time [34, 35] . Leaders in authoritarian countries, on the other hand, also behave differently with respect representation, given their means for remaining in office differ from those in democracies. Democracies (free countries) also tend to produce more women chief executives than non-democracies (not free countries), as well. If authoritarian (not free) regimes are more likely to manage pandemics according to different incentives, and to promote women in leadership positions for reasons different from free countries, we must include them in any empirical analysis of gender, leadership, and pandemic outcomes.\\nThe importance of preparation. Second, critical to this debate is the fact that disaster outcomes are not merely dependent on leadership now, but also on preparation that took place previously [36, 37] . Strong disaster preparedness systems would have protocols in place for confronting threats such as a pandemic that today\\'s leaders need only activate, adapt, and implement, meaning that women leaders today will be better placed to manage pandemics if they can rely on preparedness and policies made by leaders in the past. Consider another perspective from the disaster preparedness framework. To gauge general preparedness we calculate standardized average disaster preparedness scores according to the Hyogo Framework for Action [38] , and graph this by current leader gender as shown in Fig 2. This snapshot of pandemic management and leadership gender does suggest that women-led countries have higher average preparedness ratings than men-led countries.\\nA surface-level interpretation of the graph might suggest that preparedness levels depend on leader gender, and that women lead countries that are more prepared because of the woman\\'s leadership. But the timing of disaster preparation as, by definition, occurring before a critical event like a pandemic suggests that the relationship between disaster preparedness and leader gender is more nuanced-that in fact, some countries are more likely both to be prepared for disaster and also to elect women to leadership in government. We posit that the common factor among these countries is cultural.\\nFeminine social cultural norms. Third, cultural norms, beliefs, and behaviors can be classified according to similar attributes across countries. Hofstede [10, 11] categorizes 91 countries according to 6 specific dimensions: power distance, uncertainty avoidance, individualism versus collectivism, masculinity versus femininity, long-term versus short-term normative orientation, and indulgence versus restraint. The literature about these cultural features suggests that women-led countries should have overall better responses to the COVID-19 pandemic because they align with policies that advance communal well-being. From a theoretical perspective, people in countries with women chief executives and greater women\\'s representation in the legislature may have better social and physical outcomes-fewer deaths-during a pandemic both because of the emphasis on baseline preparation as a function of their caregiver role expectations, and also because they can demonstrate masculine leadership by acting decisively to close borders and implement other emergency executive measures unilaterally. In other words, women can both react compassionately by asking their constituents to behave in accordance with promoting the common good, and also aggressively by closing down borders. These actions fulfill both gendered traits, and work in the favor of women leaders.\\nThe cultural factor most relevant to our inquiry is the masculinity versus femininity dimension [39] . Countries that elevate feminine traits prioritize having minimal role differentiation between genders, encourage sympathy for the weak, and elect women to multiple political positions [10, 40] . Countries that prioritize feminine cultural norms should also have less to reconcile with regards to the political double bind [41] . To that end, if a woman currently leads a country that embodies feminine cultural norms, then by definition there is societywide support for policies that would benefit the public good. In turn, the woman leader should have more flexibility in the policies she can enact, which becomes especially relevant in managing a pandemic. In more feminine societies, women should be rewarded for preventive and interventive policies that elevate their country\\'s baseline resilience from pandemics, limit the immediate pandemic-related damage, and mitigate against long-term negative consequences. Other cultural features align with feminine social norms. These include less power distance (more egalitarianism), less uncertainty avoidance, more collectivism, longer-term orientation, and more indulgence (e.g., basic needs satisfied; self-fulfillment needs are met) [42] .\\nIn Fig 3 we graph Hofstede\\'s 6 cultural dimensions for 175 countries according to whether the country is led by a man or a woman, defining a country as \"woman-led\" if a woman holds executive authority in that she commands a military, which we provide subsequently in Table 3 . We have standardized these features at the mean (0), so that the height of a bar represents distance from the mean. Bars that extend above the mean show higher than average values for a particular dimension, and bars that extend below the mean show lower than average values for a dimension.\\nAcross each of these cultural dimensions, countries led by men and women show descriptive differences. For the indulgence/restraint dimension (the dark blue bars), women-led countries exhibit more indulgence than the average, and men-led countries exhibit less. Similarly, women-led countries exhibit higher-than-average long-term orientation and individualism (brown and sage bars), while men-led countries exhibit lower-than-average values of the same. Meanwhile, women-led countries are lower than the average in terms of tendency to avoid uncertainty (green), masculinity (yellow), and power distance (red), while men-led countries are the opposite. Across all aspects, women-led countries exhibit larger distances from the mean than men-led countries. We expect that a particular combination of cultural traits would support better outcomes during a pandemic regardless of whether or not the country were led by a man or woman. The include: more indulgence; less uncertainty avoidance; more collectivism; more long-term orientation; more femininity; and less power distance in society.\\nGender and pandemic policies. Given what we know about disaster management, we can see how these individual cultural aspects could provide protection against poor pandemic outcomes. For example, disasters are times of great uncertainty, when individual autonomy must be sacrificed in the process of following government guidance and believing constantly updated information [36, 43] .\\nAs with any new virus presentation, the SARS-CoV-2 virus introduced a multitude of unknown factors about its pathology and presentation in patients, uncertainty of quarantine and lockdown timelines and policies, and the complexities of therapies and vaccines that would allow states to return to a degree of normalcy [44, 45] . Successful pandemic management thus depends on the ability to mitigate that uncertainty and to adapt strategies and behaviors consistently updated with new information, rather than forcing a single course of action for the sake of constancy. It is not surprising that Erman and Medeiros find that cultures trending uncertainty avoidance are associated with higher COVID-19 fatalities, both in terms of case fatality and mortality rates [46] . Countries more able to tolerate uncertainty should be both better able to handle pandemics and more likely to elect women leaders.\\nErman and Medeiros further find that cultures trending toward individualism and longterm orientation are associated with higher COVID-19 fatalities [46] . We posit that these cultural factors could play a role in both a country\\'s ability to handle the COVID-19 pandemic, and its election of women. And, when a woman leads a country with traits more conducive to successful pandemic management, we expect them to perform better than when men lead countries with similar cultural traits.\\nThe adoption of public health strategies has presented as a mixed bag associated with a state\\'s idiosyncratic circumstances more so than any clear gender-specific issues. We obtained representative policy data for 12 of the 15 countries that are female led in our sample from the CoronaNet project. The CoronaNet project tracks covid-19 mitigation policies adopted by governments across 18 specific policy areas. We divided these policies between those that target individual behavior and those that are larger state functions, as shown in Table 1 . While the aggregated data we use do not take into account the duration of policies (for instance, a quarantine policy could be put in place for several months, but would only count as a single policy) or the changes in policy specifications, they do show patterns that emphasize government policies broadly and clearly in areas that governments have found easy to regulate or problematic in terms of balancing public health with services and the economy.\\nAs we show in Table 2 , we observe very different aggregate policies in countries led by men and women. We use data from the CoronaNet project which tracks state policies implemented during the pandemic. We coded each policy for targeting individual or state level actions. We then calculated them as percentages of total policies implemented by the country. Data were not available for Aruba, Hong Kong, Serbia, and Turks and Caicos Islands. Women-led countries overwhelmingly pursue state-level policies, whereas countries led by men pursue more individual-level policies. We selected the matching men-led countries based on our empirical analysis, discussed below, through nearest-neighbor matching to women-led countries (see Table 6 ). While all countries have pursued both individual-and state-level approaches to some degree, it is noteworthy that women-led countries have focused on the \"big picture\" whereas men-led countries have focused on regulating individual behavior. While these policies have not led to statistically significant differences in Covid-19-related fatalities, they have contributed to the perception that women-led countries are doing better given their pursuit of higher-profile macro-level policies.\\nTo test the prevailing narrative that women-led countries have fared better during the pandemic than men-led countries. Motivated by the above theoretical and empirical challenges to this narrative, we develop the following hypotheses: From the preceding discussion we generate the following hypotheses: H 1 : Countries led by women chief executives will have fewer Covid-19-related deaths.\\nH 2 : Countries with stronger feminine culture, longer-term orientation, more indulgence, less power distance, greater tolerance for uncertainty, and more collectivism will have fewer Covid-19-related fatalities.\\nH 3 : Leadership by a woman will amplify the effects of cultural traits that are already beneficial to pandemic management, compared to leadership by a man.',\n       'As background for this discussion, the life course refers to \"the age-graded sequence of roles, opportunities, constraints, and events that shape the biography from birth to death\" (Shanahan & Macmillan 2008:40). At its core, a life course perspective insists that development is lifelong and that no life stage can be understood in isolation from others. In doing so, it uniquely brings together many conceptual themes that are individually found in a variety of developmental and demographic perspectives (Elder, Johnson, & Crosnoe, 2003). Three such themes are particularly relevant to our discussion. One theme concerns continuity and discontinuity in life pathways-whether and how the development we observe during adolescence is embedded within stable trajectories across life stages or instead represents a major departure from past development and redirection of future development. Indeed, our discussion in this essay suggests that adolescence can exacerbate or buffer against early disadvantages or other childhood experiences in ways that affect adulthood. Experience in adolescence may also provide turning points that deflect earlier behavioral trajectories, and the unfolding of adolescence may allow for the accumulation of prior life advantages and risks that send young people on divergent paths into and through adulthood (refer back to Dragastin & Elder, 1975 for an early discussion of such issues). Another conceptual theme draws attention to the role of individuals in their own development. Here, we delve into the complex ways in which young people select into personal experiences, interpersonal relationships, and social settings in ways that reflect their past and contribute to their futures. As we discuss, this process of selection of person occurs through the agentic strivings of individuals as well as through the interplay of environment and biology. A final theme, the importance of historical change, is woven throughout our discussion. In general, the nature and meaning of adolescence is evolving, with a simultaneous acceleration of transitions into adult norms and values and prolongation of the achievement of autonomy for many segments of the population . The importance of historically informed approaches to adolescence is pronounced for the three topics that we have chosen to highlight here, in that the current historical moment has altered the landscape of education, puberty, and problem behavior. First, global economic restructuring has increased the lifelong returns to educational attainment to historic levels, thereby drastically increasing the long-term consequences of early academic experiences (Goldin & Katz, 2009). Second, the decline in age of puberty, especially among African-American girls, is a dramatic secular trend with implications for social behavior, fertility, and many other developmental domains as more and more young people have the appearance and reproductive capacities of adults without corresponding psychological and cognitive maturation (Ellis, 2004). Third, the echo boom (i.e., the children of the Baby Boom) has resulted in a swelling of the adolescent population (U.S. Census Bureau, 2007), which, given the age patterns of crime, has consequences for the delinquency rate and its associated policy and criminal justice responses (O\\'Brien & Stockard, 2008). These historical considerations illustrate how-across nations and cultures-developmental processes are embedded in the broader currents of history. In linking adolescence to other life stages, therefore, such historical embeddedness has to be taken seriously (see Elder, 1980, for an earlier discussion on adolescence in historical context).',\n       'The local meteorology over Guam depends on the structure of the marine boundary layer and trade-wind inversion in the dry season and on the structure and location of the monsoon trough in the wet season, thus adequate simulation even in highresolution regional models is challenging to achieve. To produce meaningful results, coarse resolution GCMs are supplemented with much higher resolution local climate models ( fig. 10). For this study, future-climate variables were produced through 20year simulations of a regional climate model with 20-km (12.4mi) resolution covering the entire tropical western Pacific Ocean domain which was then downscaled successively twice (from 20 km to 4 km and then to 0.8 km [12.4 mi to 2.5 mi to 0.5 mi]) culminating in a state-of-the-art, full-physics model with 0.8-km (0.5-mi) spatial resolution encompassing the island of Guam (Zhang, 2016) (data available online at https://cida.usgs.gov/ thredds/catalog.html?dataset=cida.usgs.gov/guam). The downscaled local model datasets have a gridded spatial resolution of 800 meters (m; 0.5 mi), and temporal resolution of 1 hour. Variables from model datasets consist of: (1) data at the ground surface such as air pressure, upward heat flux, net shortwave radiation, and downward long-wave radiation, (2) data at 2 m (6.6 ft) above the ground surface such as air temperature and relative humidity, and (3) data at 10 m (33 ft) above ground surface such as wind speed.',\n       \"The educational system is primarily viewed as a significant factor forming the basis of an individual's development and progress, and in turn, forms the core of countries' development. As such, more and more focus is being emphasized on the educational systems promotion on a global scale. In the context of Jordan, the government has made considerable efforts in developing its educational system. Such system has experienced tremendous development and increasing progress that date back to the 1920s (Al-Jaraideh, 2009 ). Owing to the significance of mathematics in the knowledge economy, Jordan's Ministry of Education (MoE) has concentrated on improving students' knowledge, skills and achievement in mathematics (Sabah & Hammouri, 2010) .\\nThe importance of mathematics has been hailed by many studies in literature. According to Drew (1996) , mathematics is the most important factor that relates to an individual's success. He proceeded to describe mathematics as a subject that is required for entry into many professions and it is important for existing as well as emerging occupations in a global economy that is based on information and technology. Saffer (1999) also stated that mathematics is not just useful in the day to day skills such as managing money but also in the most popular occupations and countless of jobs that call for some mathematical skill or another. This is the reason why mathematics is hailed at a higher rate compared to other subjects, and it has been called as the queen of all sciences and servant to all disciplines (Ajayi, Lawani, & Adeyanju, 2013) .\\nA student's proficiency in mathematics in schools is reflective of the related other variables or a combination of variables comprising academic and non-academic variables including individual characteristics, mathematics attitude , motivation, self-concept, self-confidence, self-regulation, equipment and instructional materials for effectively teaching the subject (Ma & Xu, 2004; Tella, 2007) .\\nStudies dedicated to the educational field imply that student attitudes toward a subject influence academic success (Popham, 2005; Royster, Harris, & Schoeps, 1999) . Additionally, attitude is among the factors that are gaining more focus from scholars as well as educators. More importantly, studies have highlighted the relationship between attitudes towards mathematics and achievement in the subject (Ma & Kishor, 1997) as a reciprocal influence where attitudes impact achievement and vice versa. In addition, Hammoury (2004) , in her study involving eighth grade Jordanian school students, revealed that attitude directly impacts mathematics achievement. Also, in Eleftherios and Theodosios's (2007) study, attitude is revealed to impact mathematics achievement and mathematics abilities including spatial visualization to memorize formula and procedures.\\nA second factor that can influence and determine the student's success in school is mathematics motivation. Learner's motivation is viewed as a crucial aspect of effective learning. Even psychologists are convinced that motivation is a necessary element for learning and that satisfactory school learning may not take place without enough learning motivation (Tella, 2007) . Additionally, Hammoury (2004) claimed that motivation significantly and positively impacts mathematics achievement among eighth grade students while Cassidy (2002) revealed that motivation is important in problem solving skills.\\nAnother factor that influences learning is mathematics self-regulation as evidenced by Zimmerman and Martinez-Pons's (1990) study which demonstrated its relevance for school students and academic achievement. Self-regulated learning is described as the knowledge and skills acquisition through cognitive and meta-cognitive process and actual behavior (Zimmerman, 2000) . Researchers claimed that self-regulation is crucial to the learning process (Jarvela & Jarvenoja, 2011; Zimmerman, 2008) as it assists students in creating better learning habits, strengthening study skills (Wolters, 2011) , applying learning strategies to improve academic outcomes and monitoring their achievement (Harries, Friedlander, Sadler, Frizzelle, & Graham, 2005) and evaluating their academic progress (De Bruin, Thiede, & Camp, 2001) .\\nIt is also notable that in the past three decades, educational research has stressed on self-efficacy (Joo, Bong, & Choi, 2000) . Bandura (1997) described self-efficacy as the belief in one's capabilities to organize and execute the courses of action needed to be achieved. Therefore, it can be stated that beliefs of self-efficacy can influence student's behavior through its impact upon the decisions of the tasks to engage in, the level of effort expended, and the time duration of persevering in difficult situations. A study has also revealed that self-efficacy is a major predictor of general academic achievement and in mathematics achievement in particular (Zimmerman, 2000) .\\nIn studies of meta-analysis, Hembree (1990) and Ma (1999) revealed that mathematics anxiety is inversely related to mathematics achievement. Ikegulu (2000) stated that math anxiety may hinder the developmental achievement of mathematics learners. Moreover, Gourgey (1984) claimed that math anxiety is a factor that leads students to stop trying when they encounter mathematics problems. Math anxiety may lead to higher withdrawal and/or failure rates among students who take developmental mathematics courses.\",\n       'This paper provides a novel method for modeling the complex disease progression. The mini network balance model has good performance on evaluating the AD progression which is beneficial on AD early diagnosis and facilitating therapeutic strategies.',\n       \"Background: Discrete clinical and pathological subtypes of Alzheimer's disease (AD) with variable presentations and rates of progression are well known. These subtypes may have specific patterns of regional brain atrophy, which are identifiable on MRI scans. Methods: To examine distinct regions which had distinct underlying patterns of cortical atrophy, factor analytic techniques applied to structural MRI volumetric data from cognitively normal (CN) (n = 202), amnestic mild cognitive impairment (aMCI) (n = 333) or mild AD (n = 146) subjects, in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database was applied. This revealed the existence of two neocortical (NeoC-1 and NeoC-2), and a limbic cluster of atrophic brain regions. The frequency and clinical correlates of these regional patterns of atrophy were evaluated among the three diagnostic groups, and the rates of progression from aMCI to AD, over 24 months were evaluated. Results: Discernable patterns of regional atrophy were observed in about 29% of CN, 55% of aMCI and 83% of AD subjects. Heterogeneity in clinical presentation and APOE ε4 frequency were associated with regional patterns of atrophy on MRI scans. The most rapid progression rates to dementia among aMCI subjects (n = 224), over a 24-month period, were in those with NeoC-1 regional impairment (68.2%), followed by the Limbic regional impairment (48.8%). The same pattern of results was observed when only aMCI amyloid positive subjects were examined. Conclusions: The neuroimaging results closely parallel findings described recently among AD patients with the hippocampal sparing and limbic subtypes of AD neuropathology at autopsy. We conclude that NeoC-1, Limbic and other patterns of MRI atrophy may be useful markers for predicting the rate of progression of aMCI to AD and could have utility selecting individuals at higher risk for progression in clinical trials.\",\n       nan,\n       'NPSAS:2000 included two major data collection systems: CADE and CATI. Both systems included edit checks to ensure data collected were within valid ranges. To the extent feasible, both systems incorporated across-item consistency edits. While more extensive consistency checks would have been technically possible, use of such edits was limited in order to prevent excessive interview and/or respondent burden. The CATI system included online coding systems used for the collection of industry, occupation, and major field-of-study data. Additionally, the CATI system included a coding module used to obtain IPEDS information for postsecondary institutions that the student attended (other than the NPSAS institution from which they were sampled). Below is a description of the online range and consistency checks, and the online coding systems, incorporated into the NPSAS:2000 CADE and CATI systems.',\n       \"The sample for NELS: 88/94 was created by dividing the NELS:88/92 sample into 18 groups based on their response history, dropout status, eligibility status, school sector type, race, test scores, socioeconomic status, and freshened status. Each sampling group was assigned an overall selection probability. Cases within a group were selected such that the overall group probability was met, but the probability of selection within the group was proportional to each sample member's second follow-up design weight. Assigning selection probabilities proportional to the second follow-up design weight, reduced the variability of the NELS:88/94 raw weights and consequently increased the efficiency of the resulting sample from 40.1 percent to 44.0 percent. The groups were: 0. Excluded from NELS:88/94 The NELS:88/94 sample is a spring defined sample, therefore, students who had been brought in through the freshening process but who had dropped out by the time of data collection in the year they were freshened as well as the base year dropouts were assigned to this group with a sampling probability of zero. In addition, sample members who were ineligible or out of scope (dead or out of country) for NELS:88/92 were also assigned to this group.\",\n       'The interactive influence between the diagnosis and the APOE genotype on gFCD was observed in the right DLPFC and medial prefrontal cortex (MPFC) (Figures 4A-C,E-G) , which is located in the executive control network (ECN) (DLPFC) (Vincent et al., 2008) and anterior DMN (MPFC) (Raichle, 2015) . As shown in Figures 4D,H , the interaction was observed in the alteration line between the APOE ε2 and ε4 carriers. Particularly, when compared with the CN group, the gFCDs in the right DLPFC and MPFC were decreased in MCI with APOE ε2 genotypes, but increased in MCI with ε4 carriers.',\n       nan,\n       'Estimation of evolutionary rates is an important step to characterize the genetic diversity among viral lineages and to place a timescale in phylogenetic hypotheses explaining their origin and divergence. The rate of evolution of viruses is often assessed through the number of errors occurring during replication of the viral genome (the mutation rate) and the frequency at which such mutations become fixed in the population (the substitution rate) [76] . The substitution rate depends on several factors, including the underlying mutation rate and the presence of selective forces that influence fixation of mutations in association with their fitness. Mutation rates of RNA viruses are generally higher than those of DNA viruses, due to the lack of a proofreading activity and consequent low fidelity of their RdRp [77] . However, due to the proofreading activity of Nsp14, members of the order Nidovirales have relatively lower mutation rates [78] .\\nThe substitution rate is often expressed in substitutions per nucleotide site per year (s/n/y) and can be estimated from phylogenetic reconstructions when divergence time is known for particular lineages. Although several methods have been traditionally used to estimate substitution rates, including linear regression and maximum likelihood (ML), the most popular method nowadays is the Bayesian Markov chain Monte Carlo (MCMC) approach, such as that implemented in the BEAST package [79] . Globally, substitution rates of coronaviruses have been estimated to be in the order of 10 −3 -10 −4 s/n/y [80, 81] . Studies conducted in SARS-CoV and MERS-CoV have estimated whole genome substitution rates to be between 0.80-2.38 × 10 −3 and 0.88-1.37 × 10 −3 s/n/y, respectively [82, 83] .\\nHowever, variation in the estimates for particular genes have been observed for both SARS-CoV [84] [85] [86] [87] [88] and SARS-CoV-2 [89, 90] , suggesting that the S gene and the region between ORF7b and ORF8 may be subjected to positive selective pressure in some lineages. In contrast, more conserved regions of the genome such as ORF1a and ORF1b appear to be under strong negative or purifying selection.\\nAn important application of this type of analysis is the estimation of the time to most recent common ancestor (TMRCA) between two lineages, as an approximate measure of their time since divergence. Several studies have estimated that the SARS-CoV lineage within the SARS-related coronaviruses most probably emerged between 1961-1985, while the civet SARS-CoV strains may have originated around 1986-1995 [86] [87] [88] . TMRCA estimates for the SARS-CoV and MERS-CoV strains involved in previous outbreaks have been roughly consistent with the dates when the first cases were reported [83, 88] . A preliminary study has estimated that the group containing SARS-CoV-2 and its closest bat coronavirus, RaTG13, may have diverged between 40-70 years ago [91] .',\n       'Background About 90% of the 161,000-acre Yaquina Basin is currently covered by intensively managed upland coniferous and mixed coniferous-deciduous forest in a variety of age classes (Map HAB-1). Sitka spruce and western hemlock are the primary species along the coast, while hemlock and Douglas fir dominate forests slightly farther inland. Red alder and bigleaf maple are the primary deciduous species. In the estuary study area (Map HAB-2), existing wetland habitats include intertidal and subtidal mudflats, eelgrass beds, salt marsh, and freshwater tidal marsh (ca. 2590 ac, 1.6% of basin), freshwater nontidal wetlands (1216 ac, 0.75% of basin), and permanently flooded estuarine, riverine, and lacustrine or palustrine habitats (2547 ac, 1.6% of basin). The remaining ca. 6% of the basin is urbanized.',\n       'Microarray raw signal processing for the UCSF-MAC cohort was performed using the lumi package. 24 First, within-sample raw gene expression intensities were normalized using variance-stabilized transformation (VST) 25 and interarray normalization was performed with robust spline normalization. Probes with a detection score below standard threshold (P = 0.01) for all samples were dropped along with probes not annotated within the lumiHumanAll.db database. Next, ComBat from the sva package 26 was used to perform batch correction. Outliers were removed using a connectivity Z-score (threshold > 2) calculated using the fundamentalNetworkConcepts function from the WGCNA package. 27 The AddNeuroMed cohort was processed using the same pipeline except that log2 normalization was used instead of VST because some quality control information was missing from the raw data.\\nThe raw data for the ADNI dataset is not publicly available. Therefore, we performed analysis on preprocessed array data which were normalized, using standard Robust-Multi-Array Averaging from the affy package. 28 We subsequently excluded 33 outliers using connectivity Z-scores (threshold > 2) and 11 samples with a RIN < 7, and performed batch correction using ComBat. Probes were annotated using the Affymetrix hgu219.db database from BioConductor. The SAFHS cohort was analyzed using the same pipeline as the UCSF-MAC cohort except for batch correction, which was not performed as batch information was not available.',\n       \"Main effects. It is important to note that we have no information about the mentoring and other induction support received by teachers during their second year in the profession. Thus, Table 4 shows the impact of perceived preparation quality and early career support on teachers' decisions after their second year based on their responses to the survey conducted in the spring of their first year.\\nAgain, we found that teachers' perceptions of preservice program quality were significantly related to their odds of leaving teaching, but not to their odds of moving within or across districts (Model I in Table 4 ). However, in contrast to what we found after the first year, the effect of program quality on leaving after the second year became insignificant once we controlled for the mentoring/induction support received by teachers (Model 3) .\\nSimilar to what we found for teachers' decisions following the first year, our results show that the quality and comprehensiveness of early career support significantly affected teachers' decisions to change districts and leave the profession after 2 years, whereas the availability of a mentor had marginal to no effects, depending on the subject area of the mentor. Those results held true regardless of the quality of the teachers' preservice preparation (Models II and III in Table 4 ) and provide additional support for the notion that the quality, not just availability, of induction support matters.\\nInteraction effects. Similar to our findings for teachers' decisions after the first year, we found no significant interactions between perceived preservice preparation quality and early career support in our models of teachers' decisions after the second year (Model IV in Table 4 ).\",\n       'Ethnicity showed a difference in relation to all except being good at sports. European Americans showed the hinhest percent indicating popular and African Americans showed the highest percent indicating drives a nice car, has a job, and makes a lot of money. However, in each case fewer than 45% selected these traits. Socio-economics status showed differences related to popular and good at sports but not to the other characteristics. Again, those in the lowest quartile were least likely and those in the highest quartile were most likely to indicate the characteristic. Sex of students showed a difference in relation to being good at sports with more boys than girls selecting this trait. However, fewer than half of the boys indicated this characteristic and about a third of the girls did so. Tenth graders from the South were most likely and those from the Northeast were least likely to see drMng a nice car as a characteristic of a person they admired. Family type did not show a substantial difference related to any of the characteristics. Makes a lot of money European American 22% to African American 33%',\n       'The contact of the researcher with teachers in preparatory and secondary schools informed the researcher that students were not able to keep up with needed work in the classroom particularly in applying the knowledge to specific problems and thus translate the information shared with them in class into a demonstrative learning experience. Therefore, we began conceptualizing ways new technologies can motivate students to advance organizational techniques that address school assignments. Technologies as in mobile devices allow students to engage in educational activities using what they are accustomed to, as in the smart phones to support or improve student\\'s completion of school assignment. Thus the research question of this study: \"Does changing the format of homework assignment through the use of smart phones improve student, mathematics academic outcomes whether in completion or achievement?\" The main hypothesis of this study is that \"mobile devices can increase the assignment completion rate and achievement of secondary school students.\" This proposition is based on the two main theories, the first is Bandura\\'s Social Learning Theory which suggests that the actions of a person is strongly influenced by their consequences [15] . Namely, that students realize the effects of a performing a certain behavior, they associate the behaviors with specific and possible consequences which reinforces the source behavior. The second perspective is drawn from the theory of effect originally developed by Thorndike [16] . The theory of Law of Effect suggests that when choosing a course of action that experience gained has the least level of struggle which then reinforces the behavior based upon the consequences. Thus, by engaging students in certain behavior as in school assignment through a positive reinforcement they will more likely to continue in that behavior whereas the learner might be negatively reinforced where the interaction allows for the learner to try out different actions and causes further motivation for the student to achieve a desired goal. Thus we see the engagement of these devices as interactive and in real time have different consequences in reinforcement. Using these technologies could support teachers and reinforce positively student learning behavior. The main objective of this study is to investigate the effects of using smartphones applications to increase completion rates of assignment that could impact student achievement in mathematics.',\n       \"In this investigation, private elementary school principals of Large-size schools addressed more problem matters on their school campus than private elementary school principals of Small-size schools. As such, an implication is that private elementary school principals of Large-size schools should examine the processes and structures in place on their campus to address problem matters adequately. In turn, this insight may be used to determine best practices to create a more supportive school environment. Interestingly, problem matters of theft, physical conflicts among students, and student bullying were reported as occurring at a high rate by principals of both Largesize and of Small-size private elementary schools. Accordingly, private elementary school stakeholders should examine their school culture. This examination may provide feedback to support the development of aptitudes that could positively enhance students' social, emotional, and physical development.\",\n       'Our data indicate that the SARS-CoV-2 virus TMD induces trimerization of RBD and emphasize the importance of investigating a chimeric RBD that includes the TMD of SARS-CoV-2 as a potential immunogen for vaccine development, or in anti-viral strategies targeting the RBD in SARS-CoV-2 infection. However, considering the hidden nature of the spike TMD and therefore the unlikelihood of it operating as a robust immunogen, we speculate that the employment of specific allosteric modulators of the TMD may be ideal. These allosteric modulators could include antibodies, recombinant antibody derivatives, or small molecule compounds that would bind to the ectodomain of spike and induce a conformational change that would render the TMD incapable of trimerization [15] [16] [17] .\\nGiven that many vaccines currently in development target RBD, we believe that it is necessary to have a holistic understanding of the mechanisms underlying the virulence capacity of the SARS-CoV-2 spike glycoprotein. Knowing that the TMD plays a critical role in the trimerization of spike preceding viral entry may now prompt new research streams aiming to tease apart the intricacies of these mechanisms in an effort to therapeutically target them. Coupling the targeting of trimerization with current anti-RBD strategies may yield a synergistic anti-viral strategy.\\nSupplementary Materials: The following are available online at http://www.mdpi.com/2077-0375/10/9/215/s1. Figure S1 . Uncropped blots. Table S1 . Insert DNA and Amino acid sequence.',\n       'We evaluated BMR within species by comparing populations of the same species from different locations (Fig. 5A) . For CA chickadees, BMR in Tennessee birds did not differ significantly from those in Ohio. We obtained the same results when we included mass as a covariate (see Table 1 for values). In contrast, there were significant differences for whole-organism BMR among BC chickadee populations ( , ), al-F p 40.8 P ! 0.001 4, 84 though more northerly populations did not consistently have higher BMRs. Alaskan BC had a significantly higher BMR than Minimun daily temperature in January averaged over 10 yr (1997) (1998) (1999) (2000) (2001) (2002) (2003) (2004) (2005) (2006) BC from any other location ( for all comparisons). P ! 0.001 South Dakota BC had a higher BMR than both Wisconsin and Ohio ( for both comparisons). Location was also sig-P ! 0.001 nificantly related to BMR in BC after including mass as a covariate ( , ). Again, post hoc tests revealed F p 23.6 P ! 0.001 4, 83 that Alaskan birds had a significantly higher mass-corrected BMR than birds from any other location ( for all P ! 0.001 comparisons). Among the remaining groups, New York and Wisconsin BC chickadees did not differ significantly from any other location, and the South Dakota BC chickadees had a significantly higher BMR than Ohio birds ( ). P p 0.009',\n       \"Among 1995-96 beginning postsecondary students at 4-year institutions whose high school curriculum did not exceed the core New Basics, first-generation students earned a lower average GPA in their first year (2.4) than their peers whose parents had bachelor's or advanced degrees (2.7) (Warburton, Bugarin, and Nuñez 2001) . A similar relationship held if the high school curriculum was Beyond Core New Basics I-first-generation students had an average GPA of 2.5, and students whose parents had bachelor's or advanced degrees had an average GPA of 2.8. This difference did not extend to those who had taken more challenging curricula. Regardless of parents' education, students earned an average GPA of about 2.7 when their high school curriculum was Beyond Core New Basics II and an average of about 3.1 when it was in the Rigorous category.\",\n       'Neural activity-evoked CBF response provides an index of neurovascular coupling. The CBF response evoked by whisker stimulation in the somatosensory cortex was significantly decreased in young APP mice, as compared to WT littermates (21.2 ± 0.4% vs 29.3 ± 0.9%, p b 0.05) (Fig. 1A,B) , and persisted with age (17.7 ± 1.8% vs 26.8 ± 1.8%, p b 0.01) (Fig. 1C) . Average percent flow deficit measured in APP mice was 27.8% at 3-months of age, and 34.2% at 6-months of age (Fig. 1B,C) . Pioglitazone-treatment fully normalized this response with as little as 3-days of treatment (Fig. 1B) , an effect that was maintained with long-term treatment (Fig. 1C) . A small, but non-significant, enhancing effect on the average percent flow increase was observed in WT-treated mice (Fig. 1C) .',\n       'categories, with only 50.8% of subjects at 1.5 T and 58.1% of subjects at 3 T passing stringent segmentation quality control. We also found that all algorithms identified several subjects (between 2.94% and 48.68%) across all diagnostic categories showing increases in hippocampal volume over 1 year. For any given algorithm, hippocampal \"growth\" could not entirely be explained by excluding patients with flawed hippocampal segmentations, scan-rescan variability, or MRI field strength. Furthermore, different algorithms did not uniformly identify the same subjects as hippocampal \"growers,\" and showed very poor concordance in estimates of magnitude of hippocampal volume change over time (intraclass correlation coefficient 0.319 at 1.5 T and 0.149 at 3 T). This precluded a meaningful analysis of whether hippocampal \"growth\" represents a true biological phenomenon. Taken together, our findings suggest that longitudinal hippocampal volume change should be interpreted with considerable caution as a biomarker. Hum Brain Mapp 38: 2017 .',\n       'The 2004 ARMS Phase III Core sample size was 15,900 farm/ranch operations across 15 states. The 15 states were those with the highest agricultural value of sales and included Arkansas, California, Florida, Georgia, Illinois, Indiana, Iowa, Kansas, Minnesota, Missouri, Nebraska, North Carolina, Texas, Washington, and Wisconsin. The total sample was stratified by state, ARMS farm value of sales (as maintained on the sampling frame), and type of operation (also as maintained on the sampling frame); then, five sub-samples, each of size 2,000, were systematically selected. The subsamples were drawn such that each was equally represented by the strata. Once the sub-samples were drawn, NASS Field Offices had the opportunity to remove operations from the Core sample that had previous data reporting arrangements with the office. This resulted in each sub-sample being slightly less than 2,000. The standard mail-out/mail-back data collection methodology for the 2004 ARMS Phase III Core sample consisted of: (1) a cover letter and questionnaire mailed on December 28, 2004, (2) a post-card reminder and \"thank you\" sent to the entire sample on January 13, 2005, (3) a cover letter and second questionnaire mailed to all non-respondents on January 31, 2005, and, finally, (4) starting February 21, 2005, face-to-face interviews attempted for all remaining mail nonrespondents. In addition to the standard data collection methodology, prepaid and promised indirect cash incentives -in the form of $20 automated teller machine (ATM) cards -and priority mail were used as stimuli. Combinations of these stimuli were administered to four of the five sub-samples mentioned above; a fifth sub-sample received no stimuli and served as the control for this project. Collectively, these five sub-samples formed the five treatment groups used for this project. Table 1 contains descriptions of the treatment groups. All treatment groups received cover letters that included some uses of the ARMS data. Five such uses were included in the cover letters used for treatment groups not receiving incentives, and three uses were included in the cover letters used for treatment groups that received incentives. These unintentional differences may have confounded our results, as they may have contributed positively or negatively to the response rates. However, we expect the confounding effect to be minimal since all letters were written to positively impact response. The sub-samples in the pre-paid incentive treatment groups all received cover letters that: (1) explained the incentive was a \"thank you\", (2) described the uniqueness of the ARMS, and (3) justified the use of the incentive by its overall cost savings to the government. The actual ATM card incentive was delivered to recipients in the same packet as the first questionnaire and was affixed to a standard 8½ inch x 11 inch sheet of paper that reiterated a \"thank you\" and included instructions on how to use the card. Appendix A contains the treatment group survey materials and includes copies of the cover letters used for each treatment group, a copy of the ATM card instruction sheet, and a photo of the actual ATM card that was used. The $20 ATM cards were supplied by JPMorgan Chase bank and were usable in nationwide ATM machines that displayed the NYCE ® , Pulse ® , Maestro ® , or Cirrus ® logos. The cards were also usable at point-of-sale (POS) (i.e., retail) establishments that allow the use of debit cards as payment; however, this fact was not revealed to card recipients. In addition to the $20 incentive, the ATM cards were loaded with an extra $4 to cover any transaction charges. The cards were preactivated and were immediately usable when the recipients received them. The personal identification number (PIN) needed to use the card was embossed on the front of each card after the words \"THANK YOU\". The front of each card also included the embossed message, \"FOR HELP 1-888-424-7828\"; this toll-free telephone number was answered by NASS staff. Finally, all ATM cards expired on June 30, 2005 (there was no provision for extending this date). See Appendix A for a photo of the actual ATM card. If a card recipient lost or could not use the card, a replacement could be requested by calling the toll free phone number listed on the instruction sheet. For the few cases where this occurred, only the replacement card was used in our analysis. The decision to use $20 ATM cards as incentives was essentially made by default. Actual cash was preferred by the authors; however, NASS and USDA senior management were concerned with accountability when using cash. Checks were also considered, but the U.S. Treasury Department (the would-be issuer of the checks) was concerned with logistical issues related to check usage. There were no such concerns from NASS, USDA, or the Treasury Department for the ATM cards. The decision to offer $20 as the incentive amount was also largely a result of that value being the only viable option because many, if not most, ATMs only dispense cash in $20 increments. Priority mail was also used as a stimulus because evidence from survey literature has shown the use of priority mail increases overall response rates, especially when used in combination with monetary incentives (Moore and An, 2001). Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a face-to-face interview) and Treatment Group 5\\' includes only those respondents in the treatment group who received an ATM card because they returned their questionnaires in the mail.',\n       \"We specify the error structure for the choice-specific utilities to have a nested logit form, allowing the errors to be correlated among the two graduation options, i.e., graduating with a science major (m) and graduating with a non-science major (h). In this way we account for shocks after the initial choice of college and major that may influence the value of a student individual was admitted to one of the top three UC campuses, but was rejected by one of the middle three UC campuses; and v) whether the individuals was admitted to one of the top three schools and applied to one of the bottom two schools. 31 See, for example, Hoxby (2009) page 115. continuing their education, such as a shock to one's finances or personal issues. Given our assumption regarding the error distribution, the probability of choosing to graduate from school k with major j ∈ {m, h}, conditional on X and B (but not ), is given by: where θ ≡ {α, β, φ, ρ} is the full set of parameters to be estimated and u ijk ≡ U ijk − ijk . 32 The probability of choosing not to graduate from k is then given by: 33 We estimate separate nested logit models for 4-and 5-year graduation outcomes. Note that since β j is major-specific, we must normalize one of the φ 2jk 's for each major. We do so by setting the return on both the science and non-science academic index at UC \",\n       'Demographic, neuropsychological, and functional variables (Table 1) Table 2 lists the percentage of deficits by item within each diagnostic group. Although few controls selfreported any functional deficits on the individual FAQ items, informants for the aMCI group most commonly reported deficits for item 1 (writing checks, paying bills, or balancing checkbook; 33.8%), item 2 (assembling tax records, business affairs, or other papers; 42.9%), and item 9 (remembering appointments, family occasions, holidays, and medications; 54.8%). These 3 items increased markedly in frequency on the Cochran-Armitage linear trend tests from controls to patients with aMCI to patients with AD, but the frequency of other items, such as item 5 (heating water, making a cup of coffee, turning off the stove), increased less so across groups (Table 2) . To identify those items that most commonly differentiated the control and aMCI groups, an item selection process was used. The subset selected had a median size of 6; the 6-item subset (items 2, 4, 7, 8, 9, and 10) was the most frequently selected in the 500 bootstrap samples and differentiated the control and aMCI groups with classi-\\nThe evaluation of 6 IADLs (including the ability to assemble tax records, business affairs, or other papers; to play a game of skill such as bridge or chess or work on a hobby; to keep track of current events; to pay attention to and understand a television program, book, or magazine; to remember appointments, family occasions, holidays, and medications; and to travel outside the neighborhood, drive, or arrange to take public transportation) does not represent a marked time savings compared with the 10-item FAQ for use in clinical practice. Two of these 6 items were selected in each of the 500 bootstrap samples. These 2 items (assembling tax records, business affairs, or other papers; and remembering appointments, family occasions, holidays, and medications) were highly effective in discriminating the control group from a combined aMCI and mild-AD group. Although only 3.5% (n=12) of healthy controls reported deficits on 1 of these 2 items (no controls reported deficits in both), 66.0% of informants for the aMCI group and 96.4% of informants for the mild-AD group reported deficits on 1 of these 2 items; 85.5% of the mild-AD group had informantreported deficits on both items. Comparing controls with the combined aMCI and AD groups, these numbers are reflected in the sensitivity (0.76 vs 0.81) and specificity (0.95 vs 0.92) estimates comparing the 2-item FAQ and 10-item FAQ, respectively, with a cut point of 1 functional deficit or more.\\nThe control group consisted of cognitively intact older adults who used self-report assessments, thereby making it difficult to compare these rates with those reported by informants for the aMCI and AD groups. Therefore, healthy controls were excluded from subsequent analyses.',\n       'Daily transport of dissolved atrazine, alachlor, cyanazine, metolachlor, and nitrate as nitrogen was calculated for the ten basins in the Mississippi River and Temporal Variability studies as the product of measured or estimated daily chemical concentrations and daily mean streamflow. Research has shown that less than 1% of the atrazine, cyanazine, and metolachlor transported by the Mississippi River travels in the suspended phase (Pereira and Rostad, 1990; Squillace and Thurman, 1992) . Concentrations were estimated by linear interpolation on days when no samples were collected. A concentration of zero was used when herbicide or nitrate concentrations were less than analytical reporting limits (0.05 mg l −1 or less for all chemicals). Estimates of daily transport for the period April 1, 1991 , to March 31, 1992 were summed to obtain estimates of annual transport ( Table 2) . Estimates of annual transport of agricultural chemicals in the Mississippi River and its tributaries for 1991 are similar to those reported by Goolsby et al. (1991) and Dunn (1996) , but significantly larger than those reported by Pereira et al. (1989) for 1987. Time-weighted annual mean concentrations of atrazine, alachlor, cyanazine, metolachlor, and nitrate were computed as the average of daily concentration estimates for the period April 1, 1991 to March 31, 1992 (Table 3) (Battaglin and Hay, 1996) . Annual mean concentration estimates are time-weighted, not flow weighted, because timeweighted estimates are likely more representative of an annual mean exposure expected from using rivers or some alluvial aquifers as a source of drinking water. Time-weighted concentration estimates are also relevant to public-water suppliers who must comply with federal regulations (US Environmental Protection Agency, 1991). For herbicides, which tend to occur in higher concentrations during spring runoff when flow is also high, flowweighted concentration estimates are generally larger than time-weighted estimates. For example, the flow-weighted annual mean metolachlor concentrations for Sites 6 and 9 are 0.51 and 4.12 mg l −1\\n, whereas the time-weighted annual mean concentrations for Sites 6 and 9 are 0.20 and 1.08 mg l −1 , respectively. Insufficient data were collected at the Regional Reconnaissance study sites to estimate either annual transport or annual mean concentrations of agricultural chemicals. Statistical analysis of data collected at these sites was done only on instantaneous concentration values from the post-planting samples.',\n       'At P3, the mean summer mixed layer diatom assemblages (Korb et al., 2008) are dominated by F. kerguelensis (30%), followed by Pseudo-nitzschia (20%) and C. Hyalochaete vegetative stage (10%, Fig. 5 ). Between the mixed layer and the sediment trap depth (2000 m) there are notable decreases in the contribution of Pseudo-nitzschia (20-2%), Thalassiothrix antarctica (10-1%) and E. antarctica (8-2.5%). Conversely, F. kerguelensis relative abundance is similar between the mixed layer and the sediment trap depth (30%), while the contribution of C. Hyalochaete increases (11-42%) and is present as resting spores in the trap samples. In surface sediments (Allen et al., 2005) , CRS represents 80% of the diatom assemblage, followed by F. kerguelensis (10%) and TRS (3%). Fig. 4 . Diatom fluxes at P3. Empty (white bars) and full (grey bars) diatoms fluxes for (a) the total diatom community and (b-l) the diatom species/groups contributing to more than 1% of the annual diatom flux. The relative contribution of each diatom species/group (empty þfull) to the total diatom assemblage is shown as red dots (right axis). The number in brackets is the relative contribution to the total diatom flux integrated over the deployment period and includes full and empty cells. Asterisks denote missing cups. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Table 2 ). The missing samples are from a period of low background flux with no notable surface chlorophyll a features (Fig. 2) . Consequently we decided not to include the linear interpolation of missing samples and present integrated fluxes of chemical and biological constituents as a conservative approximation of annual flux. At P3, the sediment trap deployment location is positioned in the center of a cyclonic circulation feature originating from the northern South Georgia shelf (Meredith et al., 2003) . The area downstream of South Georgia is fertilized by continuous benthic iron supply from the South Georgia shelf system, resulting in higher dissolved iron concentration when compared to the waters upstream of the island (0.3 nM versus o0.05 nM in spring, Nielsdóttir et al., 2012) . This natural iron fertilization mechanism has been suggested to support the large and recurrent phytoplankton blooms observed downstream of South Georgia (Fig. 1 , Ward et al., 2002; Borrione and Schlitzer, 2013) . The regional differences we report in deep-ocean POC fluxes at P2 and P3 are therefore consistent with the varying levels of chlorophyll a biomass observed around South Georgia resulting from differences in iron supply.\\nDeep-ocean POC fluxes at the productive P3 site lag the strong surface chlorophyll a peak (6 mg L À 1 ) by one month and exhibit no notable peaks in the time preceding surface biomass maxima. Time lags of 1-2 month between production and export have been observed previously in the Southern Ocean (Honjo et al., 2000; Buesseler et al., 2001; Rigual-Hernández et al., 2015a,b) . During a shallow sediment trap deployment (289 m) over the central Kerguelen Plateau (Rembauville et al., 2015b) , CRS also dominated carbon export and was exported one month after the surface chlorophyll a peak, a pattern very similar to what we observe at P3 despite a much deeper sediment trap depth (2000 m). Highest BSi: POC ratio was observed in spring (September/October) at both sites. Very similar patterns in export stoichiometry have been observed in other naturally fertilized area of the Southern Ocean and were attributed to changes in the diatom community structure across the season (Rembauville et al., 2015a; Salter et al., 2012) . There are no comparable peaks in either surface biomass (o2 mg L À 1 throughout the year) or deep ocean POC flux at station P2 upstream of South Georgia. Despite strong differences in surface phytoplankton biomass, primarily mediated by iron supply (Nielsdóttir et al., 2012; Borrione and Schlitzer, 2013) , the amount of POC reaching the deep-ocean differs by a factor of o2 between the two sites. This is consistent with a scenario of enhanced carbon remineralization downstream of South Georgia (Cavan et al., 2015; Le Moigne et al., 2016) . The annual POC fluxes observed at South Georgia are remarkably similar (o100 mmol m À 2 yr À 1 ) to those measured at other naturally iron-fertilized sites in the Southern Ocean (Salter et al., 2012; Rembauville et al., 2015b) . at 1031 m, Honjo et al., 2000) . The homogeneity of the deep ocean POC fluxes, despite contrasted surface primary production, highlights the inverse relationship between primary production and export efficiency in the Southern Ocean (Maiti et al., 2013) .',\n       'Konu /kavramların nasıl daha iyi/kalıcı öğretileceğini öğrendim.\\n14 Bilgi aktarma/öğretmenlik deneyimi yaşamamızı sağladı. 12 Öğrencilerime matematiği hayatlarında nasıl kullanacaklarını /yaşamla ilişkilendirmelerini öğretmemi sağladı.',\n       \"The panel acknowledges that products used for sampling are likely to be categorized as food loss according to the current calculation. The available research, however, more narrowly focuses on how sampling can boost a firm's profitability rather than the amount to be sampled. Although the panel does not expect product sampling to be a significant factor that warrants adjustment and would not affect all commodities, primary research with food retailers is likely needed to verify this assumption.\",\n       'Since each modality is affinely normalized, we are able to identify a region of interest (ROI). To select the hippocampus ROI, we do not use a segmentation step, but instead we follow an atlas-based selection method we previously proposed in [8] . The method allows for a rough extraction of the ROI. We use an MNI normalized brain atlas called Anatomic Automated Labeling Atlas (AAL) [44] . Furthermore, in order to limit the processing only to brain tissues, we also generated a mask to remove skull voxels. Both sMRI and MD images are mapped to the AAL to select the hippocampus ROI. The use of the atlas parcels helps to characterize brain abnormalities in terms of intra-ROI local pattern. In fact, the pattern overlapping with the extracted ROI mask in all modalities varies between healthy subjects and those exhibiting clinical signs of disease.',\n       'The methodology proposed in the paper can also be used to assess and compare the effectiveness of different DRR measures. In this paper, as an example, the methodology was applied to assess the effectiveness of a DRR measure in the form of a rock revetment. The construction of an adaptation option in the form of a rock revetment can be useful to reduce risks and Bbuy time^, by delaying the effects of an increase in sea level. This is clearly shown in Fig. 4 : EAD can be maintained below current levels until year 2040 by using the revetment, while EAAP will reach current levels in 2070-2100, depending on the SLR scenario. However, by the end of the century, this measure will also be ineffective, and alternative (more sustainable) solutions should be explored. The methodology could also be used as a basis to carry out a full costbenefit analysis, required to assess the financial feasibility of a possible adaptation option and to design a long-term coastal adaptation plan for the island (e.g. Haasnoot et al. 2013; Smallegan et al. 2017 ). Adaptive planning is, in general, the most effective way of dealing, in a sustainable way, with the effects of climate change and SLR on these islands, by identifying a set of adaptation options which can be applied at different times in response to changing conditions.',\n       nan,\n       \"We also wanted to know how consistent were the results across datasets, and thus we compared the classification performances obtained from ADNI, AIBL and OASIS, for the task of differentiating control subjects from patients with Alzheimer's disease. Voxel (4 mm smoothing) and regional (AAL2 atlas) features were extracted from T1w MR images and used with linear SVM classifiers. We tested two configurations: training and testing the classifiers on the same dataset, and training a classifier on ADNI and testing it on AIBL and OASIS.\\nResults are displayed in Table 11 . Performances obtained on ADNI and AIBL were comparable and much higher than those obtained on OASIS. When training on ADNI and testing on AIBL or OASIS, the balanced accuracy was at least as high as when training and testing on AIBL or OASIS respectively, suggesting that classifiers trained on ADNI generalized well to the other datasets. In particular, training on ADNI substantially improved the classification performances on OASIS. We aimed to assess whether this was due to the larger number of subjects in ADNI. To that purpose, we performed the same experiments but with subsets of participants of equal size for each dataset. We randomly sampled populations of 70 AD patients and 70 CN participants from each of the datasets, ensuring that the demographic and clinical characteristics of the subpopulations did not differ from the original ones. As can be seen from Table 11 , using the subset, the improvement disappeared for the voxel-based but remained for the regional features. Table 9 Influence of feature types. Mean balanced accuracy and standard deviation obtained for three tasks (CN vs AD, CN vs pMCI and sMCI vs pMCI) using the reference classifier (linear SVM) with voxel (reference smoothing: 4 mm) and region (reference atlas: AAL2) features extracted from T1w MRI and FDG PET images of ADNI subjects. Learning curves were computed to assess how the performance of linear SVM classifiers varies depending on the size of the training dataset. Using only ADNI participants, we tested four scenarios: voxel and region features extracted from T1w MRI and FDG PET images. As crossvalidation, 250 iterations were run where the dataset was randomly split into a test dataset (30% of the samples) and a training dataset (70% of the samples). The maximum number of subjects used for training and testing for each of the different tasks is of 362 for CN vs AD, of 313 for CN vs pMCI and of 355 for sMCI vs pMCI. For each run, 10 classifiers were trained and evaluated on the same test set using from 10% to all of the training set (from 7% to up to 70% of the samples), increasing the number of samples used by 10% on each step. Therefore, the number of participants used for training ranged from 20 to 197 for CN, 24 to 239 for sMCI, 12 to 117 for pMCI and 17 to 166 for AD. We can observe from the learning curves in Fig. 6 that, as expected, the balanced accuracy increases with the number of training samples.\\nLearning curves were also computed for the CN vs AD task when using larger datasets obtained by combining participants from ADNI and AIBL (balanced subset composed of 72 CN subjects and 72 AD subjects) and from ADNI, AIBL and OASIS. Results are displayed in Fig. 7 . We observe that for an equivalent number of subjects, combining ADNI and AIBL or only using ADNI leads to a similar balanced accuracy. For regional features, the performance is slightly higher when combining ADNI and AIBL compared to when only using ADNI, but the difference is largely within the standard deviation. The balanced accuracy keeps increasing slightly as more subjects are used for training when combining ADNI and AIBL. However, when combining ADNI, AIBL and OASIS, the performance is worse than when only using ADNI or combining ADNI and AIBL, no matter the number of subjects. This is probably due to the fact that ADNI and AIBL follow the same diagnosis and acquisition protocols, which differ from those of OASIS.\",\n       \"Four new genetic studies underscore the relevance of IL-1 to Alzheimer's pathogenesis, showing that homozygosity of a specific polymorphism in the IL-1α gene at least triples Alzheimer's risk, especially for an earlier age of onset and in combination with homozygosity for another polymorphism in the IL-1β gene [48] SOD2 A polymorphism in SOD2 is associated with development of AD [49] NOS3 NOS3 may be a new genetic risk factor of late onset AD [50] The data used in the preparation of this manuscript were obtained from the ADNI [51] database. The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of the ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD.\\nFor our predictive study, we utilized the dataset from an earlier study by Shaffer et al. [52] based on ADNI-1. That particular study identified 97 MCI patients and predicted progression to AD dementia based on their clinical parameters, MRI results, PET scans, cerebrospinal fluid (CSF) markers (tau, p-tau181P, and β-amyloid1-42), the APOE ε4 genotype, and results from at least one follow-up clinical examination. Out of the 97 patients from the earlier study, only 91 patients have corresponding SNP data in the ADNI database. Hence, for the current study, we only utilized these 91 patients. However, this reduction in the number of patients did not considerably affect the ratio of MCI progressive patients to MCI non-progressive patients. The original study had 43 MCI progressive patients and 54 MCI non-progressive patients, and the reduced dataset has 41 MCI progressive patients and 50 MCI non-progressive patients. Thus, there is still sufficient representation of the two classes of patients.\",\n       'A representative household in this economy consumes the final good in the amount C t each period, inelastically supplies labor input H, and has preferences over consumption streams given by . The representative household receives labor income, owns all the firms, and trades a one-period bond with zero net supply. As usual, if consumption grows at a constant Ct Ct , and if r denotes the one period interest rate on loans of consumption goods, these preferences imply the result 1 + r = 1 (1 + g) . (3.3) Because the price of consumption goods is always one unit of the numeraire good, r is also the one period interest rate on loans denominated in the numeraire.',\n       'Regarding the non-linearity in the processes contributing to flooding and return periods, the extreme sea-level analysis showed that the astronomical tide has an important effect on determining the return periods for possible sea levels. In fact, the CACS events can occur at any tidal phase and thus the CACS residual tide is independent of any simultaneously occurring tidal phase. However, the interaction between residual and astronomical tides is an interesting subtle point to study: if this interaction is linear, the storm tide (TSSE without waves) probability would be roughly equal for each tidal phase and would be simply equal to the sum of the residual sea level and the astronomical tide, which means independence between the astronomical tide and the residual sea level. In that case, the use of joint probability methods could be used. These methods provide the chance to source variables by taking values at the same time and to create a scenario in which a flooding event may occur. This method is usually used for independent events (Chini and Stansby, 2012). For instance, Zhong et al. (2013) assumed independence between the astronomical tide and residual tide and estimated the storm tide probability of a joint probability method. However, hydrodynamic numerical experiments for Progreso in the present study suggested that the astronomical tide and residual sea level have a non-linear relationship, and thus applying the joint probability method for this case would not be adequate. A pragmatic and simplistic approach can consist of using a large data set of sea-level reanalysis (for instance 30-years in this case), assuming that a sufficiently large number of combinations of storm surges and astronomical tidal levels are present in the data set, and it can perform an extreme analysis of the sea level as a single variable. However, if quantifying the non-linear effect is the objective of a future study for Progreso, there are other options for estimating the storm tide probability given the height of the astronomical tide and phase when the storm surge arrives (assuming that the surge can happen at any time during a tidal cycle with equal likelihood), such as the one proposed by . These authors developed an empirical function based on over 200 extreme tropical cyclone events (with both the storm tide and the storm surge simulated for the full range of tidal phases) for New York City. The implementation of this method for this zone is beyond the scope of this study, where the main objective is to study the hydrodynamic and flood-prone surface areas for Progreso during CACS events. However, this study shows the need to develop an empirical probability function based on the data for this area to estimate the probability of any storm surge for a given astronomical tidal phase as well as the wind direction and intensity with respect to the main lagoon axis.',\n       \"Abstract A large land-fast sea ice breakup occurred in 2016 in Lützow-Holm Bay, East Antarctica. The breakup caused calving from the Shirase Glacier Tongue. Although similar breakups and calving have been observed in the past, the timing and magnitudes are not well-constrained. The ice's breakup latitude during 1997-2016 was analyzed to investigate the variables controlling breakup and examine correlation with local calving for a longer period. The breakup latitude in April had a persistently high correlation with sea surface temperature (SST) in the tropical Pacific, which exceeds correlations with local atmospheric variables. The years of five out of six observed calving events from the mid-20th century can correspond to those of warm SST episodes and calving-front retreat in the 1980s to warmer SST shift. Our proposed teleconnection between tropical SST and Antarctic sea ice could lead to better predictions of breakup and might impact the glacier flux for a wider region.\\nPlain Language Summary Land-fast sea ice forms along the Antarctic coast, and it occasionally breaks up significantly. The breakup event influences the flow of glaciers, which is otherwise held back by the fast ice. The breakup of land-fast sea ice and the discharge of glaciers have significant multidecadal variability as well as interannual variability. This study explores what controls the breakup phenomena of land-fast sea ice in Antarctica and finds the linkage with tropical sea surface temperatures. We find the environmental factors which are relevant to the ice breakup, and those variables are originally driven by the teleconnection from the tropical Pacific. We believe that our study makes a significant contribution in climate science by offering a causal mechanism that explains the previously observed multidecadal variability in ice extent in this region. Our model can explain five out of the last six calving events in a major glacier connected to this bay, offering hope for future predictions of ice behavior. This will also merit the logistics to Antarctic research stations.\",\n       'Persistent racial and gender gaps are an increasing concern in many countries. In the United States, a typical black 17-year-old reads at the proficiency level of a typical white 13-year-old (Fryer and Levitt, 2006a) . Girls significantly outperform boys in reading, and boys outperform girls in mathematics. At the macroeconomic level, these gaps may be costly, given that the aggregate return to education is estimated at around 6-10% per year of schooling (Acemoglu and Angrist, 2000) . A back-of-the-envelope calculation thus suggests that there could be important gains from reducing the human-capital gap between races and genders.\\nOf course, those potential gains depend on the cost of reducing racial and gender gaps. Some claim that there are intrinsic differences between races and genders that are not reducible to social or economic factors. One of the most famous arguments is described in Herrnstein and Murray (1994) . However, this explanation has been disputed. First, there is no single factor -usually called the g factor -that explains educational or labor market outcomes (Heckman, Stixrud and Urzua, 2006) . Second, racial and gender gaps are not constant but rather are increasing with age. Fryer and Levitt (2006b) reports that there is no difference in cognitive performance for 1-year-old children. In grade 1, a few covariates for family background are enough to make racial gaps disappear (Fryer and Levitt, 2006a) . By the end of third grade, covariates do not capture the black-white test score gap (Fryer and Levitt, 2006a) ; and, indeed, the black-white test score gap increases by about 0.1 percent of a standard deviation per year that children are in school. This suggests that teachers\\' behavior may be a factor.\\nThe explanation may partly rely on the lack of minority teachers in elementary education: the fraction of minority teachers would have to (roughly) double to match the fraction of minority students. In this paper, I look at whether teachers give better subjective assessments to students of their own race and/or gender conditional on test scores. Subjective assessments are pervasive in schools: most teachers fill school records with comments on the child\\'s ability or behavior. Important decisions such as tracking, special education, and ability grouping are partly based on subjective assessments. Moreover, teachers\\' priors, beliefs, and behavior may be based on what other teachers have reported.\\nThe bottom part of Table I shows how fifth-grade teachers report their grading practices: 11% of white teachers declare they hold all children to the same standards; 19% of non-hispanic black and hispanic teachers provide the same answer. Male teachers too, more often declare holding all children to the same standards: 15% versus 12% for female teachers. Thus teachers\\' self-reported grading practices vary widely across race and gender. However, econometric work is needed to reveal teachers\\' actual grading practices.\\nI estimate the effect of being assessed by a teacher of the same race on assessments conditional on test scores. I use a unique U.S. longitudinal data set that combines test scores and teacher assessments of children\\'s skills in elementary education. I can thus compare the test scores and teacher assessments when a given child has a same-race teacher versus when the teacher is of a different race. I can also look at differences for a given teacher when assessing same-race children versus children of other races. Combining these two identification strategies, I estimate the effect of same-race and same-gender teaching on assessments conditional on test scores and on child and teacher fixed effects. This addresses three potential identification issues. First, children of different genders and races may behave differently in the classroom and during examinations; examples include differential effect of testing on boys and girls as well as effects arising from stereotype threats (Steele and Aronson, 1998) . Second, teacher assessments may capture skills that are not captured by test scores. Third, some teachers may give higher average assessments regardless of their students\\' race or gender, and this can be correlated with child characteristics.\\nThe data set is the Early Childhood Longitudinal Study, Kindergarten cohort of 1998-1999 (ECLS-K), collected by the National Center for Education Statistics of the U.S. Department of Education. It is the first large-scale U.S. study that follows a cohort of children from kindergarten entry to middle school.\\nHence this is the first paper to look at the discrepancy between test scores and teachers\\' perception of students\\' ability that uses a representative longitudinal sample of U.S. children in elementary education.\\nImportant findings are that teachers tend to give better assessments to children of their own race and ethnicity, but not significantly higher assessments to children of their own gender. Moreover, this result is mainly due to the lower grades given to non-hispanic black children and to hispanic children.\\nA number of robustness checks confirm the result of the baseline estimations. I test for endogenous mobility and allow for some correlation between race, gender, and pupil mobility. Furthermore, measurement error checks show that it would take a large amount of measurement error to otherwise explain results. The estimates are also robust to falsification checks in which test scores are regressed on assessments rather than these on teacher assessments. Finally, I show that even if relative ranking and de facto racial segregation could be a potential explanation, controlling for peers\\' test scores does not change the results.\\nThe analysis of this paper is related to Lavy (2004) . Lavy\\'s paper uses high school matriculation exams in Israel. Comparison of blind versus nonblind test scores showed that boys are likely to be overassessed in all subjects. Moreover, the size of the bias was highly sensitive to teachers\\' characteristics suggesting that teachers\\' behavior is causing grade discrimination. This paper differs from Lavy (2004) in at least three ways. First, I compare subjective assessments and test scores, where subjective assessments are based on classroom behavior and coursework; Lavy (2004) compares test scores of blind and nonblind examinations. Second, in Lavy (2004) , if tough teachers are more likely to grade boys then the effect of nonblind assessments on boys\\' test scores could be overestimated. I control for this effect in the ECLS-K by taking into account child and teacher fixed effects. This paper is also related to a small-scale experiment on fifth-grade teachers in the state of Missouri. Clifford and Walster (1973) sent teachers report cards that included child records randomly matched to photographs, and teachers were asked to assess child ability. The researchers found a significant effect of physical attractiveness on assessments but no effect of gender. Nevertheless, the study raises a number of issues. It is not clear whether this result on Missouri fifth-grade teachers is relevant to assessing discrimination in a representative U.S. classroom, since teachers were assessing students they did not know on the basis of randomly generated school records. The research reported here on the ECLS-K provides a large-scale analysis of teacher assessments in U.S. elementary education.\\nBetter teacher assessments may have beneficial or detrimental effects on performance. On the one hand, better assessments for the same ability level make it easier to get good grades and may therefore decrease the child\\'s marginal benefit of effort (cf. Coate and Loury, 1993) . On the other hand, better teacher expectations may raise student expectations or reflect greater investment in the child\\'s education.\\nThese stories can be told apart in a controlled experiment. The psychological and educational literature has debated the issue of the effect of teacher expectations at least since the Pygmalion experiment (Rosenthal and Jacobson, 1968) . In this experiment, children of an elementary school took a cognitive test at the beginning of the school year. The experimenters then selected 20% of the children and told the teachers that these children were showing \"unusual potential for intellectual growth\". Empirical results suggested that those labeled as bloomers had significantly higher IQ progress in first and second grade.\\nDiscrimination and the effect of discrimination cannot be jointly identified in the same data set.\\nIdentifying discrimination in grading by same-race or same-gender teachers requires a data set such as the ECLS-K, whereas identifying the effect of perceptions requires a controlled experiment. That is an important point, because it is tempting to go further and estimate the effect of perceptions and the amount of discrimination in the same dataset. Dee (2004) and Dee (2005b) show that being taught by a teacher of the same race or gender increases test scores. Empirical results from Project STAR\\'s experiment show that same-race teaching increases test scores for grade-1 to grade-3 children (Dee, 2004) . Other empirical results from the National Education Longitudinal Study show that same-gender teaching increases the test scores of eighth-grade children (Dee, 2005b) . This paper is different: it estimates the effect of same-race teaching on assessments conditional on test scores. That is, I examine whether teachers have incorrect perceptions of their students\\' ability -either overestimating or underestimating it. This leads to different policy implications, e.g. including or improving diversity training for teachers.\\nThe rest of the paper is structured as follows. Section 2 presents the Early Childhood Longitudinal\\nStudy. It provides a first-hand descriptive analysis of the difference between teacher assessments and test scores as well as some statistics on racial and gender diversity in U.S. elementary education. Section 3 explains main identification issues, the identification strategy, and baseline results. Section 4 checks the robustness of the results. Section 5 shows that assessment rankings are not affected by teacher-pupil racial interactions in the classroom and that relative ranking does not explain the main results. Finally, Section 6 concludes.',\n       nan,\n       \"Background: Multiple neurological disorders including Alzheimer's disease (AD), mesial temporal sclerosis, and mild traumatic brain injury manifest with volume loss on brain MRI. Subtle volume loss is particularly seen early in AD. While prior research has demonstrated the value of this additional information from quantitative neuroimaging, very few applications have been approved for clinical use. Here we describe a US FDA cleared software program, Neuroreader TM , for assessment of clinical hippocampal volume on brain MRI. Objective: To present the validation of hippocampal volumetrics on a clinical software program. Method: Subjects were drawn (n = 99) from the Alzheimer Disease Neuroimaging Initiative study. Volumetric brain MR imaging was acquired in both 1.5 T (n = 59) and 3.0 T (n = 40) scanners in participants with manual hippocampal segmentation. Fully automated hippocampal segmentation and measurement was done using a multiple atlas approach. The Dice Similarity Coefficient (DSC) measured the level of spatial overlap between Neuroreader TM and gold standard manual segmentation from 0 to 1 with 0 denoting no overlap and 1 representing complete agreement. DSC comparisons between 1.5 T and 3.0 T scanners were done using standard independent samples T-tests. Results: In the bilateral hippocampus, mean DSC was 0.87 with a range of 0.78-0.91 (right hippocampus) and 0.76-0.91 (left hippocampus). Automated segmentation agreement with manual segmentation was essentially equivalent at 1.5 T (DSC = 0.879) versus 3.0 T (DSC = 0.872).\\nThis work provides a description and validation of a software program that can be applied in measuring hippocampal volume, a biomarker that is frequently abnormal in AD and other neurological disorders.\",\n       \"Participants. Individuals from families with known ADAD mutations were recruited at 11 separate sites as part of the DIAN initiative. All participants with MRI, genetic, and clinical data that passed quality control procedures from the fourth semiannual data cutoff were included in the analysis. Of these participants, 13 with available MRI data were excluded owing to processing failures (n = 9) or excessive pathology (n = 4). After exclusions, a total of 92 NCs and 137 MCs were analyzed (Table 1) . A subset of these participants had analyzable PiB (NC = 84, MC = 116) and FDG (NC = 86, MC = 121) scans (13 PiB and 4 FDG scans were excluded for technical failures). A small number of participants also had serial MRI and PET imaging (Table S1 ). The institutional review board at Washington University provided supervisory review and human studies approval for all study procedures. Each participating institution also obtained local human studies approval. Approvals were obtained from Brown University, Butler Hospital, Columbia University, Edith Cowan University, University of Western Australia, Partners Human Research Committee, Indiana University, University of California Los Angeles, University of Melbourne, Melbourne Health, University of Pittsburgh, University of New South Wales, Neuroscience Research Australia, and University College of London. All participants or their caregivers provided written informed consent approved by their local institutional review board.\\nClinical Assessment. Each participant underwent an extensive clinical assessment including a medical history, family history of AD, and physical and neurological examinations. Dementia status was assessed using the CDR (74) . EYO was calculated as the difference between the participant's age at evaluation and the age at which parental cognitive decline began (12) . For the symptomatic MCs in this report, the parental and actual age of onsets were correlated (Pearson correlation coefficient = 0.65, P < 1.4·10\\n−07 ). The presence or absence of an ADAD mutation was determined using PCR-based amplification of the appropriate exon followed by Sanger sequencing (12) . Each participant's apolipoprotein E4 (APOE4) genotype was determined using methods described elsewhere (75) . Clinical evaluators were blind to participant mutation status. Unless medically indicated, research data were not shared with the participants.\\nMRI. Structural MRI acquisition was performed using the Alzheimer's Disease Neuroimaging Initiative (ADNI) protocol (76, 77) . All participating sites used a 3T scanner and were required to pass initial and regular follow-up quality control assessments to ensure acquisition uniformity. Scans acquired in each participant included accelerated 3D, sagittal T1-weighted images of the head (1.1-× 1.1-× 1.2-mm voxels, approximate scan time 5-6 min). These images were screened for artifacts and protocol compliance by the ADNI Imaging Core before further analysis. Volumetric segmentation and cortical surface reconstruction was performed using FreeSurfer 5.1 (78) . This procedure automatically segments subcortical and cortical structures using a probabilistic atlas. Each voxel in the brain was assigned an anatomical label (53, 79) . Segmented volumes were corrected for intracranial volume using an analysis of covariance approach (80) . For each vertex on the cortical surface, thickness was calculated as the shortest distance from the gray/white boundary to the gray/CSF boundary (81) . To facilitate group analyses, cortical thickness maps were registered to an average cortical surface and geodesically smoothed with a Gaussian 10-mm FWHM kernel. A trained rater visually verified segmentations and surfaces for accuracy. Manual edits were performed when necessary according to the FreeSurfer manual (http://surfer.nmr.mgh.harvard. edu/fswiki/). Each participant's PET data were motion-corrected and registered to his or her MRI (82, 83) . Differences owing to image resolution across scanners were minimized by smoothing PET images to a common resolution of 8 mm isotropic (84) . Using FreeSurfer ROIs, standardized uptake value ratios (SUVRs) were calculated using the brainstem as a reference region. The brainstem was used as the reference tissue as histological studies have found amyloid plaques in the cerebellar cortex of ADAD mutation carriers (85, 86) . The brainstem has been shown to be a reliable reference region for PiB (87) and FDG PET (88) . To minimize the impact of partial volume effects on the PET signal, an RSF-based approach (48) for partial volume correction was used for all regional PET measurements. Versions of Figs. 2-4 without PVC can be found in Supporting Information (Figs. 6S-S8 ). For vertex-wise group analyses, uncorrected SUVR images were sampled halfway between the pial and white surface, transformed to the average cortical surface, and smoothed with a surfaced-based Gaussian 5-mm FWHM kernel.\\nStatistical Analysis. Statistical analysis of all cross-sectional ROI and vertexwise measures was performed using the nlme package (89) in R (www. r-project.org) (90) . Carriers and noncarriers were tested for differences as a function of EYO to estimate regional differences in biomarker trajectories using a general linear mixed effects model. Histograms of the EYO distribution for each marker are shown in Fig. S5 . The statistical model included linear terms for mutation status, EYO, and the interaction between mutation status and EYO. Because some participants were recruited from the same family, a random effect of family was added to allow for possible correlations between family members. An analogous model consisting of fixed effects for region, EYO, and the interaction between EYO and region was used to estimate carrier differences in cross-sectional rates of change across brain regions. To account for baseline differences between regions, all regional values were first divided by the noncarrier mean for that region. A random effect for participant nested within family affiliation was used to account for possible correlations within participant as well as within family group. Both models included covariates for age, sex, education, and the presence of absence of an APOE4 allele. All analyses were corrected for multiple comparisons across space using the FDR.\\nTo estimate the trajectory of biomarker changes, curves were fit to the cross-sectional data using LOESS regression (91) . Movies of biomarker change as a function of EYO were also computed by fitting a LOESS curve for each vertex on the cortical surface (Movies S1-S3). For all surface-based analyses, a given participant's data were included only if they had a defined (nonzero) value at that vertex. Because there was a small amount of coverage loss throughout the study, a vertex was never analyzed with less than 96% of the available participants. The PySurfer library was used for visualization of all surface-based analyses (http://pysurfer.github.io/) (92) . All other visualizations were created using the ggplot2 package in R (93) .\\nThe quantity of longitudinal data precluded a full analysis across the full EYO range. Thus, participants were stratified into one of three groups based on their mutation and cognitive status. Any carrier with a CDR score greater than 0 at their first longitudinal visit was placed into the mutation positive, symptomatic group (MC/S). The remaining carriers were classified as asymptomatic (MC/AS). Finally, all of the noncarriers were grouped together as a control group (NC). To quantify and statistically evaluate the within participant imaging biomarker rate of change over time, general linear mixed models with random intercepts/slopes (94) at the subject level and random intercepts at the family level were used. All general linear mixed models in the longitudinal analysis were estimated using restricted maximum likelihood estimation, with the approximate F-test denominator degrees of freedom based on the method of Kenward and Roger (95) . The model included fixed effects for group membership, time from baseline assessment, and the interaction between group and time. In addition, an unstructured covariance model was used to allow the covariance in each biomarker to vary between MCs and NCs. All longitudinal analysis was performed using SAS version 9.3 (SAS Institute, Inc.).\",\n       'Continuous variables are expressed as means ± standard deviation and categorical data as absolute numbers and percentages. The normality of the data distributions was tested using the Kolmogorov-Smirnov test and differences were then evaluated using the t test for independent samples. Univariate analyses followed by a multivariate linear regression analysis with backward elimination were used to assess the associations between cardiovascular risk factors and carotid 18 F-florbetaben uptake ( mean TBR max ) [19, 20] . 18 F-florbetaben mean TBR max was treated as the response (dependent) variable and cardiovascular risk factors as the explanatory (independent) variables for the regression analysis. The explanatory variables included were male gender, age >65 years, BMI, diabetes, hypertension, history of stroke, and history of CAD. Following this, the ENTER regression was used to determine independent predictors of the response variables. All explanatory variables in the backward elimination model found to be significantly associated with the 18 F-florbetaben uptake value were retained and entered into the regression model as a block in a single step. This entry method was preferred over the forward selection of variables since only a few significant variables were left for a relatively low number of cases after excluding all the explanatory variables without a significant association with the different carotid wall 18 F-florbetaben uptake values. The results of the multiregression models are presented with the standardized regression coefficients (β), the 95% confidence intervals, and the p values indicating the statistical significance of the estimates. All statistical analyses were performed using SPSS version 16.0 (SPSS, Chicago, IL).',\n       \"Producing sufficient numbers of graduates who are prepared for science, technology, engineering, and mathematics (STEM) occupations has become a national priority in the United States. To attain this goal, some policymakers have targeted reducing STEM attrition in college, arguing that retaining more students in STEM fields in college is a low-cost, fast way to produce the STEM professionals that the nation needs (President's Council of Advisors on Science and Technology [PCAST] 2012). Within this context, this Statistical Analysis Report (SAR) presents an examination of students' attrition from STEM fields over the course of 6 years in college using data from the 2004/09 Beginning Postsecondary Students Longitudinal Study (BPS:04/09) and the associated 2009 Postsecondary Education Transcript Study (PETS:09). In this SAR, the term STEM attrition refers to enrollment choices that result in potential STEM graduates (i.e., undergraduates who declare a STEM major) moving away from STEM fields by switching majors to non-STEM fields or leaving postsecondary education before earning a degree or certificate. 1 The purpose of this study is to gain a better understanding of this attrition by: • determining rates of attrition from STEM and non-STEM fields; • identifying characteristics of students who leave STEM fields; • comparing the STEM coursetaking and performance of STEM leavers and persisters; and • examining the strength of various factors' associations with STEM attrition. Data from a cohort of students who started their postsecondary education in a bachelor's or associate's degree program in the 2003−04 academic year were used to examine students' movement into and out of STEM fields over the subsequent 6 years through 2009. Analyses were performed separately for beginning bachelor's and associate's degree students. For brevity, these two groups are frequently referred to as bachelor's or associate's degree students in this study. Selected findings from this SAR are described below.\",\n       \"Alzheimer's disease (AD) and its prodromal phase, Mild Cognitive Impairment (MCI), are the most common neurodegenerative diseases affecting elderly people. In the early stage of the disease, neural degeneration is subtle making it difficult to predict which MCI subjects will progress to AD (pMCI) and which Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). Hence, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data, but did not participate in analysis or writing of this report. ADNI investigators include (complete listing available at www.loni.ucla.edu/ADNI/Collaboration/ADNI Author ship list.pdf).\\nMCI subjects will remain stable (sMCI) during the follow up. Hereon, AD prediction, i.e., AD early detection will address the classification of MCI subjects into pMCI and sMCI subjects.\\nSeveral biomarkers have been proposed to achieve early AD diagnosis [1] . Among them, it has been established that measurements of brain atrophy extracted from structural MRI are valid markers of early stages of AD [2] . Therefore, automatic frameworks using MRI-based features have been developed to achieve computer-aided prognosis [3] [4] [5] . One part of these works focused on advanced machine learning techniques [6] while another part aimed to enhance the biomarker quality [4, 7] . Among them, patch-based methods [8, 9] demonstrated competitive AD prediction results. Despite these efforts, the current AD prognosis accuracy remains around 70%, that suggests the limitation of using (i) traditional features with advanced learning processes or (ii) high quality features with basic machine learning methods. In this paper, we propose to combine high quality biomarkers with advanced learning method to improve AD prediction accuracy.\\nTo this end, we first propose to extend the patch-based scoring method proposed in [8] . In this approach, the anatomical pattern similarity is estimated between the MCI test subject and two training populations (i.e., Cognitively Normal (CN) and AD) using a non-local patch-based scoring method. For each voxel, a score (i.e., a grade) that measures the proximity to both training populations is computed. In [8] , the a priori ROI-based strategy focused mainly on hippocampus and may discard other possible informative anatomical regions. To overcome this limitation, we propose to score the whole gray matter (GM). Moreover, to be more robust to intensity normalization discrepancies between MRI, probabilities are used in place of intensities during patch comparison. Finally, while a local patch-based strategy is used in [9] , a non-local approach is privileged to better handle inter-subject variability and registration error [8] .\\nAfterwards, an ensemble learning method [10] is considered to efficiently use the estimated advanced biomarkers. Since the scoring value, assigned to each voxel of the GM, estimates the proximity to AD and CN, it can be viewed as the posterior probability of a weak classifier. Combined together, these weak classifiers form an ensemble that can be used to classify subjects [11] . As noticed in [2] , it appears that AD-related brain alterations are mainly a region-by-region process. Hence, we propose to further use this clinical knowledge to create atlasbased anatomical sub-ensembles of weak classifiers before fusing them into intermediate classifiers. Finally, to discard brain areas that may not be related to AD, we propose to select the most relevant anatomical sub-ensembles using a Sparse Logistic Regression (SLR).\\nIn this work, the contributions are threefold: (i) unlike ROI-based approach, non-local scoring values are estimated over the whole GM and considered as weak classifiers; (ii) an advanced ensemble learning technique is used to fuse these weak classifiers into anatomical sub-ensembles; and (iii) a sparse approximation is used to efficiently select and weight the most relevant anatomical sub-ensembles. 2 Materials and Methods\",\n       nan,\n       \"The analysis began with several oneand two-sample tests of means and proportions between medical schools and other institutions, and between MDs and PhDs. Throughout the analysis, we grouped MD/PhDs with MDs because there were no significant differences in outcomes between these two groups.\\nAlthough most MDs in our sample worked at medical schools, a substantial number worked at other institutions (half at hospitals and the remainder at research institutions, higher education institutions, and other organizations).\\nNext, we used probit models to estimate the probability of an MD investigator's application receiving an R01 award conditional on the applicant's race/ ethnicity and the explanatory variables.\\nIn place of reporting probit coefficients, we report the marginal effect of the variable on the award probability, which is the change in the award probability due to each predictor separately when other variables are evaluated at their mean values. In the case of applicant race/ethnicity, the marginal effects can be interpreted as the percentage point difference in R01 award probability between applications from white investigators (the omitted reference category in the regression) and applications from investigators of a given race/ethnicity. We used heteroskedasticity-robust standard errors that were clustered on the individual applicant to adjust for the fact that applicants could (and did) submit more than one R01 application in the study sample. The resulting regression estimates are correlations between the covariate and the probability that an application will receive an R01 award, and should not be interpreted as having a causal impact. \",\n       \"Finally, in Table 3 , we compare our results with six other state-of-the-art methods that employ deep learning on ADNI. As we can see, our method with 32 images per subject significantly outperforms the state-of-the-art. Especially for the difficult AD vs. MCI and MCI vs. NC problems, we provide a 4% (over 3D CNN [14] ) and a 7% (over Autoencoder + 3D CNN [11] ) increase of accuracy over the state-of-theart, respectively. On average, our method provides a 4.5% increase of accuracy over the state-of-the-art (3D CNN [14] ). Since the accuracy values for existing methods were already reaching 90% and above, such level of increase by our method is significant considering the critical nature of the application in hand.\\nWhat makes our method more appealing is the small number of training images required. As we can see, by using our entropy-based image selection approach, we have significantly cut down the size of the training dataset. For all these methods, the training size was calculated based on the reported sample size and the cross-validation/training-testing split (e.g. in case of our 32 images per subject set, there are total 32 × 100 = 3200 images for a classification problem; a 5-fold cross-validation/ 80% -20% split therefore results in a training size of 2,560). The closest among the existing methods is the Stacked Autoencoder method [12] , which uses 21,726 training images. Using transfer learning, we have reduced the size of training data approximately 10 times. If we consider our 16/sub. method, the training size reduction is even more substantial (almost 20 times) with a slight reduction in accuracy; still superior than most of the existing methods. For a computer-aided diagnosis system this is very important. To be practical and usable in a real clinical setting, the dependence on a large training set is a problem, since physician annotated data may not be available/expensive to acquire. We believe the utilization of transfer learning with our intelligent training data selection process can be applied to other computer-aided diagnosis problems as well due to generic nature of the framework. To further validate our proposed architecture, in Table 4 , we report 3-way classification results on the same dataset. Methods that do not report 3-way results have been omitted from this table. The same architecture as in Figure 2 was used, the only change was the final classification layer, which was modified to be able to perform 3-way classification. The same 5-fold cross validation with an 80%-20% training-testing split was utilized for 3-way classification. As can be seen, even for 3-way classification, we achieve state-of-the-art results when compared to existing approaches, proving the effectiveness of our method.\\nWe also provide a deeper analysis of what information our proposed network is actually extracting from the MRI slices to arrive at a decision. Such form of analysis can provide an interpretable explanation of the proposed model, rather than a mere yes/no decision, which is important for a computeraided diagnosis system to be trustworthy. To achieve that, we present the popular Class Activation Map (CAM) [33] for two example query images. CAM is generated from a query image by mapping the convolutional feature activations to a heat map that shows the specific discriminative regions in the query image that the network focused on to arrive at a decision. CAM can be generated by projecting the weights of the output layer back to the convolutional feature maps. Further details regarding generation of CAM can be found in [33] . To generate the CAM results, we employed the 3-way classification model explained before. Figure 9 shows the CAM results overlayed on top of two query images. We intentionally picked two query images that result in accurate and inaccurate diagnosis, respectively. For the query image in, Figure 9 (a), the decision by the network matched with the ground truth (correct decision), while for the one in Figure 9 (b), the network's prediction was incorrect. In the CAM heatmap, the more red a region is, the higher attention it received from the model. These results reveal some interesting aspects of the proposed model. For the correct prediction (Figure 9(a) ), we see that the regions containing Gray Matter (GM) and Cerebral Spinal Fluid (CSF) received more attention from the network. This aligns with the neuropathology of Alzheimer's diagnosis. It is known that Alzheimer's results in significant atrophy in the GM regions, with an increased amount of CSF [34] , [10] . Indeed, our network is focusing on those regions to arrive at the correct decision. However, for the incorrect prediction (Figure 9(b) ), we see that the background to scan transition regions received more attention, likely due to the poor contrast in the MRI itself in this particular slice. This tells us that the network failed to provide a correct diagnosis due to its inability to extract accurate features. If a doctor was presented with an interpretable output like this, they will instantly be able to tell why exactly the proposed model failed, making the overall framework more trustworthy.\",\n       'All genomic DNA samples for Pfizer and Genizon were extracted from blood and quantified using Picogreen (Invitrogen Inc). The first batch of Pfizer samples (,300 cases from PrecisionMed/A3041005 and matched controls plus 489 cases from LEADe) were processed with the Illumina HumanHap550 array while all remaining samples were genotyped using the Illumina 610Quad array. All genotyping was performed at Genizon Biosciences Inc and genotype calls were generated after clustering all the data within each platform. Most of LEADe samples were processed on both 550 and 610 platforms and the genotype data concordance rates were greater than 99.99%. The ADNI genetic data set was downloaded from the ADNI web site and a similar initial QC process was performed at Pfizer (the final data set after QC includes 509376 markers in 719 subjects). The GenADA data was downloaded from dbGap and the data were imputed based on the reference haplotypes from Hapmap III using Mach [42] [43] . Genotype data from the Genizon samples were obtained from Illumina HumanHap 550 array.',\n       'The objective of the current study is to explore how principal support and cooperation among teachers may be associated with teacher job satisfaction. We were interested in determining whether ToCs, compared to White teachers in public schools, had lower levels of job satisfaction and if possible differences in satisfaction were associated with the perception of principal support and the perception of teacher cooperation. Furthermore, due to the importance of retaining ToCs, we addressed a gap in the literature by investigating whether the race/ethnicity of a teacher was moderated by the perception of principal and colleague support on job satisfaction (Dee, 2005; Egalite et al., 2015; King, 1993 , Lindsay & Hart, 2017 Grissom & Redding, 2016) . We also investigated the divergent evidence on whether teacher race is associated with job satisfaction, (Culver et al., 1990; Renzulli et al., 2007) , the positive effects of principal support (Brown & Wynn, 2009; Roberson & Roberson, 2009) , and the contradictory data regarding the benefits of teacher cooperation (Bickmore, 2013; Johnson, 2003; Madiha, 2011; Woods & Weasmer, 2004) .\\nPrior studies have discussed teacher satisfaction as applied to principal support and collegial cooperation individually (Brown & Wynn, 2009; Sass et al., 2011; Shen et al., 2012) . However, no study has used a nationally representative dataset to analyze the moderation effects between perceived principal support and teacher race, and perceived colleague cooperation and teacher race while controlling for contextual and individual-level variables. Given the limited number of studies addressing these issues, a nationally representative dataset was used to answer the following research questions:\\n1. Do ToCs differ in their levels of job satisfaction when compared with White teachers? 2. Are perceptions of principal support and perceived colleague cooperation associated with teacher satisfaction? 3. Is the association of job satisfaction and teacher race/ethnicity moderated by the perception of principal support and colleague cooperation?',\n       'Initial studies and region-specific analyses Our initial analysis of postdoc salary data for 13,079 postdocs across 52 institutions, estimated at 15-30% of the total U.S. postdoc workforce, is summarized in Table 1 . The number of science, engineering and health postdocs in the NSF\\'s 2015 GSS data (National Science Foundation, n.d.) was used as an estimate of the approximate number of total postdoc salaries which we should have received from each institution. Salaries below the $23,660 legal minimum are assumed to be reporting errors (such as postdocs paid on direct fellowships where salary was provided directly from the funder, bypassing the institution\\'s payroll system), and were excluded from the subsequent analysis in Table 1 . Institutional reporting of postdoc salaries was variable with respect to this issue. However, we independently verified that some institutions, such as the University of Washington, provided their actual annual postdoc salaries.\\nThe average, median, minimum and maximum salaries for each institution are listed in Table 1 , the most useful data for comparisons between institutions being the median salary. The lowest median salary calculated from this dataset was University of Illinois Urbana-Champaign at $27,515, and the highest was University of Maryland College Park at $56,000. However the number of salaries received from the University of Illinois was much lower than expected, and the data at the lower end of the range could be an underestimate of the actual values due to differences in salary reporting, rather than actual lower salary values. Indeed, at time of writing, we had just received updated data from the University of Illinois which is currently being analyzed.\\nFollowing this initial analysis, we separated the aggregate data from 51 institutions into geographic regions (\"Geography Atlas -Regions -Geography -U.S. Census Bureau\", n.d.) to show the distribution of postdoc numbers by institution that were grouped by U.S. region ( Figure 1 ). The majority of institutions used in data collection were from the South and Midwest (20 and 16 institutions respectively). There was no difference in salary earned overall by postdocs between different regions ( Supplementary Figure 1 ). The number of postdocs in the NSF\\'s GSS 2015 data is used as a comparison of the expected number of postdoc salaries that should have been received, to give us an indication of how much data may be missing from an institution. The number of postdoc salaries received that are greater than $23,660 are indicated and emboldened where they differ from the number of salaries received. This data was then used to calculate the average, median, maximum and minimum values, and to calculate the total cost to the institution of bringing all salaries up to the FLSA threshold salary that was proposed for December 1st 2016.',\n       'The total water footprint of economic production in the United States is 7.30 3 10 11 m 3 per year. Of this, 6.03 3 10 11 m 3 is from rainfall used to grow crops for food, feed, and fuel. Surface water (6.68 3 10 10 m 3 ) and groundwater (6.11 3 10 10 m 3 ) are valuable inputs in the production of irrigated agriculture, as well as every other economic sector. The volume of blue water used in national economic production is roughly 15 times less than the U.S. total average annual runoff (1.96 3 10 12 m 3 ; USGS, 2017) and only two-thirds the national water supply and storage capacity (1.94 3 10 11 m 3 ; U.S. Army Corps of Engineers, 2017). The entire consumption of blue water resources represents roughly one-third of withdrawals. These statistics suggest an abundance of national water resources. However, these lumped values mask localized water issues and stresses.\\nCrop production comprises the vast majority (95.4%) of total WFP. Green water comprises 86.5% of all consumptive crop water use, while surface and groundwater contribute 5.9% and 7.6%, respectively. Roughly 84% of U.S. harvested crop area is strictly rainfed, with most of this cropland dedicated to corn, soybeans, wheat, and hay and haylage, grown in the Midwest and High Plains. Corn grain and silage, hay and haylage, rice, wheat, soybeans, cotton, and almonds are among the largest irrigation users. Together, these seven crops are responsible for 75% of national groundwater consumption and 47% of national surface water consumption. Figure 1 shows the crops with the largest WFP by water source. Note the importance of rainfall on irrigated agricultural lands. Figure 2 maps crop water footprints by water source.\\nNoncrop economic sectors contribute 13.3% and 38.4% to ground and surface WFP, respectively. Figure 3 illustrates that most (1.59 3 10 10 m 3 , or 61.7%) of the annual noncrop surface WFP is due to evaporative losses from hydropower reservoirs and nonrevenue water losses in municipal distribution systems. As noted earlier, significant uncertainity exist in estimates of both sectors depending on what fraction of water losses is considered consumptive use and how water is allocated among multiple uses. Thermoelectric power generation, though the sector with the largest withdrawals, consumes the third most surface water and seventh most groundwater amongst noncrop sectors. This is because 97.5% of thermoelectric freshwater withdrawals correspond to power plants that employ once-through cooling systems, which typically consume only 1-3% of withdrawals. Figure 4 illustrates the WFP for thermoelectric power generation for each U.S. county. The direct water consumption for animal husbandry is 2.59 3 10 9 m 3 annually, with nearly half of all livestock water use occurring in the Great Plains and California (see Figure 5 ). Beef cattle are responsible for 56.0% of the water footprint of livestock production.\\nThe WFP of manufacturing and service sectors is 2.82 3 10 9 m 3 and 2.32 3 10 9 m 3 per year, respectively.\\nThe manufacturing sector relies less on groundwater than service industries, with only 21.6% of its WFP coming from groundwater sources, compared to 37.2% from the service sector. The top five water consumers are primary metal manufacturing, food manufacturing, chemical manufacturing, wholesale trade, and food and beverage stores. Whereas the first three manufacturing sectors can attribute their large WFP primarily to their high WFU $ , the two service sectors have modest WFU $ but their large WFP is due to the sheer size of their economic production within the U.S. economy.',\n       \"To test for regional differences across the whole brain, we performed a paired two-sample Student's t-test at every voxel in the brain, comparing volumetric tissue change in the group of subjects with both accelerated and nonaccelerated scans at 6 and 12 months. To avoid basing inferences on differences that would arise by chance when assessing a large number of voxels, a standard false discovery rate (FDR) correction was applied at the conventionally accepted level of 5% (q=0.05) (Benjamini et al., 1995) . For voxels that were significantly different between scan types, voxel-wise average Jacobian values were calculated for each of the groups separately. Mean difference maps were computed by subtracting the nonaccelerated mean map from the accelerated mean maps. The mean difference map was projected on to clusters of voxels that passed FDR correction allowing for visual identification of which group had higher or lower mean Jacobian values (i.e., apparently faster or slower atrophy) for a given significant cluster.\",\n       \"The ''Effect'' of Lessons in Musical Arts on Child Development Outcomes Table 3a shows the propensity score weighted estimate of the effect of lessons on key outcomes observed in the 2007 wave of the Child Development Survey. None of the effects are statistically significant-none Table 3a Effects of are more than 1.65 standard errors from zero. The lack of statistical significance reflects both the small practical magnitude of the relationships as well as the loss of statistical precision. The latter refers to the fact that the standard errors are larger than those in unadjusted analyses. Incorporating the propensity score weights inflated standard errors generally by about 50%. All in all, assuming exchangeability and including the broader array of covariates, lessons in musical arts do not influence future child development outcomes. Returning to Figure 2 , one can see the importance of adjustment for confounding-the figure shows the effect size for the adjusted and unadjusted between-group differences. Two things are striking about the figure. First, the unadjusted differences were consistent yet small. Cohen (1988) defined effect sizes of no larger than .20 SDs as ''small''; otherwise less than .50 as ''medium''. Secondly, there remain no differences between musical arts participants and nonparticipants after adjusting for observed confounding (i.e., using the IPT weights).\",\n       'China\\'s extensive coastline exposes the country to frequent typhoon strikes and inundations from storm surges. This irregular coastline is one of the longest in the world, measuring a distance of greater than 18,000 km on the mainland and more than 32,000 km when including the coastline of numerous islands (Han et al. 1995). During the 50year period from 1952-2001, China observed an average of seven typhoon strikes per year and a maximum of 12 typhoon strikes in the most active years (Xuejie et al. 2002). China\\'s National Maritime Bureau has provided statistics on the frequency of storm surge events along the coast of China. During the 50-year period from 1949-1998, storm surges exceeding 1m were observed 270 times, surges exceeding 2m were experienced 48 times, while surges greater than 3 m occurred 15 times ). These statistics reveal that surges frequently strike China, with an annual rate of 5.4 surges exceeding 1m, 0.96 surges greater than 2m, and 0.3 surges exceeding 3m. Scientific literature in English or in Chinese with English abstracts or titles provided coastal flooding observations for 76 historical typhoons that have struck China. These records are comprised of both storm surge and storm tide observations. The body of many articles was written in Chinese, which was translated to English. Fortunately, the terms \"storm surge\" and \"tide\" often translated directly from Chinese to English, however, a common term for storm surge is translated directly into English as \"highest water rising.\" The highest storm surge level observed in China occurred in July, 1980, as Typhoon No. 8007 generated a storm surge of 5.94 m at Nandu Tide Gauge in the city of Leizhou, Guangdong Province (Liu and Wang 1989;Ma 2003, Zhang 2009. This tide gauge has consistently observed high storm surge levels, as four of the six highest water levels observed in coastal flooding events in China were recorded at this location, according to available scientific literature. These storm surge observations include a 3.52m surge produced by Typhoon No. 8616 in 1986(Le 2002, a 3.84-m surge produced by Typhoon No. 9111 in 1991 (Le 2002), and a 3.66-m surge generated by Typhoon No. 0312 in 2003 (Ma 2004). A devastating storm surge that did not peak at Nandu Tide Gauge occurred in 1956, when Typhoon No. 5612 generated a storm surge that exceeded 5 m along the shore of Hangzhou Bay, east of Hangzhou, Zhejiang Province (Wang et al. 1991;Le 2002). Although an English literature review does not capture data for the majority of the storm surge events that have struck China, various peer-reviewed journal articles summarize the highest observed water levels by location or the peak water level produced by different typhoons (Le 2002;Lui 2002;Ma 2003;Zhang 2009). These summaries ensure that the largest storm surges in the history of China are documented in the scientific literature.\\nThe most deadly typhoons to impact coastal East Asia during the twentieth century both struck China, killing 50,000 people in 1912 and 60,000 people in 1922 (National Oceanic and Atmospheric Administration 1999). Although the number of deaths caused by storm surge in these events is not available, several sources provide evidence that the 1922 typhoon generated a storm surge that devastated the region around the city of Swatow, China. A New York Times article (Anonymous 1922a) reports that the typhoon generated a destructive storm surge in this region, while the Monthly Weather Review (Anonymous 1922b, pg. 435) recounts that a storm surge accompanied the storm, and that, \"houses that escaped being blown down were washed away by the waters which spread over the whole country side, and the loss of life was enormous.\" Since 1950, the most deadly storm surge events in China have killed between 1,000 and 2,000 people. Typhoon 6903 was the deadliest recent surge event in Guangdong Province, killing 1,554 people in 1969 (Zhang 2009). Typhoon Fred (Typhoon 9417) generated the most fatal surge in Zhejian Province, killing 1,216 people in 1994 (Le 2000). Some sources indicate that typhoons killed 300,000 people in China in the 1880s (Frank and Husain 1971;, however, considerable uncertainty remains regarding the timing of this event and magnitude of fatalities. Frank and Husain (1971) state that the Chinese typhoon occurred in 1881,  provides the year as 1886, and Gunn (2008) reports that an intense typhoon struck Vietnam in 1881. Bankoff (2003) indicates that a major typhoon struck Indo-China in 1881, but provides a total of 20,000 fatalities. While these sources suggest a catastrophic typhoon struck Southeast Asia sometime in the 1880s, these conflicting accounts cast doubt upon the details, specifically, the extraordinary number of deaths-five times the next highest fatality total in this region. Storm surges cause more economic loss than any other marine hazard in China (Le 2000). The surge generated by Typhoon 9615 in 1996 inflicted the most severe economic loss of any storm surge in Guangdong Province, totaling approximately 129 billion Yuan, or 20 billion U.S. dollars (Zhang 2009). Typhoon Winnie in 1997 caused 33.7 billion Yuan or approximately 5.5 billion U.S. dollars in economic loss (Le 2000), while Typhoon Fred in 1994 inflicted damage totaling 17.8 billion Yuan or nearly 3 billion U.S. dollars in Zhejian Province (Le 2000). The high population density along the coast of China increases the destructive potential for storm surges, as the 1-m coastal flood plain contains approximately 73 million people (Han et al. 1995). Past storm surge events in China have therefore caused considerable damage. For example, Typhoon Fred ruined 520 km of seawalls, flooded 189 towns and inundated more than 22 million people (Le 2000), even though the maximum surge level in this event was only 2.69 m (Le 2002). Storm surges in China also inflict severe damage on several economic sectors, including agriculture and the energy industry. Typhoon No. 9615 in 1996, for example, flooded 44,400 km 2 of farmland in Guangdong Province, the most of any surge event in the history of that province (Zhang 2009). Storm surges also threaten the onshore and offshore energy industry, including coastal nuclear and thermal power plants (Fengshu and Xinian 1989). Typhoon No. 9711 in 1997 shut down the tidal power station at Beishakou and damaged infrastructure on the Shengil oil field (Le 2000).',\n       \"Alzheimer's disease (AD) is an irreversible neuropsychiatric disorder, which often occurs in elderly and manifests clinically as memory deterioration, aphasia, social difficulties, and other symptoms (Morello et al., 2017; Tavana et al., 2018; Bregman et al., 2019) . The mortality rate of AD is high and is rising every year compared with other brain diseases (Association, 2017; Association, 2018) . This disease affects approximately 36 million people throughout the world with the incidence anticipated to triple by 2050 (Neville et al., 2015) . Moreover, the complications caused by AD also make the patient miserable (Association, 2015) . In the later stages of the disease, the patient not only needs to carry the costs of expensive cost treatment, but can also not take care of themselves and are therefore completely dependent on caregivers, placing a heavy burden on their families and society (Association, 2016) . Early diagnosis of AD can delay the disease development and improve therapeutic effects, therefore, a diagnosis study of AD is urgent.\\nWith the rapid development of neuroimaging technology, MRI technology has provided powerful support in AD research and has become an indispensable tool (Teipel et al., 2015) . Since a significant improvement in the level of modern medical technology, it has been found that the causes of AD may involve many aspects in clinical research, including the brain region and gene abnormities (Heneka et al., 2015; Zhang et al., 2015; Olsson et al., 2016) . As a result, the multimodal data fusion research of this disease is gradually becoming an emerging field, attracting widespread attention from researchers. At present, multimodal research can explore multiple potential pathogeneses of brain diseases and provided full details on the complementary advantages of information among various data points (Ning et al., 2018; Zhang et al., 2018b) . For instance, Varol et al. (2017) presented the heterogeneity through discriminative analysis (HYDRA) method to classify diseased and healthy subjects through neuroimaging and genetic data of brain diseases. The results demonstrated that the two groups of subjects could be distinguished accurately. As the diverse modal data of each disease possesses different characteristics, it is a key point in exploring the problem of multi-factor pathogenesis of brain diseases to design a reasonable scheme according to these characteristics, which is also a hot topic worthy of sustained attention by researchers.\\nMoreover, owing to the various reasons, there are a few credible public databases for Alzheimer's disease with multiple modal data such as neuroimaging, genes, proteins, and others, resulting in a small amount of available data (Zhang et al., 2018c) . In addition, the data dimension is much higher than the unimodal in multimodal research (Zhang et al., 2018a; Peng et al., 2019; Zhang et al., 2019) . These issues cause traditional methods to bottleneck with regard to data processing and analysis. However, machine learning methods have a strong adaptability in the processing of small sample data and highdimensional data, which can already be maturely applied in many studies (Dai and Xu, 2013; Jiang et al., 2018; Komiske et al., 2018) . For instance, Zhang et al. (2015) put forward a computer-aided diagnosis system based on machine learning for the classification of AD, reaching an average accuracy of 92.36%. Sun et al. (2018) proposed a novel machine learning based on the support of a vector machine to classify AD and NC, with an accuracy of 89.3%. These studies showed that machine learning achieved significant results in the field of discriminant analyses in brain science research, but those still include unilateral research such as data fusion, classification or feature extraction. Accordingly, the focus of this paper is on how to combine multimodal data of AD to design a complete brain science data analysis framework of data fusion, classification, feature extraction, and searching for lesions and disease-causing genes for the early diagnosis of this disease.\\nTo better address the above problems, this paper will study multimodal data of neuroimaging and gene data of Alzheimer's disease, and design a complete framework to realize classification, feature extraction and disease-causing factor extraction. Specifically, we first designed the fusion scheme of neuroimaging and genetic data to construct fusion features, then proposed a multimodal random forest (MRF) method to distinguish AD from NC to extract optimal fusion features. We further extracted abnormal brain regions and genes based on the optimal features. Compared to other methods, our framework was able to extract fusion features with higher differentiation ability and obtained higher classification accuracy, providing good insight into the causes of Alzheimer's disease and a new solution for the early diagnosis of this disease. The Materials and Methods describes the experimental data and the framework for data fusion, classification, feature extraction, and pathogenic factor extraction. The Results shows the results of the experiment and performance comparison. The Discussion discusses the experimental results and the Conclusion summarizes this study.\",\n       'Our study aims to recover the missing values in ARMS dataset and decide the distribution for each commodity. We apply a matrix completion approach to investigate such problem. We present the proof that there should be a unique solution to recover the target matrix when the condition about minimal sampling is achieved. We show that the Singular Value Threshold (SVT) algorithm will lead to a converged solution matrix at last. Theoretically, we can recover ARMS dataset with a large proportion of missing values based on previous work. Table 1 shows that recovery accuracy for most missing proportions is around 52%. It is quite desirable to us. The proportion of recover effect values is increasing with the expansion of confidence interval. The proportion of those values below the accuracy threshold is always larger the proportion for values above the threshold. After applying kernel density estimation, we conclude that the sample elements of one column from recovered matrix are distributed as normal distribution, which is corresponding with our column generation process. It is reasonable for us to apply the matrix completion approach to ARMS data matrix. After we recover all missing values in our target matrix and calculate the mean of net return for each commodity, we know farmers will make money when they are growing corn, soybean, wheat and cotton. It is the most profitable way for farmers to raise cattle based on the result in Table 2. Farmers will lose money when they are raising hogs due to their smaller scale compared to the large corporation. Government should not encourage farmers with small scale to raise hogs and subsidy farmers when they are raising hogs. Since large corporations on hogs make a huge amount of money, government should consider to adjust the tax policy on these companies. We apply Kullback-Leibler divergence to approximate the recovered net-return values of each commodity to some theoretical distributions. When we try to depict the trends of net returns for corn, soybean, hog, cattle and cotton, it is reasonable for us to apply normal distributions with different parameters based on values in Table 3. We can apply log-normal distribution to capture changes on trend of wheat net return.   Cai, J. F., et al., 2009) If ( * , * ) is primal-dual optimal pair for equation (12), by optimality condition, we can derive satisfying that for some ∈ γ ( ) and some * ∈ γ ( * ). Then, deduces that ( − * ) − Ω ( −1 − * ) = 0 and equations obeys that Based on the observation, there should exist Ω * = Ω , ‖ Ω ( − * )‖ = ‖ Ω ( −1 − * ) + Ω ( * − )‖ . If we assume = ‖ Ω ( − * )‖ , we can get 2 = −1 2 − 2 〈 − * , Ω ( −1 − * )〉 + 2 ‖ Ω ( * − )‖ 2 ≤ −1 2 − 2 ‖ − * ‖ 2 + 2 ‖ − * ‖ 2 For any matrix , ‖ Ω ( )‖ ≤ ‖ ‖ . Under our assumptions about the size of , we have − 2 ≥ for all ≥ 1 and some > 0 and thus 2 ≤ −1 2 − 2 ‖ − * ‖ 2 . Proof for lemma 2 (Cai, J. F., et al., 2009) Properties held by the projection 0 of one point onto a convex set C is where C = ℝ + = { ∈ ℝ : ≥ }. Because 0 ≥ 0, 〈 − 0 , − 0 〉 ≤ 0, ∀ ≥ 0. * is dual optimal, then we derive ( * , * ) ≥ ( * , ), ∀ ≥ 0. By substituting the expression in Lagrangian form, 〈 − * , ( * )〉 ≤ 0, ∀ ≥ 0, which is equivalent to 〈 − * , * + ( * ) − * 〉 ≤ 0, ∀ ≥ 0, ∀ ≥ 0. Thus, it is reasonable to conclude that * must be the projection of * + ( * ) onto the nonnegative orthant ℝ + . Proof for Lemma 3 (Cai, J. F., et al., 2009) If ( * , * ) are primal-dual optimal pair for the equation 16. Based on the optimality condition, we know for all : 〈 , − 〉 + 〈 −1 , ( ) − ( )〉 ≥ 0 〈 , − * 〉 + 〈 * , ( ) − ( * )〉 ≥ 0 for some ∈ ( ) and some * ∈ ( * ). As these two inequalities are nearly the same, we only need to prove one of them. For the first inequality, minimizes ( , −1 ) over all and therefore, ∈ ( ) and ∈ ( ), 1 ≤ ≤ , such that: Any projections onto a convex set ℝ + is a contracting projection. Therefore, ‖ − * ‖ 2 = ‖ −1 − * ‖ 2 + 2 〈 −1 − * , ( ) − ( * )〉 We replace ( ) by for short. By assuming the size of , we have 2 − ≥ for all ≥ 1 and some > 0. Finally, we get ‖ − * ‖ 2 ≤ ‖ −1 − * ‖ 2 − ‖ − * ‖ , and the conclusion as stated in Lemma 3.',\n       \"Hyperspectral sensors have been primarily available on aircraft (Fearns, Klonowski, Babcock, England, & Phillips, 2011; Lesser & Mobley, 2007; Li, Ustin, & Lay, 2005; Ozesmi & Bauer, 2002; Rosso, Ustin, & Hastings, 2005) . However, to map and monitor wetlands globally, spaceborne platforms are required, and it is ideal if the data are available to public at no cost. To date, only three hyperspectral imagers have been placed in orbit. All three were intended to be low cost, prototype sensors that were developed as a proof-of-concept with on-demand image acquisition of selected sites on the globe. None were designed to provide global wetland coverage on a regular basis.\\nIn 2000, the National Aeronautics and Space Agency (NASA) of the USA launched the first spaceborne hyperspectral sensor, Hyperion aboard the Earth Observatory 1 (EO-1), with 220 contiguous spectral bands. Images over selected sites covered a narrow strip (7.5 km by 200 km) with 34 m spatial resolution. At the time of writing this paper, NASA extended the mission one more year. However, it is highly unlikely that Hyperion will be in orbit during any date chosen for a HyspIRI launch. In 2001, the European Space Agency (ESA) launched the Compact High Resolution Imaging Spectrometer (CHRIS), aboard the Project for On-Board Autonomy 1 (Proba-1). CHRIS/Proba is a programmable sensor that can acquire 13 km × 13 km images of specified locations at five viewing angles. The sensor has built-in modes for acquiring these images with 17 m to 34 m spatial resolution, for 18 to 62 bands over a spectral range from 400 nm to 1050 nm (Barnsley, Settle, Cutter, Lobb, & Teston, 2004) . The USA Naval Research Laboratory later built the Hyperspectral Imager for the Coastal Ocean (HICO), which was situated on the International Space Station (ISS) in 2009. HICO image dimensions were 42 km × 192 km, at about 90 m spatial resolution, for 128 bands from 352 nm to 1080 nm. Because HICO was designed to observe coastal waters, its radiometric performance was superior to Hyperion and CHRIS/Proba in sensitivity and SNR. Unlike Hyperion and CHRIS/Proba, HICO could not acquire imagery above 45°latitude because of the ISS orbital characteristics. Unfortunately, HICO operations ended due to an Xclass solar storm in September 2014 (see http://hico.coas.oregonstate. edu/).\\nIn the next few years, Italy plans to launch the PRecursore IperSpettrale della Missione Applicativa (PRISMA), which is a push broom imager with 30 m spatial resolution and a 30 km swath width. PRISMA has over two hundred bands spanning 400 nm to 2500 nm. Germany plans to launch its Environmental Mapping and Analysis Program (EnMap), which is designed to acquire a 30 km × 30 km image at 30 m spatial resolution, with 245 bands going from 420 nm to 2450 nm.\\nAfter Hyperion and HICO, the only hyperspectral missions planned by the USA include the Hyperspectral Infrared Imager (HyspIRI), the Pre-Aerosol, Clouds and Ecosystems (PACE) mission, and the Geostationary Coastal and Air Pollution Events (GeoCAPE) mission. HyspIRI will include an imaging spectrometer measuring from the visible to shortwave infrared wavelengths in 10 nm contiguous bands, as well as a multispectral imager measuring from 3 μm to 12 μm in the midwave and thermal infrared wavelengths. Details regarding the mission and sensor characteristics are given in Table 1 and a comparison of the HyspIRI instrument to other existing and future sensors can be found in Table 2 .\\nBoth PRISMA and EnMap have capabilities that would be useful to study specific marsh systems, but neither are intended to produce global or synoptic maps and neither have thermal bands. Thus, unlike HyspIRI, these missions cannot be expected to support global observations of tidal wetlands (e.g., to understand the effects of global climate change), nor can they provide as extensive suite of contemporaneous data products that HyspIRI offers, such as soil moisture or evapotranspiration, as discussed later in this paper. Also, the greater temporal resolution of PACE and GeoCAPE could certainly complement HyspIRI observations. However, the respective spatial resolutions for PACE and GeoCAPE of 1000 and 250 m are far less capable of observing wetland structure. Also, only HyspIRI offers thermal bands, which can be useful in coastal wetlands studies.\\nTo illustrate the importance of spatial resolution in observing tidal wetlands, a pixelization simulation was applied to a wetland mask to derive the percentage of pixel mixing as a function of resolution for a large marsh system. Fig. 2 shows a map of wetland cover types from the United States Fish and Wildlife Service (USFWS) National Wetlands Inventory (NWI). The area shown includes a large marsh system along the eastern shore of the Chesapeake Bay (USFWS, 2014). Polygons from a NWI shape file were used to create a layer in GIS software called QGIS and rasterized to approximate one-meter resolution. This captured much of the small-scale features seen in the NWI polygon data, including variation along the marsh boundaries and within its interior, including small ponds and channels. The resulting image, with about 10 4 pixels on a side, was loaded into IDL, which is software designed for image analysis. An array of the same dimensions as the NWI image was created. Elements of this array were set to unity where image pixels were designated as being the Estuarine and Marine Wetland cover type (see Fig. 2 ) and the rest of the array was left as zero. Then the array was subsequently aggregated to increasingly coarser pixels sizes. The resulting average of ones and zeros in each aggregated pixel thus indicated the fraction of the selected wetland type that it contained. Fig. 3 uses a gray scale to illustrate the spatial distribution of this fraction within a magnified rectangular inset (identified by the small red box in Fig. 2 ), which includes an area of marsh between the Blackwater Lake in its northwestern corner and Fishing Bay in its southeastern corner. The meandering river running between these two bodies is the Blackwater River, which becomes noticeably gray at as fine as 60 m resolution. The top row of simulated images represents the effects of moderate-resolution sensors (30-90 m) and the bottom lowresolution sensors (e.g., PACE and GeoCAPE). Fig. 4 shows the fraction of the wetland type that remains unmixed as the pixel size increases. Generally, 30 m (the fourth circle) is where significant degradation begins to occur and beyond 60 m the loss is substantial. Therefore, lowresolution sensors are less appropriate for mapping, assessing, or monitoring marshes, or similar wetland systems. It should be noted that HyspIRI was originally planned to provide imagery with global coverage at 60 m resolution, a swath width of 145 km, and equatorial revisit period of 19 days. However, in response to a congressional mandate in 2014 that the USA federal government should support Sustainable Land Imaging (SLI) for the next twentyfive years, NASA instructed the HyspIRI mission management to conduct a study that would determine whether the HyspIRI mission could supplement the existing Landsat imaging capability. A review of available technology led by the Jet Propulsion Laboratory (JPL) concluded that replacing the originally proposed Offner spectrometer with a Dyson spectrometer for HyspIRI would support Landsat spatial resolution, while keeping the original spectral and radiometric characteristics. Therefore, as of this writing, HyspIRI mission will employ a VSWIR spectrometer that provides a 30 m spatial resolution equal to Landsat legacy instruments. In addition, the new spectrometer design will have Landsat's 185 km swath width, which should also decrease the equatorial revisit period to 16 days (Green, 2015) .\\nWith SNR comparable or better than the now defunct HICO instrument, HyspIRI should be more suited than Landsat to observe water optical characteristics (Devred et al., 2013) , especially along shallow coastal waters. This is useful because wetlands tend to be darker than typical terrestrial targets and it would also be useful to study the characteristics of adjacent water ecosystems and environments. This also could better support studies of interaction between coastal wetlands and estuarine and marine waters. HyspIRI's greater spectral resolution and coverage should afford it superior capabilities in discriminating species and vegetation functional types and provide a better chance at observing the spectral signatures of plant stress and changes in production. With its spectral range extending into the SWIR, the HyspIRI mission should provide broader use than HICO could in covering both aquatic and terrestrial features.\\nHyspIRI's eight thermal bands will be more than adequate to observe subtle variations in water surface temperature, which would be ideal for tracing fresh or melt water fluxes along water body margins with land and ice, respectively. Its VSWIR spectral capabilities could then observe aquatic biospheric response to contemporaneously observe coastal fluxes. The combined use of 30 m resolution satellite data from HyspIRI's optical and thermal infrared radiometers is promising for multiple applications, including the retrieval of latent and sensible heat fluxes, and soil surface moisture variations (Petropoulos, Carlson, Wooster, & Islam, 2009; Sandholt, Rasmussen, & Andersen, 2002) . This information will also be contemporaneous to the water and vegetation attributes derived from the hyperspectral measurements.\",\n       'Dependent populations are defined here as individuals who temporarily reside in facilities where they would be dependent on external assistance to evacuate and recover. Facilities with such populations include: • Adult-assistance services, such as assisted-living facilities for the elderly, continuing-care retirement communities, and skilled-nursing-care facilities; • Child services, such as child day-care services and child and youth services; • Correctional facilities; • Medical and health services, such as family-planning centers, offices of dentists, offices of physicians, and psychiatric and substance-abuse hospitals; • Medical centers; and • Schools. Results indicate that there may be significant numbers of dependent-population facilities in the extreme tsunamiinundation zone (table 3). The highest number of dependentpopulation facilities in the extreme tsunami-inundation zone were medical and health services (2,024), followed by schools (111), child services (95), and medical centers (37). Urban Honolulu has the highest number of dependent-care facilities in the extreme tsunami-inundation zone at 929, with the majority being medical and health services facilities. Kailua in Honolulu County and Hilo are the next highest communities at 308 and 303 respectively. The low number of exposed medical centers, but high number of exposed medical and health services (which includes physicians\\' offices), in some  communities suggests that medical health centers in those communities may be able to handle casualties during the immediate response phase of a disaster; however, those communities may experience difficulties in maintaining medical services during the longer-term recovery phase. On the basis of these results, a follow-up study to confirm the location and determine the size of these dependent populations may be warranted. Additional evacuation planning may be required in communities with high numbers of dependent-population facilities because of the limited mobility of certain groups at these facilities, such as those in schools and nursing homes. Also, parents may attempt to enter tsunami-prone areas to retrieve children from schools and day-care centers or adult children may attempt to enter tsunami-prone areas to retrieve their parents from elderly care facilities, which present additional evacuation issues for facility managers. In addition to unique evacuation and relief issues, many dependent-population facilities represent critical social services that, if lost, could slow community recovery following an extreme event. For example, the loss of day-care centers could keep parents at home, thereby slowing business recovery. The Infogroup Employer Database has similar issues for dependent-care facilities as for community-support businesses. In addition, some categories may contain facilities that are not as much of a concern in a particular category as others are. For example, \"schools\" may include bus depots and other school facilities at which children would not be present along with traditional elementary and secondary schools.',\n       'We computed a logistic regression relating each psychometric test and modality with Ab status, while covarying for age, gender, and education ( Table 2 ). The logistic regression results indicated that the best univariate predictor of cerebral Ab was APOE ε4 status, followed by white matter hyperintensity (WMH) volume. All other imaging and cognitive measures, including FDG-PET, hippocampal volume, ECog, and any AVLT measure, were not significant predictors of Ab status. WMH volume was not increased in APOE ε4-carrying subjects (t(85.8) 5 20.018, P 5 .99). The independence between WMH volume and APOE genotype implied that both were independent predictors of Ab status. This was confirmed by running a stepwise forward multivariate regression model, which selected only WMH volume and APOE status as independent predictors. A boxplot comparing WMH volumes in Ab1 and Ab2 subjects is shown in Fig. 1 . There was no significant association between WMH and either t or t/Ab ratio (P 5.56 and .09, n.s.).',\n       'Relative sea level (RSL), or local sea level, is the sea level measured by a tide gauge relative to a specific point on land. Tide gauges measure the combined effects of ocean volume change (GMSL), ocean and climate variability (ReMSL), and vertical land motion. The change in RSL is what coastal areas experience, and is the quantity used for assessing and planning for the coastal impacts from sea level change (NRC, 2012;Church et al., 2013).',\n       'Lumbar puncture was performed at the vertebral interspaces of L3-L4 or L4-L5 in a fasting state before treatment. The first 5 ml of CSF was used for routine examination, and the next 10 ml was used for this study. The CSF sample was collected in a sterile polypropylene tube and centrifuged for 10 min at 400 g within 10 min after completion of lumbar puncture. The supernatant was packed in a polypropylene microtube and stored at − 80°C until assays were performed.',\n       'Insular cortex',\n       \"The numbers written behind the individual choices for answer represent the number of students that chose the given answer. 50 % of students solved this task correctly. It can be assumed that the selection of the choice d) is induced by the incorrect idea that 2x must be more than x for each real number x.\\nThe second task was used for diagnosing the rate of skills development to express relationships between variables using symbolic notations. \\nOnly 22.7 % of students solved this task correctly. The most students selected the incorrect answer d). These students intended to subtract 50 % discount from the full total amount of overnight accommodations at the camp. They did not notice that amount per one night is replaced by the variable x in the calculation of discount.\\nThe average score in the pre-test was 45.98 %. This relatively low score points to the fact that some students' inquiry skills are developed on a low level. Our testing has pointed out the difficulties of students with interpretation and creation of symbolic notations expressing the relationships between variables, with making hypotheses and with finding the appropriate arguments to justify the validity of their hypotheses.\",\n       'As suggested above, the dual challenges facing efforts to develop and validate indicators for high school graduation rates are a lack of a comprehensive, authoritative source for information on high school graduation combined with a lack of consensus over how to best define and compute such an indicator. Since no established standard exists against which to test proposed indicators, analysts might pursue two general approaches to better understand high school graduation as an educational phenomenon and to explore alternative measurement strategies for developing accurate and reliable indicators. First, one might use a single computational approach to calculate graduation rates for a particular set of educational units, but do so using different data sources. Of interest here would be the stability of an indicator given variations in the types of data used, such as federally-sponsored data collections like the Common Core of Data versus information collected and reported by state or local accountability systems. An alternative approach would be to employ a single data source and compare high school graduation rate estimates calculated using multiple computational approaches. The current paper pursues the latter strategy and specifically examines the three group-level indicators described above -NCES, ACR, and CPI.\\nThe most comprehensive source of data on various forms of high school completion and dropout currently available is the Common Core of Data. Conducted by the U.S. Department of Education, the CCD is a census of public sector local educational agencies (districts) and schools for the fifty states, the District of Columbia and several other non-state jurisdictions. Annual surveys of basic demographic and educational information at the state, district, and school levels are completed by staff of the respective state education agencies. The district-level CCD database reports the number of students receiving diplomas and other kinds of high school completion credentials as well as dropout counts by grade level. Grade-specific enrollment data are available only at the school level. This data structure has two consequences with respect to calculating group-level indicators. First, we must combine information from the CCD\\'s district and school databases. Second, because completion and dropout counts are not reported for individual schools, the district is the most basic educational unit for which graduation rate indicators can be systematically constructed. Grade-specific enrollment counts at the district level are obtained by aggregating school data. High school graduation rates are first calculated at the district level and then aggregated upward to the state level (weighted according to the size of the district\\'s high school enrollment) to produce national and state level estimates.\\nDuring the foal year for our analysis (1999) (2000) , there were 14,978 regular school districts in operation throughout the fifty states and District of Columbia. In defining our target population for analytic purposes, however, it will be necessary to introduce several additional restrictions. Our objective is to identify districts that (1) are eligible for the calculation of a graduation rate and (2) should in theory have the necessary information needed to calculate such a rate. It is reasonable to assume, for instance, that we can only calculate a meaningful graduation rate for districts that contain a full complement of secondary level grades (9-12). About 27 percent of regular school districts in the country do not meet this criteria, the majority of which possess only an elementary level grade span or have ungraded enrollment.\\nIn addition, the three indicators of interest require various pieces of information from the 1995 through 2000 school years. Districts that were not in operation during this entire period of time will be legitimately missing some necessary information. Of the districts in operation during the focal year, a small fraction (3%) were not in operation during at least some part of our period of observation. Further, 4.5 percent of districts had undergone a significant change in boundaries over this period. Such events effectively change the identity of a particular district and produce large year-to-year fluctuations in enrollments, which in turn produces invalid estimates of graduation rates. 16 Taking all of these selection criteria into consideration, we arrive at a target population of 10,836 school districts for which we should be able to calculate valid graduation rates. This is only a starting point, of course, which assumes that the information expected to be reported for these districts in the CCD is actually present. We will find below that this is often not the case.\\nAs discussed earlier, one of the main points of contention in debates over measuring high school graduation rates is which credentials should be counted. The official NCES completion statistics, for instance, counts all state-issued completion credentials (although not the GED) as a means of providing a more consistent definition of a \"high school completer\" across states. The decision to count only regular diplomas when calculating graduation rates for the purposes of this study was motivated by two main considerations. First, this approach adheres to the definition of high school graduation stipulated in NCLB. Second, only a small percentage of high school completers nationwide (1.5%) were awarded a non-diploma credential in 1999-2000 and this rate never reaches 10 percent for any individual state (Young 2002 ).\\nFinally, the NCES indicator is formulated in such a way that its value is constrained to fall within the range of 0 to 100 percent, the minimum and maximum values for any percentage statistic. The same is not true for the ACR and CPI methods because values could exceed 100 percent. As a result, operational rules must be developed for handling cases in which their values exceed the maximum. This was accomplished through a combination of trimming and censoring district estimates. For the ACR method, districts with out-of-range values that fell within a reasonable margin of error (10%) were reassigned the maximum value (100%). Cases with more extreme values were assigned missing values. Similar rules were used to constrain the maximum value of CPI indicator\\'s grade-specific promotion ratio components. Following a procedure recommended by Greene (2002b) , cases are also assigned a missing value for ACR if the population adjustment factor indicates extreme growth or decline (a change of over 30%) in the district\\'s high school enrollment between 1996 and 1999.',\n       nan, nan,\n       \"We begin by analyzing the descriptive statistics for the labor market outcomes of college graduates, found in Table 21. The first set of columns includes all students whose highest degree was a bachelor's by the conclusion of the study, while the second set of columns restricts the sample to students who were not enrolled in postsecondary at the time labor market data was collected. Whether the sample is all bachelor's recipients or just those not enrolled in postsecondary, the relationship between socioeconomic background and earnings and wages is roughly equivalent. For bachelor's recipients, the mean earnings of high-SES students were approximately $5,200 more than those of low-SES students, and the median difference was slightly under $4,000. The disparities are slightly larger when the sample is restricted to respondents not enrolled in postsecondary, with a mean disparity of approximately $6,500 and a median disparity of $5,000. Similar socioeconomic differentials in wages are observed. Both the mean and median wage difference between high-SES and low-SES college graduates was approximately $2.50, while the disparities for graduates not enrolled in postsecondary was about $3.00. However, restricting the sample to students not enrolled in postsecondary is particularly important when analyzing unemployment rates. For all college graduates, high-SES students actually had the highest rates of unemployment at 12.7%, slightly higher than low-SES students and approximately 3.5% higher than students in the second lowest SES quartile. In contrast, for college graduates not enrolled in postsecondary, high-SES students had about a 3% lower rate of unemployment compared to low-SES students (6.8% vs. 9.7%), the lowest unemployment rate in the sample. These results suggest that socioeconomic background continues to exert an influence on the labor market outcomes of college graduates. However, as low-SES students have lower average ability levels and are more likely to attend less selective institutions compared to their high-SES peers, it is also important to investigate whether background ability and institutional selectivity impact the magnitude of socioeconomic disparities in the labor market outcomes of college graduates. Table 22 contains the labor market outcomes descriptives disaggregated by ability level for college graduates not enrolled in postsecondary. The first point that should be highlighted is the clear relationship between ability and earnings. The mean earnings of high-ability students is close to $40,000, while low-ability students earned approximately $26,500 on average. The same finding generally holds for both wages and unemployment rates, with high-ability students experiencing significantly better labor market outcomes on average compared to low-ability students. But in contrast to many of the analyses of postsecondary transitions, it does not appear that the magnitude of disparities in students' labor market outcomes varies consistently by background ability. For example, low-SES students have significantly higher unemployment rates in both the high-ability and low-ability subgroups, but have lower than average unemployment rates in the two middle categories of ability. We also get somewhat conflicting interpretations of the relationship between background ability and socioeconomic disparities in labor market outcomes depending on if we're considering earnings or wages. For example, low-SES students actually had the highest median earnings among low-ability students, but their mean hourly wages were more than $4.00 less than high-SES students. Part of the reason for this inconsistent relationship is likely due to the smaller cell sizes when the sample is disaggregated by ability. For example, there would be a consistent disparity in mean earnings across subgroups were it not for the second lowest ability group. At least one low-SES student in this category earned approximately $400,000, resulting in low-SES students having the highest mean earnings out of any socioeconomic subgroup in this ability category. In short, the limited sample size prevents definitive conclusions from being drawn on the degree to which ability levels impact the magnitude of socioeconomic disparities in students' labor market outcomes. Table 23 presents the labor market outcome descriptives for college graduates not enrolled in postsecondary disaggregated by institutional selectivity. While there was a clear and consistent relationship between average labor market outcomes and ability, in this instance there is less of a steady decline as we move to institutions of lower selectivity. Instead, average earnings and hourly wages are roughly similar for students who graduated from moderately selective or non-selective institutions, but these outcomes are significantly higher for students that graduated from selective institutions. The mean earnings for students who graduated from moderately selective and nonselective institutions were both approximately $32,500. In contrast, students who graduated from highly selective institutions earned approximately $10,000 more than that on average. Similarly, mean wages for students who graduated from moderately selective and non-selective colleges were both approximately $18.50, while graduates from selective institutions earned about $22.00 per hour on average. Thus, although there is not a consistent trend in labor market outcomes across levels of institutional selectivity, these results do underscore the importance of graduating from a selective institution for improving students' socioeconomic outcomes. There is a similarly inconsistent relationship in terms of how institutional selectivity impacts the magnitude of disparities in students' labor market outcomes. The mean earnings of high-SES and low-SES students are roughly equivalent for both selective and non-selective institutions, while for moderately selective graduates high-SES students earned over $5,000 more than their low-SES peers on average, suggesting that earnings disparities are greatest among graduates from moderately selective institutions. However, the reverse is true of disparities in the unemployment rates. Low-SES students are significantly more likely to be unemployed among students who graduated from selective and non-selective institutions, while the disparity is much smaller for students who graduated from moderately selective institutions. Once again, while it does appear that disparities in students' labor market outcomes stemming from socioeconomic origins do vary according to the selectivity of the institution from which they graduated, it is likely that the relatively small cell sizes when the sample is disaggregated in this fashion prevents the identification of clear trends in disparities across selectivity levels.  188 \",\n       'The authors have no competing financial interests.',\n       \"The ECLS-K reading and mathematics assessments were designed to reflect children' s knowledge and skills in both subjects over the duration of the study. The reading assessment captured information on children' s basic literacy skills, vocabulary, and comprehension. The mathematics assessment measured children' s conceptual understanding of numbers, shapes, patterns, mathematical operations, and processes for problem solving. From the start of kindergarten to the end of third grade, children' s reading scale scores, a measure of their overall reading achievement, increased an average of 81 points, and their mathematics scale scores increased about 63 points. Children' s spring third-grade reading scale scores were about 8.4 standard deviations higher than their fall kindergarten scores, and their spring third-grade mathematics scale scores were about 7.3 standard deviations higher than their fall kindergarten scores. Thus, one standard deviation in the reading score amounts to a 9.6-point difference in the reading scale score, and one standard deviation in the mathematics score amounts to an 8.6-point difference in the mathematics scale score. It is important to note that the data points represented in the figures and tables in this report cover different time spans (i.e., the kindergarten school year, the full calendar year between spring of kindergarten and spring of first grade, and 2 full calendar years between spring of first grade and spring of third grade). Thus, increases in achievement over time must be interpreted relative to the amount of time between assessments. Between the start of kindergarten and the end of third grade, the reading and mathematics achievement gaps across certain groups of children widened. Black children had made smaller gains in reading and mathematics by the end of third grade than White, Hispanic, and Asian/Pacific Islander children. 5 As the number of children' s family risk factors (e.g., living in a single-parent household, living below the federal poverty level) increased, children tended to gain less in both subject areas than children with fewer family risk factors (figures A and B). Children' s gains in their first 4 years of school did not differ substantively, however, by their sex, the type of kindergarten program they attended (i.e., half-day or full-day), or the type of school they attended (i.e., public school all 4 years, private school all 4 years, both public and private school attendance).  Black third-graders had lower achievement scores than White, Hispanic, and Asian/Pacific Islander children in all three subjects, and Hispanic third-graders had lower overall achievement scores in science compared with White children (figure C). Those with more family risk factors had lower mean achievement scores in all subjects than those with fewer family risk factors. In addition, third-graders who had always attended private schools from kindergarten through third grade had higher reading achievement scores than those who had always attended public schools. Children' s third-grade achievement did not differ substantively by their sex.\",\n       'Some of the literature on expectations related to educational outcomes was mentioned in the introduction. It was noted that individuals with more education and skills have the highest expectations for their jobs and careers; and are more easily disappointed (Tsang and Levin, 1985;Clark and Oswald, 1996). In addition, psychological theories of expectation suggest that the under-utilization of skills is a potential cause of diminished job satisfaction (Bender and Heywood, 2006a). In their paper, \"Job Satisfaction of the Highly Educated\", Bender and Heywood (2006b) report that the job satisfaction of Ph.D. level scientists in the United States is affected by gender -female scientists report lower job satisfaction than males in academic settings (but higher job satisfaction than males in non-academic settings). It may be the case that female Ph.D. graduates have higher expectations for being matched to their skill-set when they are working in the academic sector. In addition, there is also the idea that individual expectations are raised by exposure to higher standards/expectations in the external environment. The first evidence on this dynamic was produced in the organisational/management literature, when researchers explicitly considered that raising worker expectations might enhance motivation. Previous to this, it was established that \\'managers who expect more, get more\\' (e.g., Likert, 1961Likert, , 1967McGregor, 1960); this has been referred to as the Pygmalion effect. 4 Livingston (1969) was the first to form a theory where self-esteem mediates the relationship between individual expectations and performance. Subsequently, this was referred to as the Galatea effect, named after Pygmalion\\'s sculpture, as it involves working directly on the sculpture without the Gods (Eden and Ravid, 1982). 5 Babad, Inbar and Rosenthal (1982) referred to the negative impact of low expectations on achievemnet as the Golem effect. We are primarily interesyed in the Galatea effect, that is, scenarios where high selfexpectations lead to high performance. According to McNatt and Judge (2004), there are several conceptual formulations of self-expectations of performance, which include specific selfefficacy, self-confidence, and performance expectancy. It is also possible that psychometric measures such as self-esteem, or extraversion and openness (from the Big Five), are somewhat related to the Galatea effect. The first experimental evidence for the Galatea effect is provided by Eden and Ravid (1982). In this experiment, Galatea trainess were given a five-minute personal interview by a psychologist, at the end of which they were told, \"You have high potential for success\". Control groups were told that they had regular potential for success or were simply given interviews without the last sentence. The results showed a substantial rise in self-expectations due to the Galatea effect. Exisiting research demonstrates what may constitute a Galatea effect in the Ph.D. training process. Several years ago, the Cornell Higher Education Research Institute evaluated the Graduate Education Initiative (GEI) that provided 85 million dollars in financial support towards Ph.D. education in the United States (Ehrenberg, 2005). This initiative was criticised for failing to directly address more fundamental issues on departmental levels in the American universities, such as ensuring match between supervisor and student, or match between student and Ph.D. topic. Furthermore, the Cornell evaluation describes how generous financial support may even have induced students to persist with an unloved topic for longer than they might otherwise have done (Ehrenberg, 2005). This research underscores the importance of inputs into the Ph.D. training process; and these inputs are something that may lead to higher expectations for Ph.D. outcomes. We discuss further in the following section how inputs into Ph.D. training may raise self-expectations (i.e. a Galatea effect) and thereby increase expectations for post-Ph.D. educational outcomes.',\n       nan,\n       'School district characteristics considered to be the major factors a¤ecting the utility function of families are Pupil-to-teacher ratio, Expenditure per Pupil and Dropout Rate, all at the school district level. We obtain these three measurements of the school quality from the National Center for Education Statistics (NCES) survey Common Core Data …les (CCD) for year 1994 5 .\\nWe use the 1990 Census Geo…les 6 to get the geographic occupation of these school districts and match them into their prospective counties. Figure 1 shows the distribution of school districts among US counties in 1994. 95 school districts where the pupil-to-teacher ratio of these school districts ranges from 8 students per teacher to 21 students per teacher; \"Bergen County\" in New Jersey has 70 school districts, and the expenditure among these school districts goes from 1,400 dollars to 9,000 dollars per pupil. 6 The Map has 15,512 local education agencies in year 1994 where 12,920 of the 15,512 are de…ned as the \"Uni…ed\" or primary/secondary school districts that o¤er a degree up to high school diploma. \"Standalone\"school districts account for 3,582 of the total school districts where only partial degrees (not up to 12) are o¤ered. pupil averages at 3934 dollars per student, and overall dropout rate over the …rst to 12th grade ranges from zero to 39.28% per grade, averages at 7.66% per grade. The county average wage is having a mean of $11.26 per hour.\\nWe show the division of within and between county standard deviation of all school districts as well. The di¤erence in the standard deviations of all quality indicators shows that the school district qualities are not uniform within counties. For dropout rate, the between county standard deviation is very similar to the overall standard deviation even though this variable experience a lot of variations across individual school districts. This probably shows that the dropout rate has little variations within county. This has a direct impact on the performance of the Ad hoc Aggregation model.\\nOther school e¤ects studies mostly have a number of regressors, but we specify the location choice decision to be made upon the Pupil-to-teacher ratio, Expenditure per Pupil, Dropout Rate and the county mean wage. This is to make the comparison of aggregated and disaggregated models more straightforward and to simplify the data generating in the Monte Carlo experiment. For the location choice, the utility function is simulated by the sum of 1) a known part that is a linear function of the above four characteristics, 2) a heterogeneity part (which represents the unobserved family preference factor for school inputs) that interacts with the school characteristics, and 3) error term that distributed i.i.d. extreme value in all choices. More speci…cally, we de…ne equation 5 to be:\\nMost previous studies (Loeb and Bound (1995) , and Hanushek (1997)) found the marginal contribution of the three quality factors of schools are in the range of zero to 0.50 for the metrics we study here, so a coe¢ cient scale of -0.5 to 0.5 is used as \"true\" coe¢ cients in this study. We also parameterize the value of in Equation 2 to be 10. The school district outcome is generated as the one that gives the maximum utility from a sorting through the utilities that each location gives according to equation\\n1. There are 100,000 families in our sample, all having only one child age 5 to 15.',\n       'The proposed MSH-ELM shows better AD classification performance than conventional ELM using individual single modality data or simple concatenation feature of multi-modality data (MRI, PET, and CSF). This means that MSH-ELM, a deep network, discovered the optimal feature representation for AD classification that was not found in a conventional ELM, shallow network. In addition, MSH-ELM effectively extracted AD-related complementary features from multiple modalities which are helpful for discrimination of AD, MCI, and HC. As shown in Tables 2 and 3 , the ability of MSH-ELM to integrate multi-modal data is superior to that of MK-SVM, which is widely used in AD classification using multi-modal data.\\nAdditionally, we compared the classification performance of MSH-ELM with SAE which is the back propagation-based multi-layer perceptron (MLP) learning algorithm. MSH-ELM showed higher performance than that of SAE in two binary classification problems (AD vs. HC and MCI vs. HC).\\nFurthermore, the computation load of MSH-ELM is much lower than SAE as the base building block of MSH-ELM is ELM-AE, which randomly assigns the weights for hidden layer. According to Tang et al. (2016) , MLP using ELM-AE as a base building block is more efficient than the latest back propagation-based MLP (SAE, deep belief network, and deep Boltzmann machine) for MNIST and NORB dataset classification in terms of classification accuracy and computation speed.\\nAnother interesting aspect of the MSH-ELM is that the MSH-ELM exhibits excellent classification performance even though the dimension of the hidden neurons was higher than that of input neuron.\\nWe think that this is because the base building block of MSH-ELM is a sparse ELM auto-encoder. Imposing the sparsity constraint on the hidden units allows the hidden layers to have larger number of units than the input dimension (Larochelle et al., 2009; Suk and Shen, 2013) .',\n       'The SLOSH model-simulated surge-plus-tides AGL results over land and maximum envelope of water over the ocean, as rendered by the interactive SLOSH Display Program [19] , are compared to the Federal Emergency Management Agency (FEMA) Modeling Task Force (MOTF) field-verified, -ground-truth‖ Hurricane Sandy Impact Analysis graphic [20] , which depicts the final high-resolution storm surge extent (grey) and very high-resolution extent in NYC (blue) in Figure 18 to provide a more detailed verification of the inundation area. The geographical patterns of inundation agree quite well, especially at Breezy Point, Rockaway, the low-lying areas surrounding JFK airport and further east along the shores of East Bay and South Oyster Bay. The SLOSH wetting-and-drying algorithm performs skillfully inland to the west, in the area extending from south to north along the west bank of the Hudson River from Hoboken to Union City, NJ and further west in the larger Jersey City, Secaucus and Ridgefield area. Flooding over the river banks is also accurately simulated to the south along the Raritan River, the Washington Canal and the South River. The inundation area calculated from the SLOSH Best Track hindcast simulation was 561 km 2 (216 sq mi). ',\n       'For more information, contact Beverly Coleman (beverly.coleman@ed.gov).',\n       'Density (birds/ha) Figure 21. `I`iwi mean annual frequency of occurrence (proportion of stations occupied; scaled 0 to 1) and density (birds/hectare) from 1977/1979 (HFBS) to 1994 in the Mauna Loa Strip study area. Trendlines show the general linear relationship between untransformed occurrence or density and survey year, and are included for illustrative purposes only. Logistic regression of occurrences and year was non-significant (slope = 0.048; odds ratio = 1.049; 95% confidence limit = 1.013 -1.087; P = 0.174). The regression slope of log-transformed densities and year was non-significant (slope = 0.005; 95% confidence limit = -0.009 -0.019; R 2 = 0.12; P = 0.393). 1977/1979 1986 1987 1988 1989 1990 1991 1992 1993 1994 Year 0.8 0.9',\n       'Dissatisfaction. Postdocs were asked to respond to the question \"Overall, how satisfied are you with your current PostDoc experience?\" on a seven-point Likert scale ranging from \"Very dissatisfied\" to \"Very satisfied.\" The ordinal variable dissatisfaction was coded as 1 if the response was very dissatisfied, dissatisfied, or somewhat dissatisfied, 0 if the response was \"Neither satisfied nor dissatisfied,\" and -1 if the response indicated some degree of satisfaction. Binary variables were created for alternative specifications. The binary variable dissatisfied was coded as 1 if the response was very dissatisfied, dissatisfied, or somewhat dissatisfied and zero if the response was neutral or indicated some degree of satisfaction.',\n       \"We obtained a total of 138 RNA samples at baseline from age and sex-matched early stage PD, PSP patients and HC from PDBP. Standardized clinical and biospecimen collection procedures were used for all participants. Each biospecimen collection follows the PDBP protocol and mirrors the Alzheimer's Disease Neuroimaging Initiative (ADNI), BioFIND and PPMI protocols. Collected biosamples are sent to the NINDS Repository (Coriell Laboratories), undergo quality control, and are cataloged. Blinded RNA samples were shipped in dry ice to Rosalind Franklin University of Medicine and Sciences for the studies described herein.\",\n       'Rather than directly imposing sparsity regression coefficients in existing methods, we propose incorporating a structure matrix S to explicitly encode inter-target correlations via a rank minimization.\\nwhere\\ncontains the latent variables in the latent space, S ∈ R Q×Q is the structure matrix that serves to explicitly model inter-target correlations, β is the regularization parameter to control the rank of S, that is, a larger β induces lower rank, and the Frobenius norm control the shrinkage of S with the associated parameter γ . The rank minimization of the structure matrix S explores the low-rank structure existing between tasks to capture the intrinsic inter-target correlation. S is learned automatically from data without relying on any specific assumptions, which allows to adaptively cater different applications. However, the objective function in (7) is NP-hard due to the noncontinuous and non-convex nature of the rank function. The nuclear norm ||S|| * is commonly used and has been proven to be the convex envelop of the rank function over the domain ||S|| 2 ≤ 1, which provides the tightest lower bound among all convex lower bounds of the rank function Rank(S).\\nAs a consequence, the combination of the nuclear norm with the Frobenius norm on S gives rise to the matrix elastic net (MEN) (Li et al. 2012 ) as a regularizer of (7):\\nwhere the nuclear norm ||S|| * is also known as the trace norm. The MEN is an analog to the elastic-net regularization (Zou and Hastie 2005) from compressive sensing and sparse representation (Zou and Hastie 2005) . It has been shown that the elastic net often outperforms the lasso (Zou and Hastie 2005) . In the MEN, the nuclear-norm constraint enforces the low-rank property of the solution S to encode inter-target correlations, and the Frobeniusnorm constraint induces a linear shrinkage on the matrix entries leading to stable solutions (Li et al. 2012 ). The MEN regularization generalizes the matrix lasso and provides improved performance than lasso (Tibshirani 1996) . To the best of our knowledge, this is the first work that introduces the MEN to multi-target regression for robust low-rank learning, which offers a general framework to encode intertarget correlations.',\n       nan,\n       'In response to high property taxes and an unequal distribution of school funding across districts, the Michigan K-12 finance system changed dramatically with the implementation of Proposal A in 1996. Proposal A changed the power equalization finance system to a state level system and reduced the funding obtained from local property taxes while increasing the funding from state income and sales taxes. School districts with a millage rate over 18 in fiscal year 1993 had their millage rate reduced to 18 and the new law stipulated that 5. Theoretical work by Fernandez and Rogerson (2003) suggests the foundation system dominates the state-level system in terms of total welfare. districts only impose this millage on non-homestead properties. 6, 7 In addition to this non-homestead millage, the state allowed 32 of the highest-spending school districts to levy a \"hold harmless\" millage. The state also imposed a state level property tax of six mills along with increases in the sales, tobacco product, and real estate transfer taxes. Proceeds from these taxes are deposited in the Michigan School Aid Fund and distributed to the school districts through a per pupil grant. This grant varies depending on which school district the student resides in, but does not depend on the amount of local property taxes collected. The amount of the per pupil grant is the difference between the amount of local revenue that would be collected if the non-homestead millage is at its cap, usually 18 mills, and the amount required to achieve the school district\\'s per pupil grant allocation. The funds from the grant are deposited into the school district\\'s general fund to pay for labor, material, utilities, and maintenance costs. Along with property tax millages to finance capital projects, local school districts can propose a \"sinking fund\" property tax millage, the revenues of which fund certain capital expenditures and repairs. 8 Local school districts can also generate funds using voter-approved referenda for a recreational millage, which provide revenue for the operation of public recreation facilities and playgrounds. Each of the 552 local school districts in Michigan belongs to one of 57 intermediate school districts (ISDs) that provide special and vocational education. Funding of ISDs did not appreciably change due to Proposal A and a significant portion of ISD funding comes from local property taxes. The variation across ISDs with regard to property tax revenue results in vastly different services being offered across the ISDs. Some of these services are offered directly by the ISD, and others are provided by the local school districts using ISD funds. An ISD can levy three types of property tax millage: operational, special education, and vocational education. Voters must pass a referendum to change any of these millage rates and there are caps associated with all three tax millages.',\n       'Sex, female (%)\\n149 (50) 0 (0) 95 (52) 0 (0) 99 (44) 0 ( \\nAPOE genotype APOE e4\\ncarrier (%) 214 (73) 6 (2) 123 (69) 2 (1) 167 (75) 3 ( their data on the identified atrophy clusters. We characterized each atrophy cluster based on the top 100 cluster-defining features (i.e. regions of interest) in each dataset. Spatial correspondence of atrophy cluster solutions across datasets was assessed with the Dice coefficient (see Supplementary Table 3 and Supplementary Fig. 2 for comparison containing the top 200 features) (Dice, 1945) .',\n       'Ethics statement. In compliance with ethical standards of research, all research was conducted under the auspices of the Harvard University Committee on the Use of Human Subjects, IRB#15-063. All participants were provided with an opportunity to review and agree to an informed consent as part of the online survey. All data is reported in aggregate or with identifiable information removed [7]. Data collection. The sample includes graduates who earned a PhD in life, physical, computational, and social sciences or engineering, at any institution between 2004-2014, and who had worked, trained, or studied in the U.S. The sample was developed using qualitative and quantitative methods of data collection and analysis. An online survey was constructed first as a pilot, with embedded cognitive questions. The survey was then pilot tested with a small sample of respondents, revised, and a large-scale survey was launched. This large-scale data collection effort took place in the spring of 2015. Outreach to build the sample was conducted electronically via social media channels and direct emailing to potential respondents. The total number of complete responses to the final online survey was 8,099. The current analyses include a subset of relevant questions and responses. For more detailed information on sampling techniques and survey construction, see [7].',\n       'Objective: Subfield-specific measurements provide superior information in the early stages of neurodegenerative diseases compared to global hippocampal measurements. The overall goal was to systematically compare the performance of five representative manual and automated T1 and T2 based subfield labeling techniques in a subset of the ADNI2 population. Methods: The high resolution T2 weighted hippocampal images (T2-HighRes) and the corresponding T1 images from 106 ADNI2 subjects (41 controls, 57 MCI, 8 AD) were processed as follows. A. T1-based: 1. Freesurfer + Large-Diffeomorphic-Metric-Mapping in combination with shape analysis. 2. FreeSurfer 5.1 subfields using invivo atlas. B. T2-HighRes: 1. Model-based subfield segmentation using ex-vivo atlas (FreeSurfer 6.0). 2. T2-based automated multi-atlas segmentation combined with similarity-weighted voting (ASHS). 3. Manual subfield parcellation. Multiple regression analyses were used to calculate effect sizes (ES) for group, amyloid positivity in controls, and associations with cognitive/memory performance for each approach. Results: Subfield volumetry was better than whole hippocampal volumetry for the detection of the mild atrophy differences between controls and MCI (ES: 0.27 vs 0.11). T2-HighRes approaches outperformed T1 approaches for the detection of early stage atrophy (ES: 0.27 vs.0.10), amyloid positivity (ES: 0.11 vs 0.04), and cognitive associations (ES: 0.22 vs 0.19). Conclusions: T2-HighRes subfield approaches outperformed whole hippocampus and T1 subfield approaches. None of the different T2-HghRes methods tested had a clear advantage over the other methods. Each has strengths and weaknesses that need to be taken into account when deciding which one to use to get the best results from subfield volumetry.',\n       \"The substitution of capital for labor and new labor-saving technologies has reduced the labor required for farming, yet many farms today depend on hired labor in some form. Common in the literature is the assumption of perfectly substitutable farm labor. This has implications for the operator's off-farm labor decision. Intuitively, different forms of farm labor have different impacts on production. We use the Agricultural and Resource Management Survey to estimate the elasticity of substitution between hired and family labor. The results provide little evidence to support the popular homogeneity assumption and find labor can be unitary and complimentary under certain scenarios.\",\n       \"In conservation biology, knowledge of the actual or potential distribution of a species is indispensable for threatened and endangered species management and protected area planning [40] . However, knowledge and a complete understanding of population trends appear to be equally essential and historically has been a key element for determining which species needs a particular management attention. This knowledge can only be achieved by a realistic approach of trends modeling that encompasses the complexity of spatial patterns.\\nMore importantly, models that incorporate both spatial associations and associations of habitat and other factors influencing populations are critical for developing predictive models used in assessing the consequences of stressors such as habitat and climate change on bird populations. The models presented here achieve this goal, and should be useful in future modeling for management of bird populations.\\nIncorporating spatial autocorrelation in our model allowed us to obtain more precise results in regions where little information was available; this can be decisive in the management process. Surer information can help managers make more objective decisions. The use of spatial autocorrelation helps to improve the overall quality of trend estimates by explicitly accounting for underlying geographic variation in the data. In order to account for this spatial autocorrelation, and properly identify spatial patterns of trends, we used a CAR model. While the normal assumption for the CAR can be susceptible to outliers, potentially leading to local oversmoothing of avian counts [26] [41], Best et al. (1999) [41] found that the particular treatment of the spatial effects had little consequences on final model inferences, suggesting that the model framework is robust to such errors. We note that CAR models are widely-used for many applications of so-called ''small-area estimation'' [19] [20] [21] [22] in conventional survey situations such as arise in agricultural, epidemiology and census surveys. The conditional variance parameter s 2 bt that controls the amount of variability in the spatial effect, and is ''a measure of the local variability conditional on the values of neighboring random effects'' [42] . It is common to incorporate both unstructured and spatially structured random effect to determine the importance of the spatial dependency. By determining if the spatially structured random effect is dominating the unstructured random effect, then estimates will display spatial structure. If it's the opposite, the unstructured random effect will ''shrink the estimates towards the overall mean'' [25] .\\nA benefit of including autocorrelation in a model is not only that the statistical assumptions are better met, but also that the predictive power of a model is improved by incorporating additional information or predictors, such as the values at neighboring locations [43] [44] . Bahn et al. (2006) [44] pointed out that not explicitly including spatial location in distribution models is based on the implicit assumption that species' locations are independent in space and time. However, such an assumption could easily be violated if the conditions defining the species niche were autocorrelated; or if species' locations were connected through dispersal or other behaviors that lead to spatial patterning such as aggregation or regular spacing. Interestingly, it should also be noted that spatial models can also improve variable selection [45] [46] . Non-spatial models cannot account for autocorrelation and thus may incorrectly select variables purely because they have a similar autocorrelation as the dependent variable and not because they are good predictors [45] [46] [47] . Therefore, the use of a spatially meaningful component in conjunction with other spatially varying covariates can help to determine exactly what part of a spatial pattern is due to the said covariates and what part is only due to spatial autocorrelation.\",\n       'Dr. Fleisher: drafting/revising the manuscript, study concept or design, analysis or interpretation of data, acquisition of data, statistical analysis, study supervision. Dr. Truran: drafting/revising the manuscript, analysis or interpretation of data, acquisition of data, study supervision. Dr. Mai: analysis or interpretation of data. Dr. Langbaum: drafting/revising the manuscript, analysis or interpretation of data, statistical analysis. Dr. Aisen: study concept or design, analysis or interpretation of data, study supervision. Dr. Cummings: drafting/revising the manuscript, study concept or design, analysis or interpretation of data, acquisition of data, statistical analysis, study supervision, obtaining funding. Dr. Jack: drafting/ revising the manuscript, analysis or interpretation of data, contribution of vital reagents/tools/patients, acquisition of data. Dr. Weiner: drafting/ revising the manuscript, study concept or design, analysis or interpretation of data, acquisition of data, study supervision. Dr. Thomas: drafting/ revising the manuscript, statistical analysis, study supervision. Dr. Schneider: drafting/revising the manuscript, study concept or design, analysis or interpretation of data, acquisition of data. Dr. Tariot: drafting/ revising the manuscript, study concept or design, analysis or interpretation of data, study supervision.'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def docs_preprocessor(docs):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    for idx in range(len(docs)):\n        docs[idx] = str(docs[idx]).lower()  # Convert to lowercase.\n        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n    # Remove numbers, but not words that contain numbers.\n    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n    \n    # Remove words that are only one character.\n    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n    \n    # Lemmatize all words in documents.\n    lemmatizer = WordNetLemmatizer()\n    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n  \n    return docs","execution_count":36,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = docs_preprocessor(docs_orig)","execution_count":37,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\nbigram = Phrases(docs, min_count=10)\ntrigram = Phrases(bigram[docs])\n\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)\n    for token in trigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","execution_count":38,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\nprint('Number of unique words in initital documents:', len(dictionary))\n\n# Filter out words that occur less than 10 documents, or more than 20% of the documents.\ndictionary.filter_extremes(no_below=10, no_above=0.2)\nprint('Number of unique words after removing rare and common words:', len(dictionary))","execution_count":39,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"},{"output_type":"stream","text":"Number of unique words in initital documents: 18355\nNumber of unique words after removing rare and common words: 2619\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [dictionary.doc2bow(doc) for doc in docs]","execution_count":40,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corpus[0])\nprint(dictionary[0])","execution_count":55,"outputs":[{"output_type":"stream","text":"[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 3), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 7), (22, 2), (23, 1), (24, 2), (25, 1), (26, 5), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 3), (33, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 2), (39, 1), (40, 3), (41, 1), (42, 2), (43, 3), (44, 1), (45, 2), (46, 1), (47, 1), (48, 2), (49, 4), (50, 1), (51, 1), (52, 1), (53, 2), (54, 1), (55, 3), (56, 1), (57, 1), (58, 1), (59, 3), (60, 1), (61, 1), (62, 1), (63, 2), (64, 2), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 3), (71, 15), (72, 6), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 4), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 4), (86, 3), (87, 1), (88, 3), (89, 1), (90, 6), (91, 1), (92, 1), (93, 1), (94, 1), (95, 2), (96, 14), (97, 1), (98, 1), (99, 1), (100, 1), (101, 2), (102, 1), (103, 1), (104, 2), (105, 3), (106, 2), (107, 1), (108, 2), (109, 1), (110, 1), (111, 1), (112, 7), (113, 1), (114, 1), (115, 9), (116, 1), (117, 2), (118, 1), (119, 1), (120, 1), (121, 1), (122, 3), (123, 6), (124, 2), (125, 1), (126, 3), (127, 1), (128, 3), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 8), (135, 6), (136, 2), (137, 1), (138, 1), (139, 1), (140, 1), (141, 3), (142, 1), (143, 1), (144, 8), (145, 1), (146, 2), (147, 3), (148, 1), (149, 1), (150, 1), (151, 1), (152, 3), (153, 2), (154, 2), (155, 2), (156, 2), (157, 3), (158, 2), (159, 1), (160, 1), (161, 1), (162, 1), (163, 2), (164, 4), (165, 3), (166, 1), (167, 1), (168, 2), (169, 1), (170, 1), (171, 1), (172, 3), (173, 1), (174, 3), (175, 1), (176, 3), (177, 1), (178, 5), (179, 1), (180, 1), (181, 3), (182, 1), (183, 1), (184, 3), (185, 9), (186, 1), (187, 2), (188, 1), (189, 2), (190, 1), (191, 1), (192, 1), (193, 3), (194, 3), (195, 1), (196, 1), (197, 6), (198, 1), (199, 2), (200, 2), (201, 6), (202, 1), (203, 1)]\naccount\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","execution_count":41,"outputs":[{"output_type":"stream","text":"Number of unique tokens: 2619\nNumber of documents: 1000\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(dictionary))","execution_count":56,"outputs":[{"output_type":"stream","text":"2619\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set training parameters.\nnum_topics = 10\nchunksize = 500 # size of the doc looked at every pass\npasses = 20 # number of passes through documents\niterations = 100\neval_every = 1  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","execution_count":42,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 40.6 s, sys: 15.2 ms, total: 40.6 s\nWall time: 40.6 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = model.print_topics(num_words=4)\nn = 0\ntopic_kw = []\nfor topic in topics:\n    print(topic)\n    wp = model.show_topic(n,topn=4)\n    topic_keywords = \" \".join([word for word, prop in wp])\n    topic_kw.append(topic_keywords)\n    print(topic_keywords)\n    print()\n    n+=1","execution_count":151,"outputs":[{"output_type":"stream","text":"(0, '0.017*\"temperature\" + 0.016*\"normal\" + 0.015*\"during\" + 0.013*\"water\"')\ntemperature normal during water\n\n(1, '0.029*\"school\" + 0.025*\"student\" + 0.017*\"test\" + 0.014*\"high_school\"')\nschool student test high_school\n\n(2, '0.019*\"county\" + 0.011*\"farm\" + 0.011*\"climate_change\" + 0.011*\"change\"')\ncounty farm climate_change change\n\n(3, '0.020*\"response_rate\" + 0.020*\"farm\" + 0.017*\"response\" + 0.016*\"rate\"')\nresponse_rate farm response rate\n\n(4, '0.058*\"child\" + 0.023*\"health\" + 0.022*\"mental_health\" + 0.022*\"language\"')\nchild health mental_health language\n\n(5, '0.018*\"associated_with\" + 0.011*\"patient\" + 0.009*\"disease\" + 0.009*\"brain\"')\nassociated_with patient disease brain\n\n(6, '0.019*\"survey\" + 0.013*\"program\" + 0.012*\"information\" + 0.011*\"institution\"')\nsurvey program information institution\n\n(7, '0.021*\"student\" + 0.012*\"school\" + 0.011*\"science\" + 0.011*\"teacher\"')\nstudent school science teacher\n\n(8, '0.013*\"united_state\" + 0.010*\"woman\" + 0.010*\"country\" + 0.010*\"long_term\"')\nunited_state woman country long_term\n\n(9, '0.011*\"method\" + 0.009*\"image\" + 0.007*\"value\" + 0.007*\"feature\"')\nmethod image value feature\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":149,"outputs":[{"output_type":"execute_result","execution_count":149,"data":{"text/plain":"'temperature normal during water storm_surge average storm water_level region precipitation'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.print_topics())","execution_count":59,"outputs":[{"output_type":"stream","text":"[(0, '0.017*\"temperature\" + 0.016*\"normal\" + 0.015*\"during\" + 0.013*\"water\" + 0.013*\"storm_surge\" + 0.012*\"average\" + 0.011*\"storm\" + 0.011*\"water_level\" + 0.011*\"region\" + 0.011*\"precipitation\"'), (1, '0.029*\"school\" + 0.025*\"student\" + 0.017*\"test\" + 0.014*\"high_school\" + 0.013*\"score\" + 0.012*\"variable\" + 0.011*\"test_score\" + 0.010*\"standard_deviation\" + 0.009*\"difference\" + 0.008*\"grade\"'), (2, '0.019*\"county\" + 0.011*\"farm\" + 0.011*\"climate_change\" + 0.011*\"change\" + 0.010*\"land\" + 0.008*\"production\" + 0.008*\"agricultural\" + 0.007*\"rural\" + 0.007*\"crop\" + 0.007*\"state\"'), (3, '0.020*\"response_rate\" + 0.020*\"farm\" + 0.017*\"response\" + 0.016*\"rate\" + 0.016*\"sars\" + 0.012*\"farmer\" + 0.010*\"group\" + 0.010*\"more_likely\" + 0.009*\"control_group\" + 0.008*\"practice\"'), (4, '0.058*\"child\" + 0.023*\"health\" + 0.022*\"mental_health\" + 0.022*\"language\" + 0.018*\"early\" + 0.016*\"care\" + 0.015*\"literacy\" + 0.013*\"family\" + 0.012*\"early_childhood\" + 0.012*\"woman\"'), (5, '0.018*\"associated_with\" + 0.011*\"patient\" + 0.009*\"disease\" + 0.009*\"brain\" + 0.008*\"patient_with\" + 0.007*\"associated\" + 0.007*\"cognitive\" + 0.007*\"subject\" + 0.006*\"have_been\" + 0.006*\"clinical\"'), (6, '0.019*\"survey\" + 0.013*\"program\" + 0.012*\"information\" + 0.011*\"institution\" + 0.011*\"food\" + 0.010*\"data_collection\" + 0.010*\"faculty\" + 0.008*\"university\" + 0.007*\"available\" + 0.007*\"record\"'), (7, '0.021*\"student\" + 0.012*\"school\" + 0.011*\"science\" + 0.011*\"teacher\" + 0.008*\"education\" + 0.007*\"research\" + 0.006*\"learning\" + 0.006*\"this_study\" + 0.005*\"will\" + 0.005*\"mathematics\"'), (8, '0.013*\"united_state\" + 0.010*\"woman\" + 0.010*\"country\" + 0.010*\"long_term\" + 0.010*\"state\" + 0.008*\"research\" + 0.008*\"more_than\" + 0.007*\"associated_with\" + 0.007*\"have_been\" + 0.007*\"increase\"'), (9, '0.011*\"method\" + 0.009*\"image\" + 0.007*\"value\" + 0.007*\"feature\" + 0.006*\"accuracy\" + 0.006*\"classification\" + 0.005*\"parameter\" + 0.005*\"variable\" + 0.005*\"linear\" + 0.004*\"subject\"')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":43,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.gensim.prepare(model, corpus, dictionary)","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\ntopic                                                \n7      0.080965 -0.042660       1        1  18.777408\n5      0.025324  0.005433       2        1  15.400666\n9     -0.037660 -0.154507       3        1  14.171783\n1      0.064805 -0.078208       4        1  11.305674\n8     -0.061598  0.010943       5        1   8.557412\n0     -0.277175  0.191069       6        1   8.062174\n2     -0.062162 -0.024031       7        1   7.320743\n6      0.072308 -0.105512       8        1   7.313138\n3     -0.050819 -0.046335       9        1   5.127609\n4      0.246014  0.243808      10        1   3.963392, topic_info=                 Term         Freq        Total Category  logprob  loglift\n1097            child   460.000000   460.000000  Default  30.0000  30.0000\n645            school   957.000000   957.000000  Default  29.0000  29.0000\n649           student  1218.000000  1218.000000  Default  28.0000  28.0000\n71               farm   313.000000   313.000000  Default  27.0000  27.0000\n1710           county   234.000000   234.000000  Default  26.0000  26.0000\n...               ...          ...          ...      ...      ...      ...\n32    consistent_with    43.074090   225.737865  Topic10  -5.0523   1.5716\n811       development    44.716790   273.556786  Topic10  -5.0148   1.4169\n157            sample    49.722166   444.655836  Topic10  -4.9087   1.0372\n408         cognitive    43.127879   249.361575  Topic10  -5.0510   1.4733\n231   associated_with    39.607663   849.765430  Topic10  -5.1361   0.1621\n\n[608 rows x 6 columns], token_table=      Topic      Freq              Term\nterm                                   \n1997      5  0.960336             1970s\n1000      2  0.055289             1980s\n1000      4  0.055289             1980s\n1000      5  0.718755             1980s\n1000      8  0.055289             1980s\n...     ...       ...               ...\n999       9  0.124193             would\n1085      4  0.848136  year_institution\n1085      8  0.117797  year_institution\n2315      1  0.020819       young_child\n2315     10  0.957688       young_child\n\n[1908 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 6, 10, 2, 9, 1, 3, 7, 4, 5])","text/html":"\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el371406656564418087745079934\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el371406656564418087745079934_data = {\"mdsDat\": {\"x\": [0.08096522090679775, 0.025323602587784287, -0.03766033582793437, 0.06480496296770094, -0.06159822724223914, -0.27717518516957806, -0.06216233014876898, 0.0723076691026995, -0.050819230600509045, 0.2460138534240466], \"y\": [-0.04265986833405722, 0.0054326584161679065, -0.1545067647030332, -0.07820818004354801, 0.010942940844104358, 0.19106860004144405, -0.0240310812308982, -0.10551221993242442, -0.04633455552523363, 0.2438084704674775], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [18.77740805935936, 15.400666192390174, 14.17178340477268, 11.305674417908996, 8.557411633494821, 8.062173793978927, 7.320743209185461, 7.313138183204996, 5.127609126468437, 3.9633919792361416]}, \"tinfo\": {\"Term\": [\"child\", \"school\", \"student\", \"farm\", \"county\", \"high_school\", \"normal\", \"teacher\", \"survey\", \"test\", \"temperature\", \"health\", \"patient\", \"early\", \"response_rate\", \"woman\", \"language\", \"associated_with\", \"mental_health\", \"score\", \"science\", \"test_score\", \"storm_surge\", \"response\", \"variable\", \"rate\", \"program\", \"during\", \"united_state\", \"water\", \"labor_market\", \"attitude\", \"belief\", \"boy\", \"occupational\", \"father\", \"problem_solving\", \"reform\", \"psychological\", \"engage\", \"thinking\", \"lesson\", \"motivation\", \"cooperation\", \"conflict\", \"conceptual\", \"engaging\", \"political\", \"profession\", \"gender_difference\", \"peer\", \"research_question\", \"learn\", \"expectation\", \"mathematical\", \"mathematics_science\", \"intrinsic\", \"society\", \"believe\", \"learned\", \"academic_achievement\", \"science\", \"self\", \"occupation\", \"concept\", \"educational\", \"instruction\", \"classroom\", \"teacher\", \"achievement\", \"mathematics\", \"teaching\", \"student\", \"what\", \"learning\", \"education\", \"gender\", \"problem\", \"academic\", \"school\", \"knowledge\", \"research\", \"ability\", \"parent\", \"social\", \"will\", \"this_study\", \"support\", \"about\", \"work\", \"value\", \"related\", \"activity\", \"question\", \"policy\", \"group\", \"patient\", \"patient_with\", \"alzheimer_disease\", \"protein\", \"dementia\", \"apoe\", \"cancer\", \"cognitive_decline\", \"atrophy\", \"amyloid\", \"cognitively_normal\", \"white_matter\", \"human_brain\", \"alzheimer\", \"hippocampal_volume\", \"cortical\", \"mild_cognitive\", \"biomarkers\", \"hippocampal\", \"clinical_trial\", \"cortex\", \"marker\", \"apoe_genotype\", \"plasma\", \"neuroimaging_initiative\", \"mutation\", \"tissue\", \"binding\", \"carrier\", \"receptor\", \"trial\", \"gene\", \"disease\", \"progression\", \"clinical\", \"brain\", \"association_between\", \"associated_with\", \"cognitive\", \"association\", \"adni\", \"risk\", \"associated\", \"participant\", \"diagnosis\", \"subject\", \"pattern\", \"have_been\", \"group\", \"this_study\", \"difference\", \"significant\", \"region\", \"change\", \"higher\", \"individual\", \"relationship_between\", \"related\", \"compared_with\", \"spatial_resolution\", \"machine_learning\", \"atlas\", \"performed_using\", \"correlation_coefficient\", \"accuracy\", \"resolution\", \"cross_validation\", \"maximum_likelihood\", \"segmentation\", \"classifier\", \"voxels\", \"reconstruction\", \"optimal\", \"alignment\", \"anatomical\", \"template\", \"extraction\", \"normalized\", \"mapping\", \"kernel\", \"image\", \"thickness\", \"gaussian\", \"matrix\", \"covariance\", \"parameter\", \"latent\", \"dimensional\", \"annotation\", \"multi\", \"classification\", \"linear_regression\", \"time_point\", \"feature\", \"map\", \"method\", \"linear\", \"algorithm\", \"intensity\", \"spatial\", \"software\", \"proposed\", \"prediction\", \"reference\", \"derived_from\", \"correlation\", \"technique\", \"then\", \"value\", \"function\", \"regression\", \"where\", \"network\", \"this_paper\", \"variable\", \"training\", \"approach\", \"figure\", \"estimate\", \"subject\", \"scale\", \"therefore\", \"sample\", \"mean\", \"information\", \"learning\", \"school_district\", \"base_year\", \"test_score\", \"public_school\", \"education_statistic\", \"nels\", \"ecls\", \"entered\", \"district\", \"national_center_education_statistic\", \"larger_than\", \"minority\", \"race_ethnicity\", \"racial\", \"deviation\", \"first_year\", \"standard_deviation\", \"attending\", \"year_institution\", \"matching\", \"quartile\", \"factor_analysis\", \"transcript\", \"table_present\", \"enrolled\", \"dropped\", \"standard_error\", \"graduation\", \"spent\", \"administrator\", \"high_school\", \"test\", \"score\", \"statistically_significant\", \"grade\", \"school\", \"math\", \"race\", \"student\", \"variable\", \"more_likely\", \"le_than\", \"difference\", \"percent\", \"standard\", \"likely\", \"difference_between\", \"le_likely\", \"factor\", \"sample\", \"le\", \"teacher\", \"significant\", \"group\", \"education\", \"estimate\", \"relationship_between\", \"state\", \"lake\", \"migration\", \"vegetation\", \"soil\", \"canada\", \"1970s\", \"habitat\", \"elevation\", \"engineer\", \"marsh\", \"bird\", \"heavily\", \"florida\", \"funding\", \"sediment\", \"1990s\", \"freshwater\", \"grey\", \"mission\", \"annually\", \"america\", \"grant\", \"expansion\", \"1980s\", \"long_term\", \"germany\", \"mobility\", \"narrow\", \"adjacent\", \"dominated\", \"mortality\", \"united_state\", \"specie\", \"employment\", \"united\", \"position\", \"graduate_student\", \"woman\", \"site\", \"country\", \"wetland\", \"graduate\", \"long\", \"industry\", \"trend\", \"state\", \"increase\", \"research\", \"more_than\", \"area\", \"found_that\", \"population\", \"have_been\", \"term\", \"program\", \"associated_with\", \"rate\", \"period\", \"would\", \"storm_surge\", \"water_level\", \"precipitation\", \"surge\", \"hurricane\", \"flooding\", \"storm\", \"tropical_cyclone\", \"tropical\", \"ocean\", \"flux\", \"coast\", \"flood\", \"tide\", \"wind\", \"cyclone\", \"warm\", \"atmospheric\", \"august\", \"southern\", \"heat\", \"marine\", \"anomaly\", \"station\", \"eastern\", \"east\", \"shore\", \"temperature\", \"northern\", \"warming\", \"september\", \"height\", \"pacific\", \"summer\", \"season\", \"coastal\", \"south\", \"normal\", \"event\", \"water\", \"wave\", \"during\", \"western\", \"record\", \"condition\", \"average\", \"region\", \"more_than\", \"central\", \"area\", \"above\", \"surface\", \"total\", \"observed\", \"associated_with\", \"county\", \"climate_change\", \"firm\", \"fertilizer\", \"agriculture\", \"data_source\", \"payment\", \"establishment\", \"consumer\", \"facility\", \"urban\", \"domestic\", \"spending\", \"expenditure\", \"rural\", \"price\", \"extension\", \"maker\", \"agricultural\", \"business\", \"watershed\", \"contract\", \"emission\", \"planning\", \"usda\", \"recovery\", \"resource_management\", \"stream\", \"capital\", \"benchmark\", \"crop\", \"land\", \"production\", \"input\", \"climate\", \"income\", \"administrative\", \"sector\", \"environmental\", \"north_carolina\", \"scenario\", \"farm\", \"service\", \"category\", \"census_bureau\", \"change\", \"government\", \"economic\", \"farmer\", \"management\", \"resource\", \"state\", \"source\", \"will\", \"have_been\", \"area\", \"estimate\", \"research\", \"system\", \"independent_variable\", \"code\", \"modality\", \"faculty\", \"credit\", \"undergraduate\", \"consent\", \"principal_component\", \"collect\", \"package\", \"security\", \"food\", \"sheet\", \"update\", \"university\", \"doctoral\", \"file\", \"coded\", \"request\", \"salary\", \"field_study\", \"name\", \"illustrated\", \"board\", \"store\", \"supplement\", \"data_file\", \"utilization\", \"biased\", \"institution\", \"nonresponse\", \"information_about\", \"survey\", \"science_engineering\", \"list\", \"data_collection\", \"financial\", \"interview\", \"sampling\", \"collection\", \"postsecondary_education\", \"program\", \"information\", \"item\", \"degree\", \"record\", \"available\", \"enrollment\", \"report\", \"included\", \"course\", \"statistical\", \"field\", \"respondent\", \"variable\", \"about\", \"graduate_student\", \"graduate\", \"total\", \"student\", \"measure\", \"sample\", \"sars\", \"management_practice\", \"virus\", \"arm\", \"coronavirus\", \"respiratory\", \"covid\", \"response_rate\", \"transmission\", \"cropland\", \"adoption\", \"feed\", \"infection\", \"viral\", \"returned\", \"incentive\", \"card\", \"animal\", \"nitrogen\", \"targeting\", \"host\", \"manager\", \"control_group\", \"producer\", \"nutrient\", \"targeted\", \"adopt\", \"genome\", \"farmer\", \"antibody\", \"recommendation\", \"operation\", \"farm\", \"response\", \"rate\", \"practice\", \"technology\", \"china\", \"more_likely\", \"percent\", \"group\", \"cost\", \"control\", \"treatment\", \"management\", \"survey\", \"likely\", \"sample\", \"plan\", \"mental_health\", \"early_childhood\", \"childhood\", \"young_child\", \"literacy\", \"preschool\", \"care\", \"language\", \"infant\", \"mother\", \"mental\", \"health_care\", \"sample_member\", \"child\", \"socioeconomic_status\", \"well_being\", \"english\", \"living\", \"african_american\", \"birth\", \"speak\", \"health\", \"nationally_representative\", \"book\", \"retention\", \"home\", \"speaking\", \"working_with\", \"dealing_with\", \"educational_attainment\", \"early\", \"medical\", \"family\", \"community\", \"woman\", \"american\", \"skill\", \"household\", \"center\", \"consistent_with\", \"development\", \"sample\", \"cognitive\", \"associated_with\"], \"Freq\": [460.0, 957.0, 1218.0, 313.0, 234.0, 361.0, 299.0, 474.0, 373.0, 496.0, 238.0, 235.0, 288.0, 255.0, 183.0, 278.0, 161.0, 849.0, 149.0, 378.0, 401.0, 213.0, 181.0, 343.0, 490.0, 471.0, 401.0, 393.0, 282.0, 274.0, 120.1992147642216, 93.91693595142951, 75.44348411561354, 59.57004099062377, 61.662048908235676, 53.254240689862776, 51.034473784378434, 47.57133218709348, 32.5331891385264, 24.504933592251678, 32.13078733427035, 22.51110022808585, 72.20099889645219, 21.3448341594978, 17.069380930598378, 14.592294272424779, 14.464949507499362, 38.21916251960319, 13.594996802430932, 90.24018905363019, 64.10334521778576, 11.989516954860807, 43.428006790781176, 44.028087673364645, 43.8780758114778, 42.550768034099924, 67.6358385099123, 25.240461663504377, 21.900522601674712, 12.122569823446181, 83.2639390551381, 351.86232503585495, 146.84504386934273, 95.46440601139777, 111.06529043725301, 137.7912521788501, 52.62985253604786, 78.070177700237, 349.1650240850349, 134.395515614306, 154.7496510796811, 75.29374716402336, 656.1898363740257, 103.64590733054925, 186.7799847055458, 262.8848584149047, 124.87742272275794, 136.88938294558864, 124.12858193463869, 396.4561929597872, 100.97426219741587, 217.22022231544196, 120.39729134797798, 115.75624085954051, 97.48450631140983, 158.6541351532453, 177.91608915998154, 122.76585159403419, 143.0146964157838, 119.97652256390101, 150.81425817276127, 129.30410261985517, 118.81440986979513, 103.56820430669524, 106.37378276436415, 109.84852260416547, 287.10500516521455, 197.63477735938127, 140.64567810472565, 130.68104372126194, 96.4614378122129, 86.37111737137425, 85.62711685040905, 85.01591393246518, 81.69282332642717, 81.33028684519806, 78.37786194223345, 72.46566292974933, 68.40595757251769, 65.69103513743822, 63.230489094746744, 63.105546489444514, 61.75661973606737, 60.896701630453144, 59.29066233730565, 56.593058679198755, 55.84013673336098, 55.35195685184307, 55.83621228143616, 54.744172594659794, 44.770683818887434, 42.30257223727567, 41.92776574991204, 41.69441320712131, 37.88000742373415, 36.77221813136818, 56.919129445648146, 90.09401456418827, 227.18588816555638, 62.678841562922884, 162.7048883090255, 223.81090826116457, 153.77716237543774, 478.6176740850893, 178.49121555592447, 156.90889938692393, 100.10067227136872, 152.2218240761016, 182.26701362942077, 137.41771889464792, 78.28812971460454, 177.77538089955016, 113.78368219651568, 162.96441402109747, 159.45224892768138, 137.88207174662224, 131.1833857553781, 116.98309772209674, 120.4804059617974, 117.0683753311588, 110.83394468997786, 103.41436613071717, 101.50774672702327, 101.24232824319463, 95.79714510363385, 95.54967870246725, 82.18145675852536, 63.435700807835, 52.8307374173113, 49.859587774252994, 141.44106055814143, 87.0551914699525, 37.975847167027055, 37.97149565823196, 81.38422782309941, 37.72845884517265, 33.891701016369524, 24.97617166224231, 49.83680964445184, 24.820268765951894, 24.723111238395326, 23.86335179449752, 23.72440632423337, 23.099535829632917, 54.10550518969846, 21.50814532387241, 219.69572363062895, 19.7742891678914, 17.54895364548447, 76.90402427690746, 17.30052791496113, 124.87093002986913, 40.445227471186485, 15.31659038029731, 15.189918576109543, 52.32050618824257, 138.87379373654994, 75.11354663615671, 86.25857937211349, 165.54923799470035, 56.13648219798044, 253.55218920174846, 111.63630949964278, 52.86134377685209, 71.80968601393494, 96.17234820840523, 36.34128924541, 74.04848682048392, 65.99231876963022, 64.87289873970524, 79.22355230074287, 88.36144038973102, 66.9438356083917, 104.74387845552806, 178.3705436959942, 98.92254662782369, 100.26654600792259, 105.1042237585862, 73.01570443345106, 91.35107833755406, 124.3258026162232, 85.36920050881399, 94.51016157050795, 101.29506100037673, 104.71829651851358, 107.3418077128046, 85.35319637724025, 83.84465276701944, 92.19505795445521, 87.01638117050548, 84.62882156844566, 83.4541160678881, 141.76587215681957, 140.91622402333547, 211.65561355090895, 66.4546582962297, 27.954210037763364, 52.24514681275775, 25.53460426476619, 19.647305615103505, 110.39238846742735, 12.732219611244476, 57.84130605511235, 49.45800496634968, 82.23292851384124, 37.60269181010772, 73.09444229855843, 24.446986835229456, 196.25551298160664, 16.066802676761455, 36.46355947449534, 43.33642991759631, 20.116088799951786, 13.320896688586092, 25.521317888052657, 38.47895295876649, 28.66560156669524, 10.32552220188599, 104.83457630291397, 29.12551822001281, 19.60945062528872, 16.308879789452885, 269.39648891335503, 335.40737775382524, 258.55740016107023, 127.53238703940559, 161.73237067026503, 556.6528743401861, 60.43708381509794, 63.076617147687934, 484.29714563984504, 230.621670265463, 147.28730559559745, 122.79669292006146, 176.37789451278894, 120.25240098822269, 106.04313554876396, 112.27767611427494, 101.05818542345195, 71.24276873744334, 128.071473945884, 128.62951966196243, 95.25174354682251, 124.52384136932542, 108.2570127384945, 124.10892169249709, 109.4020657877703, 99.51443370810986, 97.77686193974068, 102.2944894869374, 71.74735025281737, 38.58545072671974, 42.476344290167916, 86.69411103949106, 21.790480959483865, 16.805234641499286, 66.22015819075554, 45.94242138484242, 35.48374873331047, 64.87946369121684, 31.974912333935876, 10.563220133484274, 28.796570009311687, 63.640561258347, 41.33775421093605, 11.107218683110645, 15.243666730051554, 28.724104703650717, 18.490925757919413, 16.374573814164286, 28.05892798431192, 29.08126408759603, 17.910754658222963, 12.55889927862505, 145.6398712997464, 14.031223056045071, 37.1320982586447, 9.355698113689193, 10.31731857771313, 14.236493452745199, 41.34988988943186, 182.42198864498462, 59.254922087649824, 66.45358715396549, 65.58230367838738, 57.78211664162879, 82.88749678345383, 152.3432625317357, 71.23436726711455, 151.52929169405382, 57.24792330712637, 91.29391242935768, 69.58476150943602, 54.687237581302405, 63.657484841249364, 143.03445218982415, 99.07889600728642, 119.61906686067152, 110.77099920093787, 97.92223446708017, 75.33638913010822, 83.08350966201569, 100.78226580044388, 73.41114364802309, 84.69114845561019, 104.34071491224054, 72.24392433841116, 64.30893104264716, 65.04146687256555, 180.73850751463848, 151.16943249167645, 147.40750047945096, 132.59471772520823, 91.72307944217287, 91.08017637307816, 156.60321235398186, 72.9130429479478, 67.33267110513803, 126.62471848574809, 57.3892227339503, 74.98526886873582, 51.746248152948134, 68.11643671538765, 118.78854259475428, 42.153861926575885, 38.379226892362915, 30.725630624023285, 57.93061009091912, 89.68566401257706, 68.62238193918701, 24.49086333725445, 75.36297163048184, 73.67870148818213, 110.84901986792266, 40.16191833912206, 18.135039313451568, 226.3808674696096, 92.01112732447744, 29.64811640311863, 50.651998609756625, 45.06139608098156, 67.4637719066441, 55.8010387348607, 68.8494540399073, 131.9681671650808, 110.47535578669802, 219.93829794759392, 129.0557744716277, 181.23756847890772, 84.3393569937916, 206.2369243435921, 77.13327906635314, 126.63436538140105, 137.750141262416, 160.12600088547688, 147.90473670023027, 144.5725892018772, 82.50499212911355, 114.80622495445682, 92.36235854517574, 88.64519655067303, 107.74662416352685, 91.65728021276213, 89.13169485783862, 233.25993436032488, 136.39013971561766, 34.27922975377864, 25.752712576005056, 50.658529826618285, 64.81025783518706, 33.7781119275193, 23.146273605284904, 17.90891680220313, 43.782250884412754, 61.42661030015535, 13.908148779141264, 24.589664663380315, 61.336991103484884, 93.11734056491855, 37.876572348529045, 30.321052429813218, 13.680006252120405, 99.86114890518029, 49.13836369616674, 16.539379458915143, 13.719455441785438, 15.28465566755787, 30.60155779246061, 21.79286818087525, 19.325227730358858, 27.072134950258285, 15.676358008115582, 27.644825312277337, 14.92990987594068, 89.27755570934536, 120.38198095618571, 105.52755710539326, 60.555811115078846, 76.69239953071295, 74.1494309271221, 34.5274017570222, 46.23442132006656, 62.744422089582514, 40.94228580411915, 45.50394288959326, 138.41926803151736, 72.91725822006504, 70.51191351051435, 48.6254156029476, 135.49069181843723, 45.25824759503007, 52.79564264098208, 64.66182911848284, 56.072910468905164, 54.70575569040034, 84.37950654710647, 56.84185464740959, 66.36648832010714, 65.31950969416509, 60.816534672371574, 58.324878837861505, 61.46766283280821, 53.31035194926281, 55.31713322391551, 71.41704959163874, 15.514388258621178, 118.98216845591952, 70.52760500768915, 23.09784307433469, 17.635178396703242, 37.06286672994496, 24.60542516653567, 22.338400581207964, 11.01790471645159, 138.44421255199737, 17.5516292905768, 12.337975529228762, 93.94905925654396, 47.926198004031676, 71.24781291581577, 11.400794593068063, 9.82447131450254, 32.63388585937942, 11.99416976385644, 36.91535904322592, 7.369386365228122, 18.038675220137726, 15.527751485639689, 24.27085979195522, 36.9206082623276, 8.933503183773976, 7.885849190645135, 139.4752124281279, 23.294546998070118, 68.78456910587035, 231.42171485346196, 29.57390364551391, 38.617133304928444, 124.02801648898912, 37.551440546873955, 64.3610052905189, 34.54941966630874, 58.74220528187311, 38.055504479487254, 161.3328797432095, 154.5977786011678, 75.04401691791195, 84.6130655425784, 85.46805742386833, 86.26588085280275, 49.3591595819134, 76.85811115725055, 76.77944810760327, 55.002365276283335, 62.27159037741479, 67.82634065309658, 54.1839848173057, 85.20345511148797, 73.25989589326751, 49.62120384044267, 52.17015981675627, 60.005687110941494, 70.60016808276843, 57.60277170222645, 56.49507072272811, 135.38224595824346, 55.09744238402183, 58.66726766371775, 30.67061603364885, 28.101001886034403, 26.63261152519409, 71.18085394401734, 174.76821338709124, 14.398790287756643, 26.778514409772175, 60.52215833876336, 31.223836125477398, 30.368647319340326, 47.182141404570565, 16.59996064549869, 62.12246612182694, 34.840442604290516, 59.773845815087036, 20.73475153299182, 12.7531448942115, 31.444430265773978, 23.824268598877826, 75.9057530259174, 41.19213926999468, 27.516966299900787, 29.751146447252808, 27.68348927064047, 36.28563815798732, 108.4884838721797, 14.844544690942216, 23.205246024082328, 56.551021505171136, 174.04669094682313, 144.2386531076613, 137.60066938128858, 71.94159167753857, 57.3301251759092, 44.575209019745614, 85.29905746944954, 68.00436243815403, 89.88050259366288, 54.41674621294037, 59.97883337241057, 49.58814816383382, 44.58797509099709, 57.82115226359413, 51.428698312559426, 50.683701071849555, 40.079648796370655, 148.15034382521517, 82.15273784428219, 45.52863865742875, 46.18270422358075, 99.77895030209079, 18.368913473049187, 106.64698533422083, 146.749992180009, 21.318821912728765, 48.403389355112495, 66.47878239356888, 28.947470733012235, 51.75806812300186, 387.3349114053317, 38.603775815872105, 66.78776910083431, 71.17468905085849, 28.227098737272055, 70.66594246243828, 18.20192885376676, 15.517849916153047, 154.39455266147039, 40.61961673690364, 24.89809881228895, 9.6122124049723, 68.02759269181514, 10.121069556384134, 9.594734826813703, 21.30503525656723, 28.926716255974156, 122.26809010666729, 33.81374163010845, 90.67687161980953, 59.12283366352597, 79.25691587914464, 39.590857112803015, 45.902664632619455, 40.67331114409948, 36.643413992596294, 43.07409005870096, 44.71678950847479, 49.72216590000033, 43.127878562059884, 39.60766268753024], \"Total\": [460.0, 957.0, 1218.0, 313.0, 234.0, 361.0, 299.0, 474.0, 373.0, 496.0, 238.0, 235.0, 288.0, 255.0, 183.0, 278.0, 161.0, 849.0, 149.0, 378.0, 401.0, 213.0, 181.0, 343.0, 490.0, 471.0, 401.0, 393.0, 282.0, 274.0, 121.09856517059762, 94.81572710096793, 76.3423447663986, 60.468687849368926, 62.593740229981094, 54.1529044661996, 51.933335148172254, 48.8455951281568, 33.432102339376954, 25.4035334099696, 33.3416778183237, 23.420996425480237, 75.32924879011203, 22.306124617600076, 17.971436637226603, 15.490866667603107, 15.36367192063572, 40.67130768774727, 14.495192244663928, 96.47914543426447, 68.59205255958983, 12.888330386568626, 46.94309379885971, 47.94662949069903, 48.04050552158215, 46.69632296023879, 74.31502266187782, 27.77456304014376, 24.15598763281963, 13.464358065814597, 93.09797133759233, 401.6859128550255, 165.23671054343257, 108.29583050993662, 127.69966500718259, 161.96295862774082, 58.998178671137644, 91.48418801325755, 474.51209820197914, 168.51952759094644, 198.55019617263955, 89.16702050100612, 1218.452250436556, 135.43441078044935, 291.3029027880097, 452.90185226679836, 188.21828389057856, 216.08363179498366, 189.92791568120384, 957.6509589927898, 148.55234128480274, 510.0970353110185, 198.81962083363058, 196.6655828396563, 146.20806239772156, 393.9822793421195, 530.9233377310926, 247.70036794440338, 359.8105227238884, 239.3567327962311, 507.2556998427981, 330.8183273047149, 268.41983861677517, 178.37665355220835, 212.15535779241512, 569.8730148743153, 288.0060810901724, 198.53405834169843, 141.54489218195457, 131.58248791301781, 97.36061608038519, 87.27075818797115, 86.52636601579734, 85.91510325906142, 82.59200597051446, 82.22946680478833, 79.27703016178522, 73.36690357499593, 69.30517064027907, 66.59028014691484, 64.12968066260643, 64.00653934208276, 62.65583743864261, 61.79590731119929, 60.18987970569078, 57.492278298269284, 56.74359177267411, 56.25151516501069, 56.74789173670904, 55.643348062174496, 45.6698724744533, 43.201799171398434, 42.82735096816806, 42.59359163410464, 38.77924243593575, 37.67140236878584, 58.946291108863846, 95.16843063311987, 253.10011624964005, 65.62019525759521, 186.98401344057356, 281.94704142556753, 198.4821530753307, 849.7654297513026, 249.36157526782958, 211.6093403415822, 122.84028600659096, 240.441432815328, 325.83379889670084, 225.27709172749135, 93.21945048092267, 371.26598154546525, 226.34161638219092, 595.1846270967753, 569.8730148743153, 530.9233377310926, 493.48697188883506, 359.2225923438477, 401.0662753559405, 486.10413060223124, 407.30935928686216, 352.0584589566477, 324.67930200425945, 330.8183273047149, 210.71677067321042, 96.44832518096575, 83.08007770501565, 64.37749183087865, 53.73320182617631, 50.75998960846917, 144.2345680030888, 88.8737091879801, 38.874465990251196, 38.87021621850585, 83.31568618852738, 38.62706577585976, 34.790522772181674, 25.87489332734706, 51.64132318610208, 25.719482173091972, 25.626507192861027, 24.76213685909321, 24.623223825030358, 24.000028039360142, 56.36209186581052, 22.406905729729623, 229.2471963938089, 20.673708940409412, 18.447628658787355, 80.86812593589127, 18.199162185799484, 131.3893327599455, 42.73289510468735, 16.215282471489484, 16.08858995093066, 57.40007119436488, 158.58642399310253, 83.9190062224343, 97.13338314841207, 194.23650065363597, 62.13195050118892, 328.93956325040944, 133.72408524528478, 59.2598442097635, 83.20211836485507, 118.88849233073401, 39.580186131688315, 89.33314002981508, 80.90558994407606, 81.3583890874656, 108.80438044201847, 127.47789858776925, 90.28697971932553, 197.54313656598396, 507.2556998427981, 183.38396987276326, 207.94316830644846, 259.39537306048345, 114.31475938140721, 200.00858914702178, 490.85855514242803, 177.13355605814536, 243.348555140735, 302.8669163075853, 339.7857684301125, 371.26598154546525, 202.43172698254858, 200.07362614092239, 444.6558361724322, 341.133592847432, 405.2624258571764, 291.3029027880097, 142.66528476042174, 141.81559331455952, 213.31659514167038, 67.35400703273753, 28.884406897551866, 54.440302937504526, 26.796548578538964, 20.64045983670187, 116.26677825913593, 13.644890292562174, 62.98643522874499, 54.44434601449384, 91.65763246762508, 42.878121821017196, 83.75705239746681, 28.037250781102596, 226.44178736979026, 18.698184102494064, 42.4460115821249, 52.73354713265852, 24.535340578463405, 16.47436482515113, 31.93235455226596, 48.8799447538054, 37.084478733583445, 13.475985592042129, 137.6716825310837, 38.418314504670825, 25.880450703300745, 21.668327239673054, 361.9446829442952, 496.173073617501, 378.5420773255551, 186.09948476950711, 243.00495631927365, 957.6509589927898, 90.07248587749483, 94.8759660539212, 1218.452250436556, 490.85855514242803, 298.31349017229786, 241.68177007646327, 493.48697188883506, 277.7054531631821, 235.2841607174802, 263.6013855841671, 234.6918941710655, 130.68915433329423, 388.3117162937622, 444.6558361724322, 249.25331390079356, 474.51209820197914, 359.2225923438477, 569.8730148743153, 452.90185226679836, 339.7857684301125, 324.67930200425945, 479.03915773858955, 72.64400489190648, 39.48219592328282, 44.1814416271385, 90.27383610732286, 22.759899826451715, 17.702139489683102, 71.58424543409046, 49.901787539141296, 38.563081755091886, 70.88484334631595, 35.09020414206876, 11.690526007027284, 32.4174139066447, 72.8759101535689, 47.7845275993589, 13.104055643284282, 18.651785104591283, 35.529244557451435, 22.95459358001136, 21.24874472485441, 36.894747485236095, 41.438339976135204, 25.593890314122604, 18.086837995845976, 210.79002234068318, 20.328685248710308, 54.15972952167456, 13.686128945652877, 15.322211363955343, 21.190120037006384, 61.94232248501517, 282.3335032661407, 90.7519627952351, 103.06863750265364, 101.90956923213585, 89.35343575470773, 133.30423304976674, 278.6837035838885, 117.60572191587698, 299.41543959226215, 94.11858706060599, 171.75545061860322, 126.66281752004284, 96.77693080373963, 129.07797533694395, 479.03915773858955, 320.91526461310946, 510.0970353110185, 459.69486044587177, 452.4827777187259, 250.17175668310796, 323.6936641281693, 595.1846270967753, 252.77672039169713, 401.60804526785495, 849.7654297513026, 471.77675065459715, 220.0106307586995, 314.02765612766615, 181.63251365134863, 152.06858052913213, 148.31625731905706, 133.48861395706066, 92.61696272509711, 91.97464148013414, 158.36174141601066, 73.80692499157817, 68.22662445216788, 128.6269003023877, 58.34666626533832, 76.25919903949563, 52.6505855110423, 69.49046816539354, 121.25628483781841, 43.047697564001346, 39.28198617489683, 31.620225351273874, 59.752431277880255, 92.65795247084756, 71.12079280567721, 25.38561353364192, 78.52199620792466, 76.81960983000003, 115.68806858111512, 42.07115685569803, 19.03011661858417, 238.98948373335892, 97.69218421110864, 31.718647587552617, 54.553658151981686, 48.42645888214308, 74.25454396888581, 61.07081305988291, 76.79080812182491, 158.15621681950165, 133.38256559581228, 299.2880560708409, 163.87454228210805, 274.5624416244166, 106.08271618193268, 393.48682788136983, 95.25072240839937, 220.21370877728734, 259.2327886973491, 371.00616146594353, 401.0662753559405, 459.69486044587177, 120.90275531758587, 452.4827777187259, 191.3002131982599, 161.62296890614198, 376.245028521528, 238.13627862378542, 849.7654297513026, 234.22358976567767, 137.33577341868713, 35.17675123157527, 26.657211663892518, 58.008363805436716, 75.17719525885812, 39.77104201830426, 27.294407665512157, 21.122189501284414, 51.77006024705064, 73.34141996911305, 16.632171042197093, 29.83897363488612, 74.9675857884805, 114.20852992649141, 46.89035874375868, 38.99084553679967, 17.667708473118434, 129.47056791164312, 64.49365842273914, 21.95374825254561, 18.364164511829312, 20.593654858392153, 41.58437725095632, 29.933065751198882, 27.00709597582781, 37.94397086773761, 22.016227167754643, 38.926637773204476, 21.085930231514315, 131.22051650757274, 181.47318754491528, 159.99757800716304, 90.05493545511385, 117.91229575419673, 114.11646137123674, 50.837912857436386, 71.18287871819743, 101.75677979574712, 63.90244210416587, 73.08640114508142, 313.26581308852707, 145.05716454264962, 146.53492623116836, 86.31101838851487, 486.10413060223124, 83.74430918930196, 113.62948335404815, 173.9502077209193, 136.6637589243775, 139.27971744129405, 479.03915773858955, 159.12677042888066, 393.9822793421195, 595.1846270967753, 452.4827777187259, 339.7857684301125, 510.0970353110185, 358.8189277635344, 56.213524639592094, 74.40163135945852, 16.41115162653659, 133.1278296921556, 80.75148009518405, 27.099703119484033, 21.355947651949897, 44.98946285600101, 30.14839183534788, 27.722817251863418, 13.737592572463825, 174.5682847508683, 22.75554758292143, 16.083413486531597, 122.50875617641256, 62.78452590010748, 93.73645895503506, 15.0068269288237, 13.059547284713933, 43.64808679752201, 16.303215585279133, 51.60225141183332, 10.568496787048037, 26.020085615132523, 22.617084939843945, 35.68216402305046, 54.52088446794437, 13.327255374713317, 11.964150175501278, 211.83235629457826, 35.405149627925, 105.79273116435289, 373.0851048417037, 45.4885701099919, 60.3027972891969, 226.42403316715706, 59.48381187784954, 110.16202848370651, 55.060208617286804, 104.35753538080537, 62.54299532878675, 401.60804526785495, 405.2624258571764, 169.42998841251324, 211.2251060032919, 220.21370877728734, 230.19208681314518, 96.48697008619914, 204.18802568926517, 243.83023588196642, 131.4855545147633, 176.42262469297106, 227.88758270998187, 140.53625090044895, 490.85855514242803, 359.8105227238884, 133.30423304976674, 171.75545061860322, 376.245028521528, 1218.452250436556, 311.15101057486135, 444.6558361724322, 137.36745383488602, 56.028732505486786, 59.698910153980925, 31.570467793463834, 29.00326575972261, 27.532623544476714, 74.75855605911282, 183.66927371697838, 15.29873766750105, 28.918634587313456, 65.72124781003288, 34.38584347910822, 34.155640046737766, 53.86243876782766, 19.335316765046162, 73.12611638416068, 42.24865124104857, 73.22462197084012, 25.963493966220224, 16.92420318956848, 42.3470591605112, 32.2243909613899, 109.79455205382031, 59.732237715042444, 39.95919291489442, 44.004170158883774, 41.51867842098067, 57.42017506940124, 173.9502077209193, 24.036169957243345, 37.94448733641131, 95.25755545275135, 313.26581308852707, 343.46627287511996, 471.77675065459715, 180.9561583092022, 131.43266763577367, 94.0295181901192, 298.31349017229786, 277.7054531631821, 569.8730148743153, 170.07876528076105, 269.84807775028423, 172.1692786527974, 136.6637589243775, 373.0851048417037, 263.6013855841671, 444.6558361724322, 108.62245162865386, 149.05140944208392, 83.05507173297406, 46.43091783155851, 48.03235554571979, 105.63199215142058, 19.48678271870135, 116.29042829078722, 161.34719694630206, 23.925895678972882, 54.574471154034285, 75.49163105879535, 34.06946917244445, 61.43833440059025, 460.0149394181474, 47.352140774413954, 83.50531875977612, 96.54333175115546, 38.30494964198723, 97.19075494869462, 25.44753975830824, 22.3643744402656, 235.66799472373202, 63.526622993319094, 39.017438032707716, 15.455619132770005, 109.57082482058951, 17.965549630931765, 17.7598515992808, 39.805000320160474, 55.207767314584345, 255.49652052803123, 67.58576280376388, 206.065318596961, 162.48918222403816, 278.6837035838885, 104.21433349094639, 146.72298609994576, 116.47778967709223, 109.61038460237518, 225.73786528204215, 273.55678583243593, 444.6558361724322, 249.36157526782958, 849.7654297513026], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.5816, -5.8283, -6.0473, -6.2836, -6.2491, -6.3956, -6.4382, -6.5085, -6.8885, -7.1719, -6.9009, -7.2567, -6.0913, -7.3099, -7.5334, -7.6902, -7.699, -6.7274, -7.761, -5.8683, -6.2102, -7.8867, -6.5996, -6.5859, -6.5893, -6.62, -6.1566, -7.1423, -7.2842, -7.8757, -5.9487, -4.5075, -5.3813, -5.812, -5.6606, -5.445, -6.4074, -6.0131, -4.5152, -5.4699, -5.3289, -6.0493, -3.8843, -5.7297, -5.1408, -4.799, -5.5434, -5.4516, -5.5494, -4.3882, -5.7559, -4.9898, -5.5799, -5.6192, -5.791, -5.304, -5.1894, -5.5604, -5.4078, -5.5834, -5.3547, -5.5086, -5.5932, -5.7305, -5.7038, -5.6716, -4.5126, -4.8861, -5.2262, -5.2997, -5.6033, -5.7138, -5.7225, -5.7296, -5.7695, -5.774, -5.8109, -5.8894, -5.947, -5.9875, -6.0257, -6.0277, -6.0493, -6.0633, -6.09, -6.1366, -6.15, -6.1588, -6.1501, -6.1698, -6.3709, -6.4276, -6.4365, -6.4421, -6.5381, -6.5677, -6.1309, -5.6716, -4.7467, -6.0345, -5.0805, -4.7617, -5.137, -4.0016, -4.9879, -5.1168, -5.5663, -5.1471, -4.967, -5.2495, -5.8121, -4.992, -5.4382, -5.079, -5.1007, -5.2461, -5.2959, -5.4105, -5.381, -5.4097, -5.4644, -5.5337, -5.5523, -5.555, -5.6102, -5.5297, -5.6804, -5.9393, -6.1222, -6.1801, -5.1374, -5.6228, -6.4524, -6.4525, -5.6901, -6.4589, -6.5662, -6.8714, -6.1806, -6.8777, -6.8816, -6.917, -6.9228, -6.9495, -6.0984, -7.0209, -4.6971, -7.1049, -7.2243, -5.7468, -7.2386, -5.262, -6.3894, -7.3604, -7.3687, -6.1319, -5.1558, -5.7703, -5.632, -4.9801, -6.0615, -4.5538, -5.3741, -6.1217, -5.8153, -5.5232, -6.4964, -5.7846, -5.8998, -5.9169, -5.7171, -5.6079, -5.8855, -5.4378, -4.9055, -5.495, -5.4815, -5.4344, -5.7987, -5.5746, -5.2664, -5.6423, -5.5406, -5.4713, -5.4381, -5.4133, -5.6425, -5.6604, -5.5654, -5.6232, -5.651, -5.665, -4.9092, -4.9152, -4.5084, -5.6669, -6.5328, -5.9074, -6.6233, -6.8854, -5.1593, -7.3192, -5.8057, -5.9623, -5.4538, -6.2363, -5.5716, -6.6669, -4.584, -7.0866, -6.2671, -6.0944, -6.8619, -7.274, -6.6239, -6.2133, -6.5077, -7.5288, -5.211, -6.4918, -6.8874, -7.0717, -4.2672, -4.048, -4.3083, -5.015, -4.7774, -3.5414, -5.7618, -5.719, -3.6807, -4.4226, -4.871, -5.0528, -4.6907, -5.0738, -5.1995, -5.1424, -5.2477, -5.5973, -5.0108, -5.0064, -5.3069, -5.0389, -5.1789, -5.0422, -5.1683, -5.2631, -5.2807, -5.2355, -5.3117, -5.932, -5.8359, -5.1225, -6.5034, -6.7632, -5.3919, -5.7575, -6.0158, -5.4123, -6.1199, -7.2275, -6.2246, -5.4316, -5.8631, -7.1773, -6.8607, -6.2271, -6.6676, -6.7891, -6.2506, -6.2148, -6.6995, -7.0544, -4.6037, -6.9436, -5.9704, -7.3489, -7.251, -6.9291, -5.8628, -4.3785, -5.503, -5.3884, -5.4016, -5.5282, -5.1674, -4.5587, -5.3189, -4.5641, -5.5375, -5.0708, -5.3423, -5.5832, -5.4314, -4.6218, -4.989, -4.8006, -4.8774, -5.0007, -5.2629, -5.165, -4.9719, -5.2888, -5.1459, -4.9372, -5.3048, -5.4212, -5.4098, -4.3282, -4.5069, -4.5321, -4.638, -5.0065, -5.0135, -4.4715, -5.236, -5.3156, -4.684, -5.4754, -5.208, -5.5789, -5.304, -4.7479, -5.7839, -5.8777, -6.1002, -5.466, -5.0289, -5.2966, -6.327, -5.2029, -5.2255, -4.8171, -5.8323, -6.6274, -4.103, -5.0033, -6.1359, -5.6003, -5.7172, -5.3137, -5.5035, -5.2933, -4.6427, -4.8205, -4.1319, -4.665, -4.3254, -5.0904, -4.1962, -5.1797, -4.684, -4.5998, -4.4493, -4.5287, -4.5515, -5.1124, -4.782, -4.9995, -5.0406, -4.8455, -5.0072, -5.0351, -3.9766, -4.5133, -5.8942, -6.1802, -5.5037, -5.2573, -5.909, -6.2869, -6.5435, -5.6496, -5.3109, -6.7963, -6.2265, -5.3124, -4.8949, -5.7945, -6.0169, -6.8128, -4.825, -5.5341, -6.623, -6.81, -6.7019, -6.0077, -6.3472, -6.4674, -6.1303, -6.6766, -6.1093, -6.7254, -4.937, -4.6381, -4.7698, -5.3252, -5.089, -5.1227, -5.887, -5.5951, -5.2897, -5.7166, -5.611, -4.4985, -5.1395, -5.173, -5.5446, -4.5199, -5.6164, -5.4624, -5.2596, -5.4021, -5.4268, -4.9935, -5.3885, -5.2336, -5.2495, -5.3209, -5.3628, -5.3103, -5.4527, -5.4147, -5.1592, -6.686, -4.6488, -5.1717, -6.288, -6.5578, -5.8151, -6.2248, -6.3214, -7.0282, -4.4973, -6.5626, -6.9151, -4.885, -5.5581, -5.1616, -6.9941, -7.1429, -5.9424, -6.9433, -5.8191, -7.4304, -6.5352, -6.6851, -6.2385, -5.819, -7.2379, -7.3627, -4.4899, -6.2795, -5.1968, -3.9835, -6.0409, -5.774, -4.6072, -5.802, -5.2632, -5.8854, -5.3546, -5.7887, -4.3443, -4.3869, -5.1097, -4.9897, -4.9796, -4.9703, -5.5286, -5.0858, -5.0868, -5.4204, -5.2962, -5.2108, -5.4354, -4.9827, -5.1337, -5.5233, -5.4732, -5.3333, -5.1707, -5.3742, -5.3936, -4.1646, -5.0636, -5.0008, -5.6494, -5.7369, -5.7906, -4.8075, -3.9093, -6.4056, -5.7851, -4.9697, -5.6315, -5.6593, -5.2187, -6.2633, -4.9436, -5.5219, -4.9821, -6.0409, -6.5269, -5.6245, -5.902, -4.7432, -5.3545, -5.7579, -5.6798, -5.7519, -5.4813, -4.3861, -6.3751, -5.9283, -5.0376, -3.9134, -4.1012, -4.1484, -4.7969, -5.0239, -5.2755, -4.6265, -4.8531, -4.5742, -5.076, -4.9787, -5.169, -5.2752, -5.0154, -5.1325, -5.1471, -5.3818, -3.8169, -4.4066, -4.9968, -4.9826, -4.2122, -5.9045, -4.1456, -3.8264, -5.7556, -4.9356, -4.6183, -5.4497, -4.8686, -2.8559, -5.1618, -4.6137, -4.55, -5.4749, -4.5572, -5.9136, -6.0732, -3.7757, -5.1109, -5.6004, -6.5521, -4.5953, -6.5006, -6.554, -5.7562, -5.4504, -4.009, -5.2943, -4.3079, -4.7356, -4.4425, -5.1366, -4.9886, -5.1096, -5.2139, -5.0523, -5.0148, -4.9087, -5.051, -5.1361], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6651, 1.663, 1.6607, 1.6575, 1.6575, 1.6558, 1.6551, 1.6461, 1.6453, 1.6365, 1.6355, 1.6329, 1.6301, 1.6285, 1.621, 1.6128, 1.6122, 1.6103, 1.6084, 1.6057, 1.6048, 1.6002, 1.5947, 1.5873, 1.5819, 1.5795, 1.5783, 1.5768, 1.5745, 1.5675, 1.5609, 1.5401, 1.5545, 1.5464, 1.533, 1.5109, 1.5583, 1.514, 1.3658, 1.4463, 1.4233, 1.5034, 1.0536, 1.405, 1.2281, 1.1286, 1.2622, 1.216, 1.2472, 0.7906, 1.2864, 0.8188, 1.1709, 1.1425, 1.2672, 0.7629, 0.5792, 0.9706, 0.7499, 0.9819, 0.4595, 0.7331, 0.8575, 1.1288, 0.9822, 0.0262, 1.8676, 1.8662, 1.8644, 1.8639, 1.8615, 1.8604, 1.8603, 1.8602, 1.8598, 1.8598, 1.8594, 1.8584, 1.8577, 1.8572, 1.8566, 1.8566, 1.8563, 1.8561, 1.8557, 1.855, 1.8547, 1.8546, 1.8546, 1.8545, 1.8509, 1.8497, 1.8495, 1.8494, 1.8473, 1.8466, 1.8358, 1.816, 1.7627, 1.8249, 1.7317, 1.6398, 1.6156, 1.2967, 1.5364, 1.5717, 1.6661, 1.4136, 1.2898, 1.3765, 1.6962, 1.1344, 1.183, 0.5754, 0.5971, 0.5225, 0.5459, 0.7488, 0.6681, 0.4471, 0.5692, 0.6457, 0.7081, 0.6867, 1.0825, 1.9446, 1.943, 1.9392, 1.937, 1.936, 1.9344, 1.9332, 1.9305, 1.9305, 1.9305, 1.9304, 1.9277, 1.9186, 1.9183, 1.9183, 1.918, 1.9169, 1.9167, 1.9157, 1.9131, 1.913, 1.9114, 1.9094, 1.904, 1.9037, 1.9033, 1.903, 1.8989, 1.8969, 1.8964, 1.8613, 1.8212, 1.8431, 1.8352, 1.7941, 1.8524, 1.6936, 1.7734, 1.8397, 1.8067, 1.7419, 1.8685, 1.7663, 1.7502, 1.7275, 1.6366, 1.5874, 1.6548, 1.3195, 0.9088, 1.3367, 1.2245, 1.0505, 1.5056, 1.1703, 0.5807, 1.224, 1.0081, 0.8587, 0.7769, 0.713, 1.0903, 1.0842, 0.3805, 0.5877, 0.3877, 0.7039, 2.1735, 2.1735, 2.172, 2.1664, 2.1471, 2.1387, 2.1316, 2.1306, 2.128, 2.1106, 2.0946, 2.0838, 2.0714, 2.0486, 2.0437, 2.0428, 2.0368, 2.0282, 2.0279, 1.9836, 1.9813, 1.9674, 1.9558, 1.9406, 1.9224, 1.9136, 1.9074, 1.9029, 1.9024, 1.8957, 1.8846, 1.7883, 1.7987, 1.802, 1.7727, 1.6373, 1.7809, 1.7716, 1.2572, 1.4245, 1.4741, 1.5028, 1.151, 1.3429, 1.3829, 1.3264, 1.3373, 1.5731, 1.0706, 0.9395, 1.2179, 0.8421, 0.9804, 0.6556, 0.7592, 0.9519, 0.9797, 0.6359, 2.446, 2.4354, 2.419, 2.4179, 2.4148, 2.4064, 2.3805, 2.3757, 2.3752, 2.3698, 2.3654, 2.357, 2.3399, 2.3229, 2.3134, 2.293, 2.2566, 2.2458, 2.2421, 2.1978, 2.1846, 2.1043, 2.1014, 2.0936, 2.0886, 2.0876, 2.0809, 2.078, 2.0629, 2.0606, 2.0542, 2.0216, 2.0321, 2.0195, 2.0176, 2.0225, 1.9832, 1.8544, 1.957, 1.7773, 1.9612, 1.8264, 1.8594, 1.8876, 1.7515, 1.2497, 1.2831, 1.0081, 1.0353, 0.9278, 1.2582, 1.0984, 0.6825, 1.2219, 0.9019, 0.3611, 0.5819, 1.2284, 0.8839, 2.5131, 2.5121, 2.5118, 2.5113, 2.5083, 2.5082, 2.5068, 2.5058, 2.5048, 2.5023, 2.5014, 2.5011, 2.5007, 2.498, 2.4974, 2.497, 2.4947, 2.4893, 2.487, 2.4854, 2.4822, 2.4821, 2.4769, 2.4762, 2.4753, 2.4715, 2.4698, 2.4638, 2.4581, 2.4505, 2.4438, 2.446, 2.4221, 2.4277, 2.4088, 2.337, 2.3296, 2.2099, 2.2791, 2.1026, 2.2886, 1.872, 2.307, 1.9647, 1.8857, 1.6777, 1.5204, 1.3612, 2.1359, 1.1465, 1.7899, 1.9174, 1.2675, 1.5632, 0.2631, 2.6103, 2.6075, 2.5886, 2.5799, 2.479, 2.4661, 2.4511, 2.4496, 2.4494, 2.4469, 2.4372, 2.4356, 2.421, 2.4138, 2.4103, 2.401, 2.363, 2.3587, 2.3548, 2.3425, 2.3313, 2.3229, 2.3163, 2.3078, 2.2971, 2.2798, 2.2769, 2.2748, 2.2722, 2.2692, 2.2293, 2.204, 2.1983, 2.2176, 2.1843, 2.1833, 2.2276, 2.1829, 2.1309, 2.1693, 2.1406, 1.7977, 1.9267, 1.883, 2.0406, 1.3369, 1.9991, 1.8479, 1.6249, 1.7236, 1.6799, 0.878, 1.585, 0.8333, 0.4049, 0.6076, 0.8522, 0.4984, 0.7078, 2.5994, 2.5746, 2.5593, 2.5032, 2.4801, 2.4557, 2.4241, 2.4217, 2.4123, 2.3995, 2.3949, 2.3836, 2.3558, 2.3504, 2.3501, 2.3455, 2.3412, 2.3407, 2.3309, 2.3247, 2.3086, 2.2806, 2.255, 2.2491, 2.2394, 2.2301, 2.2257, 2.2155, 2.1987, 2.1976, 2.1969, 2.185, 2.1379, 2.1849, 2.1698, 2.0136, 2.1555, 2.0781, 2.1495, 2.0408, 2.1187, 1.7035, 1.6518, 1.8011, 1.7007, 1.669, 1.634, 1.9452, 1.6384, 1.46, 1.744, 1.5741, 1.4036, 1.6624, 0.8644, 1.0239, 1.6273, 1.4239, 0.7797, -0.2328, 0.9288, 0.5524, 2.956, 2.9538, 2.9531, 2.9416, 2.9389, 2.9373, 2.9215, 2.9209, 2.9099, 2.8936, 2.8881, 2.8741, 2.853, 2.8381, 2.818, 2.8075, 2.7777, 2.7676, 2.7457, 2.6876, 2.6729, 2.6685, 2.6014, 2.5989, 2.5975, 2.5791, 2.5652, 2.5116, 2.4984, 2.4886, 2.4788, 2.4491, 2.3828, 2.1029, 1.7384, 2.0481, 2.1409, 2.2241, 1.7185, 1.5635, 1.1236, 1.8309, 1.4667, 1.7258, 1.8505, 1.1061, 1.3363, 0.7988, 1.9735, 3.222, 3.2171, 3.2084, 3.1888, 3.1711, 3.169, 3.1415, 3.1332, 3.1127, 3.1081, 3.1009, 3.0652, 3.0566, 3.0561, 3.0238, 3.0047, 2.9232, 2.9228, 2.9094, 2.893, 2.8626, 2.8052, 2.7809, 2.7789, 2.7531, 2.7514, 2.6542, 2.6123, 2.603, 2.5817, 2.4911, 2.5355, 2.4072, 2.2171, 1.9707, 2.2602, 2.066, 2.1759, 2.1324, 1.5716, 1.4169, 1.0372, 1.4733, 0.1621]}, \"token.table\": {\"Topic\": [5, 2, 4, 5, 8, 10, 2, 5, 1, 2, 3, 4, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 5, 8, 10, 1, 10, 1, 3, 5, 1, 4, 1, 2, 4, 5, 6, 7, 8, 10, 3, 5, 4, 5, 7, 9, 10, 1, 4, 2, 3, 1, 3, 9, 8, 9, 1, 10, 4, 5, 6, 7, 9, 5, 6, 7, 9, 2, 3, 4, 3, 2, 2, 1, 5, 7, 1, 2, 4, 5, 6, 7, 8, 10, 2, 3, 2, 9, 3, 5, 8, 5, 6, 7, 2, 9, 2, 2, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 8, 10, 1, 2, 4, 8, 10, 3, 6, 2, 4, 7, 1, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 1, 1, 3, 5, 7, 9, 2, 6, 8, 2, 2, 5, 9, 1, 8, 10, 1, 6, 8, 1, 7, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 5, 2, 1, 6, 7, 1, 8, 9, 2, 8, 9, 10, 2, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 4, 10, 10, 1, 6, 9, 2, 3, 4, 7, 8, 3, 1, 4, 4, 6, 7, 7, 1, 2, 3, 8, 2, 6, 5, 6, 7, 8, 10, 1, 8, 1, 2, 4, 10, 2, 2, 1, 4, 8, 9, 1, 2, 4, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 7, 8, 9, 10, 1, 5, 6, 1, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 1, 8, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 7, 5, 7, 9, 1, 2, 3, 4, 5, 7, 8, 9, 2, 9, 1, 9, 1, 2, 3, 10, 3, 2, 2, 1, 2, 3, 5, 6, 7, 8, 9, 1, 4, 5, 6, 9, 7, 1, 4, 5, 8, 10, 3, 2, 9, 4, 7, 8, 5, 6, 7, 9, 5, 9, 3, 6, 1, 4, 7, 8, 9, 10, 2, 4, 8, 4, 7, 1, 7, 9, 10, 1, 2, 3, 4, 5, 8, 2, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 3, 4, 2, 3, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 9, 3, 2, 3, 5, 9, 4, 6, 5, 8, 1, 6, 7, 3, 4, 5, 9, 1, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 10, 10, 6, 9, 1, 6, 7, 4, 1, 2, 5, 6, 7, 8, 9, 10, 1, 2, 4, 8, 10, 4, 1, 4, 7, 8, 10, 1, 2, 4, 10, 2, 3, 5, 3, 7, 1, 5, 8, 10, 1, 1, 1, 5, 8, 1, 4, 10, 4, 7, 8, 1, 4, 8, 10, 4, 1, 2, 4, 5, 6, 7, 9, 2, 7, 8, 2, 3, 4, 5, 7, 8, 9, 1, 2, 4, 5, 6, 9, 10, 5, 7, 1, 3, 4, 4, 5, 7, 8, 1, 3, 4, 5, 7, 3, 2, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 9, 2, 4, 5, 8, 1, 2, 4, 7, 8, 9, 10, 7, 9, 7, 9, 1, 1, 2, 3, 6, 8, 9, 6, 9, 7, 1, 3, 4, 5, 7, 8, 9, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 4, 7, 8, 1, 4, 7, 8, 9, 7, 1, 4, 5, 6, 6, 5, 9, 6, 2, 5, 7, 8, 9, 1, 2, 5, 7, 9, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 7, 8, 3, 1, 2, 4, 5, 10, 1, 4, 10, 2, 3, 2, 3, 9, 1, 5, 9, 1, 5, 6, 7, 8, 9, 1, 4, 1, 4, 5, 8, 5, 8, 1, 4, 5, 8, 4, 5, 8, 2, 3, 5, 1, 2, 3, 4, 5, 7, 8, 9, 10, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 5, 8, 9, 10, 8, 9, 10, 3, 6, 5, 5, 6, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 2, 1, 4, 6, 8, 10, 1, 2, 5, 9, 4, 6, 7, 8, 9, 10, 2, 6, 7, 8, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 4, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 7, 8, 3, 8, 10, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 8, 9, 10, 1, 3, 7, 1, 4, 5, 8, 1, 8, 2, 3, 5, 6, 7, 1, 4, 8, 9, 10, 1, 2, 1, 3, 4, 8, 3, 1, 2, 3, 6, 7, 1, 5, 3, 5, 7, 9, 1, 10, 4, 7, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 9, 10, 3, 4, 5, 6, 9, 1, 10, 1, 1, 3, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 5, 8, 2, 3, 8, 1, 2, 3, 6, 8, 9, 1, 10, 5, 7, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 5, 6, 10, 3, 5, 7, 1, 2, 5, 6, 7, 9, 9, 7, 9, 3, 5, 3, 5, 6, 2, 1, 5, 3, 4, 5, 1, 4, 1, 8, 1, 4, 8, 1, 4, 2, 3, 7, 8, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 10, 1, 2, 10, 10, 1, 2, 3, 7, 8, 10, 5, 2, 4, 5, 5, 6, 1, 5, 7, 8, 1, 4, 5, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 2, 5, 9, 1, 10, 1, 2, 2, 3, 2, 1, 4, 8, 10, 1, 5, 4, 4, 10, 4, 10, 1, 2, 3, 5, 7, 2, 5, 9, 4, 8, 9, 1, 2, 6, 7, 3, 6, 7, 8, 9, 5, 6, 2, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 1, 4, 8, 9, 1, 4, 6, 1, 4, 7, 8, 9, 3, 8, 4, 6, 4, 8, 9, 3, 4, 6, 1, 4, 10, 1, 2, 4, 7, 8, 10, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 8, 9, 1, 5, 8, 1, 2, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 8, 9, 1, 5, 7, 8, 2, 1, 5, 7, 8, 9, 1, 7, 1, 2, 3, 4, 5, 7, 8, 10, 1, 2, 3, 5, 6, 8, 9, 10, 4, 8, 1, 2, 5, 7, 9, 10, 6, 2, 3, 5, 6, 7, 8, 10, 3, 7, 8, 9, 3, 8, 1, 3, 6, 7, 8, 9, 1, 3, 7, 9, 2, 3, 5, 7, 9, 1, 1, 4, 5, 7, 8, 10, 2, 3, 1, 2, 3, 6, 7, 2, 1, 4, 2, 4, 1, 2, 3, 4, 5, 8, 9, 10, 1, 2, 4, 7, 8, 10, 4, 8, 10, 4, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 1, 2, 7, 9, 3, 2, 4, 6, 8, 9, 2, 7, 9, 1, 3, 4, 5, 1, 2, 3, 5, 6, 7, 9, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 4, 8, 1, 2, 3, 5, 7, 8, 9, 10, 1, 1, 3, 1, 2, 5, 6, 7, 8, 9, 10, 7, 9, 9, 1, 4, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 8, 9, 2, 4, 10, 2, 9, 1, 2, 4, 5, 6, 7, 10, 4, 6, 7, 10, 4, 8, 9, 1, 2, 3, 4, 5, 8, 9, 10, 4, 8, 10, 2, 3, 4, 5, 8, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 7, 1, 4, 8, 10, 4, 1, 5, 7, 8, 3, 5, 8, 1, 2, 3, 4, 7, 10, 4, 5, 6, 9, 1, 4, 6, 7, 8, 9, 8, 9, 5, 6, 9, 2, 3, 1, 2, 9, 10, 5, 6, 9, 1, 4, 5, 6, 7, 9, 10, 1, 5, 8, 6, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 5, 7, 8, 1, 4, 9, 10, 1, 2, 7, 8, 10, 1, 7, 1, 2, 10, 3, 4, 10, 5, 9, 1, 2, 4, 5, 6, 7, 8, 9, 1, 5, 6, 9, 5, 6, 9, 1, 2, 3, 5, 3, 1, 3, 10, 1, 10, 2, 3, 5, 1, 4, 7, 3, 4, 7, 1, 2, 3, 4, 7, 8, 9, 3, 4, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 9, 7, 8, 6, 9, 6, 6, 7, 9, 1, 4, 5, 8, 10, 1, 2, 3, 4, 6, 8, 1, 5, 6, 2, 8, 10, 1, 2, 3, 4, 5, 7, 8, 10, 3, 5, 6, 9, 6, 1, 4, 5, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 4, 7, 9, 1, 3, 7, 8, 9, 1, 9, 1, 4, 1, 4, 5, 1, 2, 3, 7, 9, 1, 3, 5, 7, 9, 5, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 1, 3, 7, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 6, 2, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 5, 7, 10, 2, 4, 10, 9, 2, 4, 7, 9, 10, 1, 2, 3, 4, 5, 8, 9, 2, 9, 6, 6, 1, 5, 8, 1, 5, 7, 9, 10, 1, 4, 5, 7, 8, 9, 10, 1, 5, 8, 5, 8, 1, 4, 5, 6, 7, 8, 7, 8, 9, 1, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 8, 9, 5, 6, 2, 9, 9, 3, 6, 5, 6, 3, 5, 6, 7, 9, 6, 3, 7, 4, 6, 10, 1, 7, 10, 5, 6, 9, 3, 5, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 1, 2, 5, 7, 8, 10, 1, 2, 3, 5, 7, 8, 9, 10, 1, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 1, 10], \"Freq\": [0.9603358966811717, 0.05528882385244293, 0.05528882385244293, 0.7187547100817581, 0.05528882385244293, 0.05528882385244293, 0.07631225226920428, 0.839434774961247, 0.60356216100228, 0.090534324150342, 0.035207792725133, 0.130771801550494, 0.010059369350037999, 0.020118738700075998, 0.10562337817539899, 0.3974314006089683, 0.013896202818495396, 0.013896202818495396, 0.10283190085686593, 0.05002633014658343, 0.06392253296507883, 0.05002633014658343, 0.20288456115003278, 0.05836405183768066, 0.04446784901918527, 0.09932032841128494, 0.036591699940999714, 0.13591202835228466, 0.057501242764428126, 0.057501242764428126, 0.4809194849388534, 0.04704647135271392, 0.03136431423514261, 0.057501242764428126, 0.6528792755675549, 0.13162888620313606, 0.16848497434001414, 0.010530310896250884, 0.031590932688752654, 0.8915339271897235, 0.09667235355069291, 0.006933150726936589, 0.977574252498059, 0.006933150726936589, 0.7951600738239847, 0.19582300325516042, 0.44333533845051265, 0.24960897206877602, 0.03352956341222364, 0.018627535229013135, 0.055882605687039406, 0.10803970432827618, 0.07823564796185517, 0.011176521137407882, 0.2610589232184709, 0.6526473080461772, 0.1376925134521148, 0.059011077193763486, 0.6884625672605741, 0.09835179532293915, 0.01967035906458783, 0.23075154554825908, 0.738404945754429, 0.8140651837511558, 0.17909434042525427, 0.24085545061440808, 0.07225663518432242, 0.6743952617203426, 0.060863117078391316, 0.9281625354454676, 0.2675151562895562, 0.730522157559942, 0.015447526277670268, 0.0463425788330108, 0.030895052555340537, 0.7723763138835134, 0.13130397336019728, 0.01723889340085605, 0.05171668020256815, 0.8791835634436586, 0.0344777868017121, 0.06749933371139323, 0.8943661716759602, 0.016874833427848308, 0.9720257908674109, 0.991135641033308, 0.9961503931822978, 0.18972890389888533, 0.7589156155955413, 0.027104129128412188, 0.1439341355218006, 0.019191218069573412, 0.09595609034786706, 0.21110339876530754, 0.009595609034786706, 0.02878682710436012, 0.10555169938265377, 0.38382436139146825, 0.9850483427345204, 0.9755523767579393, 0.17753591142029967, 0.8193965142475369, 0.9323377651956571, 0.7529856566672847, 0.18824641416682117, 0.01273528499392731, 0.9551463745445482, 0.01273528499392731, 0.33283172877504075, 0.6240594914532014, 0.9854389005623844, 0.986820801375688, 0.37805854218774354, 0.08218663960603122, 0.39038653812864826, 0.00410933198030156, 0.08218663960603122, 0.06163997970452341, 0.05967077937446681, 0.05967077937446681, 0.16354213606335347, 0.05967077937446681, 0.2165828288406573, 0.2541533195579142, 0.13481176080898058, 0.008840115462883972, 0.002210028865720993, 0.041990548448698865, 0.9819303344760086, 0.02762144390936335, 0.5585669768337922, 0.018414295939575567, 0.08286433172809005, 0.10434767699092821, 0.1074167263141908, 0.018414295939575567, 0.03375954255588854, 0.006138098646525189, 0.042966690525676324, 0.02353590685120401, 0.563684969086336, 0.018828725480963204, 0.09061324137713543, 0.12238671562626084, 0.10473478548785783, 0.005883976712801002, 0.009414362740481602, 0.015298339453282605, 0.04707181370240802, 0.11814223303964143, 0.7419332234889482, 0.028354135929513944, 0.03780551457268526, 0.07088533982378485, 0.030229418146843646, 0.7758883991023202, 0.020152945431229096, 0.06549707265149457, 0.10580296351395276, 0.9786028968867355, 0.9803851697961132, 0.9928321637965082, 0.8556980673789512, 0.1069622584223689, 0.9913967110108297, 0.9706717996171482, 0.01673572068305428, 0.021720989931589923, 0.09122815771267767, 0.17811211743903738, 0.03475358389054388, 0.15204692952112947, 0.030409385904225893, 0.060818771808451785, 0.37360102682334667, 0.02606518791790791, 0.03475358389054388, 0.008086119077230971, 0.016172238154461942, 0.08625193682379703, 0.2641465565228784, 0.06199357959210411, 0.43125968411898513, 0.008086119077230971, 0.04043059538615486, 0.08355656379805337, 0.9942489165295775, 0.9824167731485578, 0.9107472786626869, 0.04139760357557668, 0.09484997711938115, 0.7113748283953586, 0.14227496567907172, 0.16716607286452784, 0.08358303643226392, 0.6686642914581113, 0.9860638276479753, 0.9871203879701748, 0.9119354185128838, 0.05699596365705524, 0.19648264812583996, 0.03929652962516799, 0.7073375332530238, 0.23059109369381073, 0.0768636978979369, 0.6917732810814321, 0.25629565917724156, 0.07688869775317247, 0.6407391479431039, 0.9922490818630552, 0.7944754407332015, 0.20216562554371648, 0.015505400444881887, 0.031010800889763775, 0.7597646217992124, 0.015505400444881887, 0.13954860400393698, 0.015505400444881887, 0.9666123387077233, 0.9939166979958313, 0.12844674716401522, 0.12844674716401522, 0.7193017841184852, 0.07100818397452688, 0.0946775786327025, 0.8284288130361469, 0.04299580002833398, 0.008599160005666797, 0.02579748001700039, 0.9201101206063472, 0.979905681829059, 0.013648623242522197, 0.10236467431891648, 0.13648623242522198, 0.020472934863783297, 0.08871605107639428, 0.484526125109538, 0.15695916728900527, 0.5677143071054329, 0.40551021936102355, 0.02317201253491563, 0.027369669496944655, 0.10947867798777862, 0.12772512431907507, 0.08210900849083397, 0.05473933899388931, 0.10947867798777862, 0.09123223165648219, 0.07298578532518575, 0.3375592571289841, 0.0413555504741481, 0.1985066422759109, 0.00827111009482962, 0.6865021378708585, 0.0413555504741481, 0.01654222018965924, 0.117258826682635, 0.2406891705590929, 0.10697296469293018, 0.06582951673411087, 0.12137317147851692, 0.002057172397940965, 0.2777182737220303, 0.06582951673411087, 0.09347522507505691, 0.06304143086457327, 0.8412770256755123, 0.9907191618929053, 0.11698454072431799, 0.39349345516361506, 0.4785731211449373, 0.03152855001142763, 0.8764936903176882, 0.01891713000685658, 0.05675139002056974, 0.01891713000685658, 0.9837661555889743, 0.8526063541023781, 0.14210105901706302, 0.008480879738654466, 0.3307543098075241, 0.6530277398763938, 0.9902736673378258, 0.01069610157146205, 0.8717322780741571, 0.0855688125716964, 0.03208830471438615, 0.9914374884273093, 0.9834879063069694, 0.15807156052886404, 0.8346178395924021, 0.01344056550546149, 0.9542801508877657, 0.01344056550546149, 0.19990901569191036, 0.7329997242036713, 0.06416385516820312, 0.7138228887462598, 0.04411265042813965, 0.1724403607645459, 0.9893487498198996, 0.983891548924334, 0.06633853012534731, 0.033169265062673654, 0.8292316265668412, 0.06633853012534731, 0.07665953369641816, 0.03832976684820908, 0.10540685883257497, 0.00958244171205227, 0.5653640610110839, 0.11498930054462725, 0.08624197540847044, 0.14154788451263095, 0.006154255848375259, 0.04923404678700207, 0.11077660527075466, 0.03692553509025155, 0.2400159780866351, 0.04923404678700207, 0.3631010950541403, 0.07593130792998702, 0.45558784757992216, 0.028474240473745135, 0.14237120236872566, 0.004745706745624189, 0.004745706745624189, 0.18508256307934337, 0.10915125514935635, 0.8692270257228686, 0.09397048926733714, 0.03132349642244572, 0.968312510969468, 0.054005514002878764, 0.054005514002878764, 0.08100827100431815, 0.023145220286948043, 0.1427288584361796, 0.5323400665998049, 0.05786305071737011, 0.04243290385940474, 0.011572610143474021, 0.9459455213939693, 0.1404760888579012, 0.8428565331474072, 0.03543933575346171, 0.38540277631889613, 0.008859833938365428, 0.05758892059937529, 0.124037675137116, 0.048729086661009856, 0.013289750907548142, 0.13289750907548142, 0.1904864296748567, 0.09468715352063206, 0.8521843816856886, 0.10890775884259238, 0.7623543118981467, 0.10890775884259238, 0.037057888584457285, 0.340932574977007, 0.14823155433782914, 0.21122996493140653, 0.0037057888584457284, 0.018528944292228643, 0.014823155433782913, 0.2223473315067437, 0.30056136103933184, 0.6922019223936128, 0.9414454711434046, 0.965408524404315, 0.10197846170996792, 0.17257893520148418, 0.6903157408059367, 0.02353349116383875, 0.9850277824260554, 0.9868955815195294, 0.9842744295750265, 0.36453698318924305, 0.017638886283350468, 0.029398143805584115, 0.04115740132781776, 0.035277772566700935, 0.11171294646121964, 0.0764351738945187, 0.31749995310030843, 0.2972458605381152, 0.033398411296417435, 0.5076558517055451, 0.12023428066710276, 0.04007809355570092, 0.9947759755244902, 0.3574537155313347, 0.12929176944750403, 0.053237787419560485, 0.41829690115368956, 0.03802699101397178, 0.9341089346005625, 0.040129185984114656, 0.9497240682907135, 0.049534695776289016, 0.06191836972036127, 0.8792408500291301, 0.007620759516994357, 0.07620759516994356, 0.6782475970124977, 0.2286227855098307, 0.034579779241676156, 0.9336540395252563, 0.9775053890008292, 0.9756619372628773, 0.03974843073904513, 0.0927463383911053, 0.02208246152169174, 0.5476450457379551, 0.13249476913015043, 0.15899372295618053, 0.05502478599304188, 0.2384407393031815, 0.6786390272475166, 0.13301906203825423, 0.8646239032486525, 0.07536741554755264, 0.2512247184918421, 0.12561235924592104, 0.5275719088328684, 0.09941999981608587, 0.03787428564422319, 0.061545714171862685, 0.14676285687136487, 0.25091714239297863, 0.4024142849698714, 0.986024984894695, 0.18381612871421055, 0.7260737084211317, 0.08271725792139474, 0.3692103622750795, 0.17912185892553362, 0.029244385130699366, 0.0036555481413374207, 0.06579986654407358, 0.06214431840273615, 0.11332199238146004, 0.0073110962826748414, 0.16449966636018393, 0.11939293126679379, 0.8715683982475946, 0.8367352478221557, 0.11800112469286811, 0.03218212491623675, 0.11753096495739977, 0.2654578691279202, 0.10334619332461015, 0.3566456867672821, 0.028369543265579257, 0.030395939213120633, 0.016211167580331004, 0.04660710679345163, 0.03444873110820338, 0.08095720590226697, 0.14060988393551632, 0.3110461068876573, 0.430351462954156, 0.034087244590428195, 0.9250532654225263, 0.8968782921304677, 0.015804022768818815, 0.003951005692204704, 0.07902011384409408, 0.9461000093666609, 0.04300454588030277, 0.22298488041901474, 0.7645195900080505, 0.060124441809967163, 0.060124441809967163, 0.8417421853395403, 0.18876721760020274, 0.047191804400050684, 0.6606852616007095, 0.047191804400050684, 0.0742060751823987, 0.7420607518239871, 0.0742060751823987, 0.11436215093219537, 0.03812071697739846, 0.015248286790959383, 0.09911386414123599, 0.1346931999868079, 0.5235245131562721, 0.005082762263653127, 0.035579335845571894, 0.030496573581918766, 0.1213323764094036, 0.2583205433232464, 0.015655790504439174, 0.04305342388720773, 0.07827895252219587, 0.0039139476261097934, 0.47750161038539485, 0.9872967211880069, 0.9507701472816164, 0.02376925368204041, 0.008643933745845591, 0.9594766457888606, 0.025931801237536772, 0.9702742098966839, 0.3080186494463463, 0.00880053284132418, 0.03520213136529672, 0.07920479557191762, 0.46642824059018156, 0.02640159852397254, 0.061603729889269265, 0.01760106568264836, 0.5806997668118837, 0.008831935616910778, 0.2406702455608187, 0.10156725959447395, 0.06623951712683084, 0.9693811646993927, 0.8520466727036161, 0.030871256257377397, 0.006174251251475479, 0.012348502502950958, 0.09261376877213219, 0.2535875055447423, 0.09056696626597939, 0.10868035951917526, 0.5252884043426804, 0.020039362301713808, 0.040078724603427617, 0.9218106658788352, 0.19423458475463154, 0.7283796928298683, 0.05821363457769122, 0.6403499803546034, 0.24255681074038007, 0.04851136214807601, 0.9841150676381407, 0.9112404946109202, 0.05186307496640667, 0.9076038119121168, 0.025931537483203336, 0.22787695018342577, 0.031074129570467148, 0.7354210665010559, 0.7819983181734143, 0.05393091849471822, 0.16179275548415467, 0.0621845622744682, 0.3938355610716319, 0.5078405919081569, 0.0310922811372341, 0.9689706604519036, 0.22602916529166026, 0.00982735501268088, 0.00982735501268088, 0.10810090513948968, 0.00982735501268088, 0.6191233657988955, 0.00982735501268088, 0.03663754173583147, 0.8426634599241237, 0.07327508347166294, 0.014715154266469537, 0.3090182395958603, 0.2943030853293907, 0.05297455535929033, 0.17069578949104663, 0.15598063522457709, 0.0029430308532939074, 0.054920061863584696, 0.09763566553526168, 0.018306687287861566, 0.006102229095953855, 0.7871875533780472, 0.006102229095953855, 0.02440891638381542, 0.7032928475929145, 0.2735027740639112, 0.9176870296698411, 0.04171304680317459, 0.020856523401587296, 0.026678196702811782, 0.013339098351405891, 0.8136849994357593, 0.1333909835140589, 0.025647045767607096, 0.07694113730282129, 0.05129409153521419, 0.07694113730282129, 0.7694113730282129, 0.9746895926602093, 0.019316183817981367, 0.07726473527192547, 0.019316183817981367, 0.8499120879911802, 0.038632367635962735, 0.2008695507425589, 0.18541804683928514, 0.08498327146800569, 0.3296320832698403, 0.041204010408730035, 0.11331102862400759, 0.025752506505456272, 0.01802675455381939, 0.1214007350952089, 0.7891047781188578, 0.09765050650987973, 0.8938777134365914, 0.11646792465325578, 0.13102641523491276, 0.20867169833708327, 0.06308679252051355, 0.029116981163313946, 0.009705660387771315, 0.44160754764359483, 0.4405204597317551, 0.5554388405313434, 0.3736701487835193, 0.6208673241326166, 0.9787102007258133, 0.051483629319661585, 0.015445088795898477, 0.8546282467063824, 0.025741814659830792, 0.03603854052376311, 0.015445088795898477, 0.05816347070896018, 0.9015337959888827, 0.9753458211541788, 0.20624204022478018, 0.19746578319393845, 0.03510502812336684, 0.09653882733925881, 0.021940642577104274, 0.2983927390486181, 0.14042011249346736, 0.2453503714697713, 0.7360511144093138, 0.02971602217146685, 0.04292314313656323, 0.3334798043686835, 0.2377281773717348, 0.019810681447644567, 0.1683907923049788, 0.10895874796204512, 0.013207120965096377, 0.046224923377837325, 0.1600231133885208, 0.04267283023693887, 0.03200462267770415, 0.7574427367056651, 0.08405648263207371, 0.08405648263207371, 0.1176790756849032, 0.6388292680037603, 0.06724518610565897, 0.9665474726807917, 0.07133367018095872, 0.8560040421715047, 0.03566683509047936, 0.9876433375863246, 0.9894031499938529, 0.8945809213379535, 0.09254285393151243, 0.9769195679627316, 0.005728417400830457, 0.0400989218058132, 0.14893885242159188, 0.790521601314603, 0.005728417400830457, 0.1958654352100513, 0.2957967797049754, 0.2997940334847724, 0.06795331425654841, 0.055961552917157514, 0.0799450755959393, 0.16084251363487598, 0.8042125681743798, 0.038171275302071324, 0.2999171630877033, 0.5398508935578659, 0.010906078657734663, 0.021812157315469326, 0.010906078657734663, 0.016359117986601997, 0.016359117986601997, 0.04907735395980599, 0.8782051553817305, 0.04116586665851862, 0.08233173331703723, 0.9757351653664098, 0.6641225146472446, 0.04781682105460161, 0.2072062245699403, 0.04781682105460161, 0.03187788070306774, 0.9328440835052898, 0.010364934261169887, 0.04145973704467955, 0.9456917530452458, 0.042030744579788705, 0.20898577870053736, 0.13932385246702492, 0.6269573361016121, 0.19676629113305633, 0.6886820189656971, 0.09838314556652816, 0.2149399783012293, 0.10746998915061465, 0.0238822198112477, 0.5373499457530733, 0.03582332971687155, 0.0716466594337431, 0.32921139227667223, 0.6666530693602613, 0.023288926119045393, 0.13973355671427237, 0.5298230692082827, 0.3027560395475901, 0.6226358916075339, 0.37508186241417707, 0.07808775680763574, 0.7548483158071455, 0.07808775680763574, 0.07808775680763574, 0.07239672249727507, 0.6998349841403256, 0.21719016749182518, 0.08443748346939788, 0.08443748346939788, 0.8162290068708461, 0.19302545853002065, 0.27900952642066623, 0.11055094443083001, 0.2175923350702051, 0.010528661374364763, 0.0017547768957274605, 0.021057322748729526, 0.15792992061547145, 0.008773884478637303, 0.9219905804660324, 0.05587821699794136, 0.16633494128184678, 0.2738646002923336, 0.07896709333582626, 0.06888618780359311, 0.1696952431259245, 0.03360301844077713, 0.10920980993252567, 0.03696332028485484, 0.06216558411543769, 0.02970280291223224, 0.11881121164892897, 0.14427075700227088, 0.050919090706683844, 0.6534616640691093, 0.029351792801303896, 0.11740717120521559, 0.851201991237813, 0.014060585667714535, 0.9701804110723029, 0.940932853952662, 0.06194960501450642, 0.9292440752175963, 0.25418248791945697, 0.7432074918514557, 0.17185954214889523, 0.2725201311218196, 0.05155786264466857, 0.16940440583248245, 0.1546735879340057, 0.022096226847715103, 0.07610922580879646, 0.014730817898476734, 0.06628868054314531, 0.980231233032714, 0.9823844333710343, 0.02737955112514831, 0.02737955112514831, 0.06388561929201272, 0.2555424771680509, 0.620603158836695, 0.07084317209912683, 0.1180719534985447, 0.07084317209912683, 0.7320461116909772, 0.00858532775022834, 0.00858532775022834, 0.3949250765105036, 0.10302393300274008, 0.1287799162534251, 0.35199843775936196, 0.9811677739449858, 0.9933385558439402, 0.18924167176272894, 0.6623458511695512, 0.013086310529383725, 0.02617262105876745, 0.9596627721548064, 0.10940003921407239, 0.027350009803518097, 0.847850303909061, 0.0656194255077754, 0.1517449214867306, 0.14764370739249466, 0.18455463424061833, 0.02870849865965174, 0.012303642282707888, 0.3157934852561691, 0.08612549597895522, 0.0041012140942359625, 0.07010382116542228, 0.6484603457801561, 0.04381488822838892, 0.05257786587406671, 0.1752595529135557, 0.09036653346783599, 0.25863525095966855, 0.037393048331518346, 0.12152740707743462, 0.30849264873502635, 0.028044786248638758, 0.13399175652127407, 0.021812611526719032, 0.9784122300216452, 0.21303280205869293, 0.29256504816060497, 0.11077705707052032, 0.06533005929799916, 0.02272349888626058, 0.014202186803912862, 0.04260656041173858, 0.11077705707052032, 0.04828743513330373, 0.07669180874112945, 0.5683172584956032, 0.40298860147870047, 0.020666082127112843, 0.04179571847246845, 0.04179571847246845, 0.8777100879218375, 0.08783322449513085, 0.8783322449513086, 0.1110391616119354, 0.039480590795354806, 0.2097406386003224, 0.046883201569483834, 0.03701305387064513, 0.012337684623548378, 0.09376640313896767, 0.3824682233299997, 0.061688423117741885, 0.004935073849419351, 0.06616711680432225, 0.0850720073198429, 0.028357335773280964, 0.6522187227854622, 0.09452445257760321, 0.07561956206208258, 0.011104333093419747, 0.3109213266157529, 0.6773643186986046, 0.04248642727404883, 0.2785221343520979, 0.018882856566243927, 0.6561792656769765, 0.898332816262479, 0.10169805467122403, 0.02403784950798571, 0.8653625822874855, 0.036056774261978564, 0.04807569901597142, 0.02403784950798571, 0.10893045603072621, 0.018155076005121035, 0.5809624321638731, 0.24509352606913398, 0.04538769001280259, 0.9150236057841196, 0.08073737698095172, 0.11214071474608421, 0.005902142881372853, 0.4367585732215911, 0.442660716102964, 0.9818401641602064, 0.6798950398658749, 0.026926536232311876, 0.1279010471034814, 0.006731634058077969, 0.15482758333579327, 0.9909283386714779, 0.9911347826587377, 0.09367775058115645, 0.2259286925780832, 0.6612547099846338, 0.016531367749615843, 0.08676940328042598, 0.9110787344444728, 0.9208331887550713, 0.06350573715552216, 0.9360470406231011, 0.023401176015577528, 0.04413181043754612, 0.05215577597164541, 0.04413181043754612, 0.3811383628697165, 0.23269500048887953, 0.008023965534099294, 0.036107844903446824, 0.036107844903446824, 0.15245534514788658, 0.012035948301148942, 0.030606977452764528, 0.007651744363191132, 0.5432738497865703, 0.22955233089573396, 0.17599012035339603, 0.007651744363191132, 0.11585482840158512, 0.5089337104783918, 0.16136922527363642, 0.04965206931496505, 0.16136922527363642, 0.916002685810293, 0.06390716412629951, 0.8912418951830658, 0.6419434829184857, 0.28492678653601233, 0.003432852849831474, 0.06522420414679801, 0.9820248285840553, 0.05311049473042259, 0.04552328119750508, 0.018968033832293785, 0.42488395784338073, 0.19347394508939658, 0.018968033832293785, 0.034142460898128814, 0.015174427065835027, 0.19347394508939658, 0.08225892874737663, 0.8375454563369257, 0.05982467545263755, 0.014956168863159388, 0.03574875507996664, 0.8937188769991659, 0.05958125846661106, 0.04974893595089398, 0.06633191460119198, 0.09949787190178797, 0.016582978650297994, 0.6467361673616218, 0.11608085055208596, 0.047334144686324166, 0.9466828937264833, 0.02610628676832587, 0.20885029414660697, 0.7309760295131243, 0.17368948860243671, 0.09473972105587457, 0.007894976754656214, 0.031579907018624856, 0.5526483728259349, 0.1184246513198432, 0.015789953509312428, 0.11860143911173597, 0.11860143911173597, 0.692632404412538, 0.028464345386816633, 0.03795246051575551, 0.9869995583194977, 0.16980130754163875, 0.7924061018609808, 0.05122060197300317, 0.029268915413144667, 0.16829626362558184, 0.007317228853286167, 0.40976481578402535, 0.3292752983978775, 0.9816391972567639, 0.24825915281332425, 0.7447774584399728, 0.9013076130440878, 0.08047389402179356, 0.9580907700971373, 0.017742421668465506, 0.9454173706770705, 0.9777514408040487, 0.07053694081782663, 0.9169802306317463, 0.0948163033186789, 0.8154202085406385, 0.07585304265494311, 0.3219629137297501, 0.666130166337414, 0.9158937759352479, 0.062447302904676, 0.7806590121181617, 0.1762778414460365, 0.04029207804480835, 0.920843382820824, 0.06424488717354586, 0.01236581147920698, 0.9521674838989375, 0.01236581147920698, 0.01236581147920698, 0.9776122619536254, 0.0644908635832861, 0.10259910115522788, 0.255032051442995, 0.23744363410209882, 0.08794208670448105, 0.15243295028776715, 0.07914787803403295, 0.01465701445074684, 0.1831907918109946, 0.2571098832435012, 0.10605782683794424, 0.1189133210001193, 0.009641620621631293, 0.0032138735405437646, 0.07391909143250659, 0.18640466535153835, 0.044994229567612704, 0.016069367702718825, 0.2367362494147799, 0.25153226500320364, 0.5030645300064073, 0.013246501446248622, 0.09272551012374036, 0.874269095452409, 0.9929459946335331, 0.06992165908146158, 0.06992165908146158, 0.7721783220300541, 0.009120216401929773, 0.06080144267953182, 0.015200360669882954, 0.9877870034326416, 0.989532700136921, 0.9000016271102884, 0.0734695205804317, 0.7841567718138224, 0.17425706040307165, 0.11078341884257045, 0.6831644161958511, 0.1846390314042841, 0.9749468144653686, 0.09050881334399435, 0.49277020598396926, 0.09050881334399435, 0.2849351531199822, 0.04022613926399749, 0.10224173477687633, 0.043507121181649504, 0.16315170443118562, 0.24146452255815473, 0.3154266285669589, 0.0957156665996289, 0.010876780295412376, 0.028279628768072176, 0.3067369649337317, 0.6619060822254211, 0.01614405078598588, 0.09161792857117566, 0.8795321142832864, 0.9558040357021449, 0.02655011210283736, 0.06968632471648731, 0.905922221314335, 0.9721817332970225, 0.13565299591549942, 0.058136998249499756, 0.717022978410497, 0.09689499708249959, 0.21920003909892208, 0.6576001172967663, 0.9527375978307643, 0.3463114984455206, 0.6453987016484702, 0.955174699518004, 0.018368744221500078, 0.0787299911988776, 0.1574599823977552, 0.638587706390896, 0.09622554479862817, 0.017495553599750576, 0.9853322893592924, 0.15406246960459927, 0.8088279654241463, 0.31068926739753144, 0.6496230136493839, 0.028244478854321037, 0.003341262638838156, 0.25393596055169987, 0.7350777805443943, 0.003341262638838156, 0.9583322137074135, 0.14083968786872514, 0.6416030225130811, 0.0625954168305445, 0.14083968786872514, 0.05118116705421606, 0.9417334737975755, 0.07507659142138923, 0.20020424379037127, 0.02502553047379641, 0.02502553047379641, 0.7007148532662995, 0.008398552339686376, 0.3695363029462006, 0.06718841871749101, 0.041992761698431885, 0.12597828509529566, 0.38633340762557333, 0.8772267552007308, 0.009233965844218218, 0.027701897532654656, 0.06463776090952753, 0.9905143832626141, 0.007774423527653313, 0.9873517880119708, 0.06298712969782283, 0.010497854949637139, 0.30443779353947703, 0.020995709899274278, 0.5983777321293169, 0.9682168642312442, 0.019364337284624885, 0.08080313579885595, 0.9023016830872247, 0.07214274010573611, 0.7935701411630972, 0.07214274010573611, 0.951370993171728, 0.030443871781495296, 0.015221935890747648, 0.5898337590394559, 0.2694930105956135, 0.1372888921902182, 0.24414377679613908, 0.6081399531103828, 0.02663386655957881, 0.017755911039719206, 0.08877955519859604, 0.008877955519859603, 0.9965067366412398, 0.9973099913125271, 0.07510770786091107, 0.5036634527143449, 0.07510770786091107, 0.09719821017294375, 0.15021541572182215, 0.061853406473691475, 0.026508602774439203, 0.0044181004624065335, 0.008836200924813067, 0.8548933664939383, 0.05028784508787872, 0.07543176763181808, 0.9330527023433152, 0.0437368454223429, 0.0145789484741143, 0.0036009375711192515, 0.0036009375711192515, 0.4321125085343102, 0.14403750284477007, 0.007201875142238503, 0.05041312599566952, 0.1116290647046968, 0.24486375483610912, 0.9863547713283832, 0.05908805385980633, 0.04999758403522074, 0.013635704736878385, 0.04090711421063515, 0.29089503438673886, 0.3272569136850812, 0.10908563789502708, 0.03636187929834236, 0.06817852368439192, 0.24856739647439133, 0.36824799477687603, 0.36824799477687603, 0.09618996999427258, 0.04809498499713629, 0.7454722674556125, 0.09618996999427258, 0.9884380058968479, 0.4996338584280131, 0.08484348539343618, 0.20268165955098644, 0.20268165955098644, 0.004713526966302011, 0.934319601713912, 0.04917471587967958, 0.003089340666254256, 0.2687726379641203, 0.04016142866130533, 0.06178681332508513, 0.2564152752991033, 0.14210967064769578, 0.12357362665017026, 0.1050375826526447, 0.011191511457323155, 0.011191511457323155, 0.1678726718598473, 0.649107664524743, 0.033574534371969465, 0.08953209165858524, 0.02238302291464631, 0.011191511457323155, 0.38373601829961423, 0.6075820289743892, 0.37578162929281184, 0.03315720258465987, 0.016578601292329936, 0.08841920689242631, 0.3978864310159184, 0.08289300646164967, 0.9911253335079409, 0.024720170774138715, 0.8157656355465775, 0.07416051232241615, 0.037080256161208074, 0.037080256161208074, 0.012360085387069357, 0.9237030175702379, 0.06397903706376129, 0.8104011361409763, 0.06397903706376129, 0.04265269137584086, 0.15559198878201966, 0.8224147978478182, 0.6340137791185553, 0.2591589170119642, 0.004627837803785075, 0.05090621584163583, 0.04627837803785075, 0.004627837803785075, 0.9820282070175287, 0.016741378495990427, 0.2846034344318373, 0.6863965183356076, 0.05625085149474621, 0.006250094610527356, 0.06250094610527357, 0.6625100287158998, 0.2125032167579301, 0.9658374834699953, 0.1419294276387902, 0.12449949792876334, 0.21164914647889768, 0.11702952805303754, 0.40088838333061794, 0.004979979917150533, 0.960070291663877, 0.0304784219575834, 0.05597027036474081, 0.044776216291792646, 0.828360001398164, 0.011194054072948162, 0.05597027036474081, 0.9955732109777187, 0.9870752268286755, 0.9798971569415401, 0.16303013961465504, 0.8151506980732752, 0.5830359406846964, 0.039242803699931494, 0.03363668888565557, 0.016818344442827784, 0.011212229628551855, 0.23545682219958897, 0.028030574071379637, 0.05045503332848335, 0.02108015426017726, 0.02108015426017726, 0.6640248591955836, 0.02108015426017726, 0.20026146547168394, 0.06324046278053178, 0.8946336250716894, 0.043640664637643385, 0.05455083079705423, 0.8862328475724855, 0.09328766816552479, 0.027555406199992496, 0.1314180911076565, 0.06146975229229095, 0.12929844447688787, 0.15261455741534305, 0.06570904555382825, 0.059350105661522295, 0.2925112350460742, 0.08054657196920884, 0.9821773991259174, 0.10541715755799981, 0.18448002572649966, 0.052708578778999905, 0.6061486559584989, 0.9661875580981666, 0.009082086719781354, 0.018164173439562708, 0.576712506706116, 0.38598868559070754, 0.009082086719781354, 0.18513652872841846, 0.7035188091679901, 0.07405461149136738, 0.024582590958750043, 0.7989342061593764, 0.1106216593143752, 0.06145647739687511, 0.9826884056599535, 0.2992024195838998, 0.19697492622606738, 0.01745347447572749, 0.36901631748680974, 0.08726737237863744, 0.027426888461857483, 0.00480900626908929, 0.0480900626908929, 0.480900626908929, 0.3895295077962325, 0.07213509403633935, 0.38994212035048115, 0.30530352058448523, 0.006045614268999707, 0.06347894982449694, 0.1118438639764946, 0.018136842806999124, 0.02418245707599883, 0.04231929988299796, 0.01511403567249927, 0.02418245707599883, 0.2248372457048165, 0.3141561515327573, 0.00923988680978698, 0.3018363024530413, 0.033879584969218925, 0.052359358588792884, 0.015399811349644965, 0.024639698159431945, 0.03079962269928993, 0.07346170251348191, 0.09794893668464255, 0.029384681005392762, 0.27915446955123124, 0.04407702150808915, 0.048974468342321276, 0.3771034062358738, 0.01958978733692851, 0.03428212783962489, 0.15314466546179434, 0.7657233273089717, 0.4254092554521315, 0.058812339463428315, 0.02548534710081894, 0.23524935785371326, 0.11958509024230424, 0.08821850919514247, 0.017643701839028494, 0.029406169731714157, 0.931074828164369, 0.01125192150903551, 0.9789171712860892, 0.16513531490824876, 0.04307877780215185, 0.10051714820502099, 0.007179796300358641, 0.3948887965197253, 0.028719185201434565, 0.04307877780215185, 0.22257368531111787, 0.7115754988879439, 0.2635464810696088, 0.9806548205035273, 0.049809212606351363, 0.35578009004536687, 0.38424249724899623, 0.10673402701361007, 0.09250282341179539, 0.07278735053292901, 0.12810573693795504, 0.01746896412790296, 0.0873448206395148, 0.10772527878873492, 0.0291149402131716, 0.12810573693795504, 0.41925513906967105, 0.00582298804263432, 0.0435565505220402, 0.9527995426696295, 0.25880554933700073, 0.06470138733425018, 0.6470138733425018, 0.10343766405811067, 0.8792201444939407, 0.016636067890479656, 0.632170579838227, 0.12061149220597751, 0.004159016972619914, 0.012477050917859743, 0.1705196958774165, 0.03743115275357923, 0.035023653684838944, 0.017511826842419472, 0.8142999481725055, 0.13133870131814604, 0.06873153487612464, 0.7560468836373712, 0.18328409300299905, 0.011244651690709082, 0.13268688995036715, 0.2069015911090471, 0.2901120136202943, 0.004497860676283633, 0.1259400989359417, 0.11469544724523263, 0.11244651690709082, 0.06510593164715694, 0.08138241455894618, 0.8463771114130403, 0.03632387254290345, 0.18161936271451723, 0.12713355390016207, 0.03632387254290345, 0.6356677695008104, 0.0072797447436274655, 0.9827655403897078, 0.18771761011195612, 0.07409905662314056, 0.41989465419779654, 0.18277767300374673, 0.04939937108209371, 0.029639622649256226, 0.004939937108209371, 0.029639622649256226, 0.024699685541046856, 0.27364871832037063, 0.09577705141212972, 0.6293920521368525, 0.4135118294211216, 0.5816315378473856, 0.0020884435829349574, 0.0020884435829349574, 0.9953367438929592, 0.8763065587690702, 0.01742655088461219, 0.004979014538460626, 0.09958029076921251, 0.021983544384490177, 0.30776962138286246, 0.6595063315347053, 0.029058856753036044, 0.14793599801545623, 0.1347274267640762, 0.6842039908214851, 0.002641714250276004, 0.002641714250276004, 0.026044783860421115, 0.06511195965105278, 0.8985450431845284, 0.013022391930210557, 0.05619328793705314, 0.04214496595278985, 0.04214496595278985, 0.646222811276111, 0.11238657587410628, 0.0842899319055797, 0.8007225386818383, 0.1455859161239706, 0.8580183180580417, 0.1046363802509807, 0.02092727605019614, 0.012002541727102772, 0.9722058798953246, 0.8896328153504421, 0.006051923913948586, 0.03025961956974293, 0.06657116305343444, 0.018330576424665932, 0.9348593976579626, 0.0549917292739978, 0.055150671290337024, 0.11030134258067405, 0.06893833911292128, 0.08272600693550554, 0.5032498755243253, 0.020681501733876385, 0.15855817995971894, 0.08789065579336121, 0.08789065579336121, 0.7910159021402509, 0.9458691378917672, 0.10021641390957065, 0.32570334520610456, 0.06959473188164628, 0.30064924172871194, 0.08072988898270968, 0.06402715333111457, 0.02505410347739266, 0.013918946376329256, 0.019486524926860958, 0.22107767867449274, 0.09353286405459307, 0.6037121225341917, 0.02550896292397993, 0.05101792584795986, 0.5179829147440457, 0.14312685802138103, 0.020446694003054433, 0.31351597471350134, 0.663438106006332, 0.006839568103158062, 0.034197840515790306, 0.034197840515790306, 0.2530640198168483, 0.9001041695549425, 0.0720083335643954, 0.10559184692029125, 0.06335510815217475, 0.8236164059782718, 0.9095460006232264, 0.025265166683978515, 0.05053033336795703, 0.9637343858587029, 0.03323222020202424, 0.0691272748787187, 0.01256859543249431, 0.07541157259496585, 0.21366612235240326, 0.03142148858123577, 0.3582049698260878, 0.11940165660869594, 0.11311735889244878, 0.11995570731848586, 0.0224916951222161, 0.8246954878145903, 0.0224916951222161, 0.010792381801385307, 0.9713143621246777, 0.010792381801385307, 0.03364497203709559, 0.03364497203709559, 0.8074793288902943, 0.1261686451391085, 0.9953516540580195, 0.1788558857608045, 0.08942794288040225, 0.715423543043218, 0.3896346142367898, 0.5566208774811283, 0.11019045420056794, 0.2203809084011359, 0.650123679783351, 0.06702643410166453, 0.1005396511524968, 0.8378304262708067, 0.0772784068920764, 0.772784068920764, 0.11591761033811461, 0.09350395680233112, 0.07650323738372546, 0.22525953229652496, 0.4505190645930499, 0.02125089927325707, 0.0595025179651198, 0.07225305752907404, 0.1280682348291202, 0.8655646216037088, 0.7626840761264972, 0.2324370517718849, 0.025050144244258984, 0.03131268030532373, 0.03966272838674339, 0.21292622607620135, 0.29851421891075286, 0.11481316111952033, 0.17535100970981288, 0.03966272838674339, 0.05845033656993762, 0.004175024040709831, 0.026035018980517504, 0.9632957022791477, 0.00566820724802334, 0.07368669422430342, 0.2947467768972137, 0.14737338844860684, 0.05101386523221006, 0.07368669422430342, 0.35142884937744706, 0.010746940016932842, 0.19881839031325757, 0.03761429005926495, 0.6878041610837019, 0.05373470008466421, 0.005373470008466421, 0.26528617706298446, 0.7074298055012919, 0.9914010707142111, 0.006314656501364402, 0.9965176187972448, 0.18168417183932728, 0.7267366873573091, 0.04542104295983182, 0.5383879423793289, 0.39722525017011456, 0.004924279960786544, 0.058270646202640775, 0.0008207133267977574, 0.14814177095098247, 0.47944064053227053, 0.288203081668275, 0.053869734891266355, 0.005386973489126636, 0.021547893956506543, 0.016374434036426716, 0.049123302109280155, 0.9169683060398962, 0.22420164861167188, 0.6726049458350156, 0.08407561822937695, 0.4965676919285299, 0.06863130701451226, 0.04440849277409617, 0.040371357067360156, 0.16148542826944062, 0.10900266408187241, 0.03229708565388812, 0.048445628480832184, 0.31554920903362954, 0.10518306967787651, 0.5506643059606476, 0.02474895757126506, 0.9963396581732593, 0.008041060768890438, 0.09917308281631541, 0.02144282871704117, 0.08577131486816468, 0.6191616792045638, 0.15546050819854848, 0.008041060768890438, 0.19229754804203572, 0.04459073577786335, 0.22852752086154968, 0.047377656763979814, 0.18393678508368633, 0.14770681226417234, 0.1170506814168913, 0.03622997281951398, 0.7774149539529017, 0.020458288261918464, 0.1841245943572662, 0.18180095138971553, 0.04545023784742888, 0.04545023784742888, 0.02272511892371444, 0.6817535677114333, 0.1772609301836497, 0.7681306974624821, 0.73549231162373, 0.26342847837526145, 0.8411181575721007, 0.08971927014102407, 0.06728945260576806, 0.1772127060816419, 0.01107579413010262, 0.7420782067168755, 0.04430317652041048, 0.01107579413010262, 0.3804229260457533, 0.030433834083660263, 0.007608458520915066, 0.14456071189738626, 0.43368213569215874, 0.050211414379171704, 0.945648304141067, 0.9692216845650242, 0.20967120673878667, 0.087033331099119, 0.15824242018021636, 0.0909893916036244, 0.28879241682889484, 0.0435166655495595, 0.047472726054064905, 0.03560454454054868, 0.0435166655495595, 0.0826324566568617, 0.14107980404830045, 0.06046277316355734, 0.6751676336597237, 0.02821596080966009, 0.012092554632711468, 0.004687867811390239, 0.9938279760147307, 0.18223867771774074, 0.025310927460797328, 0.5315294766767439, 0.07593278238239198, 0.05568404041375412, 0.015186556476478397, 0.030373112952956794, 0.06074622590591359, 0.025310927460797328, 0.15994112076251726, 0.20492456097697526, 0.41984544200160784, 0.029988960142971987, 0.07497240035742997, 0.019992640095314658, 0.044983440214457986, 0.0049981600238286645, 0.03498712016680065, 0.009996320047657329, 0.9674122847355869, 0.9597597389779121, 0.3699841107603848, 0.45498046052966234, 0.13999398785528072, 0.02999871168327444, 0.3352649758450724, 0.2599245318349438, 0.05650533300759648, 0.05085479970683683, 0.11301066601519295, 0.08664151061164793, 0.013184577701772511, 0.0583888441078497, 0.02448564430329181, 0.014390462841895242, 0.9785514732488765, 0.10295121693353146, 0.8853804656283706, 0.9806817150847598, 0.005315684855316471, 0.19136465479139295, 0.021262739421265883, 0.09834016982335471, 0.061130375836139415, 0.2870469821870894, 0.11428722438930412, 0.15947054565949412, 0.061130375836139415, 0.11290915422843346, 0.47986390547084223, 0.30485471641677037, 0.01693637313426502, 0.07903640795990342, 0.03131619994896489, 0.8142211986730872, 0.12526479979585955, 0.9151081810978468, 0.3136448059871256, 0.011616474295819466, 0.1974800630289309, 0.29041185739548664, 0.18586358873311146, 0.17043961173524302, 0.07747255078874682, 0.10846157110424555, 0.015494510157749365, 0.4958243250479797, 0.11620882618312023, 0.015494510157749365, 0.9669819581138808, 0.01696459575638387, 0.9820213228777303, 0.9890670829103062, 0.03690077325168275, 0.0738015465033655, 0.8487177847887033, 0.009812621204610716, 0.6476329995043072, 0.14718931806916072, 0.07850096963688573, 0.10793883325071787, 0.017709552504956406, 0.007083821001982562, 0.6446277111804132, 0.13813450953865997, 0.007083821001982562, 0.0743801205208169, 0.10979922553072971, 0.10611486399617025, 0.12244022768788874, 0.7672920935107694, 0.18652756782708027, 0.7461102713083211, 0.04090458026669538, 0.05453944035559384, 0.02726972017779692, 0.02726972017779692, 0.831726465422806, 0.01363486008889846, 0.7349731625508107, 0.06681574205007369, 0.16703935512518425, 0.07503420410907456, 0.07503420410907456, 0.07503420410907456, 0.6753078369816711, 0.29768024301510243, 0.05717037779760246, 0.35090783613700816, 0.11631214793305328, 0.021685315716331966, 0.045342023770512295, 0.05322759312190573, 0.039427846756967214, 0.019713923378483607, 0.03870768839811082, 0.028521454609134286, 0.25261859796661795, 0.4706040010507157, 0.002037246757795306, 0.173165974412601, 0.03667044164031551, 0.9506253859811006, 0.022633937761454778, 0.11139488179996472, 0.8725932407663903, 0.9882927485245839, 0.9772776403114651, 0.967364527618614, 0.03152719539002133, 0.9458158617006398, 0.0692010163064865, 0.1675393026367568, 0.6592307342881083, 0.03642158752972974, 0.06555885755351352, 0.9929730354198485, 0.22775154121666827, 0.7743552401366721, 0.1225458818164582, 0.7918349286601915, 0.07541285034858966, 0.15567870637555128, 0.03592585531743491, 0.8023441020893797, 0.16797772862444027, 0.8083928190051187, 0.010498608039027517, 0.38249617981216016, 0.6056189513692536, 0.7678993794907323, 0.10337107031606012, 0.014767295759437159, 0.044301887278311476, 0.007383647879718579, 0.06645283091746722, 0.18119058734729615, 0.015420475518918822, 0.4047874823716191, 0.04626142655675647, 0.05011654543648617, 0.12336380415135058, 0.07710237759459411, 0.0038551188797297056, 0.09252285311351294, 0.9813689346505039, 0.4035714506385967, 0.07106918627597929, 0.1573674838968113, 0.017767296568994822, 0.08122192717254777, 0.0050763704482842354, 0.16752022479337977, 0.09137466806911623, 0.0025381852241421177, 0.0025381852241421177, 0.016493990416043356, 0.9813924297545796, 0.02152978420639476, 0.12917870523836855, 0.5454211998953339, 0.007176594735464919, 0.01076489210319738, 0.2834754920508643, 0.5013437416116397, 0.05013437416116398, 0.13786952894320095, 0.15040312248349194, 0.05431223867459431, 0.05013437416116398, 0.020889322567151657, 0.02924505159401232, 0.11261355359979425, 0.11261355359979425, 0.2252271071995885, 0.5630677679989713, 0.1528527792484808, 0.06368865802020034, 0.20698813856565113, 0.028659896109090154, 0.20698813856565113, 0.02229103030707012, 0.08916412122828048, 0.10508628573333056, 0.12419288313939067, 0.8481362243033576, 0.11779669781991077, 0.020819299587507133, 0.9576877810253281], \"Term\": [\"1970s\", \"1980s\", \"1980s\", \"1980s\", \"1980s\", \"1980s\", \"1990s\", \"1990s\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"above\", \"above\", \"above\", \"above\", \"above\", \"above\", \"above\", \"above\", \"above\", \"academic\", \"academic\", \"academic\", \"academic\", \"academic\", \"academic_achievement\", \"academic_achievement\", \"accuracy\", \"accuracy\", \"accuracy\", \"achievement\", \"achievement\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"adjacent\", \"adjacent\", \"administrative\", \"administrative\", \"administrative\", \"administrative\", \"administrative\", \"administrator\", \"administrator\", \"adni\", \"adni\", \"adopt\", \"adopt\", \"adopt\", \"adoption\", \"adoption\", \"african_american\", \"african_american\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agriculture\", \"agriculture\", \"agriculture\", \"agriculture\", \"algorithm\", \"algorithm\", \"algorithm\", \"alignment\", \"alzheimer\", \"alzheimer_disease\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"amyloid\", \"anatomical\", \"animal\", \"animal\", \"annotation\", \"annually\", \"annually\", \"anomaly\", \"anomaly\", \"anomaly\", \"antibody\", \"antibody\", \"apoe\", \"apoe_genotype\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"arm\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"association\", \"association\", \"association\", \"association\", \"association\", \"association_between\", \"association_between\", \"association_between\", \"association_between\", \"association_between\", \"atlas\", \"atmospheric\", \"atrophy\", \"attending\", \"attending\", \"attitude\", \"august\", \"august\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"base_year\", \"belief\", \"believe\", \"believe\", \"benchmark\", \"benchmark\", \"benchmark\", \"biased\", \"biased\", \"biased\", \"binding\", \"biomarkers\", \"bird\", \"bird\", \"birth\", \"birth\", \"birth\", \"board\", \"board\", \"board\", \"book\", \"book\", \"book\", \"boy\", \"brain\", \"brain\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"canada\", \"cancer\", \"capital\", \"capital\", \"capital\", \"card\", \"card\", \"card\", \"care\", \"care\", \"care\", \"care\", \"carrier\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"census_bureau\", \"census_bureau\", \"census_bureau\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"child\", \"child\", \"child\", \"childhood\", \"china\", \"china\", \"china\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classifier\", \"classroom\", \"classroom\", \"climate\", \"climate\", \"climate\", \"climate_change\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical_trial\", \"coast\", \"coastal\", \"coastal\", \"code\", \"code\", \"code\", \"coded\", \"coded\", \"cognitive\", \"cognitive\", \"cognitive\", \"cognitive\", \"cognitive_decline\", \"cognitively_normal\", \"collect\", \"collect\", \"collect\", \"collect\", \"collection\", \"collection\", \"collection\", \"collection\", \"collection\", \"collection\", \"collection\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"compared_with\", \"compared_with\", \"compared_with\", \"compared_with\", \"compared_with\", \"compared_with\", \"compared_with\", \"compared_with\", \"concept\", \"concept\", \"concept\", \"conceptual\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"conflict\", \"consent\", \"consent\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consistent_with\", \"consumer\", \"consumer\", \"contract\", \"contract\", \"contract\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control_group\", \"control_group\", \"cooperation\", \"coronavirus\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation_coefficient\", \"cortex\", \"cortical\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"country\", \"country\", \"country\", \"country\", \"country\", \"county\", \"course\", \"course\", \"course\", \"course\", \"course\", \"covariance\", \"covid\", \"covid\", \"credit\", \"credit\", \"credit\", \"crop\", \"crop\", \"crop\", \"crop\", \"cropland\", \"cropland\", \"cross_validation\", \"cyclone\", \"data_collection\", \"data_collection\", \"data_collection\", \"data_collection\", \"data_collection\", \"data_collection\", \"data_file\", \"data_file\", \"data_file\", \"data_source\", \"data_source\", \"dealing_with\", \"dealing_with\", \"dealing_with\", \"dealing_with\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"dementia\", \"derived_from\", \"derived_from\", \"derived_from\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"deviation\", \"deviation\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference_between\", \"difference_between\", \"difference_between\", \"difference_between\", \"difference_between\", \"dimensional\", \"disease\", \"disease\", \"disease\", \"disease\", \"district\", \"district\", \"doctoral\", \"doctoral\", \"domestic\", \"domestic\", \"domestic\", \"dominated\", \"dominated\", \"dominated\", \"dominated\", \"dropped\", \"dropped\", \"dropped\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early_childhood\", \"east\", \"east\", \"eastern\", \"eastern\", \"eastern\", \"ecls\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"education\", \"education\", \"education\", \"education\", \"education\", \"education_statistic\", \"educational\", \"educational\", \"educational\", \"educational\", \"educational\", \"educational_attainment\", \"educational_attainment\", \"educational_attainment\", \"educational_attainment\", \"elevation\", \"elevation\", \"elevation\", \"emission\", \"emission\", \"employment\", \"employment\", \"employment\", \"employment\", \"engage\", \"engaging\", \"engineer\", \"engineer\", \"engineer\", \"english\", \"english\", \"english\", \"enrolled\", \"enrolled\", \"enrolled\", \"enrollment\", \"enrollment\", \"enrollment\", \"enrollment\", \"entered\", \"environmental\", \"environmental\", \"environmental\", \"environmental\", \"environmental\", \"environmental\", \"environmental\", \"establishment\", \"establishment\", \"establishment\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"expansion\", \"expansion\", \"expectation\", \"expectation\", \"expectation\", \"expenditure\", \"expenditure\", \"expenditure\", \"expenditure\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extraction\", \"facility\", \"facility\", \"facility\", \"facility\", \"facility\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor_analysis\", \"factor_analysis\", \"faculty\", \"faculty\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"farm\", \"farm\", \"farmer\", \"farmer\", \"father\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feed\", \"feed\", \"fertilizer\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field_study\", \"field_study\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"file\", \"file\", \"file\", \"file\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"firm\", \"first_year\", \"first_year\", \"first_year\", \"flood\", \"flooding\", \"florida\", \"florida\", \"flux\", \"food\", \"food\", \"food\", \"food\", \"food\", \"found_that\", \"found_that\", \"found_that\", \"found_that\", \"found_that\", \"found_that\", \"freshwater\", \"freshwater\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"funding\", \"funding\", \"funding\", \"gaussian\", \"gender\", \"gender\", \"gender\", \"gender\", \"gender\", \"gender_difference\", \"gender_difference\", \"gender_difference\", \"gene\", \"gene\", \"genome\", \"genome\", \"genome\", \"germany\", \"germany\", \"germany\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"grade\", \"grade\", \"graduate\", \"graduate\", \"graduate\", \"graduate\", \"graduate_student\", \"graduate_student\", \"graduation\", \"graduation\", \"graduation\", \"graduation\", \"grant\", \"grant\", \"grant\", \"grey\", \"grey\", \"grey\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"habitat\", \"habitat\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health_care\", \"health_care\", \"health_care\", \"heat\", \"heat\", \"heavily\", \"height\", \"height\", \"high_school\", \"high_school\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"hippocampal\", \"hippocampal_volume\", \"home\", \"home\", \"home\", \"home\", \"home\", \"host\", \"host\", \"host\", \"host\", \"household\", \"household\", \"household\", \"household\", \"household\", \"household\", \"human_brain\", \"hurricane\", \"illustrated\", \"illustrated\", \"image\", \"image\", \"image\", \"incentive\", \"incentive\", \"incentive\", \"included\", \"included\", \"included\", \"included\", \"included\", \"included\", \"included\", \"included\", \"included\", \"income\", \"income\", \"income\", \"income\", \"income\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"independent_variable\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"industry\", \"industry\", \"industry\", \"infant\", \"infant\", \"infant\", \"infection\", \"infection\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information_about\", \"information_about\", \"information_about\", \"information_about\", \"information_about\", \"information_about\", \"input\", \"input\", \"input\", \"institution\", \"institution\", \"institution\", \"institution\", \"instruction\", \"instruction\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"interview\", \"interview\", \"interview\", \"interview\", \"interview\", \"intrinsic\", \"intrinsic\", \"item\", \"item\", \"item\", \"item\", \"kernel\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"labor_market\", \"lake\", \"land\", \"land\", \"land\", \"land\", \"language\", \"language\", \"larger_than\", \"larger_than\", \"latent\", \"latent\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le_likely\", \"le_likely\", \"le_likely\", \"le_likely\", \"le_likely\", \"le_likely\", \"le_than\", \"le_than\", \"le_than\", \"le_than\", \"le_than\", \"learn\", \"learn\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"lesson\", \"likely\", \"likely\", \"likely\", \"likely\", \"likely\", \"likely\", \"likely\", \"likely\", \"likely\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_regression\", \"linear_regression\", \"linear_regression\", \"list\", \"list\", \"list\", \"list\", \"list\", \"list\", \"literacy\", \"literacy\", \"living\", \"living\", \"living\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long_term\", \"long_term\", \"long_term\", \"long_term\", \"long_term\", \"machine_learning\", \"maker\", \"maker\", \"management\", \"management\", \"management\", \"management\", \"management\", \"management\", \"management_practice\", \"manager\", \"manager\", \"map\", \"map\", \"mapping\", \"mapping\", \"marine\", \"marker\", \"marsh\", \"marsh\", \"matching\", \"matching\", \"matching\", \"math\", \"math\", \"mathematical\", \"mathematical\", \"mathematics\", \"mathematics\", \"mathematics\", \"mathematics_science\", \"mathematics_science\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maximum_likelihood\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"medical\", \"medical\", \"medical\", \"mental\", \"mental\", \"mental\", \"mental_health\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"migration\", \"mild_cognitive\", \"minority\", \"minority\", \"mission\", \"mission\", \"mobility\", \"mobility\", \"mobility\", \"modality\", \"more_likely\", \"more_likely\", \"more_likely\", \"more_likely\", \"more_likely\", \"more_than\", \"more_than\", \"more_than\", \"more_than\", \"more_than\", \"more_than\", \"more_than\", \"more_than\", \"mortality\", \"mortality\", \"mortality\", \"mother\", \"mother\", \"motivation\", \"motivation\", \"multi\", \"multi\", \"mutation\", \"name\", \"name\", \"name\", \"name\", \"narrow\", \"narrow\", \"national_center_education_statistic\", \"nationally_representative\", \"nationally_representative\", \"nels\", \"nels\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neuroimaging_initiative\", \"nitrogen\", \"nitrogen\", \"nonresponse\", \"nonresponse\", \"nonresponse\", \"normal\", \"normal\", \"normal\", \"normal\", \"normalized\", \"north_carolina\", \"north_carolina\", \"north_carolina\", \"north_carolina\", \"northern\", \"northern\", \"nutrient\", \"nutrient\", \"nutrient\", \"nutrient\", \"nutrient\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"occupation\", \"occupation\", \"occupation\", \"occupation\", \"occupational\", \"ocean\", \"ocean\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"optimal\", \"optimal\", \"pacific\", \"pacific\", \"package\", \"package\", \"package\", \"parameter\", \"parameter\", \"parameter\", \"parent\", \"parent\", \"parent\", \"participant\", \"participant\", \"participant\", \"participant\", \"participant\", \"participant\", \"patient\", \"patient_with\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"payment\", \"payment\", \"payment\", \"peer\", \"peer\", \"peer\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"performed_using\", \"period\", \"period\", \"period\", \"period\", \"period\", \"period\", \"period\", \"period\", \"period\", \"plan\", \"plan\", \"plan\", \"planning\", \"planning\", \"planning\", \"planning\", \"plasma\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"political\", \"political\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"postsecondary_education\", \"postsecondary_education\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"precipitation\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"preschool\", \"price\", \"price\", \"price\", \"price\", \"principal_component\", \"principal_component\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem_solving\", \"producer\", \"producer\", \"producer\", \"production\", \"production\", \"production\", \"production\", \"production\", \"profession\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"progression\", \"progression\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"protein\", \"psychological\", \"public_school\", \"quartile\", \"quartile\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"race\", \"race\", \"race\", \"race\", \"race\", \"race\", \"race_ethnicity\", \"race_ethnicity\", \"race_ethnicity\", \"racial\", \"racial\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"receptor\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"reconstruction\", \"record\", \"record\", \"record\", \"record\", \"record\", \"recovery\", \"recovery\", \"recovery\", \"reference\", \"reference\", \"reference\", \"reference\", \"reform\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"relationship_between\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"request\", \"request\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research_question\", \"resolution\", \"resolution\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource_management\", \"resource_management\", \"respiratory\", \"respondent\", \"respondent\", \"respondent\", \"respondent\", \"respondent\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response_rate\", \"response_rate\", \"retention\", \"retention\", \"retention\", \"returned\", \"returned\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"rural\", \"rural\", \"rural\", \"rural\", \"salary\", \"salary\", \"salary\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample_member\", \"sample_member\", \"sample_member\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sars\", \"sars\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scenario\", \"scenario\", \"scenario\", \"school\", \"school\", \"school\", \"school\", \"school_district\", \"science\", \"science\", \"science\", \"science\", \"science_engineering\", \"science_engineering\", \"science_engineering\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"season\", \"season\", \"season\", \"season\", \"sector\", \"sector\", \"sector\", \"sector\", \"sector\", \"sector\", \"security\", \"security\", \"sediment\", \"sediment\", \"sediment\", \"segmentation\", \"segmentation\", \"self\", \"self\", \"self\", \"self\", \"september\", \"september\", \"september\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sheet\", \"sheet\", \"sheet\", \"shore\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"site\", \"site\", \"site\", \"site\", \"site\", \"skill\", \"skill\", \"skill\", \"skill\", \"social\", \"social\", \"social\", \"social\", \"social\", \"society\", \"society\", \"socioeconomic_status\", \"socioeconomic_status\", \"socioeconomic_status\", \"software\", \"software\", \"software\", \"soil\", \"soil\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"south\", \"south\", \"south\", \"south\", \"southern\", \"southern\", \"southern\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial_resolution\", \"speak\", \"speak\", \"speak\", \"speaking\", \"speaking\", \"specie\", \"specie\", \"specie\", \"spending\", \"spending\", \"spending\", \"spent\", \"spent\", \"spent\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard_deviation\", \"standard_deviation\", \"standard_error\", \"standard_error\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"station\", \"station\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistically_significant\", \"statistically_significant\", \"statistically_significant\", \"statistically_significant\", \"statistically_significant\", \"statistically_significant\", \"store\", \"store\", \"storm\", \"storm\", \"storm_surge\", \"stream\", \"stream\", \"stream\", \"student\", \"student\", \"student\", \"student\", \"student\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"summer\", \"summer\", \"summer\", \"supplement\", \"supplement\", \"supplement\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"surface\", \"surface\", \"surface\", \"surface\", \"surge\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table_present\", \"table_present\", \"table_present\", \"targeted\", \"targeted\", \"targeted\", \"targeted\", \"targeted\", \"targeting\", \"targeting\", \"teacher\", \"teacher\", \"teaching\", \"teaching\", \"teaching\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"temperature\", \"temperature\", \"template\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test_score\", \"test_score\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"thickness\", \"thinking\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"this_study\", \"tide\", \"tide\", \"time_point\", \"time_point\", \"tissue\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transcript\", \"transcript\", \"transcript\", \"transmission\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trial\", \"trial\", \"tropical\", \"tropical_cyclone\", \"undergraduate\", \"undergraduate\", \"undergraduate\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united_state\", \"united_state\", \"united_state\", \"united_state\", \"united_state\", \"united_state\", \"united_state\", \"university\", \"university\", \"university\", \"update\", \"update\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"usda\", \"usda\", \"usda\", \"utilization\", \"utilization\", \"utilization\", \"utilization\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"vegetation\", \"vegetation\", \"viral\", \"viral\", \"virus\", \"voxels\", \"warm\", \"warming\", \"warming\", \"water\", \"water\", \"water\", \"water\", \"water\", \"water_level\", \"watershed\", \"watershed\", \"wave\", \"wave\", \"wave\", \"well_being\", \"well_being\", \"well_being\", \"western\", \"western\", \"western\", \"wetland\", \"wetland\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"white_matter\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"wind\", \"wind\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"working_with\", \"working_with\", \"working_with\", \"working_with\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"year_institution\", \"year_institution\", \"young_child\", \"young_child\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 6, 10, 2, 9, 1, 3, 7, 4, 5]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el371406656564418087745079934\", ldavis_el371406656564418087745079934_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el371406656564418087745079934\", ldavis_el371406656564418087745079934_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el371406656564418087745079934\", ldavis_el371406656564418087745079934_data);\n            })\n         });\n}\n</script>"},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"for i, topic_list in enumerate(model[corpus]):\n    print(i, \"번째 문서의 topic 비율은\", topic_list)    ","execution_count":70,"outputs":[{"output_type":"stream","text":"0 번째 문서의 topic 비율은 [(1, 0.0952909), (2, 0.47193062), (3, 0.27863747), (9, 0.15323527)]\n1 번째 문서의 topic 비율은 [(0, 0.73211604), (9, 0.25910506)]\n2 번째 문서의 topic 비율은 [(0, 0.1276045), (2, 0.39247307), (3, 0.46586892)]\n3 번째 문서의 topic 비율은 [(0, 0.4551749), (8, 0.34723765), (9, 0.18889178)]\n4 번째 문서의 topic 비율은 [(2, 0.4822499), (3, 0.033397734), (4, 0.09950299), (6, 0.15200739), (9, 0.23164208)]\n5 번째 문서의 topic 비율은 [(1, 0.10829689), (3, 0.12246773), (4, 0.029945418), (5, 0.5688792), (7, 0.16405018)]\n6 번째 문서의 topic 비율은 [(1, 0.06354439), (7, 0.5796188), (9, 0.3510377)]\n7 번째 문서의 topic 비율은 [(0, 0.03492431), (2, 0.40289918), (6, 0.2936324), (7, 0.23434868), (8, 0.033492558)]\n8 번째 문서의 topic 비율은 [(2, 0.5119447), (6, 0.4800501)]\n9 번째 문서의 topic 비율은 [(2, 0.038571995), (4, 0.19401081), (7, 0.7556449)]\n10 번째 문서의 topic 비율은 [(1, 0.30824202), (6, 0.36795226), (7, 0.07691454), (8, 0.24297085)]\n11 번째 문서의 topic 비율은 [(0, 0.34154814), (8, 0.36640397), (9, 0.29109687)]\n12 번째 문서의 topic 비율은 [(9, 0.99767154)]\n13 번째 문서의 topic 비율은 [(5, 0.23416637), (6, 0.37225634), (9, 0.3921912)]\n14 번째 문서의 topic 비율은 [(5, 0.8197993), (6, 0.035335615), (9, 0.14141819)]\n15 번째 문서의 topic 비율은 [(1, 0.01379873), (2, 0.7258598), (8, 0.25921673)]\n16 번째 문서의 topic 비율은 [(6, 0.9987336)]\n17 번째 문서의 topic 비율은 [(2, 0.9921821)]\n18 번째 문서의 topic 비율은 [(1, 0.8113698), (5, 0.16263384)]\n19 번째 문서의 topic 비율은 [(1, 0.2123275), (6, 0.5475944), (7, 0.23367758)]\n20 번째 문서의 topic 비율은 [(2, 0.094340086), (5, 0.025831094), (7, 0.8780895)]\n21 번째 문서의 topic 비율은 [(1, 0.31930244), (2, 0.1438175), (3, 0.053923596), (7, 0.38409257), (8, 0.04900151), (9, 0.043990064)]\n22 번째 문서의 topic 비율은 [(2, 0.10341688), (5, 0.61831766), (9, 0.27578285)]\n23 번째 문서의 topic 비율은 [(0, 0.031711325), (5, 0.42837113), (6, 0.015023866), (9, 0.516008)]\n24 번째 문서의 topic 비율은 [(3, 0.16750364), (5, 0.6271681), (9, 0.20451048)]\n25 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n26 번째 문서의 topic 비율은 [(1, 0.1862971), (2, 0.092901416), (4, 0.17580391), (5, 0.043502565), (7, 0.50055367)]\n27 번째 문서의 topic 비율은 [(1, 0.20531122), (5, 0.52674705), (6, 0.09489621), (9, 0.17186263)]\n28 번째 문서의 topic 비율은 [(3, 0.05030845), (5, 0.5695951), (8, 0.23077631), (9, 0.14486662)]\n29 번째 문서의 topic 비율은 [(4, 0.7244366), (5, 0.2517237)]\n30 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n31 번째 문서의 topic 비율은 [(1, 0.8079212), (8, 0.16591349)]\n32 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n33 번째 문서의 topic 비율은 [(1, 0.3558423), (7, 0.5048405), (9, 0.13747671)]\n34 번째 문서의 topic 비율은 [(1, 0.23283589), (5, 0.0682469), (8, 0.038466696), (9, 0.65899605)]\n35 번째 문서의 topic 비율은 [(5, 0.75654095), (9, 0.23669603)]\n36 번째 문서의 topic 비율은 [(1, 0.6506102), (4, 0.18083638), (6, 0.15453056)]\n37 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n38 번째 문서의 topic 비율은 [(7, 0.39139426), (8, 0.60758173)]\n39 번째 문서의 topic 비율은 [(5, 0.78741837), (7, 0.20792356)]\n40 번째 문서의 topic 비율은 [(2, 0.050933383), (3, 0.1902159), (4, 0.35304484), (7, 0.041524775), (8, 0.33220285), (9, 0.031877577)]\n41 번째 문서의 topic 비율은 [(6, 0.2765818), (7, 0.53213686), (9, 0.18233475)]\n42 번째 문서의 topic 비율은 [(0, 0.12454444), (2, 0.3382611), (8, 0.10110902), (9, 0.433237)]\n43 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n44 번째 문서의 topic 비율은 [(0, 0.5679571), (2, 0.24513707), (5, 0.14965999), (6, 0.033889934)]\n45 번째 문서의 topic 비율은 [(3, 0.433455), (7, 0.5616639)]\n46 번째 문서의 topic 비율은 [(0, 0.010939506), (1, 0.082710005), (2, 0.20883648), (4, 0.27102324), (6, 0.16703266), (7, 0.2588312)]\n47 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n48 번째 문서의 topic 비율은 [(8, 0.9963816)]\n49 번째 문서의 topic 비율은 [(5, 0.9808497)]\n50 번째 문서의 topic 비율은 [(1, 0.44072857), (6, 0.33252373), (9, 0.22141057)]\n51 번째 문서의 topic 비율은 [(3, 0.6706274), (5, 0.3191136)]\n52 번째 문서의 topic 비율은 [(3, 0.16596672), (5, 0.08164348), (7, 0.2915672), (9, 0.45914245)]\n53 번째 문서의 topic 비율은 [(1, 0.92911696), (6, 0.06972278)]\n54 번째 문서의 topic 비율은 [(0, 0.10311688), (8, 0.8011525), (9, 0.09164822)]\n55 번째 문서의 topic 비율은 [(1, 0.62615025), (6, 0.11855075), (7, 0.25415987)]\n56 번째 문서의 topic 비율은 [(0, 0.020090802), (1, 0.13166018), (3, 0.12412421), (6, 0.33667323), (8, 0.38355395)]\n57 번째 문서의 topic 비율은 [(1, 0.21626659), (3, 0.3291887), (4, 0.27906722), (6, 0.17342438)]\n58 번째 문서의 topic 비율은 [(2, 0.12622592), (4, 0.025388692), (5, 0.050809342), (6, 0.5736448), (7, 0.22352669)]\n59 번째 문서의 topic 비율은 [(0, 0.11627925), (2, 0.10695271), (8, 0.24729733), (9, 0.52686185)]\n60 번째 문서의 topic 비율은 [(0, 0.2736886), (8, 0.7209506)]\n61 번째 문서의 topic 비율은 [(0, 0.124368064), (6, 0.027427599), (9, 0.8446598)]\n62 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n63 번째 문서의 topic 비율은 [(4, 0.28960434), (6, 0.25035655), (7, 0.45259535)]\n64 번째 문서의 topic 비율은 [(1, 0.30150235), (3, 0.17967322), (6, 0.44190213), (7, 0.028659979), (8, 0.047927145)]\n65 번째 문서의 topic 비율은 [(1, 0.6033897), (6, 0.3951469)]\n66 번째 문서의 topic 비율은 [(1, 0.42606807), (5, 0.57100666)]\n67 번째 문서의 topic 비율은 [(5, 0.31115255), (9, 0.68282765)]\n68 번째 문서의 topic 비율은 [(5, 0.31488556), (6, 0.11947974), (9, 0.5625847)]\n69 번째 문서의 topic 비율은 [(1, 0.16267619), (4, 0.19165447), (6, 0.08740522), (7, 0.55394334)]\n70 번째 문서의 topic 비율은 [(1, 0.21121687), (5, 0.07672164), (6, 0.27751246), (7, 0.16266574), (9, 0.27095383)]\n71 번째 문서의 topic 비율은 [(0, 0.6732665), (9, 0.30872744)]\n72 번째 문서의 topic 비율은 [(2, 0.043784343), (3, 0.024385886), (6, 0.036630403), (7, 0.8944955)]\n73 번째 문서의 topic 비율은 [(1, 0.17128035), (6, 0.16250068), (7, 0.6639927)]\n74 번째 문서의 topic 비율은 [(2, 0.8558505), (3, 0.14341755)]\n75 번째 문서의 topic 비율은 [(5, 0.22751768), (9, 0.76743597)]\n76 번째 문서의 topic 비율은 [(5, 0.9942055)]\n77 번째 문서의 topic 비율은 [(4, 0.30773404), (7, 0.6908312)]\n78 번째 문서의 topic 비율은 [(1, 0.27259773), (5, 0.10594918), (9, 0.6193866)]\n79 번째 문서의 topic 비율은 [(2, 0.22328894), (7, 0.7499225)]\n80 번째 문서의 topic 비율은 [(1, 0.07304696), (5, 0.9082887), (6, 0.01590522)]\n81 번째 문서의 topic 비율은 [(0, 0.17120944), (7, 0.81110346)]\n82 번째 문서의 topic 비율은 [(2, 0.9009884), (4, 0.07230994)]\n83 번째 문서의 topic 비율은 [(6, 0.3596528), (7, 0.6327448)]\n84 번째 문서의 topic 비율은 [(0, 0.06410494), (1, 0.82206047), (2, 0.04251155), (6, 0.042396758), (8, 0.02810959)]\n85 번째 문서의 topic 비율은 [(5, 0.13162956), (9, 0.85131705)]\n86 번째 문서의 topic 비율은 [(2, 0.5609935), (7, 0.4310367)]\n87 번째 문서의 topic 비율은 [(5, 0.9684753), (6, 0.030819304)]\n88 번째 문서의 topic 비율은 [(1, 0.20914114), (5, 0.6806669), (9, 0.10567506)]\n89 번째 문서의 topic 비율은 [(1, 0.09846244), (4, 0.41528067), (5, 0.05420934), (6, 0.3121102), (8, 0.11759683)]\n90 번째 문서의 topic 비율은 [(1, 0.1754725), (6, 0.064958125), (7, 0.5311968), (9, 0.22772501)]\n91 번째 문서의 topic 비율은 [(0, 0.043509554), (4, 0.039523076), (7, 0.91625106)]\n92 번째 문서의 topic 비율은 [(1, 0.9768489)]\n93 번째 문서의 topic 비율은 [(1, 0.4843287), (3, 0.24485937), (4, 0.261516)]\n94 번째 문서의 topic 비율은 [(1, 0.07029827), (6, 0.82346815), (8, 0.104157776)]\n95 번째 문서의 topic 비율은 [(1, 0.26450175), (4, 0.03952138), (6, 0.6929846)]\n96 번째 문서의 topic 비율은 [(2, 0.99376744)]\n97 번째 문서의 topic 비율은 [(1, 0.5147278), (2, 0.10888049), (5, 0.26415357), (9, 0.10965068)]\n98 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n99 번째 문서의 topic 비율은 [(9, 0.98107404)]\n100 번째 문서의 topic 비율은 [(1, 0.07714405), (5, 0.09966078), (9, 0.81819385)]\n101 번째 문서의 topic 비율은 [(0, 0.58468527), (3, 0.035421625), (8, 0.03677008), (9, 0.34227967)]\n102 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n103 번째 문서의 topic 비율은 [(2, 0.18650857), (4, 0.039520368), (6, 0.23385838), (7, 0.5389336)]\n104 번째 문서의 topic 비율은 [(1, 0.6372951), (6, 0.30659124), (8, 0.052434184)]\n105 번째 문서의 topic 비율은 [(9, 0.99909306)]\n106 번째 문서의 topic 비율은 [(0, 0.048823852), (2, 0.5562436), (8, 0.27896973), (9, 0.113646224)]\n107 번째 문서의 topic 비율은 [(0, 0.33493748), (8, 0.100065604), (9, 0.56314707)]\n108 번째 문서의 topic 비율은 [(2, 0.11700939), (6, 0.46750337), (8, 0.39418808)]\n109 번째 문서의 topic 비율은 [(0, 0.33433852), (1, 0.15024756), (7, 0.3909668), (8, 0.111848086)]\n110 번째 문서의 topic 비율은 [(3, 0.4604293), (6, 0.53003305)]\n111 번째 문서의 topic 비율은 [(1, 0.29175434), (6, 0.6475296), (9, 0.05819078)]\n112 번째 문서의 topic 비율은 [(0, 0.6805584), (1, 0.011336442), (5, 0.015244289), (6, 0.011738248), (7, 0.014977085), (8, 0.22465412), (9, 0.019740533)]\n113 번째 문서의 topic 비율은 [(7, 0.81519896), (8, 0.1806781)]\n114 번째 문서의 topic 비율은 [(0, 0.012300023), (2, 0.17086953), (4, 0.09051607), (5, 0.14009118), (6, 0.14485444), (7, 0.25451002), (9, 0.18606998)]\n115 번째 문서의 topic 비율은 [(2, 0.54620564), (3, 0.03228122), (6, 0.09373447), (7, 0.13117167), (8, 0.19590557)]\n116 번째 문서의 topic 비율은 [(2, 0.13617194), (4, 0.051178336), (6, 0.68045276), (7, 0.13105816)]\n117 번째 문서의 topic 비율은 [(1, 0.07412304), (7, 0.5057942), (8, 0.4177575)]\n118 번째 문서의 topic 비율은 [(3, 0.68303716), (7, 0.3055189)]\n119 번째 문서의 topic 비율은 [(1, 0.9800803)]\n120 번째 문서의 topic 비율은 [(7, 0.73211175), (8, 0.2657949)]\n121 번째 문서의 topic 비율은 [(1, 0.7746193), (4, 0.033638425), (7, 0.18985003)]\n122 번째 문서의 topic 비율은 [(0, 0.036629338), (1, 0.40451992), (7, 0.5536215)]\n123 번째 문서의 topic 비율은 [(3, 0.9979241)]\n124 번째 문서의 topic 비율은 [(0, 0.02177204), (2, 0.10822961), (3, 0.21336408), (4, 0.1875363), (6, 0.11623535), (7, 0.26904076), (8, 0.08225497)]\n125 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n","name":"stdout"},{"output_type":"stream","text":"126 번째 문서의 topic 비율은 [(0, 0.27911964), (8, 0.5497203), (9, 0.16986945)]\n127 번째 문서의 topic 비율은 [(5, 0.7821086), (9, 0.21272597)]\n128 번째 문서의 topic 비율은 [(5, 0.36642247), (8, 0.52042484), (9, 0.10977517)]\n129 번째 문서의 topic 비율은 [(5, 0.48860458), (6, 0.1301239), (9, 0.37847427)]\n130 번째 문서의 topic 비율은 [(2, 0.16241512), (3, 0.5357053), (6, 0.22033238), (9, 0.07945792)]\n131 번째 문서의 topic 비율은 [(0, 0.4272395), (2, 0.060061783), (8, 0.44939554), (9, 0.05457415)]\n132 번째 문서의 topic 비율은 [(6, 0.7016278), (7, 0.15183625), (8, 0.090194084), (9, 0.053657085)]\n133 번째 문서의 topic 비율은 [(1, 0.011337976), (5, 0.015246277), (6, 0.66436404), (7, 0.24976857), (8, 0.0100628985), (9, 0.019750534)]\n134 번째 문서의 topic 비율은 [(2, 0.28591126), (5, 0.034579244), (7, 0.67766076)]\n135 번째 문서의 topic 비율은 [(1, 0.5398357), (7, 0.076579206), (9, 0.37915692)]\n136 번째 문서의 topic 비율은 [(7, 0.9402703), (9, 0.05667473)]\n137 번째 문서의 topic 비율은 [(7, 0.93489754), (8, 0.054085337)]\n138 번째 문서의 topic 비율은 [(3, 0.9428402), (6, 0.055205524)]\n139 번째 문서의 topic 비율은 [(5, 0.9975142)]\n140 번째 문서의 topic 비율은 [(1, 0.99560064)]\n141 번째 문서의 topic 비율은 [(0, 0.90579164), (3, 0.035693612), (8, 0.058348253)]\n142 번째 문서의 topic 비율은 [(5, 0.9980802)]\n143 번째 문서의 topic 비율은 [(1, 0.0947164), (4, 0.44594783), (5, 0.08724952), (7, 0.3177954), (8, 0.04949745)]\n144 번째 문서의 topic 비율은 [(1, 0.5773814), (5, 0.13336134), (6, 0.012009475), (7, 0.276719)]\n145 번째 문서의 topic 비율은 [(2, 0.09094671), (9, 0.88818276)]\n146 번째 문서의 topic 비율은 [(0, 0.11671484), (5, 0.5898695), (6, 0.1544215), (7, 0.09481154), (9, 0.041790012)]\n147 번째 문서의 topic 비율은 [(1, 0.30941024), (4, 0.3049742), (6, 0.37713778)]\n148 번째 문서의 topic 비율은 [(1, 0.31841618), (2, 0.03304334), (7, 0.64759547)]\n149 번째 문서의 topic 비율은 [(4, 0.13246572), (9, 0.8608734)]\n150 번째 문서의 topic 비율은 [(5, 0.99771154)]\n151 번째 문서의 topic 비율은 [(0, 0.88538677), (2, 0.017929923), (3, 0.01264319), (8, 0.08393166)]\n152 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n153 번째 문서의 topic 비율은 [(2, 0.51560843), (6, 0.2445797), (7, 0.0914659), (8, 0.14316429)]\n154 번째 문서의 topic 비율은 [(1, 0.86600196), (7, 0.1322965)]\n155 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n156 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n157 번째 문서의 topic 비율은 [(6, 0.42758822), (7, 0.558028)]\n158 번째 문서의 topic 비율은 [(7, 0.8443882), (9, 0.1471152)]\n159 번째 문서의 topic 비율은 [(0, 0.16038139), (8, 0.44086078), (9, 0.397796)]\n160 번째 문서의 topic 비율은 [(2, 0.28720656), (6, 0.2449006), (7, 0.46476933)]\n161 번째 문서의 topic 비율은 [(3, 0.45734277), (5, 0.21261364), (7, 0.29008985), (9, 0.010468078)]\n162 번째 문서의 topic 비율은 [(1, 0.69057935), (6, 0.054075804), (7, 0.25202942)]\n163 번째 문서의 topic 비율은 [(5, 0.60805833), (9, 0.38671485)]\n164 번째 문서의 topic 비율은 [(9, 0.94571424)]\n165 번째 문서의 topic 비율은 [(0, 0.013290287), (2, 0.6695225), (3, 0.08487934), (7, 0.16236803), (8, 0.06947355)]\n166 번째 문서의 topic 비율은 [(5, 0.23681119), (9, 0.760124)]\n167 번째 문서의 topic 비율은 [(5, 0.037210044), (7, 0.14859994), (8, 0.057441168), (9, 0.7522376)]\n168 번째 문서의 topic 비율은 [(2, 0.064197324), (5, 0.47752634), (6, 0.025334772), (7, 0.12291604), (9, 0.30942944)]\n169 번째 문서의 topic 비율은 [(1, 0.011336864), (5, 0.015244999), (6, 0.011738657), (7, 0.01497749), (8, 0.01006192), (9, 0.9071732)]\n170 번째 문서의 topic 비율은 [(9, 0.9868041)]\n171 번째 문서의 topic 비율은 [(9, 0.9550439)]\n172 번째 문서의 topic 비율은 [(6, 0.031587653), (9, 0.960345)]\n173 번째 문서의 topic 비율은 [(1, 0.68288374), (4, 0.04486189), (6, 0.033177376), (7, 0.23506823)]\n174 번째 문서의 topic 비율은 [(5, 0.53300947), (6, 0.4590348)]\n175 번째 문서의 topic 비율은 [(0, 0.082652904), (1, 0.0429896), (2, 0.01896746), (8, 0.16575436), (9, 0.68898493)]\n176 번째 문서의 topic 비율은 [(0, 0.35879952), (2, 0.2370036), (3, 0.16546723), (4, 0.10515546), (5, 0.12705132)]\n177 번째 문서의 topic 비율은 [(1, 0.03333233), (5, 0.14295867), (9, 0.82250345)]\n178 번째 문서의 topic 비율은 [(2, 0.038461827), (7, 0.43979847), (8, 0.5213754)]\n179 번째 문서의 topic 비율은 [(1, 0.053753555), (6, 0.056357563), (7, 0.160533), (8, 0.7283289)]\n180 번째 문서의 topic 비율은 [(1, 0.5212844), (2, 0.09030835), (6, 0.35251215)]\n181 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n182 번째 문서의 topic 비율은 [(1, 0.5535366), (6, 0.25154364), (9, 0.19306266)]\n183 번째 문서의 topic 비율은 [(6, 0.94657445), (9, 0.010461674)]\n184 번째 문서의 topic 비율은 [(1, 0.011336052), (3, 0.8944033), (5, 0.015243702), (6, 0.011737807), (7, 0.014975927), (8, 0.0100612035), (9, 0.019736558)]\n185 번째 문서의 topic 비율은 [(0, 0.038573172), (1, 0.17303775), (5, 0.21585415), (7, 0.11260239), (9, 0.45688915)]\n186 번째 문서의 topic 비율은 [(0, 0.057567358), (2, 0.7410569), (6, 0.04907788), (9, 0.15103702)]\n187 번째 문서의 topic 비율은 [(1, 0.11838305), (4, 0.6162158), (5, 0.26256981)]\n188 번째 문서의 topic 비율은 [(3, 0.34445506), (4, 0.07850339), (5, 0.096324995), (9, 0.46878764)]\n189 번째 문서의 topic 비율은 [(0, 0.011960303), (1, 0.017574854), (2, 0.013968284), (3, 0.010789315), (5, 0.023633149), (6, 0.018197754), (7, 0.023218611), (8, 0.41299817), (9, 0.45869684)]\n190 번째 문서의 topic 비율은 [(2, 0.42602035), (3, 0.21165681), (6, 0.12803441), (9, 0.22147553)]\n191 번째 문서의 topic 비율은 [(5, 0.99923474)]\n192 번째 문서의 topic 비율은 [(1, 0.09528728), (6, 0.65922713), (7, 0.14057426), (8, 0.10257717)]\n193 번째 문서의 topic 비율은 [(0, 0.4015802), (1, 0.09183886), (7, 0.0616785), (9, 0.4435479)]\n194 번째 문서의 topic 비율은 [(7, 0.8817642), (8, 0.09172453), (9, 0.021250432)]\n195 번째 문서의 topic 비율은 [(1, 0.19832885), (5, 0.20035897), (6, 0.45256203), (7, 0.071774825), (9, 0.069933966)]\n196 번째 문서의 topic 비율은 [(1, 0.044745125), (4, 0.113696724), (7, 0.57755244), (8, 0.26317155)]\n197 번째 문서의 topic 비율은 [(5, 0.9961816)]\n198 번째 문서의 topic 비율은 [(5, 0.19071554), (9, 0.80493987)]\n199 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n200 번째 문서의 topic 비율은 [(1, 0.6814159), (7, 0.31537858)]\n201 번째 문서의 topic 비율은 [(5, 0.016577277), (7, 0.9191889), (9, 0.04963685)]\n202 번째 문서의 topic 비율은 [(2, 0.1430797), (5, 0.847466)]\n203 번째 문서의 topic 비율은 [(3, 0.23111495), (5, 0.49738222), (6, 0.06082249), (7, 0.20587376)]\n204 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n205 번째 문서의 topic 비율은 [(8, 0.94568866), (9, 0.010463032)]\n206 번째 문서의 topic 비율은 [(0, 0.52900684), (9, 0.468877)]\n207 번째 문서의 topic 비율은 [(5, 0.5136791), (9, 0.47886547)]\n208 번째 문서의 topic 비율은 [(4, 0.037367236), (7, 0.91971344), (9, 0.03990291)]\n209 번째 문서의 topic 비율은 [(5, 0.9794085)]\n210 번째 문서의 topic 비율은 [(0, 0.081719905), (3, 0.21753773), (5, 0.5002008), (7, 0.1952907)]\n211 번째 문서의 topic 비율은 [(0, 0.4756052), (1, 0.21620736), (9, 0.30482537)]\n212 번째 문서의 topic 비율은 [(1, 0.6185673), (4, 0.36786523)]\n213 번째 문서의 topic 비율은 [(3, 0.05424192), (5, 0.17070544), (9, 0.76447767)]\n214 번째 문서의 topic 비율은 [(0, 0.042457357), (6, 0.28170523), (7, 0.16814472), (8, 0.36720166), (9, 0.13919798)]\n215 번째 문서의 topic 비율은 [(0, 0.36543927), (7, 0.0970871), (8, 0.46731818), (9, 0.069346815)]\n216 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n217 번째 문서의 topic 비율은 [(1, 0.013782466), (2, 0.010954135), (4, 0.8701704), (5, 0.018533623), (6, 0.01427107), (7, 0.018207982), (8, 0.012232489), (9, 0.024007216)]\n218 번째 문서의 topic 비율은 [(0, 0.037536543), (1, 0.12542056), (2, 0.073255375), (6, 0.20098792), (7, 0.52647334), (8, 0.03118212)]\n219 번째 문서의 topic 비율은 [(4, 0.09733469), (7, 0.29478258), (8, 0.60714525)]\n220 번째 문서의 topic 비율은 [(5, 0.83200514), (9, 0.1650138)]\n221 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n222 번째 문서의 topic 비율은 [(3, 0.13109483), (6, 0.100352086), (8, 0.7645724)]\n223 번째 문서의 topic 비율은 [(3, 0.5668438), (8, 0.42746764)]\n224 번째 문서의 topic 비율은 [(0, 0.01037793), (2, 0.46329907), (3, 0.13078505), (6, 0.15610196), (7, 0.23715553)]\n225 번째 문서의 topic 비율은 [(5, 0.04314237), (7, 0.36599377), (8, 0.061967462), (9, 0.5270869)]\n226 번째 문서의 topic 비율은 [(2, 0.44458827), (3, 0.5535828)]\n227 번째 문서의 topic 비율은 [(5, 0.78505576), (6, 0.11365105), (9, 0.09618576)]\n228 번째 문서의 topic 비율은 [(1, 0.19596004), (2, 0.41765568), (5, 0.20620522), (8, 0.037197996), (9, 0.14171919)]\n229 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n230 번째 문서의 topic 비율은 [(0, 0.019799525), (1, 0.3069055), (7, 0.67182744)]\n231 번째 문서의 topic 비율은 [(1, 0.28421602), (5, 0.4647552), (6, 0.227303)]\n232 번째 문서의 topic 비율은 [(1, 0.38856867), (2, 0.010953531), (5, 0.018532613), (6, 0.5026407), (7, 0.018207412), (8, 0.0122318175), (9, 0.023997314)]\n233 번째 문서의 topic 비율은 [(0, 0.39547348), (8, 0.24913163), (9, 0.35410818)]\n234 번째 문서의 topic 비율은 [(0, 0.5537032), (5, 0.101308815), (8, 0.112049505), (9, 0.23037837)]\n235 번째 문서의 topic 비율은 [(1, 0.14053303), (2, 0.260559), (4, 0.13333128), (5, 0.09214414), (6, 0.28839263), (7, 0.083938755)]\n236 번째 문서의 topic 비율은 [(2, 0.039818358), (4, 0.07600306), (7, 0.8795423)]\n237 번째 문서의 topic 비율은 [(2, 0.015883712), (3, 0.015306223), (4, 0.024556), (7, 0.9423171)]\n238 번째 문서의 topic 비율은 [(5, 0.99311346)]\n239 번째 문서의 topic 비율은 [(0, 0.1287761), (7, 0.24842197), (9, 0.61628425)]\n240 번째 문서의 topic 비율은 [(0, 0.46926975), (8, 0.4884529)]\n241 번째 문서의 topic 비율은 [(1, 0.68492746), (4, 0.02024231), (6, 0.017874962), (7, 0.26491952), (9, 0.011697654)]\n242 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n243 번째 문서의 topic 비율은 [(1, 0.023770755), (4, 0.76685196), (7, 0.20330255)]\n244 번째 문서의 topic 비율은 [(5, 0.23877233), (6, 0.020677326), (9, 0.73489726)]\n245 번째 문서의 topic 비율은 [(3, 0.27339762), (6, 0.17023726), (7, 0.44130757), (9, 0.113092706)]\n246 번째 문서의 topic 비율은 [(1, 0.35204554), (7, 0.33653513), (8, 0.29158), (9, 0.015398911)]\n247 번째 문서의 topic 비율은 [(5, 0.95486444), (8, 0.03880139)]\n248 번째 문서의 topic 비율은 [(1, 0.30101252), (2, 0.2995691), (7, 0.3046154), (8, 0.092857786)]\n249 번째 문서의 topic 비율은 [(0, 0.35831156), (1, 0.14383402), (8, 0.3799904), (9, 0.11329813)]\n250 번째 문서의 topic 비율은 [(5, 0.011249781), (7, 0.011052275), (9, 0.9315)]\n251 번째 문서의 topic 비율은 [(4, 0.044681456), (5, 0.9192156), (8, 0.030394597)]\n252 번째 문서의 topic 비율은 [(0, 0.16009223), (2, 0.35882375), (8, 0.419539), (9, 0.057528082)]\n253 번째 문서의 topic 비율은 [(3, 0.93824136), (9, 0.0115426695)]\n254 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n255 번째 문서의 topic 비율은 [(3, 0.4219963), (4, 0.070579045), (5, 0.28468966), (6, 0.18789297), (7, 0.034351327)]\n256 번째 문서의 topic 비율은 [(5, 0.41122165), (9, 0.5843013)]\n257 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n258 번째 문서의 topic 비율은 [(3, 0.26240996), (5, 0.6499651), (7, 0.08675904)]\n259 번째 문서의 topic 비율은 [(0, 0.60175955), (7, 0.098569475), (8, 0.29070944)]\n260 번째 문서의 topic 비율은 [(0, 0.38175347), (3, 0.16990322), (6, 0.43346223)]\n261 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n","name":"stdout"},{"output_type":"stream","text":"262 번째 문서의 topic 비율은 [(3, 0.057625737), (6, 0.19072762), (9, 0.73958653)]\n263 번째 문서의 topic 비율은 [(0, 0.049546514), (1, 0.6785463), (2, 0.19747895), (6, 0.07184309)]\n264 번째 문서의 topic 비율은 [(2, 0.16593969), (5, 0.17513165), (6, 0.1089099), (9, 0.54652405)]\n265 번째 문서의 topic 비율은 [(1, 0.62590325), (4, 0.37324655)]\n266 번째 문서의 topic 비율은 [(1, 0.38793784), (4, 0.097306795), (7, 0.5133096)]\n267 번째 문서의 topic 비율은 [(1, 0.15554574), (5, 0.52019334), (9, 0.3226965)]\n268 번째 문서의 topic 비율은 [(0, 0.013489011), (3, 0.56693494), (4, 0.06047239), (7, 0.35639733)]\n269 번째 문서의 topic 비율은 [(2, 0.28610998), (5, 0.05984633), (7, 0.53653336), (9, 0.11671262)]\n270 번째 문서의 topic 비율은 [(5, 0.9117108), (6, 0.084759265)]\n271 번째 문서의 topic 비율은 [(6, 0.9939321)]\n272 번째 문서의 topic 비율은 [(1, 0.06307154), (6, 0.44973344), (7, 0.21781349), (8, 0.26624033)]\n273 번째 문서의 topic 비율은 [(6, 0.43662003), (7, 0.3664656), (8, 0.19564573)]\n274 번째 문서의 topic 비율은 [(2, 0.13108814), (6, 0.5542386), (9, 0.3116566)]\n275 번째 문서의 topic 비율은 [(1, 0.14214282), (2, 0.10387945), (6, 0.4631778), (8, 0.16400406), (9, 0.12597436)]\n276 번째 문서의 topic 비율은 [(7, 0.6391736), (9, 0.34301984)]\n277 번째 문서의 topic 비율은 [(5, 0.6747811), (7, 0.0806041), (8, 0.07598937), (9, 0.16693005)]\n278 번째 문서의 topic 비율은 [(5, 0.050631), (7, 0.08104138), (9, 0.86675715)]\n279 번째 문서의 topic 비율은 [(1, 0.6827161), (3, 0.065062486), (6, 0.1989902), (7, 0.05123409)]\n280 번째 문서의 topic 비율은 [(1, 0.16353093), (4, 0.3846533), (6, 0.057170164), (7, 0.39344877)]\n281 번째 문서의 topic 비율은 [(1, 0.9693896)]\n282 번째 문서의 topic 비율은 [(2, 0.41754624), (3, 0.36840922), (8, 0.20828329)]\n283 번째 문서의 topic 비율은 [(1, 0.19635946), (2, 0.031492114), (3, 0.4870262), (5, 0.07401632), (9, 0.21057999)]\n284 번째 문서의 topic 비율은 [(4, 0.24999084), (5, 0.059826367), (7, 0.53527313), (8, 0.1536671)]\n285 번째 문서의 topic 비율은 [(1, 0.558434), (4, 0.06865935), (6, 0.3665003)]\n286 번째 문서의 topic 비율은 [(1, 0.99748325)]\n287 번째 문서의 topic 비율은 [(2, 0.1228758), (9, 0.85714096)]\n288 번째 문서의 topic 비율은 [(0, 0.072776705), (3, 0.27812108), (5, 0.16807061), (8, 0.36797428), (9, 0.10822606)]\n289 번째 문서의 topic 비율은 [(2, 0.03461476), (9, 0.9650115)]\n290 번째 문서의 topic 비율은 [(6, 0.4764819), (9, 0.50239354)]\n291 번째 문서의 topic 비율은 [(0, 0.0321714), (7, 0.26521957), (9, 0.6956312)]\n292 번째 문서의 topic 비율은 [(0, 0.13217737), (9, 0.8600406)]\n293 번째 문서의 topic 비율은 [(3, 0.04877089), (4, 0.34989518), (6, 0.1537756), (7, 0.42657736)]\n294 번째 문서의 topic 비율은 [(1, 0.16170813), (5, 0.6565518), (9, 0.17716727)]\n295 번째 문서의 topic 비율은 [(2, 0.983153)]\n296 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n297 번째 문서의 topic 비율은 [(0, 0.030761704), (5, 0.78241605), (7, 0.17620903)]\n298 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n299 번째 문서의 topic 비율은 [(0, 0.1149028), (3, 0.109437555), (8, 0.46337244), (9, 0.30975696)]\n300 번째 문서의 topic 비율은 [(3, 0.9987593)]\n301 번째 문서의 topic 비율은 [(0, 0.21127099), (6, 0.769746)]\n302 번째 문서의 topic 비율은 [(1, 0.5137511), (4, 0.17110625), (6, 0.11849988), (7, 0.18955383)]\n303 번째 문서의 topic 비율은 [(0, 0.08559575), (2, 0.5609998), (6, 0.021024821), (9, 0.32900983)]\n304 번째 문서의 topic 비율은 [(0, 0.126538), (8, 0.39229247), (9, 0.4773472)]\n305 번째 문서의 topic 비율은 [(0, 0.31297308), (2, 0.099210605), (9, 0.5844824)]\n306 번째 문서의 topic 비율은 [(1, 0.43701065), (2, 0.053412464), (5, 0.18384159), (6, 0.31767824)]\n307 번째 문서의 topic 비율은 [(5, 0.94510907), (9, 0.05050349)]\n308 번째 문서의 topic 비율은 [(2, 0.8267175), (3, 0.05107945), (7, 0.11878643)]\n309 번째 문서의 topic 비율은 [(2, 0.6588526), (6, 0.11300898), (9, 0.22644354)]\n310 번째 문서의 topic 비율은 [(1, 0.09048522), (2, 0.6059511), (3, 0.18962964), (7, 0.110566765)]\n311 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n312 번째 문서의 topic 비율은 [(0, 0.32217687), (9, 0.67280823)]\n313 번째 문서의 topic 비율은 [(1, 0.40563688), (7, 0.27848256), (8, 0.31454158)]\n314 번째 문서의 topic 비율은 [(4, 0.16476794), (7, 0.654592), (9, 0.17889445)]\n315 번째 문서의 topic 비율은 [(0, 0.02634757), (1, 0.08618374), (2, 0.11788066), (8, 0.7678373)]\n316 번째 문서의 topic 비율은 [(6, 0.6087295), (9, 0.37193704)]\n317 번째 문서의 topic 비율은 [(5, 0.8981815), (8, 0.097163424)]\n318 번째 문서의 topic 비율은 [(1, 0.12655157), (4, 0.54965025), (6, 0.32064626)]\n319 번째 문서의 topic 비율은 [(2, 0.30532825), (3, 0.31073648), (4, 0.04867101), (5, 0.04611914), (6, 0.055974156), (9, 0.23048846)]\n320 번째 문서의 topic 비율은 [(1, 0.92257446), (6, 0.07341168)]\n321 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n322 번째 문서의 topic 비율은 [(1, 0.52346784), (5, 0.26382032), (6, 0.14123775), (8, 0.061598662)]\n323 번째 문서의 topic 비율은 [(0, 0.035084393), (5, 0.11581558), (6, 0.23948936), (9, 0.60559803)]\n324 번째 문서의 topic 비율은 [(0, 0.08753982), (9, 0.9053699)]\n325 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n326 번째 문서의 topic 비율은 [(5, 0.9964535)]\n327 번째 문서의 topic 비율은 [(4, 0.024978662), (6, 0.05607478), (7, 0.910101)]\n328 번째 문서의 topic 비율은 [(1, 0.22718213), (6, 0.24487783), (9, 0.5264984)]\n329 번째 문서의 topic 비율은 [(2, 0.17394187), (3, 0.8242595)]\n330 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n331 번째 문서의 topic 비율은 [(5, 0.66920924), (9, 0.32786945)]\n332 번째 문서의 topic 비율은 [(7, 0.9069024), (8, 0.044831377), (9, 0.046938293)]\n333 번째 문서의 topic 비율은 [(0, 0.02659786), (1, 0.039083887), (2, 0.031063298), (3, 0.023993762), (4, 0.01993173), (5, 0.05255628), (6, 0.040468894), (7, 0.05163328), (8, 0.034688402), (9, 0.6799826)]\n334 번째 문서의 topic 비율은 [(1, 0.44843042), (2, 0.023354717), (4, 0.2590984), (6, 0.18270777), (7, 0.015774159), (9, 0.069266126)]\n335 번째 문서의 topic 비율은 [(6, 0.042272132), (7, 0.9509163)]\n336 번째 문서의 topic 비율은 [(3, 0.031871185), (5, 0.7223226), (6, 0.0656911), (7, 0.078774735), (9, 0.10002183)]\n337 번째 문서의 topic 비율은 [(5, 0.95289445), (6, 0.04511779)]\n338 번째 문서의 topic 비율은 [(0, 0.048880298), (1, 0.060375717), (5, 0.24643376), (7, 0.099351086), (9, 0.5423378)]\n339 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n340 번째 문서의 topic 비율은 [(6, 0.3194706), (7, 0.43144518), (9, 0.2420062)]\n341 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n342 번째 문서의 topic 비율은 [(2, 0.18173087), (4, 0.024825113), (6, 0.041611098), (7, 0.731572), (8, 0.013792203)]\n343 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n344 번째 문서의 topic 비율은 [(4, 0.12556756), (6, 0.051483322), (7, 0.80287415), (9, 0.018213699)]\n345 번째 문서의 topic 비율은 [(0, 0.10133159), (3, 0.5887948), (4, 0.038352624), (7, 0.26435196)]\n346 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n347 번째 문서의 topic 비율은 [(0, 0.062576406), (1, 0.31202877), (4, 0.23066759), (6, 0.38906506)]\n348 번째 문서의 topic 비율은 [(5, 0.10648075), (6, 0.016770732), (9, 0.87508214)]\n349 번째 문서의 topic 비율은 [(2, 0.03206972), (6, 0.29579732), (7, 0.40249413), (8, 0.2639039)]\n350 번째 문서의 topic 비율은 [(1, 0.32287765), (7, 0.22190286), (8, 0.45282915)]\n351 번째 문서의 topic 비율은 [(2, 0.085045196), (5, 0.16161622), (6, 0.28377488), (7, 0.46242577)]\n352 번째 문서의 topic 비율은 [(1, 0.12152453), (5, 0.43735814), (9, 0.43740845)]\n353 번째 문서의 topic 비율은 [(0, 0.047312867), (2, 0.07390818), (6, 0.13320465), (7, 0.46946618), (8, 0.19848183), (9, 0.0746171)]\n354 번째 문서의 topic 비율은 [(5, 0.8115983), (6, 0.07814987), (8, 0.10447187)]\n355 번째 문서의 topic 비율은 [(0, 0.09300612), (6, 0.39205474), (9, 0.49815756)]\n356 번째 문서의 topic 비율은 [(7, 0.9429269), (9, 0.011546203)]\n357 번째 문서의 topic 비율은 [(1, 0.11443632), (2, 0.095347725), (4, 0.036223475), (6, 0.081600785), (7, 0.25525326), (8, 0.4026564), (9, 0.014197478)]\n358 번째 문서의 topic 비율은 [(1, 0.6550156), (4, 0.32915595), (9, 0.014139812)]\n359 번째 문서의 topic 비율은 [(0, 0.1252805), (8, 0.87039894)]\n360 번째 문서의 topic 비율은 [(1, 0.3183928), (6, 0.083300315), (7, 0.4637321), (8, 0.13379128)]\n361 번째 문서의 topic 비율은 [(0, 0.99544185)]\n362 번째 문서의 topic 비율은 [(1, 0.16497675), (5, 0.3524692), (6, 0.10026114), (9, 0.38033208)]\n363 번째 문서의 topic 비율은 [(3, 0.6165371), (5, 0.38092482)]\n364 번째 문서의 topic 비율은 [(2, 0.99667746)]\n365 번째 문서의 topic 비율은 [(4, 0.039968155), (5, 0.8405915), (6, 0.11799274)]\n366 번째 문서의 topic 비율은 [(1, 0.17098244), (5, 0.41360775), (9, 0.40881896)]\n367 번째 문서의 topic 비율은 [(5, 0.95730335), (7, 0.041993223)]\n368 번째 문서의 topic 비율은 [(1, 0.33482334), (9, 0.65858203)]\n369 번째 문서의 topic 비율은 [(5, 0.108030625), (7, 0.1656376), (9, 0.7249836)]\n370 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n371 번째 문서의 topic 비율은 [(5, 0.7936707), (9, 0.20273529)]\n372 번째 문서의 topic 비율은 [(0, 0.20437546), (3, 0.5085252), (6, 0.2665577)]\n373 번째 문서의 topic 비율은 [(7, 0.06827221), (9, 0.92635375)]\n374 번째 문서의 topic 비율은 [(1, 0.07425175), (4, 0.032077387), (6, 0.08598902), (8, 0.80659723)]\n375 번째 문서의 topic 비율은 [(2, 0.03753439), (4, 0.023286121), (6, 0.118395664), (7, 0.5217895), (8, 0.29515857)]\n376 번째 문서의 topic 비율은 [(5, 0.99759084)]\n377 번째 문서의 topic 비율은 [(2, 0.51312536), (3, 0.03918533), (5, 0.44339064)]\n378 번째 문서의 topic 비율은 [(7, 0.91193175), (9, 0.08571787)]\n379 번째 문서의 topic 비율은 [(5, 0.95469517), (7, 0.03695174)]\n380 번째 문서의 topic 비율은 [(2, 0.06102251), (3, 0.03958701), (7, 0.817307), (9, 0.08006383)]\n381 번째 문서의 topic 비율은 [(5, 0.40281022), (9, 0.5951813)]\n382 번째 문서의 topic 비율은 [(5, 0.8860484), (9, 0.112172)]\n383 번째 문서의 topic 비율은 [(5, 0.29411936), (6, 0.060131326), (9, 0.6353389)]\n384 번째 문서의 topic 비율은 [(1, 0.11250914), (3, 0.32396936), (4, 0.40144175), (6, 0.1586912)]\n385 번째 문서의 topic 비율은 [(1, 0.57514834), (5, 0.112281926), (6, 0.029965604), (9, 0.27893084)]\n386 번째 문서의 topic 비율은 [(3, 0.9953007)]\n387 번째 문서의 topic 비율은 [(5, 0.36760908), (8, 0.1601808), (9, 0.46743298)]\n388 번째 문서의 topic 비율은 [(0, 0.107097074), (1, 0.369468), (3, 0.18922614), (7, 0.32880443)]\n389 번째 문서의 topic 비율은 [(5, 0.2956985), (7, 0.0145852985), (9, 0.68857694)]\n390 번째 문서의 topic 비율은 [(4, 0.34176186), (7, 0.6116576), (9, 0.03847287)]\n391 번째 문서의 topic 비율은 [(2, 0.9359213), (9, 0.060029272)]\n","name":"stdout"},{"output_type":"stream","text":"392 번째 문서의 topic 비율은 [(0, 0.079475224), (7, 0.17551623), (9, 0.7424693)]\n393 번째 문서의 topic 비율은 [(1, 0.31698996), (5, 0.21362714), (6, 0.026489472), (9, 0.44144207)]\n394 번째 문서의 topic 비율은 [(4, 0.074964106), (5, 0.7083601), (9, 0.21418819)]\n395 번째 문서의 topic 비율은 [(4, 0.023486074), (5, 0.76415044), (6, 0.13099426), (8, 0.049050696), (9, 0.031635724)]\n396 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n397 번째 문서의 topic 비율은 [(0, 0.026598103), (1, 0.039084096), (2, 0.031063585), (3, 0.023993984), (4, 0.019931914), (5, 0.052557677), (6, 0.652343), (7, 0.05163344), (8, 0.03468872), (9, 0.068105556)]\n398 번째 문서의 topic 비율은 [(0, 0.13819334), (2, 0.32680812), (8, 0.41854575), (9, 0.11572691)]\n399 번째 문서의 topic 비율은 [(1, 0.56859183), (3, 0.05113138), (4, 0.16254586), (9, 0.21485372)]\n400 번째 문서의 topic 비율은 [(3, 0.33619574), (5, 0.6422572)]\n401 번째 문서의 topic 비율은 [(6, 0.2148282), (7, 0.38453215), (8, 0.39945862)]\n402 번째 문서의 topic 비율은 [(6, 0.3819867), (9, 0.5705838)]\n403 번째 문서의 topic 비율은 [(5, 0.21981053), (9, 0.77826905)]\n404 번째 문서의 topic 비율은 [(7, 0.98892134)]\n405 번째 문서의 topic 비율은 [(0, 0.053668875), (2, 0.3211911), (4, 0.13997617), (5, 0.08174573), (6, 0.3559235), (9, 0.046819266)]\n406 번째 문서의 topic 비율은 [(1, 0.086047955), (5, 0.37804988), (9, 0.5254262)]\n407 번째 문서의 topic 비율은 [(4, 0.97653025)]\n408 번째 문서의 topic 비율은 [(1, 0.2619767), (4, 0.11098955), (6, 0.27679116), (7, 0.34302065)]\n409 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n410 번째 문서의 topic 비율은 [(1, 0.8429405), (9, 0.14044559)]\n411 번째 문서의 topic 비율은 [(1, 0.44358337), (2, 0.098740324), (7, 0.3679303), (8, 0.08652768)]\n412 번째 문서의 topic 비율은 [(1, 0.35093936), (7, 0.45911378), (8, 0.18010533)]\n413 번째 문서의 topic 비율은 [(1, 0.16570483), (7, 0.011636328), (9, 0.8203954)]\n414 번째 문서의 topic 비율은 [(1, 0.26000074), (5, 0.2961439), (7, 0.09348011), (9, 0.34305367)]\n415 번째 문서의 topic 비율은 [(4, 0.11862775), (5, 0.6996651), (6, 0.022563174), (8, 0.15562326)]\n416 번째 문서의 topic 비율은 [(0, 0.029070644), (5, 0.8127291), (7, 0.0916211), (9, 0.065457)]\n417 번째 문서의 topic 비율은 [(1, 0.12395103), (2, 0.13133007), (5, 0.03864329), (6, 0.7032829)]\n418 번째 문서의 topic 비율은 [(0, 0.038759526), (5, 0.14366129), (6, 0.043363467), (7, 0.039854247), (8, 0.5650966), (9, 0.16871578)]\n419 번째 문서의 topic 비율은 [(5, 0.9786022)]\n420 번째 문서의 topic 비율은 [(6, 0.94657975), (9, 0.01045836)]\n421 번째 문서의 topic 비율은 [(0, 0.22202517), (2, 0.5132325), (8, 0.25963512)]\n422 번째 문서의 topic 비율은 [(0, 0.02131914), (5, 0.90743196), (6, 0.06944774)]\n423 번째 문서의 topic 비율은 [(5, 0.5012094), (9, 0.48961246)]\n424 번째 문서의 topic 비율은 [(2, 0.9967697)]\n425 번째 문서의 topic 비율은 [(2, 0.397086), (3, 0.06501603), (6, 0.049904782), (8, 0.24989493), (9, 0.23672919)]\n426 번째 문서의 topic 비율은 [(0, 0.08325142), (2, 0.049684014), (6, 0.067952156), (8, 0.3318005), (9, 0.4656986)]\n427 번째 문서의 topic 비율은 [(2, 0.21285252), (5, 0.4641224), (8, 0.3165047)]\n428 번째 문서의 topic 비율은 [(2, 0.99854344)]\n429 번째 문서의 topic 비율은 [(1, 0.30317003), (2, 0.10512914), (6, 0.3870286), (7, 0.19991715)]\n430 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n431 번째 문서의 topic 비율은 [(0, 0.03945896), (2, 0.03998614), (4, 0.024737816), (7, 0.6243935), (8, 0.26946017)]\n432 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n433 번째 문서의 topic 비율은 [(0, 0.028720114), (2, 0.14903185), (6, 0.6422291), (7, 0.17698938)]\n434 번째 문서의 topic 비율은 [(0, 0.054807525), (2, 0.021598045), (4, 0.1979936), (5, 0.03614295), (7, 0.68527687)]\n435 번째 문서의 topic 비율은 [(0, 0.97695434)]\n436 번째 문서의 topic 비율은 [(0, 0.20567021), (6, 0.6167898), (9, 0.15535542)]\n437 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n438 번째 문서의 topic 비율은 [(1, 0.53996795), (6, 0.32941747), (9, 0.12771696)]\n439 번째 문서의 topic 비율은 [(5, 0.99366933)]\n440 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n441 번째 문서의 topic 비율은 [(4, 0.22267877), (5, 0.25590345), (7, 0.5186978)]\n442 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n443 번째 문서의 topic 비율은 [(1, 0.41860485), (7, 0.42978504), (9, 0.14943923)]\n444 번째 문서의 topic 비율은 [(1, 0.013782278), (2, 0.010953992), (5, 0.018533198), (6, 0.014270726), (7, 0.01820894), (8, 0.87538356), (9, 0.023998396)]\n445 번째 문서의 topic 비율은 [(8, 0.9883623)]\n446 번째 문서의 topic 비율은 [(1, 0.2912598), (6, 0.5791572), (7, 0.07613556), (8, 0.052115113)]\n447 번째 문서의 topic 비율은 [(1, 0.099756315), (5, 0.5839828), (9, 0.31288648)]\n448 번째 문서의 topic 비율은 [(1, 0.13159434), (2, 0.06960755), (4, 0.47286722), (6, 0.24891791), (7, 0.075218864)]\n449 번째 문서의 topic 비율은 [(2, 0.3013835), (5, 0.3903273), (8, 0.30319947)]\n450 번째 문서의 topic 비율은 [(7, 0.6457253), (9, 0.30346704)]\n451 번째 문서의 topic 비율은 [(0, 0.24255899), (2, 0.3052074), (5, 0.14363596), (7, 0.042013507), (8, 0.12009943), (9, 0.14516237)]\n452 번째 문서의 topic 비율은 [(1, 0.011347187), (5, 0.9025785), (6, 0.011749341), (7, 0.014992272), (8, 0.010071088), (9, 0.019768026)]\n453 번째 문서의 topic 비율은 [(2, 0.35347834), (3, 0.12719248), (5, 0.055878233), (8, 0.39845017), (9, 0.0617958)]\n454 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n455 번째 문서의 topic 비율은 [(1, 0.46017998), (8, 0.188142), (9, 0.34469402)]\n456 번째 문서의 topic 비율은 [(3, 0.07981488), (6, 0.20163012), (7, 0.46881145), (8, 0.24704334)]\n457 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n458 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n459 번째 문서의 topic 비율은 [(0, 0.23017116), (2, 0.06495362), (5, 0.69738364)]\n460 번째 문서의 topic 비율은 [(2, 0.107189335), (8, 0.33654433), (9, 0.55433017)]\n461 번째 문서의 topic 비율은 [(0, 0.11626253), (2, 0.12470091), (8, 0.7527293)]\n462 번째 문서의 topic 비율은 [(1, 0.046960372), (5, 0.43551365), (7, 0.21235672), (9, 0.2995745)]\n463 번째 문서의 topic 비율은 [(0, 0.18441695), (5, 0.11975492), (6, 0.039279457), (7, 0.032749616), (9, 0.6217998)]\n464 번째 문서의 topic 비율은 [(0, 0.03339771), (1, 0.2245204), (2, 0.0641962), (3, 0.021841226), (5, 0.32424617), (9, 0.3300558)]\n465 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n466 번째 문서의 topic 비율은 [(4, 0.013339768), (5, 0.51150125), (6, 0.048044626), (9, 0.4247743)]\n467 번째 문서의 topic 비율은 [(1, 0.44475883), (5, 0.012946861), (7, 0.48198268), (9, 0.016773826)]\n468 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n469 번째 문서의 topic 비율은 [(6, 0.039301094), (7, 0.0885336), (9, 0.8677107)]\n470 번째 문서의 topic 비율은 [(1, 0.32158357), (6, 0.16466899), (9, 0.5094102)]\n471 번째 문서의 topic 비율은 [(5, 0.25169724), (9, 0.7312509)]\n472 번째 문서의 topic 비율은 [(3, 0.04275522), (6, 0.83782303), (7, 0.09619369), (8, 0.022743614)]\n473 번째 문서의 topic 비율은 [(1, 0.24744093), (2, 0.45405477), (7, 0.21864457), (8, 0.07457414)]\n474 번째 문서의 topic 비율은 [(7, 0.9404528), (9, 0.058288887)]\n475 번째 문서의 topic 비율은 [(1, 0.6828653), (5, 0.119004644), (7, 0.18623413)]\n476 번째 문서의 topic 비율은 [(1, 0.11853314), (5, 0.06957012), (6, 0.26279247), (7, 0.41204464), (8, 0.13540488)]\n477 번째 문서의 topic 비율은 [(2, 0.4216147), (4, 0.09978206), (6, 0.47342622)]\n478 번째 문서의 topic 비율은 [(0, 0.1601897), (1, 0.06618724), (2, 0.6315153), (9, 0.13974771)]\n479 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n480 번째 문서의 topic 비율은 [(3, 0.023990368), (6, 0.092188194), (7, 0.22102608), (8, 0.66201746)]\n481 번째 문서의 topic 비율은 [(0, 0.05771331), (1, 0.41262242), (3, 0.45407754), (6, 0.07330351)]\n482 번째 문서의 topic 비율은 [(2, 0.39557466), (6, 0.4886584), (8, 0.104143254)]\n483 번째 문서의 topic 비율은 [(0, 0.19049296), (3, 0.1607753), (8, 0.29864717), (9, 0.34729683)]\n484 번째 문서의 topic 비율은 [(0, 0.15631963), (2, 0.114857905), (7, 0.13804731), (8, 0.2865066), (9, 0.30281255)]\n485 번째 문서의 topic 비율은 [(4, 0.28835547), (5, 0.5182157), (9, 0.19130652)]\n486 번째 문서의 topic 비율은 [(1, 0.020337917), (2, 0.02263461), (6, 0.37838143), (7, 0.39411938), (8, 0.013666833), (9, 0.17000233)]\n487 번째 문서의 topic 비율은 [(0, 0.59165883), (5, 0.15502086), (9, 0.25193724)]\n488 번째 문서의 topic 비율은 [(7, 0.17745776), (9, 0.8184661)]\n489 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n490 번째 문서의 topic 비율은 [(0, 0.08337959), (1, 0.041077603), (5, 0.12368619), (9, 0.75046873)]\n491 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n492 번째 문서의 topic 비율은 [(2, 0.37494075), (6, 0.16409358), (7, 0.15815121), (8, 0.09536867), (9, 0.2041174)]\n493 번째 문서의 topic 비율은 [(5, 0.89572686), (9, 0.10266974)]\n494 번째 문서의 topic 비율은 [(5, 0.5975725), (6, 0.085604526), (9, 0.31520736)]\n495 번째 문서의 topic 비율은 [(0, 0.020927325), (3, 0.2323641), (5, 0.613102), (8, 0.12685996)]\n496 번째 문서의 topic 비율은 [(0, 0.9226312), (5, 0.01124933), (7, 0.01105166), (9, 0.014564593)]\n497 번째 문서의 topic 비율은 [(2, 0.028430562), (4, 0.4335351), (5, 0.0541361), (7, 0.48351306)]\n498 번째 문서의 topic 비율은 [(0, 0.011960962), (1, 0.017575817), (2, 0.013969054), (3, 0.010789909), (5, 0.023634933), (6, 0.01819875), (7, 0.8486996), (8, 0.01559925), (9, 0.030608494)]\n499 번째 문서의 topic 비율은 [(1, 0.38219136), (4, 0.05992214), (5, 0.31980228), (7, 0.10231763), (9, 0.13296941)]\n500 번째 문서의 topic 비율은 [(0, 0.5319347), (2, 0.42163944)]\n501 번째 문서의 topic 비율은 [(7, 0.9967193)]\n502 번째 문서의 topic 비율은 [(8, 0.48526606), (9, 0.4950029)]\n503 번째 문서의 topic 비율은 [(4, 0.02474668), (5, 0.11497964), (7, 0.4668195), (9, 0.3913836)]\n504 번째 문서의 topic 비율은 [(7, 0.8639314), (8, 0.09989435)]\n505 번째 문서의 topic 비율은 [(1, 0.040645495), (2, 0.16648789), (3, 0.7870474)]\n506 번째 문서의 topic 비율은 [(1, 0.70350623), (7, 0.07799547), (9, 0.2129572)]\n507 번째 문서의 topic 비율은 [(1, 0.20990078), (5, 0.7660008), (9, 0.021911018)]\n508 번째 문서의 topic 비율은 [(1, 0.59096116), (2, 0.14645235), (9, 0.25827095)]\n509 번째 문서의 topic 비율은 [(7, 0.99272984)]\n510 번째 문서의 topic 비율은 [(2, 0.018641707), (7, 0.9779995)]\n511 번째 문서의 topic 비율은 [(7, 0.45256752), (9, 0.50189394)]\n512 번째 문서의 topic 비율은 [(5, 0.81423354), (9, 0.18369122)]\n513 번째 문서의 topic 비율은 [(6, 0.09146997), (8, 0.20130488), (9, 0.70387954)]\n514 번째 문서의 topic 비율은 [(5, 0.11031344), (9, 0.88516283)]\n515 번째 문서의 topic 비율은 [(3, 0.055698857), (5, 0.5592097), (8, 0.15764888), (9, 0.22354835)]\n516 번째 문서의 topic 비율은 [(0, 0.14299922), (2, 0.09286195), (3, 0.2963485), (8, 0.4640772)]\n517 번째 문서의 topic 비율은 [(2, 0.05583915), (3, 0.63784724), (7, 0.07500176), (8, 0.033120204), (9, 0.19762862)]\n518 번째 문서의 topic 비율은 [(1, 0.37752184), (5, 0.59664744), (9, 0.02335276)]\n519 번째 문서의 topic 비율은 [(5, 0.966605), (7, 0.026634896)]\n520 번째 문서의 topic 비율은 [(1, 0.29233935), (4, 0.022854054), (5, 0.022663206), (6, 0.069383256), (8, 0.10676991), (9, 0.4849983)]\n521 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n522 번째 문서의 topic 비율은 [(1, 0.20705287), (3, 0.19849989), (5, 0.56819266), (7, 0.02277148)]\n523 번째 문서의 topic 비율은 [(5, 0.23233865), (9, 0.73970276)]\n524 번째 문서의 topic 비율은 [(3, 0.46187553), (5, 0.0949238), (6, 0.05155699), (8, 0.09231913), (9, 0.29808322)]\n525 번째 문서의 topic 비율은 [(6, 0.085350394), (9, 0.9100169)]\n526 번째 문서의 topic 비율은 [(1, 0.60315174), (8, 0.13479275), (9, 0.2578471)]\n527 번째 문서의 topic 비율은 [(1, 0.07922547), (2, 0.27862525), (5, 0.08786423), (6, 0.22019507), (8, 0.17523912), (9, 0.1581317)]\n528 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n529 번째 문서의 topic 비율은 [(4, 0.330206), (5, 0.1500319), (7, 0.51318836)]\n530 번째 문서의 topic 비율은 [(0, 0.06922916), (5, 0.92039007)]\n531 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n532 번째 문서의 topic 비율은 [(5, 0.99737144)]\n533 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n534 번째 문서의 topic 비율은 [(0, 0.23673223), (6, 0.45260003), (9, 0.2627837)]\n","name":"stdout"},{"output_type":"stream","text":"535 번째 문서의 topic 비율은 [(4, 0.54328793), (7, 0.4540123)]\n536 번째 문서의 topic 비율은 [(0, 0.05270029), (3, 0.19912656), (4, 0.10219177), (5, 0.56048256), (9, 0.08351936)]\n537 번째 문서의 topic 비율은 [(2, 0.076366395), (3, 0.09637805), (5, 0.2457631), (6, 0.29024848), (8, 0.1779117), (9, 0.11212374)]\n538 번째 문서의 topic 비율은 [(3, 0.09819973), (5, 0.44215557), (8, 0.45618078)]\n539 번째 문서의 topic 비율은 [(1, 0.013589356), (2, 0.4378987), (3, 0.018318132), (6, 0.43676955), (8, 0.092614405)]\n540 번째 문서의 topic 비율은 [(6, 0.039710764), (7, 0.9531364)]\n541 번째 문서의 topic 비율은 [(7, 0.93735737), (9, 0.058849193)]\n542 번째 문서의 topic 비율은 [(1, 0.60368544), (5, 0.09194125), (6, 0.14851853), (9, 0.15429299)]\n543 번째 문서의 topic 비율은 [(0, 0.44303045), (8, 0.3983675), (9, 0.15647274)]\n544 번째 문서의 topic 비율은 [(1, 0.08334861), (4, 0.092838906), (6, 0.51690644), (9, 0.30525738)]\n545 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n546 번째 문서의 topic 비율은 [(5, 0.9984708)]\n547 번째 문서의 topic 비율은 [(5, 0.011251861), (6, 0.48411655), (7, 0.011054245), (9, 0.45603493)]\n548 번째 문서의 topic 비율은 [(3, 0.072446905), (5, 0.618981), (9, 0.29827207)]\n549 번째 문서의 topic 비율은 [(2, 0.76762784), (3, 0.05059228), (6, 0.17606245)]\n550 번째 문서의 topic 비율은 [(0, 0.63854223), (1, 0.039083015), (2, 0.031062722), (3, 0.023993319), (4, 0.019931361), (5, 0.052555416), (6, 0.040468134), (7, 0.05163201), (8, 0.034687757), (9, 0.068044014)]\n551 번째 문서의 topic 비율은 [(0, 0.22641778), (1, 0.117666125), (2, 0.29989126), (8, 0.33639956), (9, 0.017228926)]\n552 번째 문서의 topic 비율은 [(0, 0.06661528), (3, 0.20441134), (8, 0.14822854), (9, 0.5772768)]\n553 번째 문서의 topic 비율은 [(0, 0.6586876), (5, 0.023708537), (9, 0.3164059)]\n554 번째 문서의 topic 비율은 [(3, 0.5030881), (5, 0.36102784), (9, 0.12148268)]\n555 번째 문서의 topic 비율은 [(5, 0.24870823), (9, 0.7493545)]\n556 번째 문서의 topic 비율은 [(5, 0.03555763), (7, 0.39864767), (9, 0.55711174)]\n557 번째 문서의 topic 비율은 [(1, 0.4244626), (6, 0.46836224), (9, 0.10241916)]\n558 번째 문서의 topic 비율은 [(5, 0.9917267)]\n559 번째 문서의 topic 비율은 [(3, 0.07802274), (6, 0.33762434), (7, 0.57814676)]\n560 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n561 번째 문서의 topic 비율은 [(0, 0.21348971), (2, 0.1604594), (8, 0.27431834), (9, 0.34940124)]\n562 번째 문서의 topic 비율은 [(2, 0.12583727), (4, 0.822365), (9, 0.010456884)]\n563 번째 문서의 topic 비율은 [(2, 0.043910317), (6, 0.09765781), (7, 0.6141046), (9, 0.24139237)]\n564 번째 문서의 topic 비율은 [(3, 0.39649063), (5, 0.5951149)]\n565 번째 문서의 topic 비율은 [(0, 0.023641016), (1, 0.071551114), (7, 0.7869099), (8, 0.11697395)]\n566 번째 문서의 topic 비율은 [(2, 0.2391548), (3, 0.22451586), (7, 0.5349789)]\n567 번째 문서의 topic 비율은 [(0, 0.03529155), (8, 0.8605178), (9, 0.102806926)]\n568 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n569 번째 문서의 topic 비율은 [(3, 0.28306583), (5, 0.28720126), (9, 0.41134137)]\n570 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n571 번째 문서의 topic 비율은 [(1, 0.33829334), (7, 0.65582526)]\n572 번째 문서의 topic 비율은 [(1, 0.26953876), (7, 0.72781456)]\n573 번째 문서의 topic 비율은 [(1, 0.53310895), (7, 0.14640534), (9, 0.31978336)]\n574 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n575 번째 문서의 topic 비율은 [(0, 0.026598107), (1, 0.039084103), (2, 0.031063588), (3, 0.023993988), (4, 0.019931916), (5, 0.052557684), (6, 0.6523385), (7, 0.051633447), (8, 0.034688726), (9, 0.06810991)]\n576 번째 문서의 topic 비율은 [(5, 0.40010107), (9, 0.59388036)]\n577 번째 문서의 topic 비율은 [(1, 0.350067), (7, 0.12055678), (9, 0.5280998)]\n578 번째 문서의 topic 비율은 [(5, 0.17109641), (9, 0.82118547)]\n579 번째 문서의 topic 비율은 [(1, 0.22779147), (2, 0.06768502), (5, 0.364622), (9, 0.3360911)]\n580 번째 문서의 topic 비율은 [(6, 0.46308696), (7, 0.5255638)]\n581 번째 문서의 topic 비율은 [(1, 0.1759048), (5, 0.10060992), (9, 0.7170089)]\n582 번째 문서의 topic 비율은 [(1, 0.39926377), (4, 0.13745363), (5, 0.061610784), (7, 0.39846537)]\n583 번째 문서의 topic 비율은 [(5, 0.38451868), (6, 0.050406046), (9, 0.56412065)]\n584 번째 문서의 topic 비율은 [(1, 0.092963636), (7, 0.9007957)]\n585 번째 문서의 topic 비율은 [(5, 0.90887284), (7, 0.022081334), (9, 0.068654574)]\n586 번째 문서의 topic 비율은 [(5, 0.9536182), (8, 0.040282752)]\n587 번째 문서의 topic 비율은 [(0, 0.5657481), (8, 0.13046469), (9, 0.29974544)]\n588 번째 문서의 topic 비율은 [(4, 0.5159966), (5, 0.12820937), (7, 0.35446915)]\n589 번째 문서의 topic 비율은 [(5, 0.38425717), (9, 0.6023437)]\n590 번째 문서의 topic 비율은 [(0, 0.5721398), (1, 0.1853011), (5, 0.20268092), (9, 0.03746434)]\n591 번째 문서의 topic 비율은 [(0, 0.040945932), (2, 0.6195986), (8, 0.042828623), (9, 0.2947995)]\n592 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n593 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n594 번째 문서의 topic 비율은 [(3, 0.3997127), (7, 0.486675), (8, 0.08456234)]\n595 번째 문서의 topic 비율은 [(4, 0.28777266), (7, 0.70786583)]\n596 번째 문서의 topic 비율은 [(0, 0.07120706), (9, 0.9233208)]\n597 번째 문서의 topic 비율은 [(5, 0.91396445), (8, 0.08241025)]\n598 번째 문서의 topic 비율은 [(6, 0.29562968), (7, 0.27215892), (8, 0.4274989)]\n599 번째 문서의 topic 비율은 [(0, 0.053770248), (2, 0.025285305), (5, 0.012306353), (6, 0.07063228), (8, 0.18070899), (9, 0.6565181)]\n600 번째 문서의 topic 비율은 [(2, 0.014458306), (4, 0.04013632), (6, 0.27045906), (9, 0.67197204)]\n601 번째 문서의 topic 비율은 [(1, 0.36703667), (3, 0.050689943), (4, 0.12540066), (6, 0.36611485), (7, 0.08958046)]\n602 번째 문서의 topic 비율은 [(2, 0.5706583), (5, 0.028374037), (7, 0.39865434)]\n603 번째 문서의 topic 비율은 [(3, 0.3783699), (9, 0.60877407)]\n604 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n605 번째 문서의 topic 비율은 [(2, 0.4487687), (5, 0.47622716), (7, 0.012720807), (9, 0.01676496)]\n606 번째 문서의 topic 비율은 [(1, 0.025413014), (6, 0.9714472)]\n607 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n608 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n609 번째 문서의 topic 비율은 [(1, 0.5135562), (2, 0.15147667), (6, 0.32695955)]\n610 번째 문서의 topic 비율은 [(1, 0.3845173), (3, 0.01282204), (4, 0.07365455), (5, 0.45923504), (7, 0.06794288)]\n611 번째 문서의 topic 비율은 [(0, 0.20455678), (1, 0.34608123), (9, 0.44339183)]\n612 번째 문서의 topic 비율은 [(2, 0.96868485)]\n613 번째 문서의 topic 비율은 [(9, 0.9508159)]\n614 번째 문서의 topic 비율은 [(1, 0.22235109), (7, 0.772039)]\n615 번째 문서의 topic 비율은 [(0, 0.06776157), (5, 0.30233005), (9, 0.62672406)]\n616 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n617 번째 문서의 topic 비율은 [(1, 0.2156022), (2, 0.5420359), (3, 0.02496761), (6, 0.11855441), (9, 0.09648422)]\n618 번째 문서의 topic 비율은 [(2, 0.40249294), (6, 0.13156341), (7, 0.2111676), (8, 0.1954427), (9, 0.059016876)]\n619 번째 문서의 topic 비율은 [(5, 0.8973509), (9, 0.09944954)]\n620 번째 문서의 topic 비율은 [(4, 0.35173258), (5, 0.45632166), (8, 0.18250403)]\n621 번째 문서의 topic 비율은 [(3, 0.2293387), (5, 0.52592754), (9, 0.22634006)]\n622 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n623 번째 문서의 topic 비율은 [(0, 0.12586193), (2, 0.36371738), (3, 0.3616472), (6, 0.14204696)]\n624 번째 문서의 topic 비율은 [(1, 0.07156706), (5, 0.85958445), (9, 0.06641891)]\n625 번째 문서의 topic 비율은 [(1, 0.37328333), (4, 0.22502722), (5, 0.017193552), (7, 0.3795705)]\n626 번째 문서의 topic 비율은 [(5, 0.053899128), (6, 0.049023483), (9, 0.8911534)]\n627 번째 문서의 topic 비율은 [(0, 0.94966), (6, 0.042346362)]\n628 번째 문서의 topic 비율은 [(7, 0.93631303), (9, 0.012887236)]\n629 번째 문서의 topic 비율은 [(1, 0.8319341), (2, 0.033632826), (8, 0.12692304)]\n630 번째 문서의 topic 비율은 [(6, 0.1880065), (7, 0.7223959), (8, 0.08483316)]\n631 번째 문서의 topic 비율은 [(0, 0.49286085), (2, 0.13026832), (4, 0.038823508), (6, 0.063503474), (9, 0.27244964)]\n632 번째 문서의 topic 비율은 [(9, 0.986104)]\n633 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n634 번째 문서의 topic 비율은 [(5, 0.9854246)]\n635 번째 문서의 topic 비율은 [(2, 0.51971555), (3, 0.21759365), (8, 0.24938148)]\n636 번째 문서의 topic 비율은 [(2, 0.038891893), (4, 0.093606696), (7, 0.8633591)]\n637 번째 문서의 topic 비율은 [(1, 0.13964972), (4, 0.072142325), (6, 0.7149301), (7, 0.07138474)]\n638 번째 문서의 topic 비율은 [(1, 0.051114507), (5, 0.41687506), (6, 0.09333023), (9, 0.42544475)]\n639 번째 문서의 topic 비율은 [(1, 0.6390704), (4, 0.12509345), (6, 0.095480055), (7, 0.09392444), (9, 0.046057906)]\n640 번째 문서의 topic 비율은 [(5, 0.17962073), (6, 0.12621102), (9, 0.68621236)]\n641 번째 문서의 topic 비율은 [(1, 0.28360713), (4, 0.28804812), (7, 0.25408277), (9, 0.17173918)]\n642 번째 문서의 topic 비율은 [(1, 0.96570593)]\n643 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n644 번째 문서의 topic 비율은 [(1, 0.6169023), (6, 0.18344945), (7, 0.054934356), (9, 0.14265144)]\n645 번째 문서의 topic 비율은 [(3, 0.26255488), (5, 0.7161186), (9, 0.01667882)]\n646 번째 문서의 topic 비율은 [(0, 0.14373061), (2, 0.83979684), (9, 0.015122649)]\n647 번째 문서의 topic 비율은 [(1, 0.6983266), (7, 0.23835303), (9, 0.062021855)]\n648 번째 문서의 topic 비율은 [(0, 0.47613075), (2, 0.31893173), (3, 0.13071485), (7, 0.07158039)]\n649 번째 문서의 topic 비율은 [(3, 0.14620014), (4, 0.16473827), (5, 0.62262714), (8, 0.06567456)]\n650 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n651 번째 문서의 topic 비율은 [(2, 0.15717106), (8, 0.75397164), (9, 0.08460075)]\n652 번째 문서의 topic 비율은 [(4, 0.16913065), (5, 0.7027507), (9, 0.11840186)]\n653 번째 문서의 topic 비율은 [(1, 0.5879625), (6, 0.2093808), (7, 0.20099454)]\n654 번째 문서의 topic 비율은 [(5, 0.8522388), (6, 0.08531294), (9, 0.0604276)]\n655 번째 문서의 topic 비율은 [(2, 0.10534257), (6, 0.674992), (8, 0.2167239)]\n656 번째 문서의 topic 비율은 [(1, 0.9844307)]\n657 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n658 번째 문서의 topic 비율은 [(5, 0.32948464), (7, 0.1608087), (9, 0.5061298)]\n659 번째 문서의 topic 비율은 [(5, 0.9705649)]\n660 번째 문서의 topic 비율은 [(0, 0.03473755), (3, 0.020484166), (4, 0.015789274), (5, 0.033810183), (7, 0.58972096), (8, 0.22272216), (9, 0.08222729)]\n661 번째 문서의 topic 비율은 [(0, 0.17982277), (2, 0.22278066), (6, 0.14038093), (9, 0.45202205)]\n662 번째 문서의 topic 비율은 [(1, 0.12763831), (7, 0.87005395)]\n663 번째 문서의 topic 비율은 [(1, 0.28739554), (5, 0.69441694)]\n664 번째 문서의 topic 비율은 [(1, 0.63445306), (2, 0.31161335), (9, 0.011547307)]\n665 번째 문서의 topic 비율은 [(1, 0.09325892), (6, 0.8627982), (7, 0.037544645)]\n666 번째 문서의 topic 비율은 [(3, 0.29666284), (7, 0.6854984)]\n667 번째 문서의 topic 비율은 [(2, 0.2909966), (5, 0.2614934), (7, 0.25979432), (8, 0.114083245), (9, 0.07095497)]\n668 번째 문서의 topic 비율은 [(9, 0.9733611)]\n669 번째 문서의 topic 비율은 [(0, 0.08358802), (5, 0.042766824), (9, 0.8691001)]\n670 번째 문서의 topic 비율은 [(2, 0.1575373), (3, 0.37567103), (5, 0.25076073), (8, 0.21272825)]\n671 번째 문서의 topic 비율은 [(0, 0.43345475), (6, 0.2668657), (8, 0.2827431)]\n672 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n673 번째 문서의 topic 비율은 [(2, 0.06407594), (5, 0.92450064)]\n674 번째 문서의 topic 비율은 [(3, 0.023802415), (6, 0.18044195), (7, 0.06373787), (9, 0.7294061)]\n675 번째 문서의 topic 비율은 [(0, 0.1419074), (2, 0.27181575), (4, 0.24953173), (5, 0.08092734), (7, 0.084875196), (8, 0.1705554)]\n676 번째 문서의 topic 비율은 [(0, 0.8286832), (2, 0.039196465), (9, 0.13081548)]\n677 번째 문서의 topic 비율은 [(0, 0.026975475), (5, 0.035985414), (9, 0.93190175)]\n678 번째 문서의 topic 비율은 [(1, 0.35179272), (3, 0.013180732), (6, 0.5247229), (8, 0.031717405), (9, 0.07757089)]\n679 번째 문서의 topic 비율은 [(5, 0.73374796), (7, 0.13619015), (8, 0.1283404)]\n680 번째 문서의 topic 비율은 [(2, 0.029127358), (9, 0.9615454)]\n681 번째 문서의 topic 비율은 [(1, 0.726136), (4, 0.2557137)]\n682 번째 문서의 topic 비율은 [(5, 0.7453485), (9, 0.25142878)]\n683 번째 문서의 topic 비율은 [(1, 0.23023166), (8, 0.71645683), (9, 0.011542935)]\n684 번째 문서의 topic 비율은 [(2, 0.1511182), (7, 0.5696989), (8, 0.24993347), (9, 0.02790101)]\n685 번째 문서의 topic 비율은 [(4, 0.218214), (5, 0.51685554), (6, 0.20890161), (8, 0.05121378)]\n","name":"stdout"},{"output_type":"stream","text":"686 번째 문서의 topic 비율은 [(2, 0.41862854), (3, 0.20056415), (8, 0.26962182), (9, 0.10901795)]\n687 번째 문서의 topic 비율은 [(3, 0.13221613), (5, 0.65788174), (9, 0.20574148)]\n688 번째 문서의 topic 비율은 [(1, 0.6120032), (4, 0.063329846), (6, 0.15358537), (7, 0.17003354)]\n689 번째 문서의 topic 비율은 [(2, 0.04888494), (6, 0.12754738), (7, 0.82141984)]\n690 번째 문서의 topic 비율은 [(1, 0.2178762), (7, 0.011487259), (9, 0.7688486)]\n691 번째 문서의 topic 비율은 [(5, 0.10781072), (6, 0.021607738), (7, 0.09878982), (9, 0.77095497)]\n692 번째 문서의 topic 비율은 [(5, 0.28906998), (7, 0.6957824)]\n693 번째 문서의 topic 비율은 [(2, 0.6674397), (3, 0.047369387), (5, 0.05092834), (6, 0.22878613)]\n694 번째 문서의 topic 비율은 [(5, 0.08315284), (6, 0.2951789), (9, 0.6181558)]\n695 번째 문서의 topic 비율은 [(1, 0.29603162), (2, 0.22347157), (4, 0.47684744)]\n696 번째 문서의 topic 비율은 [(4, 0.13534641), (6, 0.30558196), (7, 0.55786383)]\n697 번째 문서의 topic 비율은 [(0, 0.041228708), (1, 0.08571506), (4, 0.36353868), (5, 0.074612245), (7, 0.43389428)]\n698 번째 문서의 topic 비율은 [(1, 0.4023456), (6, 0.02830101), (7, 0.3031836), (8, 0.26536745)]\n699 번째 문서의 topic 비율은 [(2, 0.60316294), (6, 0.38308898)]\n700 번째 문서의 topic 비율은 [(0, 0.061164208), (2, 0.35680252), (3, 0.26492393), (8, 0.049078457), (9, 0.267226)]\n701 번째 문서의 topic 비율은 [(1, 0.6761438), (4, 0.040533874), (5, 0.0660621), (7, 0.21682294)]\n702 번째 문서의 topic 비율은 [(5, 0.5731522), (9, 0.37622297)]\n703 번째 문서의 topic 비율은 [(2, 0.055340596), (4, 0.45102957), (6, 0.15245174), (7, 0.11756398), (8, 0.17433652), (9, 0.047899105)]\n704 번째 문서의 topic 비율은 [(0, 0.029478297), (2, 0.40987265), (3, 0.42174274), (7, 0.08398258), (8, 0.05428311)]\n705 번째 문서의 topic 비율은 [(1, 0.16183649), (2, 0.3266637), (3, 0.47020492)]\n706 번째 문서의 topic 비율은 [(3, 0.097436845), (5, 0.7070505), (8, 0.09478655), (9, 0.09500425)]\n707 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n708 번째 문서의 topic 비율은 [(1, 0.550739), (7, 0.11489277), (9, 0.33309194)]\n709 번째 문서의 topic 비율은 [(1, 0.15964061), (2, 0.2661034), (3, 0.49905136), (6, 0.068016574)]\n710 번째 문서의 topic 비율은 [(3, 0.25293517), (5, 0.33517694), (9, 0.39573273)]\n711 번째 문서의 topic 비율은 [(0, 0.11373529), (1, 0.35485202), (5, 0.26043835), (6, 0.13996486), (8, 0.05818282), (9, 0.06923525)]\n712 번째 문서의 topic 비율은 [(1, 0.22396593), (2, 0.040096696), (3, 0.17264958), (6, 0.29896045), (7, 0.26131785)]\n713 번째 문서의 topic 비율은 [(2, 0.24583966), (3, 0.56698364), (6, 0.015387668), (8, 0.16986084)]\n714 번째 문서의 topic 비율은 [(7, 0.9993241)]\n715 번째 문서의 topic 비율은 [(1, 0.34821075), (6, 0.6501503)]\n716 번째 문서의 topic 비율은 [(1, 0.09506759), (5, 0.55981505), (7, 0.064780675), (9, 0.2790896)]\n717 번째 문서의 topic 비율은 [(6, 0.5261955), (8, 0.42569822), (9, 0.010468408)]\n718 번째 문서의 topic 비율은 [(6, 0.41072196), (8, 0.5878441)]\n719 번째 문서의 topic 비율은 [(3, 0.21952038), (4, 0.07572279), (7, 0.68915147)]\n720 번째 문서의 topic 비율은 [(1, 0.6782382), (4, 0.26332447), (7, 0.044104647)]\n721 번째 문서의 topic 비율은 [(2, 0.13797934), (5, 0.32839623), (8, 0.5282884)]\n722 번째 문서의 topic 비율은 [(5, 0.9841652)]\n723 번째 문서의 topic 비율은 [(0, 0.08411385), (4, 0.22281227), (7, 0.6690423), (8, 0.018206973)]\n724 번째 문서의 topic 비율은 [(0, 0.2665775), (2, 0.44481215), (9, 0.2814181)]\n725 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n726 번째 문서의 topic 비율은 [(0, 0.011960404), (1, 0.017575016), (2, 0.013968403), (3, 0.0107894065), (5, 0.023633817), (6, 0.84368324), (7, 0.023218174), (8, 0.015598522), (9, 0.030610168)]\n727 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n728 번째 문서의 topic 비율은 [(7, 0.44103652), (8, 0.5433736)]\n729 번째 문서의 topic 비율은 [(3, 0.24514562), (5, 0.5454632), (7, 0.08303544), (8, 0.12502475)]\n730 번째 문서의 topic 비율은 [(0, 0.35905233), (2, 0.17755301), (7, 0.40267122), (9, 0.059140775)]\n731 번째 문서의 topic 비율은 [(0, 0.31844005), (2, 0.38164693), (8, 0.29804415)]\n732 번째 문서의 topic 비율은 [(2, 0.49652752), (5, 0.012946734), (6, 0.4254982), (7, 0.012719416), (9, 0.01676239)]\n733 번째 문서의 topic 비율은 [(1, 0.4120462), (5, 0.53138274), (8, 0.054349985)]\n734 번째 문서의 topic 비율은 [(1, 0.16995357), (5, 0.64082), (9, 0.18378852)]\n735 번째 문서의 topic 비율은 [(4, 0.3275313), (6, 0.04804379), (8, 0.22392978), (9, 0.39782417)]\n736 번째 문서의 topic 비율은 [(3, 0.112533845), (5, 0.092609435), (6, 0.47186193), (9, 0.3173361)]\n737 번째 문서의 topic 비율은 [(0, 0.1642321), (4, 0.14431936), (7, 0.21722348), (9, 0.46644711)]\n738 번째 문서의 topic 비율은 [(3, 0.27130952), (5, 0.08058058), (9, 0.64025027)]\n739 번째 문서의 topic 비율은 [(5, 0.45114464), (6, 0.18342413), (9, 0.32246608)]\n740 번째 문서의 topic 비율은 [(3, 0.68481475), (4, 0.07185231), (6, 0.19928488), (8, 0.038595792)]\n741 번째 문서의 topic 비율은 [(5, 0.7299125), (9, 0.25958848)]\n742 번째 문서의 topic 비율은 [(1, 0.37643513), (7, 0.5980819), (9, 0.022242066)]\n743 번째 문서의 topic 비율은 [(0, 0.3371828), (1, 0.626813)]\n744 번째 문서의 topic 비율은 [(1, 0.24038303), (5, 0.42121395), (7, 0.3274788)]\n745 번째 문서의 topic 비율은 [(2, 0.022074187), (5, 0.36425927), (6, 0.019215643), (7, 0.05771333), (9, 0.536306)]\n746 번째 문서의 topic 비율은 [(2, 0.029637188), (4, 0.51246023), (7, 0.45732835)]\n747 번째 문서의 topic 비율은 [(0, 0.019946104), (2, 0.16498785), (3, 0.07937237), (6, 0.65256625), (9, 0.08146554)]\n748 번째 문서의 topic 비율은 [(5, 0.5600287), (9, 0.4375242)]\n749 번째 문서의 topic 비율은 [(0, 0.18735439), (5, 0.1817784), (9, 0.6269154)]\n750 번째 문서의 topic 비율은 [(5, 0.98942333)]\n751 번째 문서의 topic 비율은 [(5, 0.9846128)]\n752 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n753 번째 문서의 topic 비율은 [(0, 0.09607183), (9, 0.8968379)]\n754 번째 문서의 topic 비율은 [(5, 0.5885196), (9, 0.40492043)]\n755 번째 문서의 topic 비율은 [(1, 0.090242945), (6, 0.2023601), (7, 0.62564605), (9, 0.07880129)]\n756 번째 문서의 topic 비율은 [(1, 0.20213093), (5, 0.7782017)]\n757 번째 문서의 topic 비율은 [(1, 0.043871764), (3, 0.10204134), (6, 0.84172803)]\n758 번째 문서의 topic 비율은 [(1, 0.1904413), (2, 0.10113848), (3, 0.19536084), (6, 0.04969484), (9, 0.4624801)]\n759 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n760 번째 문서의 topic 비율은 [(0, 0.011655963), (1, 0.14055437), (3, 0.13670564), (4, 0.05893683), (6, 0.05763091), (7, 0.5937521)]\n761 번째 문서의 topic 비율은 [(9, 0.99203074)]\n762 번째 문서의 topic 비율은 [(1, 0.2607349), (3, 0.06443472), (5, 0.6720231)]\n763 번째 문서의 topic 비율은 [(1, 0.093856566), (3, 0.059984732), (6, 0.0786325), (7, 0.7667733)]\n764 번째 문서의 topic 비율은 [(1, 0.124442324), (4, 0.08296471), (7, 0.38357314), (8, 0.4060056)]\n765 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n766 번째 문서의 topic 비율은 [(0, 0.3315305), (1, 0.01757563), (2, 0.013968905), (3, 0.010789795), (5, 0.023634499), (6, 0.018198574), (7, 0.023218913), (8, 0.015599084), (9, 0.53652096)]\n767 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n768 번째 문서의 topic 비율은 [(4, 0.03510572), (7, 0.9646827)]\n769 번째 문서의 topic 비율은 [(1, 0.37476885), (4, 0.13259028), (6, 0.15381034), (7, 0.33435962)]\n770 번째 문서의 topic 비율은 [(1, 0.3706523), (4, 0.079474226), (7, 0.5298402)]\n771 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n772 번째 문서의 topic 비율은 [(3, 0.49147344), (5, 0.34572396), (8, 0.16130227)]\n773 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n774 번째 문서의 topic 비율은 [(5, 0.8977261), (9, 0.08585997)]\n775 번째 문서의 topic 비율은 [(1, 0.5036055), (7, 0.49036625)]\n776 번째 문서의 topic 비율은 [(1, 0.058825478), (4, 0.32997677), (7, 0.16765806), (8, 0.39231983), (9, 0.048965383)]\n777 번째 문서의 topic 비율은 [(5, 0.6693051), (9, 0.32823396)]\n778 번째 문서의 topic 비율은 [(1, 0.3857907), (8, 0.6084137)]\n779 번째 문서의 topic 비율은 [(1, 0.22584634), (7, 0.056468915), (9, 0.71556264)]\n780 번째 문서의 topic 비율은 [(1, 0.5463284), (6, 0.3633487), (9, 0.08795001)]\n781 번째 문서의 topic 비율은 [(0, 0.17889713), (2, 0.40400958), (8, 0.39474925)]\n782 번째 문서의 topic 비율은 [(0, 0.20269385), (3, 0.26261297), (8, 0.13007288), (9, 0.40033993)]\n783 번째 문서의 topic 비율은 [(0, 0.089560375), (7, 0.43509677), (9, 0.46104145)]\n784 번째 문서의 topic 비율은 [(0, 0.1753879), (2, 0.33515978), (3, 0.16797522), (7, 0.25940788), (8, 0.0576622)]\n785 번째 문서의 topic 비율은 [(1, 0.5949965), (4, 0.40300167)]\n786 번째 문서의 topic 비율은 [(0, 0.09994302), (2, 0.5836324), (6, 0.13923368), (8, 0.12754896), (9, 0.04674481)]\n787 번째 문서의 topic 비율은 [(0, 0.42917776), (2, 0.13724528), (8, 0.42598122)]\n788 번째 문서의 topic 비율은 [(2, 0.035465173), (3, 0.7344955), (6, 0.17183076), (9, 0.05694167)]\n789 번째 문서의 topic 비율은 [(1, 0.32137942), (4, 0.29257876), (6, 0.1735762), (7, 0.2029352)]\n790 번째 문서의 topic 비율은 [(2, 0.13133053), (4, 0.08842841), (5, 0.1480305), (6, 0.62770873)]\n791 번째 문서의 topic 비율은 [(5, 0.8179912), (8, 0.11257482), (9, 0.06820114)]\n792 번째 문서의 topic 비율은 [(6, 0.06053355), (7, 0.9304454)]\n793 번째 문서의 topic 비율은 [(1, 0.4486194), (6, 0.4776715), (8, 0.06566488)]\n794 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n","name":"stdout"},{"output_type":"stream","text":"795 번째 문서의 topic 비율은 [(7, 0.9930074)]\n796 번째 문서의 topic 비율은 [(0, 0.14867309), (5, 0.25249022), (7, 0.10608335), (8, 0.40746915), (9, 0.08160391)]\n797 번째 문서의 topic 비율은 [(5, 0.32729962), (9, 0.66484237)]\n798 번째 문서의 topic 비율은 [(0, 0.019037925), (1, 0.06822479), (5, 0.48600364), (9, 0.4243374)]\n799 번째 문서의 topic 비율은 [(4, 0.057746384), (5, 0.79253405), (7, 0.09299394), (8, 0.05514204)]\n800 번째 문서의 topic 비율은 [(0, 0.08030703), (2, 0.3805983), (3, 0.1400537), (6, 0.025109863), (8, 0.37105954)]\n801 번째 문서의 topic 비율은 [(1, 0.56052047), (7, 0.41975284)]\n802 번째 문서의 topic 비율은 [(0, 0.29765525), (8, 0.6297137), (9, 0.068385996)]\n803 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n804 번째 문서의 topic 비율은 [(0, 0.5622952), (9, 0.43619972)]\n805 번째 문서의 topic 비율은 [(0, 0.07973692), (2, 0.030753355), (9, 0.88502437)]\n806 번째 문서의 topic 비율은 [(1, 0.8417273), (4, 0.14067838), (6, 0.014263885)]\n807 번째 문서의 topic 비율은 [(1, 0.9938428)]\n808 번째 문서의 topic 비율은 [(0, 0.12690666), (6, 0.41960087), (9, 0.43954128)]\n809 번째 문서의 topic 비율은 [(3, 0.0350381), (5, 0.9594049)]\n810 번째 문서의 topic 비율은 [(0, 0.10886815), (1, 0.3392182), (2, 0.18197344), (8, 0.23138283), (9, 0.13687891)]\n811 번째 문서의 topic 비율은 [(0, 0.9689469), (8, 0.025797863)]\n812 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n813 번째 문서의 topic 비율은 [(0, 0.05251873), (7, 0.812533), (8, 0.13099067)]\n814 번째 문서의 topic 비율은 [(5, 0.64293706), (8, 0.25112745), (9, 0.10429302)]\n815 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n816 번째 문서의 topic 비율은 [(1, 0.36921299), (7, 0.50546646), (9, 0.12419048)]\n817 번째 문서의 topic 비율은 [(0, 0.101642035), (5, 0.41442126), (7, 0.03960482), (8, 0.18323907), (9, 0.2602205)]\n818 번째 문서의 topic 비율은 [(1, 0.80997944), (6, 0.18775548)]\n819 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n820 번째 문서의 topic 비율은 [(2, 0.22667278), (8, 0.7600349)]\n821 번째 문서의 topic 비율은 [(1, 0.86792725), (7, 0.094420515), (8, 0.03323215)]\n822 번째 문서의 topic 비율은 [(1, 0.62597513), (4, 0.03578784), (6, 0.031162078), (7, 0.3065603)]\n823 번째 문서의 topic 비율은 [(5, 0.072964355), (9, 0.9058425)]\n824 번째 문서의 topic 비율은 [(2, 0.059147134), (3, 0.11400776), (5, 0.63340074), (7, 0.19220416)]\n825 번째 문서의 topic 비율은 [(5, 0.9921237)]\n826 번째 문서의 topic 비율은 [(1, 0.6937878), (6, 0.3027014)]\n827 번째 문서의 topic 비율은 [(7, 0.12568489), (9, 0.85575306)]\n828 번째 문서의 topic 비율은 [(5, 0.9979356)]\n829 번째 문서의 topic 비율은 [(1, 0.2599956), (7, 0.38249582), (9, 0.34784928)]\n830 번째 문서의 topic 비율은 [(4, 0.25375086), (5, 0.74250156)]\n831 번째 문서의 topic 비율은 [(0, 0.053662214), (3, 0.040509), (5, 0.15106143), (9, 0.75156224)]\n832 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n833 번째 문서의 topic 비율은 [(0, 0.033655904), (3, 0.06419691), (5, 0.41984645), (8, 0.116205215), (9, 0.36498278)]\n834 번째 문서의 topic 비율은 [(5, 0.7830359), (8, 0.08566496), (9, 0.13042782)]\n835 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n836 번째 문서의 topic 비율은 [(2, 0.6761199), (6, 0.31823415)]\n837 번째 문서의 topic 비율은 [(1, 0.013579312), (5, 0.03082309), (7, 0.8908758), (8, 0.06422233)]\n838 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n839 번째 문서의 topic 비율은 [(1, 0.26147556), (7, 0.3699012), (8, 0.3653861)]\n840 번째 문서의 topic 비율은 [(3, 0.1848077), (5, 0.078568764), (9, 0.73368925)]\n841 번째 문서의 topic 비율은 [(0, 0.44260615), (8, 0.056278057), (9, 0.49617663)]\n842 번째 문서의 topic 비율은 [(1, 0.13341834), (7, 0.85964686)]\n843 번째 문서의 topic 비율은 [(0, 0.3401374), (2, 0.22200488), (6, 0.24940412), (9, 0.18649983)]\n844 번째 문서의 topic 비율은 [(2, 0.8106463), (3, 0.046134677), (9, 0.13950582)]\n845 번째 문서의 topic 비율은 [(0, 0.059935004), (1, 0.16938311), (3, 0.11634115), (7, 0.65240735)]\n846 번째 문서의 topic 비율은 [(2, 0.17253359), (3, 0.34829965), (5, 0.4154887), (6, 0.056496944)]\n847 번째 문서의 topic 비율은 [(5, 0.47438434), (6, 0.022045322), (9, 0.5014083)]\n848 번째 문서의 topic 비율은 [(3, 0.43242225), (5, 0.56093204)]\n849 번째 문서의 topic 비율은 [(1, 0.4891557), (6, 0.18831305), (7, 0.09347116), (9, 0.22593097)]\n850 번째 문서의 topic 비율은 [(0, 0.3223924), (2, 0.11668681), (5, 0.13315375), (8, 0.42432913)]\n851 번째 문서의 topic 비율은 [(0, 0.027840717), (5, 0.124095924), (9, 0.84548414)]\n852 번째 문서의 topic 비율은 [(0, 0.02509207), (7, 0.031580683), (9, 0.94041306)]\n853 번째 문서의 topic 비율은 [(2, 0.17377868), (7, 0.26048934), (8, 0.5625401)]\n854 번째 문서의 topic 비율은 [(1, 0.03722211), (7, 0.96069777)]\n855 번째 문서의 topic 비율은 [(1, 0.7010261), (3, 0.023570165), (4, 0.2682439)]\n856 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n857 번째 문서의 topic 비율은 [(3, 0.2627225), (7, 0.7196993), (8, 0.014603205)]\n858 번째 문서의 topic 비율은 [(5, 0.05934341), (6, 0.031482022), (9, 0.90833443)]\n859 번째 문서의 topic 비율은 [(0, 0.11447425), (3, 0.48472747), (5, 0.3966635)]\n860 번째 문서의 topic 비율은 [(2, 0.032900773), (7, 0.45478627), (8, 0.161817), (9, 0.34829715)]\n861 번째 문서의 topic 비율은 [(0, 0.52551645), (2, 0.19123411), (8, 0.28077832)]\n862 번째 문서의 topic 비율은 [(2, 0.27679306), (4, 0.12564), (7, 0.5958923)]\n863 번째 문서의 topic 비율은 [(4, 0.31391025), (6, 0.6304869), (9, 0.011553182)]\n864 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n865 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n866 번째 문서의 topic 비율은 [(5, 0.76223457), (6, 0.22939712)]\n867 번째 문서의 topic 비율은 [(1, 0.73277783), (5, 0.12535763), (9, 0.11635922)]\n868 번째 문서의 topic 비율은 [(0, 0.08107929), (3, 0.09049242), (6, 0.3696361), (7, 0.09000685), (9, 0.36656648)]\n869 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n870 번째 문서의 topic 비율은 [(7, 0.95273584)]\n871 번째 문서의 topic 비율은 [(0, 0.18616539), (2, 0.16154821), (8, 0.08859714), (9, 0.5593438)]\n872 번째 문서의 topic 비율은 [(2, 0.5850528), (3, 0.08832743), (6, 0.05924992), (7, 0.15912116), (8, 0.06168099), (9, 0.045629136)]\n873 번째 문서의 topic 비율은 [(5, 0.8605006), (6, 0.13393262)]\n874 번째 문서의 topic 비율은 [(5, 0.4061345), (9, 0.58881956)]\n875 번째 문서의 topic 비율은 [(0, 0.7297893), (8, 0.25768209)]\n876 번째 문서의 topic 비율은 [(0, 0.34945506), (8, 0.32076105), (9, 0.32688025)]\n877 번째 문서의 topic 비율은 [(5, 0.6511639), (6, 0.096757114), (8, 0.24686585)]\n878 번째 문서의 topic 비율은 [(0, 0.16648929), (1, 0.10664709), (2, 0.43056512), (7, 0.28983492)]\n879 번째 문서의 topic 비율은 [(2, 0.20389655), (5, 0.65611756), (7, 0.13429566)]\n880 번째 문서의 topic 비율은 [(3, 0.2222972), (5, 0.58491486), (9, 0.19056007)]\n881 번째 문서의 topic 비율은 [(0, 0.017803693), (1, 0.06549059), (4, 0.11172743), (5, 0.53255934), (8, 0.063638166), (9, 0.20276894)]\n882 번째 문서의 topic 비율은 [(0, 0.07399371), (1, 0.17044766), (4, 0.07454811), (6, 0.06315997), (7, 0.61487687)]\n883 번째 문서의 topic 비율은 [(1, 0.027986702), (2, 0.3330447), (6, 0.10019142), (7, 0.35678768), (8, 0.1773699)]\n884 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n885 번째 문서의 topic 비율은 [(3, 0.060368147), (5, 0.35699153), (6, 0.40558544), (9, 0.17167115)]\n886 번째 문서의 topic 비율은 [(1, 0.5158266), (4, 0.1124232), (5, 0.29223245), (7, 0.07724333)]\n887 번째 문서의 topic 비율은 [(2, 0.13489938), (5, 0.8351835)]\n888 번째 문서의 topic 비율은 [(5, 0.0884794), (7, 0.72641426), (8, 0.18191484)]\n889 번째 문서의 topic 비율은 [(4, 0.48640335), (7, 0.35828567), (9, 0.14801216)]\n890 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n891 번째 문서의 topic 비율은 [(0, 0.016502649), (1, 0.024249515), (2, 0.01927323), (3, 0.014886936), (4, 0.012366647), (5, 0.032609645), (6, 0.025109002), (7, 0.79125816), (8, 0.021522427), (9, 0.04222175)]\n892 번째 문서의 topic 비율은 [(0, 0.49544874), (8, 0.39580387), (9, 0.10425877)]\n893 번째 문서의 topic 비율은 [(1, 0.26631123), (3, 0.42921504), (7, 0.29446343)]\n894 번째 문서의 topic 비율은 [(6, 0.030541109), (9, 0.9579106)]\n895 번째 문서의 topic 비율은 [(0, 0.40873334), (5, 0.27358672), (8, 0.0968106), (9, 0.21895076)]\n896 번째 문서의 topic 비율은 [(2, 0.46432862), (3, 0.16105826), (7, 0.06328522), (8, 0.27321345), (9, 0.037742294)]\n897 번째 문서의 topic 비율은 [(5, 0.5866181), (9, 0.40145028)]\n898 번째 문서의 topic 비율은 [(2, 0.40401128), (3, 0.5837935)]\n899 번째 문서의 topic 비율은 [(6, 0.11252131), (7, 0.43527654), (8, 0.44791552)]\n900 번째 문서의 topic 비율은 [(3, 0.44808757), (5, 0.51811725), (9, 0.0279109)]\n901 번째 문서의 topic 비율은 [(1, 0.3261722), (6, 0.08890347), (7, 0.25106326), (9, 0.3330926)]\n902 번째 문서의 topic 비율은 [(0, 0.23112418), (2, 0.5817196), (5, 0.035609778), (7, 0.053445), (8, 0.09585877)]\n903 번째 문서의 topic 비율은 [(2, 0.104966916), (3, 0.89089847)]\n904 번째 문서의 topic 비율은 [(5, 0.012945924), (6, 0.5212724), (7, 0.012718722), (8, 0.401651), (9, 0.016761802)]\n905 번째 문서의 topic 비율은 [(1, 0.09993096), (2, 0.029640174), (4, 0.104737714), (6, 0.26722586), (7, 0.40690097), (8, 0.090624094)]\n906 번째 문서의 topic 비율은 [(5, 0.86094105), (8, 0.12165696), (9, 0.016955255)]\n907 번째 문서의 topic 비율은 [(2, 0.42326558), (6, 0.57506883)]\n908 번째 문서의 topic 비율은 [(0, 0.26626694), (3, 0.08222137), (5, 0.17530526), (8, 0.4719737)]\n909 번째 문서의 topic 비율은 [(4, 0.015703516), (5, 0.9749466)]\n910 번째 문서의 topic 비율은 [(1, 0.961025)]\n911 번째 문서의 topic 비율은 [(1, 0.11310833), (2, 0.06778918), (6, 0.31948367), (7, 0.36638787), (9, 0.13098115)]\n912 번째 문서의 topic 비율은 [(3, 0.08502268), (7, 0.34718874), (8, 0.53357035), (9, 0.024103234)]\n913 번째 문서의 topic 비율은 [(2, 0.07429512), (4, 0.21199432), (7, 0.7118414)]\n914 번째 문서의 topic 비율은 [(0, 0.4621808), (9, 0.53275174)]\n915 번째 문서의 topic 비율은 [(7, 0.9985629)]\n916 번째 문서의 topic 비율은 [(5, 0.40734383), (9, 0.5741605)]\n917 번째 문서의 topic 비율은 [(5, 0.9779123), (9, 0.019366179)]\n918 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n919 번째 문서의 topic 비율은 [(3, 0.021943169), (6, 0.970873)]\n920 번째 문서의 topic 비율은 [(1, 0.789283), (4, 0.20589861)]\n921 번째 문서의 topic 비율은 [(0, 0.13798016), (5, 0.8498923)]\n922 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n923 번째 문서의 topic 비율은 [(3, 0.32179397), (5, 0.39009196), (9, 0.28589877)]\n924 번째 문서의 topic 비율은 [(0, 0.16339481), (8, 0.7468848), (9, 0.0787723)]\n925 번째 문서의 topic 비율은 [(1, 0.15801829), (6, 0.13828568), (9, 0.6991595)]\n926 번째 문서의 topic 비율은 [(1, 0.37916693), (7, 0.6173772)]\n927 번째 문서의 topic 비율은 [(1, 0.39499578), (4, 0.15515175), (7, 0.44484898)]\n928 번째 문서의 topic 비율은 [(7, 0.9972317)]\n929 번째 문서의 topic 비율은 [(1, 0.2909464), (7, 0.70317125)]\n930 번째 문서의 topic 비율은 [(3, 0.47901556), (5, 0.09316138), (7, 0.4224198)]\n931 번째 문서의 topic 비율은 [(0, 0.15565878), (1, 0.17508122), (5, 0.5041424), (8, 0.160681)]\n932 번째 문서의 topic 비율은 [(1, 0.9538318), (7, 0.03956812)]\n933 번째 문서의 topic 비율은 [(0, 0.20670876), (3, 0.20300399), (5, 0.32371536), (8, 0.21888408), (9, 0.04035959)]\n934 번째 문서의 topic 비율은 [(5, 0.71833366), (9, 0.27709538)]\n935 번째 문서의 topic 비율은 [(3, 0.7982244), (6, 0.20036745)]\n936 번째 문서의 topic 비율은 [(1, 0.36059675), (7, 0.30627197), (9, 0.3285962)]\n937 번째 문서의 topic 비율은 [(3, 0.20496479), (4, 0.025768712), (5, 0.45005006), (6, 0.31711823)]\n938 번째 문서의 topic 비율은 [(0, 0.17806973), (1, 0.16529098), (2, 0.06704181), (8, 0.58719534)]\n","name":"stdout"},{"output_type":"stream","text":"939 번째 문서의 topic 비율은 [(0, 0.4528143), (5, 0.09794881), (8, 0.2837268), (9, 0.16420206)]\n940 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n941 번째 문서의 topic 비율은 [(2, 0.21329702), (6, 0.44706118), (8, 0.32771683)]\n942 번째 문서의 topic 비율은 [(5, 0.23953019), (6, 0.04448056), (9, 0.7097679)]\n943 번째 문서의 topic 비율은 [(2, 0.3690035), (6, 0.25467354), (8, 0.19392368), (9, 0.17850044)]\n944 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n945 번째 문서의 topic 비율은 [(0, 0.04861052), (5, 0.2677436), (9, 0.68223304)]\n946 번째 문서의 topic 비율은 [(5, 0.942347), (9, 0.05425471)]\n947 번째 문서의 topic 비율은 [(2, 0.8620313), (4, 0.06699958), (9, 0.060655024)]\n948 번째 문서의 topic 비율은 [(0, 0.38848594), (9, 0.6094257)]\n949 번째 문서의 topic 비율은 [(0, 0.39966264), (2, 0.07793236), (7, 0.27322033), (8, 0.105260104), (9, 0.14173864)]\n950 번째 문서의 topic 비율은 [(1, 0.5068338), (4, 0.06213986), (7, 0.3854949), (9, 0.04517743)]\n951 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n952 번째 문서의 topic 비율은 [(5, 0.5464045), (6, 0.036736835), (9, 0.41422072)]\n953 번째 문서의 topic 비율은 [(0, 0.013301736), (5, 0.4536603), (6, 0.12061098), (9, 0.41188344)]\n954 번째 문서의 topic 비율은 [(1, 0.34732607), (5, 0.27895322), (6, 0.053067677), (9, 0.31852975)]\n955 번째 문서의 topic 비율은 [(1, 0.09108155), (6, 0.3673905), (7, 0.44744506), (8, 0.09189005)]\n956 번째 문서의 topic 비율은 [(0, 0.017991465), (1, 0.025766445), (5, 0.40723664), (7, 0.051898252), (9, 0.49639094)]\n957 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n958 번째 문서의 topic 비율은 [(1, 0.6100805), (6, 0.14475037), (9, 0.24213734)]\n959 번째 문서의 topic 비율은 [(5, 0.22093387), (9, 0.7778012)]\n960 번째 문서의 topic 비율은 [(0, 0.09486577), (5, 0.41156948), (9, 0.4863595)]\n961 번째 문서의 topic 비율은 [(1, 0.2549519), (4, 0.066744186), (5, 0.14518754), (7, 0.530575)]\n962 번째 문서의 topic 비율은 [(0, 0.08008789), (3, 0.08472435), (6, 0.63468194), (9, 0.19860263)]\n963 번째 문서의 topic 비율은 [(0, 0.27050132), (2, 0.5228356), (3, 0.04805315), (8, 0.11896537), (9, 0.03812873)]\n964 번째 문서의 topic 비율은 [(1, 0.08551891), (5, 0.43336853), (9, 0.4769012)]\n965 번째 문서의 topic 비율은 [(1, 0.76888853), (2, 0.09514234), (7, 0.1326617)]\n966 번째 문서의 topic 비율은 [(0, 0.91811097), (4, 0.02260968), (8, 0.058277406)]\n967 번째 문서의 topic 비율은 [(5, 0.4637128), (7, 0.1619005), (9, 0.37347895)]\n968 번째 문서의 topic 비율은 [(0, 0.024204332), (2, 0.048981342), (3, 0.12528999), (6, 0.06366732), (9, 0.73642343)]\n969 번째 문서의 topic 비율은 [(0, 0.100010134), (2, 0.03283591), (6, 0.044350915), (8, 0.29401195), (9, 0.5282623)]\n970 번째 문서의 topic 비율은 [(0, 0.058416028), (1, 0.17831874), (2, 0.3583671), (4, 0.2968184), (5, 0.048246812), (8, 0.058587063)]\n971 번째 문서의 topic 비율은 [(1, 0.27815935), (5, 0.6573909), (6, 0.020653736), (9, 0.04029132)]\n972 번째 문서의 topic 비율은 [(0, 0.29204687), (2, 0.40340012), (9, 0.2909988)]\n973 번째 문서의 topic 비율은 [(5, 0.8495237), (6, 0.124589846)]\n974 번째 문서의 topic 비율은 [(0, 0.026597852), (1, 0.039083727), (2, 0.031063288), (3, 0.023993757), (4, 0.019931724), (5, 0.66449416), (6, 0.04046887), (7, 0.05163295), (8, 0.03468839), (9, 0.06804525)]\n975 번째 문서의 topic 비율은 [(1, 0.31829113), (7, 0.47602382), (9, 0.20079459)]\n976 번째 문서의 topic 비율은 [(1, 0.5611972), (2, 0.120994925), (6, 0.053630434), (7, 0.11804542), (9, 0.1456729)]\n977 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n978 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n979 번째 문서의 topic 비율은 [(1, 0.58067834), (7, 0.41850054)]\n980 번째 문서의 topic 비율은 [(0, 0.011960306), (1, 0.017574852), (2, 0.013968288), (3, 0.010789318), (5, 0.023633553), (6, 0.84367317), (7, 0.023219818), (8, 0.015598394), (9, 0.030619541)]\n981 번째 문서의 topic 비율은 [(1, 0.8303643), (4, 0.072478466), (7, 0.09576477)]\n982 번째 문서의 topic 비율은 [(2, 0.07881912), (3, 0.050132897), (7, 0.70614433), (8, 0.16331357)]\n983 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n984 번째 문서의 topic 비율은 [(1, 0.7345107), (2, 0.15439686), (9, 0.1032594)]\n985 번째 문서의 topic 비율은 [(0, 0.0700669), (5, 0.07349963), (6, 0.0383852), (7, 0.04606779), (9, 0.7700751)]\n986 번째 문서의 topic 비율은 [(0, 0.5674089), (8, 0.17492644), (9, 0.25309914)]\n987 번째 문서의 topic 비율은 [(0, 0.016501788), (1, 0.024248261), (2, 0.019272227), (3, 0.7740986), (4, 0.012366003), (5, 0.03260748), (6, 0.025107928), (7, 0.032035258), (8, 0.021521308), (9, 0.042241156)]\n988 번째 문서의 topic 비율은 [(0, 0.062934294), (1, 0.4716212), (8, 0.090789825), (9, 0.36711833)]\n989 번째 문서의 topic 비율은 [(6, 0.9875503)]\n990 번째 문서의 topic 비율은 [(5, 0.6866253), (6, 0.30150637)]\n991 번째 문서의 topic 비율은 [(5, 0.082566194), (7, 0.1221436), (9, 0.7924822)]\n992 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n993 번째 문서의 topic 비율은 [(1, 0.5813285), (2, 0.2711346), (3, 0.012715709), (8, 0.13299935)]\n994 번째 문서의 topic 비율은 [(5, 0.71094567), (9, 0.27605838)]\n995 번째 문서의 topic 비율은 [(4, 0.15534598), (6, 0.6455105), (9, 0.19565473)]\n996 번째 문서의 topic 비율은 [(5, 0.41273752), (9, 0.58351487)]\n997 번째 문서의 topic 비율은 [(2, 0.98847747)]\n998 번째 문서의 topic 비율은 [(2, 0.13823217), (5, 0.04062204), (6, 0.10192902), (8, 0.13340668), (9, 0.5848469)]\n999 번째 문서의 topic 비율은 [(5, 0.034343965), (8, 0.9557505)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\ndf_test = pd.read_csv('./df_test.csv')\ndocs_test_orig = array(df_test['text'])\ndocs_test = docs_preprocessor(docs_test_orig)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\nbigram_test = Phrases(docs_test, min_count=10)\ntrigram_test = Phrases(bigram[docs_test])\n\nfor idx in range(len(docs_test)):\n    for token in bigram_test[docs_test[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs_test[idx].append(token)\n    for token in trigram_test[docs_test[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs_test[idx].append(token)","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary representation of the documents.\ndictionary_test = Dictionary(docs_test)\nprint('Number of unique words in initital documents:', len(dictionary_test))\n\n# Filter out words that occur less than 10 documents, or more than 20% of the documents.\ndictionary_test.filter_extremes(no_below=10, no_above=0.2)\nprint('Number of unique words after removing rare and common words:', len(dictionary_test))","execution_count":68,"outputs":[{"output_type":"stream","text":"Number of unique words in initital documents: 3858\nNumber of unique words after removing rare and common words: 273\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_test = [dictionary_test.doc2bow(doc) for doc in docs_test]","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, topic_list in enumerate(model[corpus_test]):\n    print(i, \"번째 문서의 topic 비율은\", topic_list)    ","execution_count":71,"outputs":[{"output_type":"stream","text":"0 번째 문서의 topic 비율은 [(0, 0.058057558), (2, 0.60112077), (3, 0.07169372), (9, 0.2660747)]\n1 번째 문서의 topic 비율은 [(2, 0.87499017), (9, 0.10095728)]\n2 번째 문서의 topic 비율은 [(1, 0.08115631), (2, 0.6320003), (3, 0.22650416), (9, 0.056705162)]\n3 번째 문서의 topic 비율은 [(2, 0.7553299), (3, 0.07291451), (7, 0.05190968), (9, 0.11490869)]\n4 번째 문서의 topic 비율은 [(1, 0.034973405), (2, 0.5485551), (3, 0.2087791), (4, 0.031195594), (9, 0.1736587)]\n5 번째 문서의 topic 비율은 [(0, 0.02659665), (1, 0.03908196), (2, 0.64301735), (3, 0.023992673), (4, 0.019930825), (5, 0.052553892), (6, 0.040467046), (7, 0.05163062), (8, 0.034686822), (9, 0.06804218)]\n6 번째 문서의 topic 비율은 [(1, 0.025857), (2, 0.635465), (3, 0.094110936), (9, 0.24317142)]\n7 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n8 번째 문서의 topic 비율은 [(0, 0.08742313), (2, 0.5612366), (3, 0.061289545), (7, 0.014137866), (9, 0.27372614)]\n9 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n10 번째 문서의 topic 비율은 [(0, 0.03587534), (2, 0.68054914), (3, 0.16160916), (9, 0.12078794)]\n11 번째 문서의 topic 비율은 [(2, 0.8508721), (3, 0.13984482)]\n12 번째 문서의 topic 비율은 [(2, 0.58672374), (6, 0.13998981), (9, 0.26044476)]\n13 번째 문서의 topic 비율은 [(0, 0.12400579), (2, 0.55985004), (3, 0.06401776), (9, 0.2475766)]\n14 번째 문서의 topic 비율은 [(0, 0.092608236), (2, 0.78890437), (3, 0.10797267)]\n15 번째 문서의 topic 비율은 [(0, 0.11904339), (2, 0.4380402), (9, 0.43520692)]\n16 번째 문서의 topic 비율은 [(0, 0.056340255), (1, 0.07009641), (2, 0.69392973), (3, 0.07203389), (9, 0.105608396)]\n17 번째 문서의 topic 비율은 [(0, 0.12578003), (2, 0.28066766), (3, 0.5250213), (6, 0.06262713)]\n18 번째 문서의 topic 비율은 [(0, 0.1125854), (1, 0.11661576), (2, 0.26659888), (3, 0.3901689), (9, 0.11116474)]\n19 번째 문서의 topic 비율은 [(1, 0.13193619), (2, 0.39933088), (3, 0.16440067), (6, 0.05388029), (9, 0.24639069)]\n20 번째 문서의 topic 비율은 [(0, 0.10536593), (2, 0.015525734), (3, 0.6786449), (9, 0.19228444)]\n21 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n22 번째 문서의 topic 비율은 [(0, 0.23662512), (1, 0.015229421), (2, 0.40783578), (3, 0.33609176)]\n23 번째 문서의 topic 비율은 [(0, 0.07347662), (1, 0.11942657), (2, 0.3409159), (3, 0.38973528), (6, 0.07096181)]\n24 번째 문서의 topic 비율은 [(2, 0.52303565), (3, 0.20647524), (9, 0.25833637)]\n25 번째 문서의 topic 비율은 [(0, 0.14326677), (1, 0.01623108), (2, 0.27263767), (3, 0.4296728), (9, 0.13589196)]\n26 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n27 번째 문서의 topic 비율은 [(0, 0.01195958), (1, 0.017573794), (2, 0.0139674395), (3, 0.010788662), (5, 0.023632769), (6, 0.018196637), (7, 0.023217376), (8, 0.84109074), (9, 0.030610831)]\n28 번째 문서의 topic 비율은 [(0, 0.1735418), (3, 0.49906498), (9, 0.31730127)]\n29 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n30 번째 문서의 topic 비율은 [(0, 0.27236214), (2, 0.11442107), (9, 0.6098568)]\n31 번째 문서의 topic 비율은 [(0, 0.9004021), (2, 0.076744236)]\n32 번째 문서의 topic 비율은 [(0, 0.74910444), (8, 0.10215971), (9, 0.1247465)]\n33 번째 문서의 topic 비율은 [(0, 0.38687384), (9, 0.5575749)]\n34 번째 문서의 topic 비율은 [(0, 0.4045845), (3, 0.30498686), (9, 0.26679003)]\n35 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n36 번째 문서의 topic 비율은 [(0, 0.28488898), (2, 0.19268088), (7, 0.06910503), (9, 0.44844806)]\n37 번째 문서의 topic 비율은 [(0, 0.11493557), (3, 0.040101804), (7, 0.4235831), (9, 0.40936348)]\n38 번째 문서의 topic 비율은 [(0, 0.4732381), (2, 0.12048443), (3, 0.107638106), (9, 0.28400826)]\n39 번째 문서의 topic 비율은 [(0, 0.41163084), (3, 0.031184297), (7, 0.252078), (8, 0.074714385), (9, 0.22870737)]\n40 번째 문서의 topic 비율은 [(0, 0.27204156), (5, 0.21246912), (7, 0.31461594), (9, 0.18778402)]\n41 번째 문서의 topic 비율은 [(0, 0.20778473), (1, 0.15351868), (3, 0.3502402), (9, 0.27431396)]\n42 번째 문서의 topic 비율은 [(0, 0.19372112), (1, 0.15464528), (2, 0.15731533), (7, 0.01477579), (8, 0.08121224), (9, 0.39714357)]\n43 번째 문서의 topic 비율은 [(0, 0.08804923), (2, 0.27327514), (3, 0.16528764), (5, 0.05433988), (6, 0.030373124), (7, 0.053812485), (9, 0.33392638)]\n44 번째 문서의 topic 비율은 [(0, 0.105584644), (1, 0.10979063), (2, 0.32479268), (3, 0.11068283), (8, 0.07826629), (9, 0.2683601)]\n45 번째 문서의 topic 비율은 [(0, 0.3656648), (6, 0.17327425), (9, 0.43887332)]\n46 번째 문서의 topic 비율은 [(0, 0.117737636), (6, 0.25960466), (9, 0.58377)]\n47 번째 문서의 topic 비율은 [(0, 0.28239995), (1, 0.011338123), (3, 0.45204738), (5, 0.015247574), (6, 0.17937557), (7, 0.014978893), (8, 0.010063036), (9, 0.019755896)]\n48 번째 문서의 topic 비율은 [(0, 0.36273035), (2, 0.21894817), (3, 0.10838288), (8, 0.040845808), (9, 0.2623853)]\n49 번째 문서의 topic 비율은 [(0, 0.41313842), (2, 0.10800424), (3, 0.022030985), (9, 0.4538207)]\n50 번째 문서의 topic 비율은 [(0, 0.4649093), (7, 0.07875955), (9, 0.44253042)]\n51 번째 문서의 topic 비율은 [(0, 0.2747408), (2, 0.34449643), (3, 0.051873527), (6, 0.06148016), (9, 0.2585751)]\n52 번째 문서의 topic 비율은 [(0, 0.6843676), (2, 0.16029452), (9, 0.152528)]\n53 번째 문서의 topic 비율은 [(0, 0.26407254), (1, 0.025200691), (2, 0.14210267), (3, 0.06251917), (6, 0.030494153), (7, 0.06114077), (8, 0.06951966), (9, 0.34491068)]\n54 번째 문서의 topic 비율은 [(0, 0.31191579), (2, 0.12867053), (3, 0.07206164), (7, 0.23832609), (9, 0.24687053)]\n55 번째 문서의 topic 비율은 [(0, 0.08787664), (2, 0.16736212), (3, 0.04983287), (7, 0.21714279), (9, 0.47590867)]\n56 번째 문서의 topic 비율은 [(0, 0.29870567), (1, 0.07632677), (3, 0.12231789), (7, 0.09566173), (8, 0.06299106), (9, 0.34145537)]\n57 번째 문서의 topic 비율은 [(0, 0.2404207), (2, 0.16323425), (6, 0.096302345), (7, 0.29079685), (9, 0.20703049)]\n58 번째 문서의 topic 비율은 [(0, 0.2850254), (1, 0.08724944), (2, 0.16669248), (3, 0.15599695), (6, 0.09105534), (9, 0.2123015)]\n59 번째 문서의 topic 비율은 [(0, 0.25201324), (2, 0.15327448), (6, 0.17441389), (7, 0.2091296), (9, 0.2056716)]\n60 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n61 번째 문서의 topic 비율은 [(0, 0.3519073), (1, 0.04003349), (2, 0.26615298), (3, 0.16837187), (9, 0.1714933)]\n62 번째 문서의 topic 비율은 [(0, 0.20293419), (3, 0.1189511), (8, 0.15722524), (9, 0.5147634)]\n63 번째 문서의 topic 비율은 [(0, 0.2920462), (3, 0.048518356), (7, 0.2612841), (9, 0.38855198)]\n64 번째 문서의 topic 비율은 [(0, 0.4014554), (1, 0.08159038), (2, 0.20219022), (3, 0.03727759), (7, 0.073775746), (9, 0.20183422)]\n65 번째 문서의 topic 비율은 [(0, 0.3673679), (2, 0.34967256), (7, 0.08826111), (9, 0.19130887)]\n66 번째 문서의 topic 비율은 [(0, 0.23202585), (2, 0.20472726), (3, 0.09142793), (9, 0.4707533)]\n67 번째 문서의 topic 비율은 [(0, 0.16539302), (1, 0.15446322), (2, 0.28044045), (3, 0.07405289), (8, 0.21864997), (9, 0.1055272)]\n68 번째 문서의 topic 비율은 [(3, 0.16287906), (8, 0.66676813), (9, 0.16078258)]\n69 번째 문서의 topic 비율은 [(0, 0.4071946), (2, 0.34999546), (7, 0.12188396), (9, 0.119183026)]\n70 번째 문서의 topic 비율은 [(0, 0.16779004), (2, 0.2442657), (3, 0.099992275), (5, 0.18250926), (9, 0.30310002)]\n71 번째 문서의 topic 비율은 [(2, 0.06782052), (3, 0.1554418), (8, 0.4379148), (9, 0.3351197)]\n72 번째 문서의 topic 비율은 [(0, 0.1518059), (2, 0.37313586), (3, 0.06097178), (7, 0.11370805), (9, 0.2974613)]\n73 번째 문서의 topic 비율은 [(2, 0.15038905), (3, 0.12960714), (8, 0.4365746), (9, 0.27535844)]\n74 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n75 번째 문서의 topic 비율은 [(0, 0.17827679), (2, 0.3831284), (3, 0.26713338), (9, 0.16871053)]\n76 번째 문서의 topic 비율은 [(0, 0.108730145), (2, 0.39991343), (3, 0.28773636), (9, 0.19592473)]\n77 번째 문서의 topic 비율은 [(0, 0.016501276), (1, 0.02424752), (2, 0.778496), (3, 0.0148856975), (4, 0.012365618), (5, 0.032606218), (6, 0.025106868), (7, 0.03203403), (8, 0.021520639), (9, 0.04223611)]\n78 번째 문서의 topic 비율은 [(0, 0.016502263), (1, 0.024248948), (2, 0.7784877), (3, 0.014886588), (4, 0.012366358), (5, 0.032607794), (6, 0.025108341), (7, 0.032036), (8, 0.021521926), (9, 0.042234138)]\n79 번째 문서의 topic 비율은 [(0, 0.42820978), (1, 0.21105966), (3, 0.24673267), (9, 0.089915425)]\n80 번째 문서의 topic 비율은 [(0, 0.7781527), (1, 0.15280811), (5, 0.011253748), (7, 0.011056695), (9, 0.014580874)]\n81 번째 문서의 topic 비율은 [(0, 0.21169786), (2, 0.23186548), (3, 0.20527853), (9, 0.34716728)]\n82 번째 문서의 topic 비율은 [(0, 0.32895216), (2, 0.21170598), (3, 0.11348951), (5, 0.11614799), (6, 0.22319931)]\n83 번째 문서의 topic 비율은 [(0, 0.13477086), (3, 0.5665726), (8, 0.077897884), (9, 0.21216086)]\n84 번째 문서의 topic 비율은 [(0, 0.4346349), (1, 0.013783542), (2, 0.010954952), (5, 0.018535068), (6, 0.014272008), (7, 0.01820928), (8, 0.012233402), (9, 0.46188584)]\n85 번째 문서의 topic 비율은 [(0, 0.011963067), (1, 0.017578911), (2, 0.013971512), (3, 0.6111669), (5, 0.023639627), (6, 0.2432742), (7, 0.023223553), (8, 0.015601996), (9, 0.030615436)]\n86 번째 문서의 topic 비율은 [(0, 0.011963065), (1, 0.017578907), (2, 0.013971509), (3, 0.61102647), (5, 0.023639621), (6, 0.24341466), (7, 0.02322355), (8, 0.015601994), (9, 0.030615434)]\n87 번째 문서의 topic 비율은 [(0, 0.41202605), (1, 0.11881529), (3, 0.4379612)]\n88 번째 문서의 topic 비율은 [(0, 0.19082208), (2, 0.433174), (3, 0.15103738), (9, 0.21808705)]\n89 번째 문서의 topic 비율은 [(2, 0.21414092), (9, 0.74846965)]\n90 번째 문서의 topic 비율은 [(0, 0.3960302), (1, 0.40391535), (2, 0.019273048), (3, 0.014886795), (4, 0.01236653), (5, 0.03260991), (6, 0.02510869), (7, 0.03203666), (8, 0.021522228), (9, 0.042250652)]\n91 번째 문서의 topic 비율은 [(1, 0.47542346), (3, 0.44450834), (5, 0.012947509), (7, 0.0127200205), (9, 0.01677052)]\n92 번째 문서의 topic 비율은 [(0, 0.20356931), (2, 0.09600337), (3, 0.14773788), (8, 0.1710426), (9, 0.37998715)]\n93 번째 문서의 topic 비율은 [(1, 0.30185515), (3, 0.09876587), (6, 0.14566806), (8, 0.29501864), (9, 0.15481476)]\n94 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n95 번째 문서의 topic 비율은 [(0, 0.21433967), (2, 0.15906283), (7, 0.049343247), (8, 0.13312371), (9, 0.44216782)]\n96 번째 문서의 topic 비율은 [(0, 0.1792576), (1, 0.1489667), (2, 0.19583887), (7, 0.06085641), (8, 0.10527619), (9, 0.30743808)]\n97 번째 문서의 topic 비율은 [(0, 0.30851522), (1, 0.089669526), (2, 0.11432708), (6, 0.25022835), (9, 0.23123674)]\n98 번째 문서의 topic 비율은 [(0, 0.22448452), (2, 0.20269534), (3, 0.04790133), (6, 0.09136957), (8, 0.04935895), (9, 0.38247615)]\n99 번째 문서의 topic 비율은 [(0, 0.45829317), (6, 0.22509938), (8, 0.27636394)]\n100 번째 문서의 topic 비율은 [(0, 0.6124086), (2, 0.31682077), (5, 0.011255712), (7, 0.011058175), (9, 0.01458317)]\n101 번째 문서의 topic 비율은 [(0, 0.52215874), (2, 0.42704123), (9, 0.010464967)]\n102 번째 문서의 topic 비율은 [(0, 0.27676642), (2, 0.35180563), (6, 0.26132292), (9, 0.10511125)]\n103 번째 문서의 topic 비율은 [(0, 0.6384687), (1, 0.03908823), (2, 0.031066868), (3, 0.02399652), (4, 0.019934021), (5, 0.05256232), (6, 0.040473536), (7, 0.05163908), (8, 0.034692388), (9, 0.06807836)]\n104 번째 문서의 topic 비율은 [(0, 0.14626956), (1, 0.08859928), (2, 0.22068213), (6, 0.07771803), (7, 0.059232846), (8, 0.12035932), (9, 0.28572574)]\n105 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n106 번째 문서의 topic 비율은 [(0, 0.2652414), (2, 0.19757868), (3, 0.15714025), (8, 0.06920511), (9, 0.3081844)]\n107 번째 문서의 topic 비율은 [(0, 0.085229024), (1, 0.17296126), (2, 0.20940551), (6, 0.08692453), (8, 0.07597277), (9, 0.36530578)]\n108 번째 문서의 topic 비율은 [(0, 0.19047666), (1, 0.06542562), (2, 0.18852983), (3, 0.08546704), (8, 0.18554187), (9, 0.2831097)]\n109 번째 문서의 topic 비율은 [(0, 0.19887127), (2, 0.2675543), (3, 0.11733698), (9, 0.41108453)]\n110 번째 문서의 topic 비율은 [(0, 0.09961039), (1, 0.17953928), (2, 0.32881072), (3, 0.13769028), (7, 0.040895414), (8, 0.07158843), (9, 0.14105782)]\n111 번째 문서의 topic 비율은 [(0, 0.0685402), (1, 0.100715145), (2, 0.08004722), (3, 0.061829686), (4, 0.05136221), (5, 0.13543263), (6, 0.10428453), (7, 0.13305333), (8, 0.08938877), (9, 0.17534631)]\n112 번째 문서의 topic 비율은 [(0, 0.18676776), (1, 0.05152091), (2, 0.26845545), (3, 0.06719317), (8, 0.11757429), (9, 0.3072465)]\n113 번째 문서의 topic 비율은 [(0, 0.24365792), (2, 0.16978611), (3, 0.048677854), (8, 0.11304341), (9, 0.4229911)]\n114 번째 문서의 topic 비율은 [(0, 0.20911144), (2, 0.13262968), (3, 0.19382976), (8, 0.19653793), (9, 0.2639576)]\n115 번째 문서의 topic 비율은 [(0, 0.24661396), (1, 0.06342743), (2, 0.1374901), (3, 0.071362846), (6, 0.062463623), (8, 0.0897515), (9, 0.3279225)]\n116 번째 문서의 topic 비율은 [(0, 0.1581996), (1, 0.06596371), (2, 0.23884016), (3, 0.0746677), (6, 0.13985139), (8, 0.21561429), (9, 0.10386343)]\n117 번째 문서의 topic 비율은 [(0, 0.08261867), (3, 0.16125993), (5, 0.14098318), (6, 0.589577)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_topic_per_doc(ldamodel, corpus):\n    topic_table = pd.DataFrame()\n    \n    for i, topic_list in enumerate(ldamodel[corpus]):\n        doc = topic_list[0] if ldamodel.per_word_topics else topic_list\n        doc = sorted(doc, key = lambda x: (x[1]), reverse = True)  # 가중치 높은순으로 정렬\n        \n        for j, (topic_num, prop_topic) in enumerate(doc):\n            if j==0:\n                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index = True)\n            else:\n                break\n    return(topic_table)        ","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = model.print_topics(num_words=4)\ntopics = np.array(topics)","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics[1,1]","execution_count":127,"outputs":[{"output_type":"execute_result","execution_count":127,"data":{"text/plain":"'0.029*\"school\" + 0.025*\"student\" + 0.017*\"test\" + 0.014*\"high_school\"'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_table = make_topic_per_doc(model, corpus_test)\ntest_table[:num[0]]","execution_count":114,"outputs":[{"output_type":"execute_result","execution_count":114,"data":{"text/plain":"      0       1                                                  2\n0   2.0  0.6011  [(0, 0.058069143), (2, 0.6011216), (3, 0.07168...\n1   2.0  0.8750                  [(2, 0.8749784), (9, 0.10096897)]\n2   2.0  0.6320  [(1, 0.08115659), (2, 0.6320003), (3, 0.226504...\n3   2.0  0.7553  [(2, 0.75533456), (3, 0.07291584), (7, 0.05189...\n4   2.0  0.5486  [(1, 0.034976296), (2, 0.5485558), (3, 0.20877...\n5   2.0  0.6430  [(0, 0.026596647), (1, 0.039081957), (2, 0.643...\n6   2.0  0.6355  [(1, 0.025857763), (2, 0.63546485), (3, 0.0941...\n7   9.0  0.1753  [(0, 0.0685402), (1, 0.100715145), (2, 0.08004...\n8   2.0  0.5612  [(0, 0.08742295), (2, 0.56123525), (3, 0.06128...\n9   9.0  0.1753  [(0, 0.0685402), (1, 0.100715145), (2, 0.08004...\n10  2.0  0.6806  [(0, 0.035871215), (2, 0.68055326), (3, 0.1616...\n11  2.0  0.8508                   [(2, 0.8508482), (3, 0.1398687)]\n12  2.0  0.5867  [(2, 0.5867271), (6, 0.1396301), (9, 0.26080108)]\n13  2.0  0.5598  [(0, 0.12400462), (2, 0.5598435), (3, 0.064012...\n14  2.0  0.7889  [(0, 0.09261422), (2, 0.788885), (3, 0.10798604)]\n15  2.0  0.4380  [(0, 0.11904314), (2, 0.43803838), (9, 0.43520...\n16  2.0  0.6939  [(0, 0.05634215), (1, 0.07009841), (2, 0.69392...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>0.6011</td>\n      <td>[(0, 0.058069143), (2, 0.6011216), (3, 0.07168...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>0.8750</td>\n      <td>[(2, 0.8749784), (9, 0.10096897)]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>0.6320</td>\n      <td>[(1, 0.08115659), (2, 0.6320003), (3, 0.226504...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>0.7553</td>\n      <td>[(2, 0.75533456), (3, 0.07291584), (7, 0.05189...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>0.5486</td>\n      <td>[(1, 0.034976296), (2, 0.5485558), (3, 0.20877...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.0</td>\n      <td>0.6430</td>\n      <td>[(0, 0.026596647), (1, 0.039081957), (2, 0.643...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.0</td>\n      <td>0.6355</td>\n      <td>[(1, 0.025857763), (2, 0.63546485), (3, 0.0941...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.0</td>\n      <td>0.1753</td>\n      <td>[(0, 0.0685402), (1, 0.100715145), (2, 0.08004...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.0</td>\n      <td>0.5612</td>\n      <td>[(0, 0.08742295), (2, 0.56123525), (3, 0.06128...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9.0</td>\n      <td>0.1753</td>\n      <td>[(0, 0.0685402), (1, 0.100715145), (2, 0.08004...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.0</td>\n      <td>0.6806</td>\n      <td>[(0, 0.035871215), (2, 0.68055326), (3, 0.1616...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2.0</td>\n      <td>0.8508</td>\n      <td>[(2, 0.8508482), (3, 0.1398687)]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.0</td>\n      <td>0.5867</td>\n      <td>[(2, 0.5867271), (6, 0.1396301), (9, 0.26080108)]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.0</td>\n      <td>0.5598</td>\n      <td>[(0, 0.12400462), (2, 0.5598435), (3, 0.064012...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2.0</td>\n      <td>0.7889</td>\n      <td>[(0, 0.09261422), (2, 0.788885), (3, 0.10798604)]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2.0</td>\n      <td>0.4380</td>\n      <td>[(0, 0.11904314), (2, 0.43803838), (9, 0.43520...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.0</td>\n      <td>0.6939</td>\n      <td>[(0, 0.05634215), (1, 0.07009841), (2, 0.69392...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter \ndef modefinder(numbers): # 최빈값\n    c = Counter(numbers) \n    mode = c.most_common(1) \n    return mode[0][0]","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topics_idx = []\nfor i in range(len(num)):\n    max = modefinder(test_table[:num[i]][0])\n    top_topics_idx.append(max)","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topics = []\nfor i in range(len(num)):\n    ind = int(top_topics_idx[i])\n    top_topics.append(topic_kw[ind])","execution_count":153,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\nsubmission['PredictionString'] = top_topics","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":155,"outputs":[{"output_type":"execute_result","execution_count":155,"data":{"text/plain":"                                     Id                   PredictionString\n0  2100032a-7c33-4bff-97ef-690822c43466  county farm climate_change change\n1  2f392438-e215-4169-bebf-21ac4ff253e1  county farm climate_change change\n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6  county farm climate_change change\n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60         method image value feature","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>county farm climate_change change</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>county farm climate_change change</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>county farm climate_change change</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>method image value feature</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":156,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# reference\n[1] Simple EDA and preprocessing of DataFrame, https://www.kaggle.com/tanlikesmath/simple-eda-and-preprocessing-of-dataframe\n[2] [ShowUsTheData] EDA & Visualization Utils, https://www.kaggle.com/subinium/showusthedata-eda-visualization-utils\n[3] [ShowUsTheData] Topic Modeling with LDA, https://www.kaggle.com/subinium/showusthedata-topic-modeling-with-lda\n\n'''","execution_count":157,"outputs":[{"output_type":"execute_result","execution_count":157,"data":{"text/plain":"'\\n# reference\\n[1] Simple EDA and preprocessing of DataFrame, https://www.kaggle.com/tanlikesmath/simple-eda-and-preprocessing-of-dataframe\\n[2] [ShowUsTheData] EDA & Visualization Utils, https://www.kaggle.com/subinium/showusthedata-eda-visualization-utils\\n[3] [ShowUsTheData] Topic Modeling with LDA, https://www.kaggle.com/subinium/showusthedata-topic-modeling-with-lda\\n\\n'"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}